<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/procjx.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/procjxfavicon32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/procjxfavicon16x16.ico">
  <link rel="mask-icon" href="/images/procjx.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.4.2',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

<!-- Google Adsense -->
<!--
<script async src="//pagead2.googlesyndication.com/
pagead/js/adsbygoogle.js"></script>
<script>
(adsbygoogle = window.adsbygoogle || []).push({
google_ad_client: "pub-1179774715076800",
enable_page_level_ads: true
});
</script>
-->

<script data-ad-client="ca-pub-1179774715076800" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>


<meta name="google-site-verification" content="cEiGwg0T8Rj5msmuEcGYZTh5nnf05EhCXy0gp2Ml5BI" />
<meta name="baidu-site-verification" content="noSKHe8MJs" />

  <meta name="description" content="目录  1. jiant: A Software Toolkit for Research on General-Purpose Text  Understanding Models [PDF] 摘要  2. Data Augmentation using Pre-trained Transformer Models [PDF] 摘要  3. Unsupervised Adversarial Do">
<meta property="og:type" content="article">
<meta property="og:title" content="【arxiv论文】 Computation and Language 2020-03-05">
<meta property="og:url" content="https:&#x2F;&#x2F;procjx.github.io&#x2F;2020&#x2F;03&#x2F;05&#x2F;%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-03-05&#x2F;index.html">
<meta property="og:site_name" content="PROCJX&#39;s BLOGS">
<meta property="og:description" content="目录  1. jiant: A Software Toolkit for Research on General-Purpose Text  Understanding Models [PDF] 摘要  2. Data Augmentation using Pre-trained Transformer Models [PDF] 摘要  3. Unsupervised Adversarial Do">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2020-03-12T13:13:14.681Z">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://procjx.github.io/2020/03/05/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-03-05/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>【arxiv论文】 Computation and Language 2020-03-05 | PROCJX's BLOGS</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">PROCJX's BLOGS</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">WITH LOVE OF WORLD</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-fw fa-download"></i>资源</a>

  </li>
        <li class="menu-item menu-item-arxiv">

    <a href="/arxiv/" rel="section"><i class="fa fa-fw fa-file-pdf-o"></i>arxiv论文</a>

  </li>
        <li class="menu-item menu-item-deadline">

    <a href="/deadline/" rel="section"><i class="fa fa-fw fa-calendar"></i>会议截稿</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://procjx.github.io/2020/03/05/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-03-05/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/procjx.png">
      <meta itemprop="name" content="PROCJX">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PROCJX's BLOGS">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          【arxiv论文】 Computation and Language 2020-03-05
        </h2>

        <div class="post-meta">
        
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-03-05 11:30:11" itemprop="dateCreated datePublished" datetime="2020-03-05T11:30:11+08:00">2020-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-12 21:13:14" itemprop="dateModified" datetime="2020-03-12T21:13:14+08:00">2020-03-12</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/arxiv/" itemprop="url" rel="index">
                    <span itemprop="name">arxiv</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/arxiv/CL/" itemprop="url" rel="index">
                    <span itemprop="name">CL</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
              <span>22k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
              <span>36 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> jiant: A Software Toolkit for Research on General-Purpose Text  Understanding Models <a href="https://arxiv.org/pdf/2003.02249" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Data Augmentation using Pre-trained Transformer Models <a href="https://arxiv.org/pdf/2003.02245" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Unsupervised Adversarial Domain Adaptation for Implicit Discourse  Relation Classification <a href="https://arxiv.org/pdf/2003.02244" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Evaluating Low-Resource Machine Translation between Chinese and  Vietnamese with Back-Translation <a href="https://arxiv.org/pdf/2003.02197" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Sequential Neural Networks for Noetic End-to-End Response Selection <a href="https://arxiv.org/pdf/2003.02126" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Posterior-GAN: Towards Informative and Coherent Response Generation with  Posterior Generative Adversarial Network <a href="https://arxiv.org/pdf/2003.02020" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Restoration of Fragmentary Babylonian Texts Using Recurrent Neural  Networks <a href="https://arxiv.org/pdf/2003.01912" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> SeMemNN: A Semantic Matrix-Based Memory Neural Network for Text  Classification <a href="https://arxiv.org/pdf/2003.01857" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> HyperEmbed: Tradeoffs Between Resources and Performance in NLP Tasks  with Hyperdimensional Computing enabled Embedding of n-gram Statistics <a href="https://arxiv.org/pdf/2003.01821" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> AlignTTS: Efficient Feed-Forward Text-to-Speech System without Explicit  Alignment <a href="https://arxiv.org/pdf/2003.01950" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> GraphTTS: graph-to-sequence modelling in neural text-to-speech <a href="https://arxiv.org/pdf/2003.01924" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> On Emergent Communication in Competitive Multi-Agent Teams <a href="https://arxiv.org/pdf/2003.01848" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Discover Your Social Identity from What You Tweet: a Content Based  Approach <a href="https://arxiv.org/pdf/2003.01797" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Untangling in Invariant Speech Recognition <a href="https://arxiv.org/pdf/2003.01787" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Phonetic Feedback for Speech Enhancement With and Without Parallel  Speech Data <a href="https://arxiv.org/pdf/2003.01769" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Towards Real-time Mispronunciation Detection in Kids' Speech <a href="https://arxiv.org/pdf/2003.01765" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. jiant: A Software Toolkit for Research on General-Purpose Text  Understanding Models</b>  <a href="https://arxiv.org/pdf/2003.02249" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Pruksachatkun%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yada Pruksachatkun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yeres%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Phil Yeres</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haokun Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Phang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jason Phang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Htut%2C+P+M" target="_blank" rel="noopener" style="color:#0000EE;">Phu Mon Htut</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alex Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tenney%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ian Tenney</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bowman%2C+S+R" target="_blank" rel="noopener" style="color:#0000EE;">Samuel R. Bowman</a><br>
<font size="3">
Abstract: We introduce jiant, an open source toolkit for conducting multitask and transfer learning experiments on English NLU tasks. jiant enables modular and configuration-driven experimentation with state-of-the-art models and implements a broad set of tasks for probing, transfer learning, and multitask training experiments. jiant implements over 50 NLU tasks, including all GLUE and SuperGLUE benchmark tasks. We demonstrate that jiant reproduces published performance on a variety of tasks and models, including BERT and RoBERTa. jiant is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：介绍jiant，一个开源工具包进行英语自然语言理解任务，多任务和迁移学习实验。 jiant使模块化和配置驱动的实验与国家的最先进的车型，并实施一系列针对探测任务，传递学习和多任务训练实验。 50个NLU任务jiant工具，包括所有的胶水，强力胶基准任务。我们证明在各种不同的任务和模型，包括BERT和罗伯塔公布业绩的是jiant再现。 jiant可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper2"> <b>2. Data Augmentation using Pre-trained Transformer Models</b>  <a href="https://arxiv.org/pdf/2003.02245" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Varun Kumar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Choudhary%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ashutosh Choudhary</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eunah Cho</a><br>
<font size="3">
Abstract: Language model based pre-trained models such as BERT have provided significant gains across different NLP tasks. In this paper, we study different types of pre-trained transformer based models such as auto-regressive models (GPT-2), auto-encoder models (BERT), and seq2seq models (BART) for conditional data augmentation. We show that prepending the class labels to text sequences provides a simple yet effective way to condition the pre-trained models for data augmentation. On three classification benchmarks, pre-trained Seq2Seq model outperforms other models. Further, we explore how different pre-trained model based data augmentation differs in-terms of data diversity, and how well such methods preserve the class-label information. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：语言模型基于预先训练模式，如BERT提供了在不同的NLP任务显著的收益。在本文中，我们研究了不同类型的预训练的基于变压器的模型，比如有条件数据增强自回归模型（GPT-2），自动编码器模型（BERT），以及seq2seq模型（BART）。我们发现，在前面加上类标签文本序列提供了一个简单而有效的方法来调理增强数据预先训练的模式。三个分类基准，预先训练Seq2Seq模型优于其他车型。此外，我们探讨，如何条件不同的预先训练的基于模型的数据增强不同数据的多样性，以及如何以及这种方法保留类的标签信息。</font>
</div>


<hr>
<div id="paper3"> <b>3. Unsupervised Adversarial Domain Adaptation for Implicit Discourse  Relation Classification</b>  <a href="https://arxiv.org/pdf/2003.02244" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hsin-Ping Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J+J" target="_blank" rel="noopener" style="color:#0000EE;">Junyi Jessy Li</a><br>
<font size="3">
Abstract: Implicit discourse relations are not only more challenging to classify, but also to annotate, than their explicit counterparts. We tackle situations where training data for implicit relations are lacking, and exploit domain adaptation from explicit relations (Ji et al., 2015). We present an unsupervised adversarial domain adaptive network equipped with a reconstruction component. Our system outperforms prior works and other adversarial benchmarks for unsupervised domain adaptation. Additionally, we extend our system to take advantage of labeled data if some are available. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：隐性语篇关系不仅更具挑战性进行分类，而且还注释，比他们的同行明确。我们解决了隐含关系训练数据匮乏的地方的情况，并利用从显性关系领域适应性（Ji等人，2015年）。我们提出装备有一重建成分的无监督对抗性域自适应网络。我们的系统优于之前的作品和无监督领域适应性等对抗性的基准。此外，我们扩展我们的系统采取标记数据的优势，如果一些可用。</font>
</div>


<hr>
<div id="paper4"> <b>4. Evaluating Low-Resource Machine Translation between Chinese and  Vietnamese with Back-Translation</b>  <a href="https://arxiv.org/pdf/2003.02197" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongzheng Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Heyan Huang</a><br>
<font size="3">
Abstract: Back translation (BT) has been widely used and become one of standard techniques for data augmentation in Neural Machine Translation (NMT), BT has proven to be helpful for improving the performance of translation effectively, especially for low-resource scenarios. While most works related to BT mainly focus on European languages, few of them study languages in other areas around the world. In this paper, we investigate the impacts of BT on Asia language translations between the extremely low-resource Chinese and Vietnamese language pair. We evaluate and compare the effects of different sizes of synthetic data on both NMT and Statistical Machine Translation (SMT) models for Chinese to Vietnamese and Vietnamese to Chinese, with character-based and word-based settings. Some conclusions from previous works are partially confirmed and we also draw some other interesting findings and conclusions, which are beneficial to understand BT further. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：回译（BT）已被广泛使用，成为在神经机器翻译（NMT）数据增强标准技术之一，BT已经被证明是有效地提高翻译的性能，特别是对于低资源场景很有帮助。虽然与BT大多数作品主要集中在欧洲语言，他们几个人的研究在世界各地的其他地区语言。在本文中，我们研究了BT对亚洲语言翻译极低的资源的中国和越南的语言对之间的影响。我们评价和比较不同的尺寸上都NMT和统计机器翻译（SMT）模型对中国越南和越南到中国的综合数据，与基于词的基于字符和设置的影响。从以前的作品有些结论部分证实，我们也借鉴了其他一些有趣的发现和结论，这有利于进一步了解BT。</font>
</div>


<hr>
<div id="paper5"> <b>5. Sequential Neural Networks for Noetic End-to-End Response Selection</b>  <a href="https://arxiv.org/pdf/2003.02126" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qian Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wen Wang</a><br>
<font size="3">
Abstract: The noetic end-to-end response selection challenge as one track in the 7th Dialog System Technology Challenges (DSTC7) aims to push the state of the art of utterance classification for real world goal-oriented dialog systems, for which participants need to select the correct next utterances from a set of candidates for the multi-turn context. This paper presents our systems that are ranked top 1 on both datasets under this challenge, one focused and small (Advising) and the other more diverse and large (Ubuntu). Previous state-of-the-art models use hierarchy-based (utterance-level and token-level) neural networks to explicitly model the interactions among different turns' utterances for context modeling. In this paper, we investigate a sequential matching model based only on chain sequence for multi-turn response selection. Our results demonstrate that the potentials of sequential matching approaches have not yet been fully exploited in the past for multi-turn response selection. In addition to ranking top 1 in the challenge, the proposed model outperforms all previous models, including state-of-the-art hierarchy-based models, on two large-scale public multi-turn response selection benchmark datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：智力的终端到终端的反应选择的挑战，因为在第7对话系统的技术挑战（DSTC7）目标之一的轨道，推动艺术的真实世界面向目标的对话系统，其参与者需要话语分类的状态从一组的多匝背景候选中选择正确的下话语。本文介绍了我们正在根据这一挑战排名榜首1在两个数据集中系统，一个专注和小（通知）和其他更加多样化和大型（Ubuntu的）。上一页国家的最先进的机型采用层次结构为基础的（话语级别和标记级别）神经网络，以明确的相互作用不同转弯话语的上下文建模中的模型。在本文中，我们调查仅仅基于多转响应选定链序列连续匹配模型。我们的研究结果表明，顺序匹配方法的潜力尚未完全过去，多转反应选择利用。除了挑战排名前1，该模型优于以前的所有型号，包括国家的最先进的基于层次的模型，两个大型公共多圈响应的选择标准数据集。</font>
</div>


<hr>
<div id="paper6"> <b>6. Posterior-GAN: Towards Informative and Coherent Response Generation with  Posterior Generative Adversarial Network</b>  <a href="https://arxiv.org/pdf/2003.02020" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shaoxiong Feng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongshen Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kan Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dawei Yin</a><br>
<font size="3">
Abstract: Neural conversational models learn to generate responses by taking into account the dialog history. These models are typically optimized over the query-response pairs with a maximum likelihood estimation objective. However, the query-response tuples are naturally loosely coupled, and there exist multiple responses that can respond to a given query, which leads the conversational model learning burdensome. Besides, the general dull response problem is even worsened when the model is confronted with meaningless response training instances. Intuitively, a high-quality response not only responds to the given query but also links up to the future conversations, in this paper, we leverage the query-response-future turn triples to induce the generated responses that consider both the given context and the future conversations. To facilitate the modeling of these triples, we further propose a novel encoder-decoder based generative adversarial learning framework, Posterior Generative Adversarial Network (Posterior-GAN), which consists of a forward and a backward generative discriminator to cooperatively encourage the generated response to be informative and coherent by two complementary assessment perspectives. Experimental results demonstrate that our method effectively boosts the informativeness and coherence of the generated response on both automatic and human evaluation, which verifies the advantages of considering two assessment perspectives. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：神经会话模型学会考虑对话历史产生响应。这些模型在用最大似然估计目标的查询和响应对优化典型。但是，查询响应元组自然松散耦合，并存在可以对给定的查询，从而导致会话模型学习负担响应多个响应。此外，当模型正面临着毫无意义的应对训练情况一般沉闷的响应问题甚至恶化。直观地说，一个高品质的响应，不仅响应给定的查询，但还链接到未来的对话，在本文中，我们利用查询响应，未来又将三元组诱导考虑给定的背景和双方产生的响应未来的对话。为了便于这些三元组的建模，我们进一步提出了一种基于生成对抗学习框架的新的编码器 - 解码器，后剖成对抗性网络（后路-GAN），其由前向和后向生成鉴别器的协同促进所产生的响应为内容丰富，由两个互补的评估观点一致。实验结果表明，该方法有效地提升在自动和人工评估，从而验证考虑两种评价视点的优势产生响应的信息量和连贯性。</font>
</div>


<hr>
<div id="paper7"> <b>7. Restoration of Fragmentary Babylonian Texts Using Recurrent Neural  Networks</b>  <a href="https://arxiv.org/pdf/2003.01912" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Fetaya%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Ethan Fetaya</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lifshitz%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yonatan Lifshitz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Aaron%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Elad Aaron</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gordin%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shai Gordin</a><br>
<font size="3">
Abstract: The main source of information regarding ancient Mesopotamian history and culture are clay cuneiform tablets. Despite being an invaluable resource, many tablets are fragmented leading to missing information. Currently these missing parts are manually completed by experts. In this work we investigate the possibility of assisting scholars and even automatically completing the breaks in ancient Akkadian texts from Achaemenid period Babylonia by modelling the language using recurrent neural networks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：信息关于古代美索不达米亚的历史和文化的主要来源是粘土楔形文字片。尽管是一个非常宝贵的资源，许多片支离破碎导致丢失的信息。目前，这些缺失的部分是由人工完成的专家。在这项工作中，我们协助调查的学者，甚至通过模拟使用递归神经网络的语言自动完成从阿契美尼德时期巴比伦古阿卡德文字，断裂的可能性。</font>
</div>


<hr>
<div id="paper8"> <b>8. SeMemNN: A Semantic Matrix-Based Memory Neural Network for Text  Classification</b>  <a href="https://arxiv.org/pdf/2003.01857" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Changzeng Fu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chaoran Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ishi%2C+C+T" target="_blank" rel="noopener" style="color:#0000EE;">Carlos Toshinori Ishi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yoshikawa%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuichiro Yoshikawa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ishiguro%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hiroshi Ishiguro</a><br>
<font size="3">
Abstract: Text categorization is the task of assigning labels to documents written in a natural language, and it has numerous real-world applications including sentiment analysis as well as traditional topic assignment tasks. In this paper, we propose 5 different configurations for the semantic matrix-based memory neural network with end-to-end learning manner and evaluate our proposed method on two corpora of news articles (AG news, Sogou news). The best performance of our proposed method outperforms the baseline VDCNN models on the text classification task and gives a faster speed for learning semantics. Moreover, we also evaluate our model on small scale datasets. The results show that our proposed method can still achieve better results in comparison to VDCNN on the small scale dataset. This paper is to appear in the Proceedings of the 2020 IEEE 14th International Conference on Semantic Computing (ICSC 2020), San Diego, California, 2020. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：文本分类是分配标签写在一个自然语言文档的任务，它有许多现实世界的应用，包括情感分析，以及传统的话题分配任务。在本文中，我们提出了与终端到终端的学习方式，基于语义矩阵存储神经网络5个不同的配置和评估我们提出了新闻报道的两个语料库（AG新闻，搜狗新闻）方法。我们提出的方法的最佳性能优于对文本分类的任务基线VDCNN模型，并给出了学习语义更快的速度。此外，我们还评估了小规模的数据集模型。结果表明，该方法仍然可以取得更好的成绩相比，VDCNN在小规模数据集。本文是出现在语义计算的2020年IEEE第14届国际大会（ICSC 2020）的诉讼，圣迭戈，加利福尼亚州，2020年</font>
</div>


<hr>
<div id="paper9"> <b>9. HyperEmbed: Tradeoffs Between Resources and Performance in NLP Tasks  with Hyperdimensional Computing enabled Embedding of n-gram Statistics</b>  <a href="https://arxiv.org/pdf/2003.01821" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Alonso%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pedro Alonso</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shridhar%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kumar Shridhar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kleyko%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Denis Kleyko</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Osipov%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Evgeny Osipov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liwicki%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marcus Liwicki</a><br>
<font size="3">
Abstract: Recent advances in Deep Learning have led to a significant performance increase on several NLP tasks, however, the models become more and more computationally demanding. Therefore, this paper tackles the domain of computationally efficient algorithms for NLP tasks. In particular, it investigates distributed representations of n-gram statistics of texts. The representations are formed using hyperdimensional computing enabled embedding. These representations then serve as features, which are used as input to standard classifiers. We investigate the applicability of the embedding on one large and three small standard datasets for classification tasks using nine classifiers. The embedding achieved on par F1 scores while decreasing the time and memory requirements by several times compared to the conventional n-gram statistics, e.g., for one of the classifiers on a small dataset, the memory reduction was 6.18 times; while train and test speed-ups were 4.62 and 3.84 times, respectively. For many classifiers on the large dataset, the memory reduction was about 100 times and train and test speed-ups were over 100 times. More importantly, the usage of distributed representations formed via hyperdimensional computing allows dissecting the strict dependency between the dimensionality of the representation and the parameters of n-gram statistics, thus, opening a room for tradeoffs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深学的最新进展已经导致了一些NLP任务显著的性能提升，但是，模型越来越多的计算能力的要求。因此，本文铲球对NLP任务的高效计算算法的域。特别是，调查分布文本的n-gram统计表示。这些表示使用超维度计算启用了嵌入形成。这些表述则充当功能，这被用作输入标准分类。我们研究了嵌入在一个大的和三个小数据集标准对采用九个分类分类任务的适用性。看齐F1分数实现同时降低相比于常规的n-gram统计数倍的时间和内存需求，嵌入例如，用于在一个小数据集的分类器中的一个，所述存储器减少为6.18倍;而火车和测试速度起坐分别为4.62和3.84倍。有关大数据集众多分类，存储量减少了约100倍，培养和测试速度起坐均超过100倍。更重要的是，经由超维度计算形成分布表示的使用允许解剖表示的维数和n-gram中的统计参数之间的严格相关性，因此，开启了折衷的余地。</font>
</div>


<hr>
<div id="paper10"> <b>10. AlignTTS: Efficient Feed-Forward Text-to-Speech System without Explicit  Alignment</b>  <a href="https://arxiv.org/pdf/2003.01950" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Zeng%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhen Zeng</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Wang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianzong Wang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Cheng%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Ning Cheng</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Xia%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tian Xia</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Xiao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jing Xiao</a><br>
<font size="3">
Abstract: Targeting at both high efficiency and performance, we propose AlignTTS to predict the mel-spectrum in parallel. AlignTTS is based on a Feed-Forward Transformer which generates mel-spectrum from a sequence of characters, and the duration of each character is determined by a duration predictor.Instead of adopting the attention mechanism in Transformer TTS to align text to mel-spectrum, the alignment loss is presented to consider all possible alignments in training by use of dynamic programming. Experiments on the LJSpeech dataset show that our model achieves not only state-of-the-art performance which outperforms Transformer TTS by 0.03 in mean option score (MOS), but also a high efficiency which is more than 50 times faster than real-time. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在两个高效率和性能定位，我们提出AlignTTS预测并行梅尔频谱。 AlignTTS是基于前馈变压器，其从字符序列生成梅尔频谱，并且确定每一个字符的持续时间被持续时间predictor.Instead采用在变压器TTS注意机制来对齐文本到梅尔谱，提出对准损失利用动态规划的考虑培训所有可能的路线。在LJSpeech数据集的实验表明我们的模型实现不仅是国家的最先进的，其平均选项得分（MOS）优于变压器TTS 0.03的表现，也是一种高效率的比实时更快的超过50倍。</font>
</div>


<hr>
<div id="paper11"> <b>11. GraphTTS: graph-to-sequence modelling in neural text-to-speech</b>  <a href="https://arxiv.org/pdf/2003.01924" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Sun%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aolan Sun</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Wang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianzong Wang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Cheng%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Ning Cheng</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Peng%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huayi Peng</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Zeng%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhen Zeng</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Xiao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jing Xiao</a><br>
<font size="3">
Abstract: This paper leverages the graph-to-sequence method in neural text-to-speech (GraphTTS), which maps the graph embedding of the input sequence to spectrograms. The graphical inputs consist of node and edge representations constructed from input texts. The encoding of these graphical inputs incorporates syntax information by a GNN encoder module. Besides, applying the encoder of GraphTTS as a graph auxiliary encoder (GAE) can analyse prosody information from the semantic structure of texts. This can remove the manual selection of reference audios process and makes prosody modelling an end-to-end procedure. Experimental analysis shows that GraphTTS outperforms the state-of-the-art sequence-to-sequence models by 0.24 in Mean Opinion Score (MOS). GAE can adjust the pause, ventilation and tones of synthesised audios automatically. This experimental conclusion may give some inspiration to researchers working on improving speech synthesis prosody. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：利用神经文本到语音（GraphTTS），该图形中嵌入输入序列谱图映射图对序列的方法。所述图形输入包括从输入的文本构成节点和边表示。这些图形输入的编码包含由GNN编码器模块的语法信息。此外，施加GraphTTS的编码器的图辅助编码器（GAE）可以分析从文本的语义结构韵律信息。这可以去除参考音频处理的手动选择和使韵律模型的端至端的过程。实验分析显示，GraphTTS 0.24在平均意见得分（MOS）优于状态的最先进的序列到序列的机型。 GAE可以自动调整合成音的暂停，通风和音调。这个实验的结论可能会提供一些灵感，在提高语音合成韵律工作的研究人员。</font>
</div>


<hr>
<div id="paper12"> <b>12. On Emergent Communication in Competitive Multi-Agent Teams</b>  <a href="https://arxiv.org/pdf/2003.01848" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+P+P" target="_blank" rel="noopener" style="color:#0000EE;">Paul Pu Liang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jeffrey Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Salakhutdinov%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruslan Salakhutdinov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Morency%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Louis-Philippe Morency</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kottur%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Satwik Kottur</a><br>
<font size="3">
Abstract: Several recent works have found the emergence of grounded compositional language in the communication protocols developed by mostly cooperative multi-agent systems when learned end-to-end to maximize performance on a downstream task. However, human populations learn to solve complex tasks involving communicative behaviors not only in fully cooperative settings but also in scenarios where competition acts as an additional external pressure for improvement. In this work, we investigate whether competition for performance from an external, similar agent team could act as a social influence that encourages multi-agent populations to develop better communication protocols for improved performance, compositionality, and convergence speed. We start from Task & Talk, a previously proposed referential game between two cooperative agents as our testbed and extend it into Task, Talk & Compete, a game involving two competitive teams each consisting of two aforementioned cooperative agents. Using this new setting, we provide an empirical study demonstrating the impact of competitive influence on multi-agent teams. Our results show that an external competitive influence leads to improved accuracy and generalization, as well as faster emergence of communicative languages that are more informative and compositional. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近的一些作品已经发现接地组成的语言中所学到的端至端时，上下游任务，最大限度地提高性能的主要合作多智能体系统开发的通信协议的出现。然而，人类群体学会解决不仅涉及在完全合作的设置，而且在场景中的竞争作为改善额外的外部压力的交际行为，复杂的任务。在这项工作中，我们研究了来自外部，类似的代理团队绩效的竞争是否能为鼓励多主体人群制定，以改善性能，组合性和收敛速度更好的通信协议的社会影响作用。我们从任务与交流，为我们的测试平台两个互惠代理之间的先前提出的指称游戏开始，并延伸到任务，对话和竞争，涉及到两个有竞争力的球队分别由上述两个合作代理的游戏。使用这个新的设置，我们提供了一个实证研究表明竞争影响多代理团队的影响。我们的研究结果表明，在外部竞争的影响导致提高精度和泛化，以及更具信息和组成交际语言的更快出现。</font>
</div>


<hr>
<div id="paper13"> <b>13. Discover Your Social Identity from What You Tweet: a Content Based  Approach</b>  <a href="https://arxiv.org/pdf/2003.01797" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Binxuan Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Carley%2C+K+M" target="_blank" rel="noopener" style="color:#0000EE;">Kathleen M. Carley</a><br>
<font size="3">
Abstract: An identity denotes the role an individual or a group plays in highly differentiated contemporary societies. In this paper, our goal is to classify Twitter users based on their role identities. We first collect a coarse-grained public figure dataset automatically, then manually label a more fine-grained identity dataset. We propose a hierarchical self-attention neural network for Twitter user role identity classification. Our experiments demonstrate that the proposed model significantly outperforms multiple baselines. We further propose a transfer learning scheme that improves our model's performance by a large margin. Such transfer learning also greatly reduces the need for a large amount of human labeled data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：身份表示角色的个人或在高度分化当代社会的基团戏剧。在本文中，我们的目标是基于角色的身份Twitter用户进行分类。我们首先会自动收集粗粒度的公众人物数据集，然后手动标注更细粒度的身份数据集。我们提出了Twitter的用户角色身份分类分层的自我关注的神经网络。我们的实验表明，该模型显著优于多个基准。我们进一步建议，提高了一大截我们的模型的性能转移的学习方案。这种转移学习也大大减少了对大量的人力标记数据的需要。</font>
</div>


<hr>
<div id="paper14"> <b>14. Untangling in Invariant Speech Recognition</b>  <a href="https://arxiv.org/pdf/2003.01787" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Stephenson%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cory Stephenson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feather%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jenelle Feather</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Padhy%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Suchismita Padhy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Elibol%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Oguz Elibol</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hanlin Tang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=McDermott%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Josh McDermott</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chung%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">SueYeon Chung</a><br>
<font size="3">
Abstract: Encouraged by the success of deep neural networks on a variety of visual tasks, much theoretical and experimental work has been aimed at understanding and interpreting how vision networks operate. Meanwhile, deep neural networks have also achieved impressive performance in audio processing applications, both as sub-components of larger systems and as complete end-to-end systems by themselves. Despite their empirical successes, comparatively little is understood about how these audio models accomplish these tasks. In this work, we employ a recently developed statistical mechanical theory that connects geometric properties of network representations and the separability of classes to probe how information is untangled within neural networks trained to recognize speech. We observe that speaker-specific nuisance variations are discarded by the network's hierarchy, whereas task-relevant properties such as words and phonemes are untangled in later layers. Higher level concepts such as parts-of-speech and context dependence also emerge in the later layers of the network. Finally, we find that the deep representations carry out significant temporal untangling by efficiently extracting task-relevant features at each time step of the computation. Taken together, these findings shed light on how deep auditory models process time dependent input signals to achieve invariant speech recognition, and show how different concepts emerge through the layers of the network. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：通过深层神经网络对各种视觉任务成功的鼓舞，许多理论和实验工作已经瞄准理解和解释视觉网络是如何运作的。同时，深层神经网络也取得了骄人的业绩在音频处理应用中，无论是作为较大系统的子组件和完整自己的终端到终端系统。尽管他们的成功经验，相对较少了解关于这些音频模式如何完成这些任务。在这项工作中，我们采用连接网络表示的几何特性和类探头的可分性信息如何被训练识别语音神经网络内解开的一个新近开发的统计力学理论。我们观察到的说话者特定滋扰变化是由网络的层次结构丢弃，而与任务相关的属性，如单词和音素在以后层解开。更高层次的概念，例如部件的词性和上下文依赖性也出现在网络的后面的层。最后，我们发现，深表示通过在计算中的每一步高效提取与任务相关的功能进行显著时间解开。总之，这些研究结果揭示听觉模型有多深加工时间相关的输入信号，以实现不变的语音识别，并显示不同的概念如何通过网络的各层出现光。</font>
</div>


<hr>
<div id="paper15"> <b>15. Phonetic Feedback for Speech Enhancement With and Without Parallel  Speech Data</b>  <a href="https://arxiv.org/pdf/2003.01769" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Plantinga%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peter Plantinga</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Bagchi%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Deblin Bagchi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Fosler-Lussier%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eric Fosler-Lussier</a><br>
<font size="3">
Abstract: While deep learning systems have gained significant ground in speech enhancement research, these systems have yet to make use of the full potential of deep learning systems to provide high-level feedback. In particular, phonetic feedback is rare in speech enhancement research even though it includes valuable top-down information. We use the technique of mimic loss to provide phonetic feedback to an off-the-shelf enhancement system, and find gains in objective intelligibility scores on CHiME-4 data. This technique takes a frozen acoustic model trained on clean speech to provide valuable feedback to the enhancement model, even in the case where no parallel speech data is available. Our work is one of the first to show intelligibility improvement for neural enhancement systems without parallel speech data, and we show phonetic feedback can improve a state-of-the-art neural enhancement system trained with parallel speech data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：虽然深学习系统在语音增强的研究已经获得了显著地，这些系统还没有充分利用深度学习系统的全部潜能提供高层次的反馈。特别是，语音反馈是，即使它包含有价值的自上而下的信息，语音增强研究少见。我们使用模拟损失的技术来提供语音反馈关闭的，现成的增强系统，并找到磬-4数据的客观清晰度得分收益。这种技术需要训练有素的清洁讲话冻结的声学模型提供有价值的反馈，以增强模式，即使在没有并行语音数据是可用的情况下。我们的工作是第一次，以示对神经增强系统的清晰度提高无并行语音数据之一，我们将展示语音反馈可以改善与并行语音数据训练一个国家的最先进的神经增强系统。</font>
</div>


<hr>
<div id="paper16"> <b>16. Towards Real-time Mispronunciation Detection in Kids' Speech</b>  <a href="https://arxiv.org/pdf/2003.01765" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Plantinga%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peter Plantinga</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Fosler-Lussier%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eric Fosler-Lussier</a><br>
<font size="3">
Abstract: Modern mispronunciation detection and diagnosis systems have seen significant gains in accuracy due to the introduction of deep learning. However, these systems have not been evaluated for the ability to be run in real-time, an important factor in applications that provide rapid feedback. In particular, the state-of-the-art uses bi-directional recurrent networks, where a uni-directional network may be more appropriate. Teacher-student learning is a natural approach to use to improve a uni-directional model, but when using a CTC objective, this is limited by poor alignment of outputs to evidence. We address this limitation by trying two loss terms for improving the alignments of our models. One loss is an "alignment loss" term that encourages outputs only when features do not resemble silence. The other loss term uses a uni-directional model as teacher model to align the bi-directional model. Our proposed model uses these aligned bi-directional models as teacher models. Experiments on the CSLU kids' corpus show that these changes decrease the latency of the outputs, and improve the detection rates, with a trade-off between these goals. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现代读音错误检测与诊断系统，已经看到了精度显著收益由于引入深的学问。然而，这些系统目前尚未评估在实时运行的能力，在提供快速反馈应用的重要因素。特别地，国家的最先进的用途双向复发性网络，其中的单向网络可以是更合适的。教师与学生的学习是一种自然的方法来使用，以提高单向模式，但使用CTC的目标时，这是通过对证据的输出对准差的限制。我们应对努力为改善我们的模型的比对2点损失而言，这限制。一个亏损是一个“对准损失”一词，鼓励输出只有当功能并不像沉默。其他损耗项采用了单向模型作为教师模型对准双向模式。我们提出的模型使用这些对准双向模型作为教师的模型。在CSLU孩子的语料实验表明，这些变化降低了输出的延迟时间，提高检测率，这些目标之间的权衡。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>PROCJX
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://procjx.github.io/2020/03/05/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-03-05/" title="【arxiv论文】 Computation and Language 2020-03-05">https://procjx.github.io/2020/03/05/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-03-05/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">

        
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2020/03/04/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-03-04/" rel="next" title="【arxiv论文】 Computer Vision and Pattern Recognition 2020-03-04">
                  <i class="fa fa-chevron-left"></i> 【arxiv论文】 Computer Vision and Pattern Recognition 2020-03-04
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/2020/03/05/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-03-05/" rel="prev" title="【arxiv论文】 Computer Vision and Pattern Recognition 2020-03-05">
                  【arxiv论文】 Computer Vision and Pattern Recognition 2020-03-05 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>

        
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang -->
<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9197824246"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="gitalk-container"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#目录"><span class="nav-number">1.</span> <span class="nav-text">目录</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#摘要"><span class="nav-number">2.</span> <span class="nav-text">摘要</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="PROCJX"
    src="/images/procjx.png">
  <p class="site-author-name" itemprop="name">PROCJX</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">306</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">71</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/procjx" title="GitHub &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;procjx" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:procjx@gmail.com" title="E-Mail &amp;rarr; mailto:procjx@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>


<!--
      
        <script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
        <script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
        <div class="widget-wrap">
            <h3 class="widget-title">标签云</h3>
            <div id="myCanvasContainer" class="widget tagcloud">
                <canvas width="250" height="250" id="resCanvas" style="width=100%">
                    <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AAAI/" rel="tag">AAAI</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ACL/" rel="tag">ACL</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Accepted-Papers/" rel="tag">Accepted Papers</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ArXiv/" rel="tag">ArXiv</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/" rel="tag">BERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS20SI/" rel="tag">CS20SI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS224d/" rel="tag">CS224d</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/" rel="tag">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CVPR/" rel="tag">CVPR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Context/" rel="tag">Context</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cross-Lingual/" rel="tag">Cross Lingual</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dialog-System/" rel="tag">Dialog System</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse/" rel="tag">Discourse</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Ranking/" rel="tag">Discourse Ranking</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Structure/" rel="tag">Discourse Structure</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Document-NMT/" rel="tag">Document NMT</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EMNLP/" rel="tag">EMNLP</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Extractive/" rel="tag">Extractive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICLR/" rel="tag">ICLR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICML/" rel="tag">ICML</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IJCAI/" rel="tag">IJCAI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Inter-Sentence/" rel="tag">Inter-Sentence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Keyphrase-Generation/" rel="tag">Keyphrase Generation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NAACL/" rel="tag">NAACL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIPS/" rel="tag">NIPS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NMT/" rel="tag">NMT</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Relation-Extraction/" rel="tag">Neural Relation Extraction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RST/" rel="tag">RST</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Relation-Constraints/" rel="tag">Relation Constraints</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Summarization/" rel="tag">Summarization</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Translation/" rel="tag">Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Word-Translation/" rel="tag">Word Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/alias/" rel="tag">alias</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/git/" rel="tag">git</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pip/" rel="tag">pip</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/screen/" rel="tag">screen</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/" rel="tag">shell</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tgz/" rel="tag">tgz</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tts/" rel="tag">tts</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1/" rel="tag">冒泡</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/" rel="tag">冒泡排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%99%E4%BD%9C%E5%8A%A9%E6%89%8B/" rel="tag">写作助手</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8E%8B%E7%BC%A9/" rel="tag">压缩</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/" rel="tag">发送邮件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%88%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">合并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%8E%E5%8F%B0/" rel="tag">后台</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/" rel="tag">基数排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F/" rel="tag">希尔排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6/" rel="tag">归并</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">归并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/" rel="tag">快速排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F/" rel="tag">批量</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4/" rel="tag">批量删除</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F/" rel="tag">排序</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5/" rel="tag">插入</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/" rel="tag">插入排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%99%E7%A8%8B/" rel="tag">教程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/" rel="tag">斐波那契数列</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9D%80%E6%AD%BB%E8%BF%9B%E7%A8%8B/" rel="tag">杀死进程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B1%89%E8%AF%BA%E5%A1%94/" rel="tag">汉诺塔</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%A7%A3%E5%8E%8B/" rel="tag">解压</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%B0%B7%E6%AD%8C%E7%BF%BB%E8%AF%91/" rel="tag">谷歌翻译</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BF%AD%E4%BB%A3%E5%9B%9E%E7%BF%BB/" rel="tag">迭代回翻</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9/" rel="tag">选择</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/" rel="tag">选择排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%99%84%E4%BB%B6/" rel="tag">附件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3/" rel="tag">非监督</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%A2%86%E5%9F%9F%E9%80%82%E5%BA%94/" rel="tag">领域适应</a><span class="tag-list-count">1</span></li></ul>
                </canvas>
            </div>
        </div>
        
-->
        
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-hengfu -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9879871597"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-chuizhi -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="1662238719"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-zhengfangxing -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="6699421902"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PROCJX</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.4.2
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>












        
      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  <script src="/js/local-search.js"></script>













  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '2286ab64f5194d9d79ce',
      clientSecret: 'f912492bec2391664b40478f50f2f943376768d6',
      repo: 'procjx.github.io',
      owner: 'procjx',
      admin: ['procjx'],
      id: '32a97815f6df607d153e7908f75fc3f1',
        language: 'zh-CN',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
