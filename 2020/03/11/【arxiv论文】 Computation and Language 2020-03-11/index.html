<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/procjx.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/procjxfavicon32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/procjxfavicon16x16.ico">
  <link rel="mask-icon" href="/images/procjx.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.4.2',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

<!-- Google Adsense -->
<!--
<script async src="//pagead2.googlesyndication.com/
pagead/js/adsbygoogle.js"></script>
<script>
(adsbygoogle = window.adsbygoogle || []).push({
google_ad_client: "pub-1179774715076800",
enable_page_level_ads: true
});
</script>
-->

<script data-ad-client="ca-pub-1179774715076800" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>


<meta name="google-site-verification" content="cEiGwg0T8Rj5msmuEcGYZTh5nnf05EhCXy0gp2Ml5BI" />
<meta name="baidu-site-verification" content="noSKHe8MJs" />

  <meta name="description" content="目录  1. Multi-SimLex: A Large-Scale Evaluation of Multilingual and Cross-Lingual  Lexical Semantic Similarity [PDF] 摘要  2. Video Caption Dataset for Describing Human Actions in Japanese [PDF] 摘要  3. Un">
<meta property="og:type" content="article">
<meta property="og:title" content="【arxiv论文】 Computation and Language 2020-03-11">
<meta property="og:url" content="https:&#x2F;&#x2F;procjx.github.io&#x2F;2020&#x2F;03&#x2F;11&#x2F;%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-03-11&#x2F;index.html">
<meta property="og:site_name" content="PROCJX&#39;s BLOGS">
<meta property="og:description" content="目录  1. Multi-SimLex: A Large-Scale Evaluation of Multilingual and Cross-Lingual  Lexical Semantic Similarity [PDF] 摘要  2. Video Caption Dataset for Describing Human Actions in Japanese [PDF] 摘要  3. Un">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2020-03-12T13:14:46.793Z">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://procjx.github.io/2020/03/11/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-03-11/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>【arxiv论文】 Computation and Language 2020-03-11 | PROCJX's BLOGS</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">PROCJX's BLOGS</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">WITH LOVE OF WORLD</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-fw fa-download"></i>资源</a>

  </li>
        <li class="menu-item menu-item-arxiv">

    <a href="/arxiv/" rel="section"><i class="fa fa-fw fa-file-pdf-o"></i>arxiv论文</a>

  </li>
        <li class="menu-item menu-item-deadline">

    <a href="/deadline/" rel="section"><i class="fa fa-fw fa-calendar"></i>会议截稿</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://procjx.github.io/2020/03/11/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-03-11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/procjx.png">
      <meta itemprop="name" content="PROCJX">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PROCJX's BLOGS">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          【arxiv论文】 Computation and Language 2020-03-11
        </h2>

        <div class="post-meta">
        
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-03-11 11:29:23" itemprop="dateCreated datePublished" datetime="2020-03-11T11:29:23+08:00">2020-03-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-12 21:14:46" itemprop="dateModified" datetime="2020-03-12T21:14:46+08:00">2020-03-12</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/arxiv/" itemprop="url" rel="index">
                    <span itemprop="name">arxiv</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/arxiv/CL/" itemprop="url" rel="index">
                    <span itemprop="name">CL</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
              <span>20k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
              <span>33 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Multi-SimLex: A Large-Scale Evaluation of Multilingual and Cross-Lingual  Lexical Semantic Similarity <a href="https://arxiv.org/pdf/2003.04866" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Video Caption Dataset for Describing Human Actions in Japanese <a href="https://arxiv.org/pdf/2003.04865" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Undersensitivity in Neural Reading Comprehension <a href="https://arxiv.org/pdf/2003.04808" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Efficient Intent Detection with Dual Sentence Encoders <a href="https://arxiv.org/pdf/2003.04807" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Learning to Respond with Stickers: A Framework of Unifying  Multi-Modality in Multi-Turn Dialog <a href="https://arxiv.org/pdf/2003.04679" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> A Framework for Evaluation of Machine Reading Comprehension Gold  Standards <a href="https://arxiv.org/pdf/2003.04642" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Combining Pretrained High-Resource Embeddings and Subword  Representations for Low-Resource Languages <a href="https://arxiv.org/pdf/2003.04419" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> GenNet : Reading Comprehension with Multiple Choice Questions using  Generation and Selection model <a href="https://arxiv.org/pdf/2003.04360" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> ReZero is All You Need: Fast Convergence at Large Depth <a href="https://arxiv.org/pdf/2003.04887" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> On the coexistence of competing languages <a href="https://arxiv.org/pdf/2003.04748" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Neuro-symbolic Architectures for Context Understanding <a href="https://arxiv.org/pdf/2003.04707" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Ecological Semantics: Programming Environments for Situated Language  Understanding <a href="https://arxiv.org/pdf/2003.04567" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Multi-SimLex: A Large-Scale Evaluation of Multilingual and Cross-Lingual  Lexical Semantic Similarity</b>  <a href="https://arxiv.org/pdf/2003.04866" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Vuli%C4%87%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ivan Vulić</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Baker%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simon Baker</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ponti%2C+E+M" target="_blank" rel="noopener" style="color:#0000EE;">Edoardo Maria Ponti</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Petti%2C+U" target="_blank" rel="noopener" style="color:#0000EE;">Ulla Petti</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Leviant%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ira Leviant</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wing%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kelly Wing</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Majewska%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Olga Majewska</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bar%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eden Bar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Malone%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matt Malone</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Poibeau%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thierry Poibeau</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Reichart%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roi Reichart</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Korhonen%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anna Korhonen</a><br>
<font size="3">
Abstract: We introduce Multi-SimLex, a large-scale lexical resource and evaluation benchmark covering datasets for 12 typologically diverse languages, including major languages (e.g., Mandarin Chinese, Spanish, Russian) as well as less-resourced ones (e.g., Welsh, Kiswahili). Each language dataset is annotated for the lexical relation of semantic similarity and contains 1,888 semantically aligned concept pairs, providing a representative coverage of word classes (nouns, verbs, adjectives, adverbs), frequency ranks, similarity intervals, lexical fields, and concreteness levels. Additionally, owing to the alignment of concepts across languages, we provide a suite of 66 cross-lingual semantic similarity datasets. Due to its extensive size and language coverage, Multi-SimLex provides entirely novel opportunities for experimental evaluation and analysis. On its monolingual and cross-lingual benchmarks, we evaluate and analyze a wide array of recent state-of-the-art monolingual and cross-lingual representation models, including static and contextualized word embeddings (such as fastText, M-BERT and XLM), externally informed lexical representations, as well as fully unsupervised and (weakly) supervised cross-lingual word embeddings. We also present a step-by-step dataset creation protocol for creating consistent, Multi-Simlex-style resources for additional languages. We make these contributions -- the public release of Multi-SimLex datasets, their creation protocol, strong baseline results, and in-depth analyses which can be be helpful in guiding future developments in multilingual lexical semantics and representation learning -- available via a website which will encourage community effort in further expansion of Multi-Simlex to many more languages. Such a large-scale semantic resource could inspire significant further advances in NLP across languages. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文介绍了多SimLex，大规模的词汇资源，覆盖12种类型学的不同语言的数据集，包括主要语言（例如，国语中国语，西班牙语，俄语），以及资源充足的少的人（例如，威尔士的评价基准，斯瓦希里语）。每个语言数据集注释的语义相似度的词法关系和包含1888语义上对准的概念对，提供词类（名词，动词，形容词，副词），频率行列，相似的间隔，词汇字段，和具体层次的代表覆盖。另外，由于概念跨语言的对准，我们提供了一套66跨语言的语义相似度的数据集。由于其广泛的大小和语言覆盖，多SimLex提供了实验评估和分析完全新颖的机会。它的单语和跨语种基准，我们评估和分析的宽状态的最先进的最近的单语和跨语言表示模型，包括静态和情境化字的嵌入（如fastText，M-BERT和XLM）的阵列的，外部通知词汇表示，以及完全无人监督和（弱）监督跨语言字的嵌入。我们还提出一个一步一步的数据集创建协议对其他语言的创建一致的，多Simlex式的资源。我们做出这些贡献 - 多SimLex数据集，他们的创作协议，强大的基准结果的公开发布，并深入分析其可能是在多语种词汇语义和代表学习指导未来的发展有帮助的 - 可通过网站这将鼓励多Simlex到更多的语言进一步扩大社会各界共同努力。如此大规模的语义资源可以跨越语言激励在NLP显著的进一步发展。</font>
</div>


<hr>
<div id="paper2"> <b>2. Video Caption Dataset for Describing Human Actions in Japanese</b>  <a href="https://arxiv.org/pdf/2003.04865" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shigeto%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yutaro Shigeto</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yoshikawa%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuya Yoshikawa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiaqing Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Takeuchi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Akikazu Takeuchi</a><br>
<font size="3">
Abstract: In recent years, automatic video caption generation has attracted considerable attention. This paper focuses on the generation of Japanese captions for describing human actions. While most currently available video caption datasets have been constructed for English, there is no equivalent Japanese dataset. To address this, we constructed a large-scale Japanese video caption dataset consisting of 79,822 videos and 399,233 captions. Each caption in our dataset describes a video in the form of "who does what and where." To describe human actions, it is important to identify the details of a person, place, and action. Indeed, when we describe human actions, we usually mention the scene, person, and action. In our experiments, we evaluated two caption generation methods to obtain benchmark results. Further, we investigated whether those generation methods could specify "who does what and where." </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近年来，自动字幕生成已经吸引了相当多的关注。本文重点介绍日本字幕的生成用于描述人的行为。虽然大多数现有的视频字幕的数据集已经构建了英语，没有等价日本数据集。为了解决这个问题，我们构建一个大型的日本视频字幕数据集，包括79822个视频和字幕399233。在我们的数据集中的每个标题描述视频的形式“谁做什么，在哪里。”为了描述人的行为，以识别人物，地点和行动的细节是很重要的。事实上，当我们描述人的行为，我们通常提到的场景，人物和动作。在我们的实验中，我们评价了两种字幕生成方法来获取基准测试结果。此外，我们研究了这些发电方式是否可以指定“谁做什么，在哪里。”</font>
</div>


<hr>
<div id="paper3"> <b>3. Undersensitivity in Neural Reading Comprehension</b>  <a href="https://arxiv.org/pdf/2003.04808" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Welbl%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Johannes Welbl</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Minervini%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pasquale Minervini</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bartolo%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Max Bartolo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Stenetorp%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pontus Stenetorp</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Riedel%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sebastian Riedel</a><br>
<font size="3">
Abstract: Current reading comprehension models generalise well to in-distribution test sets, yet perform poorly on adversarially selected inputs. Most prior work on adversarial inputs studies oversensitivity: semantically invariant text perturbations that cause a model's prediction to change when it should not. In this work we focus on the complementary problem: excessive prediction undersensitivity, where input text is meaningfully changed but the model's prediction does not, even though it should. We formulate a noisy adversarial attack which searches among semantic variations of the question for which a model erroneously predicts the same answer, and with even higher probability. Despite comprising unanswerable questions, both SQuAD2.0 and NewsQA models are vulnerable to this attack. This indicates that although accurate, models tend to rely on spurious patterns and do not fully consider the information specified in a question. We experiment with data augmentation and adversarial training as defences, and find that both substantially decrease vulnerability to attacks on held out data, as well as held out attack spaces. Addressing undersensitivity also improves results on AddSent and AddOneSent, and models furthermore generalise better when facing train/evaluation distribution mismatch: they are less prone to overly rely on predictive cues present only in the training set, and outperform a conventional model by as much as 10.9% F1. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：当前的阅读理解模式推广以及在分布测试集，但表现不佳，adversarially选择的输入。在对抗投入研究过度敏感最优先的工作：语义不变文字扰动时，它不应该是导致模型的预测变化。在这项工作中，我们注重补充问题：过多的预测undersensitivity，在输入文本时意味深长地改变，但模型预测不对，即使它应该。我们制定一个嘈杂的敌对攻击该问题的语义变化中搜索该模型预测错误相同的答案，并具有更高的概率。尽管包括无法回答的问题，既SQuAD2.0和NewsQA模型很容易受到这种攻击。这表明，虽然准确，模型往往依靠虚假的模式，不充分考虑在一个问题中指定的信息。我们用数据增强和对抗性训练作为防御试验，发现两者显着降低脆弱性就伸出数据攻击，以及举行了进攻的空间。面对火车/评估分布不匹配时，寻址undersensitivity也提高了AddSent和AddOneSent结果和模型进一步广义含更好：它们不易过分依赖预测的线索只有在训练集展示，以及多达10.9优于传统模式％F1。</font>
</div>


<hr>
<div id="paper4"> <b>4. Efficient Intent Detection with Dual Sentence Encoders</b>  <a href="https://arxiv.org/pdf/2003.04807" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Casanueva%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Iñigo Casanueva</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tem%C4%8Dinas%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tadas Temčinas</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gerz%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniela Gerz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Henderson%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matthew Henderson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vuli%C4%87%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ivan Vulić</a><br>
<font size="3">
Abstract: Building conversational systems in new domains and with added functionality requires resource-efficient models that work under low-data regimes (i.e., in few-shot setups). Motivated by these requirements, we introduce intent detection methods backed by pretrained dual sentence encoders such as USE and ConveRT. We demonstrate the usefulness and wide applicability of the proposed intent detectors, showing that: 1) they outperform intent detectors based on fine-tuning the full BERT-Large model or using BERT as a fixed black-box encoder on three diverse intent detection data sets; 2) the gains are especially pronounced in few-shot setups (i.e., with only 10 or 30 annotated examples per intent); 3) our intent detectors can be trained in a matter of minutes on a single CPU; and 4) they are stable across different hyperparameter settings. In hope of facilitating and democratizing research focused on intention detection, we release our code, as well as a new challenging single-domain intent detection dataset comprising 13,083 annotated examples over 77 intents. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在新的领域，并与附加功能建立对话系统，需要节约资源的模式，低数据传输机制（即，在为数不多的镜头设置）下工作。通过这些要求的推动下，我们通过引入双预先训练句子的编码器，如使用和转换支持的意图的检测方法。我们证明了该意图检测器的有用性和广泛的适用性，这表明：1）它们优于基于微调在三个不同的意图的检测的数据集的完整BERT-Large模式或使用BERT作为固定黑箱编码器意图检测器; 2）增益在几次设置（即尤其显着，每意图仅10或30注释的例子）; 3）我们的意图的检测器可以在单个CPU上几分钟内就被训练; 4）他们是在不同的超参数设置稳定。在促进和民主化研究的希望集中在意向检测，我们发布的代码，以及新的具有挑战性的单域意图检测，包括超过77个意图13083个注释实例数据集。</font>
</div>


<hr>
<div id="paper5"> <b>5. Learning to Respond with Stickers: A Framework of Unifying  Multi-Modality in Multi-Turn Dialog</b>  <a href="https://arxiv.org/pdf/2003.04679" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shen Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiuying Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chang Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Li Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dongyan Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rui Yan</a><br>
<font size="3">
Abstract: Stickers with vivid and engaging expressions are becoming increasingly popular in online messaging apps, and some works are dedicated to automatically select sticker response by matching text labels of stickers with previous utterances. However, due to their large quantities, it is impractical to require text labels for the all stickers. Hence, in this paper, we propose to recommend an appropriate sticker to user based on multi-turn dialog context history without any external labels. Two main challenges are confronted in this task. One is to learn semantic meaning of stickers without corresponding text labels. Another challenge is to jointly model the candidate sticker with the multi-turn dialog context. To tackle these challenges, we propose a sticker response selector (SRS) model. Specifically, SRS first employs a convolutional based sticker image encoder and a self-attention based multi-turn dialog encoder to obtain the representation of stickers and utterances. Next, deep interaction network is proposed to conduct deep matching between the sticker with each utterance in the dialog history. SRS then learns the short-term and long-term dependency between all interaction results by a fusion network to output the the final matching score. To evaluate our proposed method, we collect a large-scale real-world dialog dataset with stickers from one of the most popular online chatting platform. Extensive experiments conducted on this dataset show that our model achieves the state-of-the-art performance for all commonly-used metrics. Experiments also verify the effectiveness of each component of SRS. To facilitate further research in sticker selection field, we release this dataset of 340K multi-turn dialog and sticker pairs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：以生动，引人入胜的表情贴纸正在成为在线消息应用越来越普及，有的作品致力于通过与以前的话语贴纸匹配的文本标签自动选择贴纸响应。然而，由于其数量大，是不切实际的要求对所有的贴纸文本标签。因此，在本文中，我们提出建议适当贴纸基于多轮对话的上下文历史的用户，无需任何外部的标签。两个主要挑战所面临的这一任务。一是学习贴的语义没有相应的文字标签。另一个挑战是共同的模型与多轮对话语境候选贴纸。为了应对这些挑战，我们提出了一个标签响应选择（SRS）模型。具体而言，SRS第一采用卷积基于不干胶贴纸图像编码器和一个自关注基于多匝对话编码器以获得贴和话语的表示。接下来，深交互网络提出了在对话历史每个话语的贴纸之间进行深匹配。 SRS然后通过熔融网络来输出最终的匹配分数获悉所有交互结果之间的短期和长期的依赖性。为了评估我们提出的方法，我们将收集从最流行的在线聊天平台之一贴纸大规模真实世界的对话集。在这个数据集上，我们的模型实现了国家的最先进的性能为所有常用的指标进行了广泛的实验。实验还证实SRS的每个组件的有效性。为了便于在贴纸的选择领域的进一步研究，我们发布的340K多轮对话和贴纸对这个数据集。</font>
</div>


<hr>
<div id="paper6"> <b>6. A Framework for Evaluation of Machine Reading Comprehension Gold  Standards</b>  <a href="https://arxiv.org/pdf/2003.04642" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Schlegel%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Viktor Schlegel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Valentino%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marco Valentino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Freitas%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">André Freitas</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nenadic%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Goran Nenadic</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Batista-Navarro%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Riza Batista-Navarro</a><br>
<font size="3">
Abstract: Machine Reading Comprehension (MRC) is the task of answering a question over a paragraph of text. While neural MRC systems gain popularity and achieve noticeable performance, issues are being raised with the methodology used to establish their performance, particularly concerning the data design of gold standards that are used to evaluate them. There is but a limited understanding of the challenges present in this data, which makes it hard to draw comparisons and formulate reliable hypotheses. As a first step towards alleviating the problem, this paper proposes a unifying framework to systematically investigate the present linguistic features, required reasoning and background knowledge and factual correctness on one hand, and the presence of lexical cues as a lower bound for the requirement of understanding on the other hand. We propose a qualitative annotation schema for the first and a set of approximative metrics for the latter. In a first application of the framework, we analyse modern MRC gold standards and present our findings: the absence of features that contribute towards lexical ambiguity, the varying factual correctness of the expected answers and the presence of lexical cues, all of which potentially lower the reading comprehension complexity and quality of the evaluation data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：机阅读理解（MRC）是在一段文字回答问题的任务。虽然神经系统MRC得到普及并取得显着的性能，问题被提出以用于建立自己的表现，特别是有关的被用来评估他们的黄金标准的数据设计的方法。有，但是目前在这个数据，这使得它很难得出比较，并制定可靠的假设的理解有限的挑战。作为有助于减轻该问题的第一步骤中，提出了一种统一的框架系统地研究本语言特征，一方面需要推理和背景知识和事实正确性，和词汇线索的存在，作为一个下界的理解的要求另一方面。我们提出了一个质的注释架构的第一和后者的一组近似度量。在此框架的第一个应用，我们分析现代MRC金标准，目前我们的研究结果：即对词汇歧义贡献没有特色，期望答案的不同事实的正确性和词汇线索的存在，所有这些都可能降低阅读理解的复杂性和评估数据的质量。</font>
</div>


<hr>
<div id="paper7"> <b>7. Combining Pretrained High-Resource Embeddings and Subword  Representations for Low-Resource Languages</b>  <a href="https://arxiv.org/pdf/2003.04419" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Reid%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Machel Reid</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marrese-Taylor%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Edison Marrese-Taylor</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Matsuo%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yutaka Matsuo</a><br>
<font size="3">
Abstract: The contrast between the need for large amounts of data for current Natural Language Processing (NLP) techniques, and the lack thereof, is accentuated in the case of African languages, most of which are considered low-resource. To help circumvent this issue, we explore techniques exploiting the qualities of morphologically rich languages (MRLs), while leveraging pretrained word vectors in well-resourced languages. In our exploration, we show that a meta-embedding approach combining both pretrained and morphologically-informed word embeddings performs best in the downstream task of Xhosa-English translation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：大量数据的当前自然语言处理（NLP）技术，以及缺乏在非洲语言，其中大部分被认为是低的资源的情况下，加剧了需求之间的对比。为了帮助规避这个问题，我们探索利用技术形态丰富的语言（最大残留限量）的质量，同时利用预先训练词矢量在资源充足的语言。在我们的探索，我们表明，元嵌入的方式在科萨英语翻译的下游任务都预先训练和形态知情字的嵌入进行最佳结合。</font>
</div>


<hr>
<div id="paper8"> <b>8. GenNet : Reading Comprehension with Multiple Choice Questions using  Generation and Selection model</b>  <a href="https://arxiv.org/pdf/2003.04360" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ingale%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vaishali Ingale</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pushpender Singh</a><br>
<font size="3">
Abstract: Multiple-choice machine reading comprehension is difficult task as its required machines to select the correct option from a set of candidate or possible options using the given passage and question.Reading Comprehension with Multiple Choice Questions task,required a human (or machine) to read a given passage, question pair and select the best one option from n given options. There are two different ways to select the correct answer from the given passage. Either by selecting the best match answer to by eliminating the worst match answer. Here we proposed GenNet model, a neural network-based model. In this model first we will generate the answer of the question from the passage and then will matched the generated answer with given answer, the best matched option will be our answer. For answer generation we used S-net (Tan et al., 2017) model trained on SQuAD and to evaluate our model we used Large-scale RACE (ReAding Comprehension Dataset From Examinations) (Lai et al.,2017). </font>
<br>
<font size="2" style="line-height:30px;">
摘要：多项选择题机器阅读理解是其所需的机器使用给定的通道和question.Reading理解与多项选择题的任务从一组的候选人或可能的选择正确的选项艰巨的任务，需要一个人（或机器）阅读给定的通道，问题对选择由正给定的选项最好的一种选择。有两种不同的方式来选择从给定的通道正确的答案。无论是通过消除最恶劣的比赛答案中选择最匹配的答案。在这里，我们提出GENNET模型，基于神经网络模型。在这个模型中我们首先会产生从通道的问题的答案，然后将匹配给定答案中产生答案，最佳匹配的选择将是我们的答案。对于答案代我们使用S-NET（Tan等，2017）模型中训练的阵容，来评估我们的模型中，我们使用大型RACE（阅读理解数据集从考试）（Lai等，2017）。</font>
</div>


<hr>
<div id="paper9"> <b>9. ReZero is All You Need: Fast Convergence at Large Depth</b>  <a href="https://arxiv.org/pdf/2003.04887" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bachlechner%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Bachlechner</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Majumder%2C+B+P" target="_blank" rel="noopener" style="color:#0000EE;">Bodhisattwa Prasad Majumder</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+H+H" target="_blank" rel="noopener" style="color:#0000EE;">Huanru Henry Mao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cottrell%2C+G+W" target="_blank" rel="noopener" style="color:#0000EE;">Garrison W. Cottrell</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=McAuley%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Julian McAuley</a><br>
<font size="3">
Abstract: Deep networks have enabled significant performance gains across domains, but they often suffer from vanishing/exploding gradients. This is especially true for Transformer architectures where depth beyond 12 layers is difficult to train without large datasets and computational budgets. In general, we find that inefficient signal propagation impedes learning in deep networks. In Transformers, multi-head self-attention is the main cause of this poor signal propagation. To facilitate deep signal propagation, we propose ReZero, a simple change to the architecture that initializes an arbitrary layer as the identity map, using a single additional learned parameter per layer. We apply this technique to language modeling and find that we can easily train ReZero-Transformer networks over a hundred layers. When applied to 12 layer Transformers, ReZero converges 56% faster on enwiki8. ReZero applies beyond Transformers to other residual networks, enabling 1,500% faster convergence for deep fully connected networks and 32% faster convergence for a ResNet-56 trained on CIFAR 10. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深网络使跨域显著的性能提升，但他们往往消失/爆炸梯度受到影响。这是变压器的架构，其中深度超过12层，是很难培养没有大型数据集和计算的预算来说尤其如此。在一般情况下，我们发现，低效率的信号传播阻碍了深刻的网络学习。在变压器，多头的自我关注的是这个可怜的信号传播的主要原因。为了便于深信号传播，我们建议重新调零，一个简单的改变来初始化的任意层作为恒等映射的体系结构，使用每层单个附加学习参数。我们应用此技术的语言模型和发现，我们可以很容易地超过一百层训练重新调零变压器网络。当施加到12个变压器，重新调零收敛于enwiki8快56％。重新调零适用超出变压器到其它残余的网络，使1500％更快的收敛为深全连接网络，32％用于RESNET-56上训练CIFAR 10更快的收敛。</font>
</div>


<hr>
<div id="paper10"> <b>10. On the coexistence of competing languages</b>  <a href="https://arxiv.org/pdf/2003.04748" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cond-mat?searchtype=author&query=Luck%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jean-Marc Luck</a>, 
<a href="https://arxiv.org/search/cond-mat?searchtype=author&query=Mehta%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anita Mehta</a><br>
<font size="3">
Abstract: We investigate the evolution of competing languages, a subject where much previous literature suggests that the outcome is always the domination of one language over all the others. Since coexistence of languages is observed in reality, we here revisit the question of language competition, with an emphasis on uncovering the ways in which coexistence might emerge. We find that this emergence is related to symmetry breaking, and explore two particular scenarios -- the first relating to an imbalance in the population dynamics of language speakers in a single geographical area, and the second to do with spatial heterogeneity, where language preferences are specific to different geographical regions. For each of these, the investigation of paradigmatic situations leads us to a quantitative understanding of the conditions leading to language coexistence. We also obtain predictions of the number of surviving languages as a function of various model parameters. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们研究了竞争性语言的进化，在以前的许多文献表明，结局总是一种语言比所有其他人的统治的对象。由于语言共存的现实观察，我们在这里重温语言竞争的问题，对揭示其共存可能出现的方式为重点。我们发现，这个出现有关对称性破缺，探索两种特殊情形 - 第一与在语言的人在一个地理区域的人口动态失衡，而第二与空间异质性，其中的语言偏好做具体到不同的地理区域。对于这些，的范式情况的调查使我们领先的语言共存的条件的定量理解。我们也获得仅存的语言为各种模型参数的函数的数量的预测。</font>
</div>


<hr>
<div id="paper11"> <b>11. Neuro-symbolic Architectures for Context Understanding</b>  <a href="https://arxiv.org/pdf/2003.04707" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Oltramari%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alessandro Oltramari</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Francis%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jonathan Francis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Henson%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cory Henson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kaixin Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wickramarachchi%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruwan Wickramarachchi</a><br>
<font size="3">
Abstract: Computational context understanding refers to an agent's ability to fuse disparate sources of information for decision-making and is, therefore, generally regarded as a prerequisite for sophisticated machine reasoning capabilities, such as in artificial intelligence (AI). Data-driven and knowledge-driven methods are two classical techniques in the pursuit of such machine sense-making capability. However, while data-driven methods seek to model the statistical regularities of events by making observations in the real-world, they remain difficult to interpret and they lack mechanisms for naturally incorporating external knowledge. Conversely, knowledge-driven methods, combine structured knowledge bases, perform symbolic reasoning based on axiomatic principles, and are more interpretable in their inferential processing; however, they often lack the ability to estimate the statistical salience of an inference. To combat these issues, we propose the use of hybrid AI methodology as a general framework for combining the strengths of both approaches. Specifically, we inherit the concept of neuro-symbolism as a way of using knowledge-bases to guide the learning progress of deep neural networks. We further ground our discussion in two applications of neuro-symbolism and, in both cases, show that our systems maintain interpretability while achieving comparable performance, relative to the state-of-the-art. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：计算上下文的理解是指试剂的融合不同的信息来源的决策，并，因此，通常被视为成熟的机器的推理能力，如在人工智能（AI）的先决条件的能力。数据驱动和知识驱动的方法都在追求这样的机器意义建构能力的两种经典技术。然而，虽然数据驱动的方法试图通过在真实世界的观察到的事件的统计规律的模型，但它们仍然难以解释，他们缺乏自然引入外部知识的机制。相反，知识驱动的方法，结合结构化的知识基础，进行符号推理基础上的公理原则，并在他们的推理处理更可解释的;然而，他们往往缺乏估计推理的统计显着性的能力。为了解决这些问题，我们建议使用混合AI方法学为结合两种方法的优点的总体框架。具体来说，我们继承神经象征的概念，利用知识基础，指导深层神经网络的学习进度的一种方式。我们进一步地我们的讨论中神经象征的两个应用程序，并在这两种情况下，显示我们的系统中保持可解释性，同时实现相当的性能，相对于国家的最先进的。</font>
</div>


<hr>
<div id="paper12"> <b>12. Ecological Semantics: Programming Environments for Situated Language  Understanding</b>  <a href="https://arxiv.org/pdf/2003.04567" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tamari%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ronen Tamari</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Stanovsky%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gabriel Stanovsky</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shahaf%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dafna Shahaf</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tsarfaty%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Reut Tsarfaty</a><br>
<font size="3">
Abstract: Large-scale natural language understanding (NLU) systems have made impressive progress: they can be applied flexibly across a variety of tasks, and employ minimal structural assumptions. However, extensive empirical research has shown this to be a double-edged sword, coming at the cost of shallow understanding: inferior generalization, grounding and explainability. Grounded language learning approaches offer the promise of deeper understanding by situating learning in richer, more structured training environments, but are limited in scale to relatively narrow, predefined domains. How might we enjoy the best of both worlds: grounded, general NLU? Following extensive contemporary cognitive science, we propose treating environments as ``first-class citizens'' in semantic representations, worthy of research and development in their own right. Importantly, models should also be partners in the creation and configuration of environments, rather than just actors within them, as in existing approaches. To do so, we argue that models must begin to understand and program in the language of affordances (which define possible actions in a given situation) both for online, situated discourse comprehension, as well as large-scale, offline common-sense knowledge mining. To this end we propose an environment-oriented ecological semantics, outlining theoretical and practical approaches towards implementation. We further provide actual demonstrations building upon interactive fiction programming languages. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：大型自然语言理解（NLU）系统已经取得了重大进展：他们可以灵活地在各种任务中得到应用，并采用最小的结构假设。然而，大量的实证研究表明这是一个双刃剑，在认识肤浅的成本来：劣质泛化，接地和explainability。接地的语言学习方法通​​过更丰富，更结构化的训练环境情境的学习提供更深入的了解的承诺，但在规模上受限于相对狭窄的，预定义的域。我们如何享受两全其美：接地，一般NLU？经过广泛的当代认知科学，我们建议治疗环境中的语义表示``一等公民“”，值得在他们自己的权利的研究和开发。重要的是，车型也应该是在环境的创建和配置的合作伙伴，而不仅仅是演员在其中，如在现有的方法。要做到这一点，我们认为，模型必须开始启示（限定在特定情况下可能采取的行动）的语言理解和程序都在网上，位于语篇理解，以及大型，离线常识性的知识挖掘。为此，我们提出了一个面向环境生态语义，勾勒朝着实现理论和实践方法。我们进一步提供实际的示范建设在互动小说的编程语言。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>PROCJX
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://procjx.github.io/2020/03/11/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-03-11/" title="【arxiv论文】 Computation and Language 2020-03-11">https://procjx.github.io/2020/03/11/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-03-11/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">

        
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2020/03/10/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-03-10/" rel="next" title="【arxiv论文】 Computer Vision and Pattern Recognition 2020-03-10">
                  <i class="fa fa-chevron-left"></i> 【arxiv论文】 Computer Vision and Pattern Recognition 2020-03-10
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/2020/03/11/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-03-11/" rel="prev" title="【arxiv论文】 Computer Vision and Pattern Recognition 2020-03-11">
                  【arxiv论文】 Computer Vision and Pattern Recognition 2020-03-11 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>

        
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang -->
<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9197824246"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="gitalk-container"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#目录"><span class="nav-number">1.</span> <span class="nav-text">目录</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#摘要"><span class="nav-number">2.</span> <span class="nav-text">摘要</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="PROCJX"
    src="/images/procjx.png">
  <p class="site-author-name" itemprop="name">PROCJX</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">334</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">71</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/procjx" title="GitHub &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;procjx" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:procjx@gmail.com" title="E-Mail &amp;rarr; mailto:procjx@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>


<!--
      
        <script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
        <script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
        <div class="widget-wrap">
            <h3 class="widget-title">标签云</h3>
            <div id="myCanvasContainer" class="widget tagcloud">
                <canvas width="250" height="250" id="resCanvas" style="width=100%">
                    <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AAAI/" rel="tag">AAAI</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ACL/" rel="tag">ACL</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Accepted-Papers/" rel="tag">Accepted Papers</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ArXiv/" rel="tag">ArXiv</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/" rel="tag">BERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS20SI/" rel="tag">CS20SI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS224d/" rel="tag">CS224d</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/" rel="tag">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CVPR/" rel="tag">CVPR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Context/" rel="tag">Context</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cross-Lingual/" rel="tag">Cross Lingual</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dialog-System/" rel="tag">Dialog System</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse/" rel="tag">Discourse</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Ranking/" rel="tag">Discourse Ranking</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Structure/" rel="tag">Discourse Structure</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Document-NMT/" rel="tag">Document NMT</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EMNLP/" rel="tag">EMNLP</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Extractive/" rel="tag">Extractive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICLR/" rel="tag">ICLR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICML/" rel="tag">ICML</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IJCAI/" rel="tag">IJCAI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Inter-Sentence/" rel="tag">Inter-Sentence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Keyphrase-Generation/" rel="tag">Keyphrase Generation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NAACL/" rel="tag">NAACL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIPS/" rel="tag">NIPS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NMT/" rel="tag">NMT</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Relation-Extraction/" rel="tag">Neural Relation Extraction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RST/" rel="tag">RST</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Relation-Constraints/" rel="tag">Relation Constraints</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Summarization/" rel="tag">Summarization</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Translation/" rel="tag">Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Word-Translation/" rel="tag">Word Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/alias/" rel="tag">alias</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/git/" rel="tag">git</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pip/" rel="tag">pip</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/screen/" rel="tag">screen</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/" rel="tag">shell</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tgz/" rel="tag">tgz</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tts/" rel="tag">tts</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1/" rel="tag">冒泡</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/" rel="tag">冒泡排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%99%E4%BD%9C%E5%8A%A9%E6%89%8B/" rel="tag">写作助手</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8E%8B%E7%BC%A9/" rel="tag">压缩</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/" rel="tag">发送邮件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%88%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">合并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%8E%E5%8F%B0/" rel="tag">后台</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/" rel="tag">基数排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F/" rel="tag">希尔排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6/" rel="tag">归并</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">归并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/" rel="tag">快速排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F/" rel="tag">批量</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4/" rel="tag">批量删除</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F/" rel="tag">排序</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5/" rel="tag">插入</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/" rel="tag">插入排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%99%E7%A8%8B/" rel="tag">教程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/" rel="tag">斐波那契数列</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9D%80%E6%AD%BB%E8%BF%9B%E7%A8%8B/" rel="tag">杀死进程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B1%89%E8%AF%BA%E5%A1%94/" rel="tag">汉诺塔</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%A7%A3%E5%8E%8B/" rel="tag">解压</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%B0%B7%E6%AD%8C%E7%BF%BB%E8%AF%91/" rel="tag">谷歌翻译</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BF%AD%E4%BB%A3%E5%9B%9E%E7%BF%BB/" rel="tag">迭代回翻</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9/" rel="tag">选择</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/" rel="tag">选择排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%99%84%E4%BB%B6/" rel="tag">附件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3/" rel="tag">非监督</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%A2%86%E5%9F%9F%E9%80%82%E5%BA%94/" rel="tag">领域适应</a><span class="tag-list-count">1</span></li></ul>
                </canvas>
            </div>
        </div>
        
-->
        
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-hengfu -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9879871597"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-chuizhi -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="1662238719"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-zhengfangxing -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="6699421902"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PROCJX</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.4.2
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>












        
      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  <script src="/js/local-search.js"></script>













  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '2286ab64f5194d9d79ce',
      clientSecret: 'f912492bec2391664b40478f50f2f943376768d6',
      repo: 'procjx.github.io',
      owner: 'procjx',
      admin: ['procjx'],
      id: 'd7da703d8adb6b24c2da19083fbbdd1a',
        language: 'zh-CN',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
