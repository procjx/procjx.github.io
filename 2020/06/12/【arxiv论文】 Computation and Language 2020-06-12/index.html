<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/procjx.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/procjxfavicon32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/procjxfavicon16x16.ico">
  <link rel="mask-icon" href="/images/procjx.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.4.2',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

<!-- Google Adsense -->
<!--
<script async src="//pagead2.googlesyndication.com/
pagead/js/adsbygoogle.js"></script>
<script>
(adsbygoogle = window.adsbygoogle || []).push({
google_ad_client: "pub-1179774715076800",
enable_page_level_ads: true
});
</script>
-->

<script data-ad-client="ca-pub-1179774715076800" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>


<meta name="google-site-verification" content="cEiGwg0T8Rj5msmuEcGYZTh5nnf05EhCXy0gp2Ml5BI" />
<meta name="baidu-site-verification" content="noSKHe8MJs" />

  <meta name="description" content="目录  1. Teaching Pre-Trained Models to Systematically Reason Over Implicit  Knowledge [PDF] 摘要  2. Multi-hop Reading Comprehension across Documents with Path-based Graph  Convolutional Network [PDF] 摘要">
<meta property="og:type" content="article">
<meta property="og:title" content="【arxiv论文】 Computation and Language 2020-06-12">
<meta property="og:url" content="https:&#x2F;&#x2F;procjx.github.io&#x2F;2020&#x2F;06&#x2F;12&#x2F;%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-06-12&#x2F;index.html">
<meta property="og:site_name" content="PROCJX&#39;s BLOGS">
<meta property="og:description" content="目录  1. Teaching Pre-Trained Models to Systematically Reason Over Implicit  Knowledge [PDF] 摘要  2. Multi-hop Reading Comprehension across Documents with Path-based Graph  Convolutional Network [PDF] 摘要">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2020-06-12T12:38:36.536Z">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://procjx.github.io/2020/06/12/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-06-12/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>【arxiv论文】 Computation and Language 2020-06-12 | PROCJX's BLOGS</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">PROCJX's BLOGS</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">WITH LOVE OF WORLD</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-fw fa-download"></i>资源</a>

  </li>
        <li class="menu-item menu-item-arxiv">

    <a href="/arxiv/" rel="section"><i class="fa fa-fw fa-file-pdf-o"></i>arxiv论文</a>

  </li>
        <li class="menu-item menu-item-deadline">

    <a href="/deadline/" rel="section"><i class="fa fa-fw fa-calendar"></i>会议截稿</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://procjx.github.io/2020/06/12/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-06-12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/procjx.png">
      <meta itemprop="name" content="PROCJX">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PROCJX's BLOGS">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          【arxiv论文】 Computation and Language 2020-06-12
        </h2>

        <div class="post-meta">
        
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-06-12 11:31:40 / 修改时间：20:38:36" itemprop="dateCreated datePublished" datetime="2020-06-12T11:31:40+08:00">2020-06-12</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/arxiv/" itemprop="url" rel="index">
                    <span itemprop="name">arxiv</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/arxiv/CL/" itemprop="url" rel="index">
                    <span itemprop="name">CL</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
              <span>31k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
              <span>51 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Teaching Pre-Trained Models to Systematically Reason Over Implicit  Knowledge <a href="https://arxiv.org/pdf/2006.06609" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Multi-hop Reading Comprehension across Documents with Path-based Graph  Convolutional Network <a href="https://arxiv.org/pdf/2006.06478" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> A Probabilistic Model with Commonsense Constraints for Pattern-based  Temporal Fact Extraction <a href="https://arxiv.org/pdf/2006.06436" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> CoSDA-ML: Multi-Lingual Code-Switching Data Augmentation for Zero-Shot  Cross-Lingual NLP <a href="https://arxiv.org/pdf/2006.06402" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Provenance for Linguistic Corpora Through Nanopublications <a href="https://arxiv.org/pdf/2006.06341" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine  Translation Evaluation Metrics <a href="https://arxiv.org/pdf/2006.06264" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Augmenting Data for Sarcasm Detection with Unlabeled Conversation  Context <a href="https://arxiv.org/pdf/2006.06259" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Performance in the Courtroom: Automated Processing and Visualization of  Appeal Court Decisions in France <a href="https://arxiv.org/pdf/2006.06251" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Discrete Latent Variable Representations for Low-Resource Text  Classification <a href="https://arxiv.org/pdf/2006.06226" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> A Monolingual Approach to Contextualized Word Embeddings for  Mid-Resource Languages <a href="https://arxiv.org/pdf/2006.06202" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Emora STDM: A Versatile Framework for Innovative Dialogue System  Development <a href="https://arxiv.org/pdf/2006.06143" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Towards Unified Dialogue System Evaluation: A Comprehensive Analysis of  Current Evaluation Protocols <a href="https://arxiv.org/pdf/2006.06110" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Report from the NSF Future Directions Workshop, Toward User-Oriented  Agents: Research Directions and Challenges <a href="https://arxiv.org/pdf/2006.06026" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Disentangled Non-Local Neural Networks <a href="https://arxiv.org/pdf/2006.06668" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> VirTex: Learning Visual Representations from Textual Annotations <a href="https://arxiv.org/pdf/2006.06666" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Exploring Weaknesses of VQA Models through Attribution Driven Insights <a href="https://arxiv.org/pdf/2006.06637" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Deep Differential System Stability -- Learning advanced computations  from examples <a href="https://arxiv.org/pdf/2006.06462" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Extracting and categorising the reactions to COVID-19 by the South  African public -- A social media study <a href="https://arxiv.org/pdf/2006.06336" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Mental Workload and Language Production in Non-Native Speaker IPA  Interaction <a href="https://arxiv.org/pdf/2006.06331" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> See what I'm saying? Comparing Intelligent Personal Assistant use for  Native and Non-Native Language Speakers <a href="https://arxiv.org/pdf/2006.06328" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Transparency in Language Generation: Levels of Automation <a href="https://arxiv.org/pdf/2006.06295" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> XiaoiceSing: A High-Quality and Integrated Singing Voice Synthesis  System <a href="https://arxiv.org/pdf/2006.06261" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Large-Scale Adversarial Training for Vision-and-Language Representation  Learning <a href="https://arxiv.org/pdf/2006.06195" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> PeopleMap: Visualization Tool for Mapping Out Researchers using Natural  Language Processing <a href="https://arxiv.org/pdf/2006.06105" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Teaching Pre-Trained Models to Systematically Reason Over Implicit  Knowledge</b>  <a href="https://arxiv.org/pdf/2006.06609" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Talmor%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alon Talmor</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tafjord%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Oyvind Tafjord</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Clark%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peter Clark</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yoav Goldberg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Berant%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jonathan Berant</a><br>
<font size="3">
 Abstract: To what extent can a neural network systematically reason over symbolic facts? Evidence suggests that large pre-trained language models (LMs) acquire some reasoning capacity, but this ability is difficult to control. Recently, it has been shown that Transformer-based models succeed in consistent reasoning over explicit symbolic facts, under a "closed-world" assumption. However, in an open-domain setup, it is desirable to tap into the vast reservoir of implicit knowledge already encoded in the parameters of pre-trained LMs. In this work, we provide a first demonstration that LMs can be trained to reliably perform systematic reasoning combining both implicit, pre-trained knowledge and explicit natural language statements. To do this, we describe a procedure for automatically generating datasets that teach a model new reasoning skills, and demonstrate that models learn to effectively perform inference which involves implicit taxonomic and world knowledge, chaining and counting. Finally, we show that "teaching" models to reason generalizes beyond the training distribution: they successfully compose the usage of multiple reasoning skills in single examples. Our work paves a path towards open-domain systems that constantly improve by interacting with users who can instantly correct a model by adding simple natural language statements.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在何种程度上可以一个神经网络系统的理智战胜了象征性的事实吗？有证据表明，大型预训练的语言模型（LMS）获得了一些推理能力，但这种能力是难以控制。最近，它已经表明，基于变压器的模型在明确的象征事实相符推理成功，下一个“封闭的世界”的假设。然而，在开放域的设置，希望进军在预先训练的LMS参数已编码的隐性知识方面的巨大资源。在这项工作中，我们提供了一个首次证明LMS能够通过训练来可靠地执行系统的推理组合这两个隐含的，预先训练知识和显性的自然语言语句。要做到这一点，我们描述了一个程序自动生成，教一个新的模型推理能力的数据集，并证明模型学习如何有效地进行推理涉及隐分类和世界的知识，链接和计数。最后，我们表明，“教学”模式，以超越训练分布的原因概括：他们成功地组成的多个推理技巧的使用单一实例。我们的工作铺平了道路，朝着这一不断与谁可以通过添加简单的自然语言的语句立即纠正模型用户互动提高开放领域系统的路径。</font>
</div>


<hr>
<div id="paper2"> <b>2. Multi-hop Reading Comprehension across Documents with Path-based Graph  Convolutional Network</b>  <a href="https://arxiv.org/pdf/2006.06478" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zeyun Tang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yongliang Shen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinyin Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiale Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weiming Lu</a><br>
<font size="3">
 Abstract: Multi-hop reading comprehension across multiple documents attracts much attention recently. In this paper, we propose a novel approach to tackle this multi-hop reading comprehension problem. Inspired by human reasoning processing, we construct a path-based reasoning graph from supporting documents. This graph can combine both the idea of the graph-based and path-based approaches, so it is better for multi-hop reasoning. Meanwhile, we propose Gated-RGCN to accumulate evidence on the path-based reasoning graph, which contains a new question-aware gating mechanism to regulate the usefulness of information propagating across documents and add question information during reasoning. We evaluate our approach on WikiHop dataset, and our approach achieves state-of-the-art accuracy against previously published approaches. Especially, our ensemble model surpasses human performance by 4.2%.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在多个文档多跳阅读理解最近备受瞩目。在本文中，我们提出了一种新的方法来解决这个多跳阅读理解问题。通过人类的推理处理的启发，我们从支持文件构建一个基于路径的推理图。该图可以结合基于图形和基于路径的方法的两个想法，所以它是多跳推理更好。同时，我们提出门控RGCN对基于路径的推理图，其中包含一个新的问题意识的门控机制，规范信息传播的跨文档的有效性和推理过程中添加问题的资料积累的证据。我们评估我们的WikiHop数据集的方法，而我们的方法实现对此前公布的方案国家的最先进的精度。特别是，我们的集成模型4.2％，超过了人体机能。</font>
</div>


<hr>
<div id="paper3"> <b>3. A Probabilistic Model with Commonsense Constraints for Pattern-based  Temporal Fact Extraction</b>  <a href="https://arxiv.org/pdf/2006.06436" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tong Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Meng Jiang</a><br>
<font size="3">
 Abstract: Textual patterns (e.g., Country's president Person) are specified and/or generated for extracting factual information from unstructured data. Pattern-based information extraction methods have been recognized for their efficiency and transferability. However, not every pattern is reliable: A major challenge is to derive the most complete and accurate facts from diverse and sometimes conflicting extractions. In this work, we propose a probabilistic graphical model which formulates fact extraction in a generative process. It automatically infers true facts and pattern reliability without any supervision. It has two novel designs specially for temporal facts: (1) it models pattern reliability on two types of time signals, including temporal tag in text and text generation time; (2) it models commonsense constraints as observable variables. Experimental results demonstrate that our model significantly outperforms existing methods on extracting true temporal facts from news data.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：文本模式（例如，国总统人）被指定和/或用于提取从非结构化数据的事实信息生成的。基于模式的信息提取方法已被确认为他们的效率和转移性。然而，并不是每一个模式是可靠的：一个主要挑战是派生从不同和有时是相互冲突提取最完整，最准确的事实。在这项工作中，我们提出这实际上制定提取在生成过程概率图形模型。它会自动推断出真正的事实和模式的可靠性没有任何监督。它具有专门用于时间事实两种新的设计：在两种类型的时间信号，包括在文本和文本生成时间的时间标签的（1）它的模型图案的可靠性; （2）它的模型常识约束作为可观察的变量。实验结果表明，我们的模型显著优于从新闻的数据中提取真实时间的事实现有的方法。</font>
</div>


<hr>
<div id="paper4"> <b>4. CoSDA-ML: Multi-Lingual Code-Switching Data Augmentation for Zero-Shot  Cross-Lingual NLP</b>  <a href="https://arxiv.org/pdf/2006.06402" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Libo Qin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ni%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minheng Ni</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yue Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Che%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wanxiang Che</a><br>
<font size="3">
 Abstract: Multi-lingual contextualized embeddings, such as multilingual-BERT (mBERT), have shown success in a variety of zero-shot cross-lingual tasks. However, these models are limited by having inconsistent contextualized representations of subwords across different languages. Existing work addresses this issue by bilingual projection and fine-tuning technique. We propose a data augmentation framework to generate multi-lingual code-switching data to fine-tune mBERT, which encourages model to align representations from source and multiple target languages once by mixing their context information. Compared with the existing work, our method does not rely on bilingual sentences for training, and requires only one training process for multiple target languages. Experimental results on five tasks with 19 languages show that our method leads to significantly improved performances for all the tasks compared with mBERT.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：多国语言情境的嵌入，例如多语言-BERT（mBERT），已经显示出了多种零射门跨语言任务中取得成功。然而，这些模型由具有不同语言的子词不一致的情境表示限制。通过双语投影和微调技术现有的工作解决了这个问题。我们提出了一个数据增强框架生成多语种码转换数据进行微调mBERT，鼓励模型从源和多目标语言对齐表示一旦通过混合它们的上下文信息。与现有工作相比，我们的方法不依赖于双语句子进行训练，并且只需要一个多目标语言的训练过程。与19种语言五项任务的实验结果表明，该方法导致显著改善了所有与mBERT相比的任务性能。</font>
</div>


<hr>
<div id="paper5"> <b>5. Provenance for Linguistic Corpora Through Nanopublications</b>  <a href="https://arxiv.org/pdf/2006.06341" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lek%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Timo Lek</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=de+Groot%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anna de Groot</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kuhn%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tobias Kuhn</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Morante%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roser Morante</a><br>
<font size="3">
 Abstract: Research in Computational Linguistics is dependent on text corpora for training and testing new tools and methodologies. While there exists a plethora of annotated linguistic information, these corpora are often not interoperable without significant manual work. Moreover, these annotations might have adapted and might have evolved into different versions, making it challenging for researchers to know the data's provenance and merge it with other annotated corpora. In other words, these variations affect the interoperability between existing corpora. This paper addresses this issue with a case study on event annotated corpora and by creating a new, more interoperable representation of this data in the form of nanopublications. We demonstrate how linguistic annotations from separate corpora can be merged through a similar format to thereby make annotation content simultaneously accessible. The process for developing the nanopublications is described, and SPARQL queries are performed to extract interesting content from the new representations. The queries show that information of multiple corpora can now be retrieved more easily and effectively with the automated interoperability of the information of different corpora in a uniform data format.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：研究计算语言学取决于语料库进行训练和测试新的工具和方法。虽然存在的注释语言信息过多，这些语料库往往没有显著的手工工作不能互操作。此外，这些注释可能已经适应，并有可能演变成不同的版本，使其成为具有挑战性的研究人员知道数据的出处，并与其他标注的语料进行合并。换句话说，这些变化影响现有的语料库之间的互操作性。本文将解决这个问题与事件注释语料的案例研究，并在nanopublications的形式创建该数据的新的，更好的互操作性表示。我们演示了如何从不同的语料库语言学注释可以通过类似的形式合并，从而使注释内容的同时访问。为发展nanopublications的过程描述，并且SPARQL查询的执行来提取新的表述有趣的内容。的查询显示多个语料库的信息，现在可以与不同的语料库以统一的数据格式的信息的自动互操作性更容易且有效地检索。</font>
</div>


<hr>
<div id="paper6"> <b>6. Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine  Translation Evaluation Metrics</b>  <a href="https://arxiv.org/pdf/2006.06264" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Mathur%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nitika Mathur</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Baldwin%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tim Baldwin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cohn%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Trevor Cohn</a><br>
<font size="3">
 Abstract: Automatic metrics are fundamental for the development and evaluation of machine translation systems. Judging whether, and to what extent, automatic metrics concur with the gold standard of human evaluation is not a straightforward problem. We show that current methods for judging metrics are highly sensitive to the translations used for assessment, particularly the presence of outliers, which often leads to falsely confident conclusions about a metric's efficacy. Finally, we turn to pairwise system ranking, developing a method for thresholding performance improvement under an automatic metric against human judgements, which allows quantification of type I versus type II errors incurred, i.e., insignificant human differences in system quality that are accepted, and significant human differences that are rejected. Together, these findings suggest improvements to the protocols for metric evaluation and system performance evaluation in machine translation.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自动度量是机器翻译系统的开发和评估的基础。判断是否以及在何种程度上，自动度量与人工评估的金标准同意不是一个简单的问题。我们表明，判断当前的度量方法是用于评估翻译，离群的特别存在，高度敏感的这往往会导致错误的结论充满信心有关指标的功效。最后，我们来看看两两系统排名中，开发了一种用于在自动度量对人的判断，这使得I型相比发生II类错误，即在系统质量显着人类个体差异所接受，并显著的量化阈值处理性能改进被拒绝的人的差异。总之，这些发现表明改进为机器翻译度量评估和系统性能评估的协议。</font>
</div>


<hr>
<div id="paper7"> <b>7. Augmenting Data for Sarcasm Detection with Unlabeled Conversation  Context</b>  <a href="https://arxiv.org/pdf/2006.06259" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hankyol Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Youngjae Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gunhee Kim</a><br>
<font size="3">
 Abstract: We present a novel data augmentation technique, CRA (Contextual Response Augmentation), which utilizes conversational context to generate meaningful samples for training. We also mitigate the issues regarding unbalanced context lengths by changing the input-output format of the model such that it can deal with varying context lengths effectively. Specifically, our proposed model, trained with the proposed data augmentation technique, participated in the sarcasm detection task of FigLang2020, have won and achieves the best performance in both Reddit and Twitter datasets.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出一个新的数据增强技术，CRA（上下文响应增强），其利用会话语境生成用于训练有意义的样品。我们还通过改变模式使得它可以有效处理不同的语境长度的输入输出格式减轻有关不平衡背景下长度的问题。具体来说，我们提出的模型，与所提出的数据增强技术的培训，参加FigLang2020的嘲讽检测任务，赢得并实现两者Reddit和Twitter的数据集最佳的性能。</font>
</div>


<hr>
<div id="paper8"> <b>8. Performance in the Courtroom: Automated Processing and Visualization of  Appeal Court Decisions in France</b>  <a href="https://arxiv.org/pdf/2006.06251" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Boniol%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Paul Boniol</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Panagopoulos%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">George Panagopoulos</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xypolopoulos%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christos Xypolopoulos</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hamdani%2C+R+E" target="_blank" rel="noopener" style="color:#0000EE;">Rajaa El Hamdani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Amariles%2C+D+R" target="_blank" rel="noopener" style="color:#0000EE;">David Restrepo Amariles</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vazirgiannis%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michalis Vazirgiannis</a><br>
<font size="3">
 Abstract: Artificial Intelligence techniques are already popular and important in the legal domain. We extract legal indicators from judicial judgment to decrease the asymmetry of information of the legal system and the access-to-justice gap. We use NLP methods to extract interesting entities/data from judgments to construct networks of lawyers and judgments. We propose metrics to rank lawyers based on their experience, wins/loss ratio and their importance in the network of lawyers. We also perform community detection in the network of judgments and propose metrics to represent the difficulty of cases capitalising on communities features.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人工智能技术已经流行和重要的法律领域。我们提取的司法判决的法律指标下降的法律制度和接入到正义差距信息的不对称。我们用NLP方法从判决中提取有趣的实体/数据构建的律师和判断网络。我们建议指标，根据他们的经验，胜/损失率和他们的律师在网络中的重要性级别的律师。我们也判断网络中进行社区发现并提出的指标来表示资本对社区功能的情况下难度。</font>
</div>


<hr>
<div id="paper9"> <b>9. Discrete Latent Variable Representations for Low-Resource Text  Classification</b>  <a href="https://arxiv.org/pdf/2006.06226" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuning Jin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wiseman%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sam Wiseman</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Stratos%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Karl Stratos</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Livescu%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Karen Livescu</a><br>
<font size="3">
 Abstract: While much work on deep latent variable models of text uses continuous latent variables, discrete latent variables are interesting because they are more interpretable and typically more space efficient. We consider several approaches to learning discrete latent variable models for text in the case where exact marginalization over these variables is intractable. We compare the performance of the learned representations as features for low-resource document and sentence classification. Our best models outperform the previous best reported results with continuous representations in these low-resource settings, while learning significantly more compressed representations. Interestingly, we find that an amortized variant of Hard EM performs particularly well in the lowest-resource regimes.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：虽然在文本的深层潜变量模型很多工作采用连续隐变量，因为他们更可解释的，通常更节省空间的离散隐变量很有趣。我们认为几种方法来学习离散潜变量模型，在这些变数确切的边缘化是棘手的情况下的文本。我们比较了解到表示作为特征的低资源文件和句子分类的性能。我们最好的榜样胜过在这些低资源设置连续表示以前最好的业绩报告，一边学习显著更多的压缩表示。有趣的是，我们发现，硬EM执行中特别最低资源制度的摊销变型。</font>
</div>


<hr>
<div id="paper10"> <b>10. A Monolingual Approach to Contextualized Word Embeddings for  Mid-Resource Languages</b>  <a href="https://arxiv.org/pdf/2006.06202" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Su%C3%A1rez%2C+P+O" target="_blank" rel="noopener" style="color:#0000EE;">Pedro Ortiz Suárez</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Romary%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Laurent Romary</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sagot%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Benoît Sagot</a><br>
<font size="3">
 Abstract: We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (ELMo) for several mid-resource languages. We then compare the performance of OSCAR-based and Wikipedia-based ELMo embeddings for these languages on the part-of-speech tagging and parsing tasks. We show that, despite the noise in the Common-Crawl-based OSCAR data, embeddings trained on OSCAR perform much better than monolingual embeddings trained on Wikipedia. They actually equal or improve the current state of the art in tagging and parsing for all five languages. In particular, they also improve over multilingual Wikipedia-based contextual embeddings (multilingual BERT), which almost always constitutes the previous state of the art, thereby showing that the benefit of a larger, more diverse corpus surpasses the cross-lingual benefit of multilingual embedding architectures.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们通过语言分类，过滤和清洗使用多语言语料库OSCAR从普通抓取提取，培养了数月中资源语言和英语情境字的嵌入（ELMO）。然后，我们比较基于OSCAR和基于维基百科ELMO的嵌入这些语言对部分的语音标签化和分析任务的性能。我们表明，尽管在基于共抓取-OSCAR数据中的噪声，培训了OSCAR的嵌入进行更好的培训相比维基百科上的嵌入单语。他们实际上等于或提高标记和解析所有五种语言艺术的当前状态。特别地，它们还改善对多语言基于维基百科上下文的嵌入（多种语言BERT），这几乎总是构成现有技术的先前的状态，从而显示出一个更大，更多样化的语料库的益处超过多种语言嵌入的跨语种益处架构。</font>
</div>


<hr>
<div id="paper11"> <b>11. Emora STDM: A Versatile Framework for Innovative Dialogue System  Development</b>  <a href="https://arxiv.org/pdf/2006.06143" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Finch%2C+J+D" target="_blank" rel="noopener" style="color:#0000EE;">James D. Finch</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+J+D" target="_blank" rel="noopener" style="color:#0000EE;">Jinho D. Choi</a><br>
<font size="3">
 Abstract: This demo paper presents Emora STDM (State Transition Dialogue Manager), a dialogue system development framework that provides novel workflows for rapid prototyping of chat-based dialogue managers as well as collaborative development of complex interactions. Our framework caters to a wide range of expertise levels by supporting interoperability between two popular approaches, state machine and information state, to dialogue management. Our Natural Language Expression package allows seamless integration of pattern matching, custom NLP modules, and database querying, that makes the workflows much more efficient. As a user study, we adopt this framework to an interdisciplinary undergraduate course where students with both technical and non-technical backgrounds are able to develop creative dialogue managers in a short period of time.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文演示呈现Emora STDM（状态转换对话管理器），对话系统开发框架，它提供了基于聊天的对话经理的快速原型以及复杂的互动协作开发新的工作流程。我们的框架迎合广泛的专业知识水平的两个流行的方法，状态机和信息状态，对话管理之间的互操作性支持。我们的自然语言表达软件包允许模式匹配，自定义NLP模块和数据库查询，使工作流程更加高效的无缝集成。作为用户研究中，我们采用这个框架，一个跨学科的本科课程，让学生有技术和非技术背景能够开发创造性的对话经理在很短的时间周期。</font>
</div>


<hr>
<div id="paper12"> <b>12. Towards Unified Dialogue System Evaluation: A Comprehensive Analysis of  Current Evaluation Protocols</b>  <a href="https://arxiv.org/pdf/2006.06110" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Finch%2C+S+E" target="_blank" rel="noopener" style="color:#0000EE;">Sarah E. Finch</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+J+D" target="_blank" rel="noopener" style="color:#0000EE;">Jinho D. Choi</a><br>
<font size="3">
 Abstract: As conversational AI-based dialogue management has increasingly become a trending topic, the need for a standardized and reliable evaluation procedure grows even more pressing. The current state of affairs suggests various evaluation protocols to assess chat-oriented dialogue management systems, rendering it difficult to conduct fair comparative studies across different approaches and gain an insightful understanding of their values. To foster this research, a more robust evaluation protocol must be set in place. This paper presents a comprehensive synthesis of both automated and human evaluation methods on dialogue systems, identifying their shortcomings while accumulating evidence towards the most effective evaluation dimensions. A total of 20 papers from the last two years are surveyed to analyze three types of evaluation protocols: automated, static, and interactive. Finally, the evaluation dimensions used in these papers are compared against our expert evaluation on the system-user dialogue data collected from the Alexa Prize 2020.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于AI-作为对话的对话管理已日益成为一个热门话题，需要一个标准化的，可靠的评估程序的增长更为严峻。事务的当前状态显示不同的评估方案，评估导向的聊天对话管理系统，使其难以在不同的方法进行公平比较研究，并获得他们的价值观有见地的理解。为了促进这项研究，更强劲的评价协议必须在地方设置。本文介绍了对话系统自动和人工评估方法，确定各自的缺点，而对积累的最有效的评价维度的证据进行全面综合。从过去两年共有20篇论文被调查，分析三种评价协议：自动，静态和互动。最后，在这些论文中使用的评价维度是对我们从Alexa的奖2020采集系统用户对话数据专家评价比较。</font>
</div>


<hr>
<div id="paper13"> <b>13. Report from the NSF Future Directions Workshop, Toward User-Oriented  Agents: Research Directions and Challenges</b>  <a href="https://arxiv.org/pdf/2006.06026" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Eskenazi%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maxine Eskenazi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tiancheng Zhao</a><br>
<font size="3">
 Abstract: This USER Workshop was convened with the goal of defining future research directions for the burgeoning intelligent agent research community and to communicate them to the National Science Foundation. It took place in Pittsburgh Pennsylvania on October 24 and 25, 2019 and was sponsored by National Science Foundation Grant Number IIS-1934222. Any opinions, findings and conclusions or future directions expressed in this document are those of the authors and do not necessarily reflect the views of the National Science Foundation. The 27 participants presented their individual research interests and their personal research goals. In the breakout sessions that followed, the participants defined the main research areas within the domain of intelligent agents and they discussed the major future directions that the research in each area of this domain should take   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：该用户研讨会与定义为新兴的智能代理研究界未来的研究方向的目标召集和他们沟通，以美国国家科学基金会。它发生在宾夕法尼亚州匹兹堡于2019年10月24和25，是由美国国家科学基金会资助号IIS赞助-1934222。本文档中的任何意见，研究成果和结论或未来的发展方向是那些作者，并不一定反映国家科学基金会的意见。 27名学员提出了他们的个人研究兴趣和个人的研究目标。在分组会议随后，与会的智能代理域内定义的主要研究领域，他们讨论了今后的主要方向，在这个领域的各个方面的研究应该采取</font>
</div>


<hr>
<div id="paper14"> <b>14. Disentangled Non-Local Neural Networks</b>  <a href="https://arxiv.org/pdf/2006.06668" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minghao Yin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhuliang Yao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yue Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiu Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zheng Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stephen Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Han Hu</a><br>
<font size="3">
 Abstract: The non-local block is a popular module for strengthening the context modeling ability of a regular convolutional neural network. This paper first studies the non-local block in depth, where we find that its attention computation can be split into two terms, a whitened pairwise term accounting for the relationship between two pixels and a unary term representing the saliency of every pixel. We also observe that the two terms trained alone tend to model different visual clues, e.g. the whitened pairwise term learns within-region relationships while the unary term learns salient boundaries. However, the two terms are tightly coupled in the non-local block, which hinders the learning of each. Based on these findings, we present the disentangled non-local block, where the two terms are decoupled to facilitate learning for both terms. We demonstrate the effectiveness of the decoupled design on various tasks, such as semantic segmentation on Cityscapes, ADE20K and PASCAL Context, object detection on COCO, and action recognition on Kinetics.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：非本地块是加强常规卷积神经网络的上下文建模能力的流行的模块。本文首先研究在深度非局部块，我们发现它的注意力计算可以分成两个方面，一个白化成对项占两个像素，并表示每个像素的显着性一元项之间的关系。我们还注意到，仅仅训练了两个词往往不同的视觉线索，例如模拟白化成对项获悉内区域的关系，而一元长期学习突出的边界。然而，这两个术语紧密耦合在非局部块，这妨碍各学习。基于这些发现，我们现在的解开的非本地块，其中两个术语分离，以便学习两个词。我们展示的各种任务，如城市景观，ADE20K和PASCAL上下文语义分割，目标检测的COCO，以及动作识别的动力学解耦设计的有效性。</font>
</div>


<hr>
<div id="paper15"> <b>15. VirTex: Learning Visual Representations from Textual Annotations</b>  <a href="https://arxiv.org/pdf/2006.06666" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Desai%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Karan Desai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Johnson%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Justin Johnson</a><br>
<font size="3">
 Abstract: The de-facto approach to many vision tasks is to start from pretrained visual representations, typically learned via supervised training on ImageNet. Recent methods have explored unsupervised pretraining to scale to vast quantities of unlabeled images. In contrast, we aim to learn high-quality visual representations from fewer images. To this end, we revisit supervised pretraining, and seek data-efficient alternatives to classification-based pretraining. We propose VirTex -- a pretraining approach using semantically dense captions to learn visual representations. We train convolutional networks from scratch on COCO Captions, and transfer them to downstream recognition tasks including image classification, object detection, and instance segmentation. On all tasks, VirTex yields features that match or exceed those learned on ImageNet -- supervised or unsupervised -- despite using up to ten times fewer images.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：事实上的方法很多视觉任务是从预训练的视觉表现，通常是通过对ImageNet监督培训学到启动。最近的方法已经探索监督的训练前向规模浩大的数量未标记的图像。相比之下，我们的目标是从较少的图像学习高品质的视觉表现。为此，我们重新审视监督训练前，并寻求数据有效替代基于分类的训练前。我们提出的Virtex  - 使用语义密集字幕学习视觉表现一个训练前的办法。我们培养卷积网络从COCO标题划伤，并将其传送到下游的识别任务，包括图像分类，目标检测和实例分割。在所有任务，VIRTEX收益率的特点是匹配或超过那些ImageNet了解到 - 监督或无人监督 - 尽管使用高达十倍较少的图像。</font>
</div>


<hr>
<div id="paper16"> <b>16. Exploring Weaknesses of VQA Models through Attribution Driven Insights</b>  <a href="https://arxiv.org/pdf/2006.06637" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Halbe%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shaunak Halbe</a><br>
<font size="3">
 Abstract: Deep Neural Networks have been successfully used for the task of Visual Question Answering for the past few years owing to the availability of relevant large scale datasets. However these datasets are created in artificial settings and rarely reflect the real world scenario. Recent research effectively applies these VQA models for answering visual questions for the blind. Despite achieving high accuracy these models appear to be susceptible to variation in input questions.We analyze popular VQA models through the lens of attribution (input's influence on predictions) to gain valuable insights. Further, We use these insights to craft adversarial attacks which inflict significant damage to these systems with negligible change in meaning of the input questions. We believe this will enhance development of systems more robust to the possible variations in inputs when deployed to assist the visually impaired.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深层神经网络已经被成功地用于可视化问题回答为由于相关大型数据集的可用性，在过去几年的任务。然而，这些数据集在人工设置创建，很少反映真实世界的场景。最近的研究有效地应用这些模型VQA回答视觉问题，为盲人。尽管实现高精度这些模型似乎容易受到输入变化questions.We通过归属（上预测输入的影响力），以获得宝贵的见解的镜头分析流行VQA车型。此外，我们利用这些资料来手艺这造成这些系统在输入问题，这意味着可以忽略不计的变化显著损害对抗性攻击。我们相信，出动协助视障人士的时候，这将提升系统更稳健的输入可能的变化发展。</font>
</div>


<hr>
<div id="paper17"> <b>17. Deep Differential System Stability -- Learning advanced computations  from examples</b>  <a href="https://arxiv.org/pdf/2006.06462" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Charton%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">François Charton</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hayat%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Amaury Hayat</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lample%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guillaume Lample</a><br>
<font size="3">
 Abstract: Can advanced mathematical computations be learned from examples? Using transformers over large generated datasets, we train models to learn properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect estimates of qualitative characteristics of the systems, and good approximations of numerical quantities, demonstrating that neural networks can learn advanced theorems and complex computations without built-in mathematical knowledge.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：CAN先进的数学计算从例子学到什么？使用变压器过大产生的数据集，我们训练模式学习微分系统，如局部稳定性，行为的无限性和可控性的特性。我们实现了系统的质量特征，以及数字量的很好的近似近乎完美的估计，表明神经网络可以学习先进的定理和复杂的计算，而不内置的数学知识。</font>
</div>


<hr>
<div id="paper18"> <b>18. Extracting and categorising the reactions to COVID-19 by the South  African public -- A social media study</b>  <a href="https://arxiv.org/pdf/2006.06336" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Marivate%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vukosi Marivate</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Moodley%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Avashlin Moodley</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Saba%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Athandiwe Saba</a><br>
<font size="3">
 Abstract: Social Media can be used to extract discussion topics during a disaster. With the COVID-19 pandemic impact on South Africa, we need to understand how the law and regulation promulgated by the government in response to the pandemic contrasts with discussion topics social media users have been engaging in. In this work, we expand on traditional media analysis by using Social Media discussions driven by or directed to South African government officials. We find topics that are similar as well as different in some cases. The findings can inform further study into social media during disaster settings in South Africa and beyond.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：社会化媒体可以用于灾难发生过程中提取的讨论主题。随着南非COVID-19大流行的影响，我们需要了解如何通过政府的响应与讨论主题的社交媒体用户中一直从事大流行对比颁布的法律，法规。在这项工作中，我们对传统媒体扩大通过使用社会化媒体的讨论通过推动或引导到南非政府官员分析。我们发现，在某些情况下，类似的还有不同的主题。这些发现可以在南非和超越灾难设置通知进一步研究社交媒体。</font>
</div>


<hr>
<div id="paper19"> <b>19. Mental Workload and Language Production in Non-Native Speaker IPA  Interaction</b>  <a href="https://arxiv.org/pdf/2006.06331" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunhan Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Edwards%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Justin Edwards</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cooney%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Orla Cooney</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bleakley%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anna Bleakley</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Doyle%2C+P+R" target="_blank" rel="noopener" style="color:#0000EE;">Philip R.Doyle</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Clark%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Leigh Clark</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rough%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniel Rough</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cowan%2C+B+R" target="_blank" rel="noopener" style="color:#0000EE;">Benjamin R. Cowan</a><br>
<font size="3">
 Abstract: Through proliferation on smartphones and smart speakers, intelligent personal assistants (IPAs) have made speech a common interaction modality. Yet, due to linguistic coverage and varying levels of functionality, many speakers engage with IPAs using a non-native language. This may impact the mental workload and pattern of language production displayed by non-native speakers. We present a mixed-design experiment, wherein native (L1) and non-native (L2) English speakers completed tasks with IPAs through smartphones and smart speakers. We found significantly higher mental workload for L2 speakers during IPA interactions. Contrary to our hypotheses, we found no significant differences between L1 and L2 speakers in terms of number of turns, lexical complexity, diversity, or lexical adaptation when encountering errors. These findings are discussed in relation to language production and processing load increases for L2 speakers in IPA interaction.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：通过对智能手机和智能扬声器扩散，智能个人助理（IPAS）已经取得讲话共同交互形态。然而，由于语言的覆盖范围和功能的不同级别，许多发言者从事与投资促进机构使用非母语语言。这可能会影响非母语的语言显示生产的脑力负荷和模式。我们提出了一个混合设计实验，其中天然（L1）和非本地（L2）英语的人通过智能手机和智能扬声器完成了投资促进机构的任务。我们在IPA相互作用发现L2音箱显著较高的心理负荷。相反，我们的假设，我们发现L1和L2音箱之间没有显著差异的匝数，词汇的复杂性，多样性，或词汇适应方面遇到错误时。这些发现相对于为L2扬声器在IPA交互语言的生产和加工负荷增大了讨论。</font>
</div>


<hr>
<div id="paper20"> <b>20. See what I'm saying? Comparing Intelligent Personal Assistant use for  Native and Non-Native Language Speakers</b>  <a href="https://arxiv.org/pdf/2006.06328" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunhan Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rough%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniel Rough</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bleakley%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anna Bleakley</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Edwards%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Justin Edwards</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cooney%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Orla Cooney</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Doyle%2C+P+R" target="_blank" rel="noopener" style="color:#0000EE;">Philip R. Doyle</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Clark%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Leigh Clark</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cowan%2C+B+R" target="_blank" rel="noopener" style="color:#0000EE;">Benjamin R. Cowan</a><br>
<font size="3">
 Abstract: Limited linguistic coverage for Intelligent Personal Assistants (IPAs) means that many interact in a non-native language. Yet we know little about how IPAs currently support or hinder these users. Through native (L1) and non-native (L2) English speakers interacting with Google Assistant on a smartphone and smart speaker, we aim to understand this more deeply. Interviews revealed that L2 speakers prioritised utterance planning around perceived linguistic limitations, as opposed to L1 speakers prioritising succinctness because of system limitations. L2 speakers see IPAs as insensitive to linguistic needs resulting in failed interaction. L2 speakers clearly preferred using smartphones, as visual feedback supported diagnoses of communication breakdowns whilst allowing time to process query results. Conversely, L1 speakers preferred smart speakers, with audio feedback being seen as sufficient. We discuss the need to tailor the IPA experience for L2 users, emphasising visual feedback whilst reducing the burden of language production.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：有限的语言覆盖智能个人助理（IPAS）意味着在非母语很多互动。然而，我们很少知道目前的投资促进机构如何支持或阻碍这些用户。通过本机（L1）和非本地（L2）英语的人与谷歌助理在智能手机和智能交互的音箱，我们的目标是更深刻地理解这一点。采访发现，L2音箱优先话语规划围绕感知语言的限制，相对于L1音箱由于系统限制，优先简洁。 L2音箱看到投资促进机构的不敏感，从而导致失败的交互语言需求。 L2扬声器使用智能手机清楚地优选的，因为视觉反馈支持的通信故障诊断的同时允许的时间来处理查询结果。相反地​​，L1扬声器优选智能扬声器，用音频反馈被视为足够的。我们讨论是否需要裁缝为L2用户体验IPA，强调视觉反馈，同时减少语言产生的负担。</font>
</div>


<hr>
<div id="paper21"> <b>21. Transparency in Language Generation: Levels of Automation</b>  <a href="https://arxiv.org/pdf/2006.06295" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Edwards%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Justin Edwards</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Perrone%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Allison Perrone</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Doyle%2C+P+R" target="_blank" rel="noopener" style="color:#0000EE;">Philip R. Doyle</a><br>
<font size="3">
 Abstract: Language models and conversational systems are growing increasingly advanced, creating outputs that may be mistaken for humans. Consumers may thus be misled by advertising, media reports, or vagueness regarding the role of automation in the production of language. We propose a taxonomy of language automation, based on the SAE levels of driving automation, to establish a shared set of terms for describing automated language. It is our hope that the proposed taxonomy can increase transparency in this rapidly advancing field.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：语言模型和对话系统变得越来越先进，创造产出可能被误认为是人类。消费者因此，可以通过广告，媒体的报道，或含糊关于自动化的生产语言的作用被误导。我们提出语言自动化的分类的基础上，推动自动化水平SAE，建立一个共享的一套术语用于描述自动化语言。我们希望，所提出的分类可以提高在这个快速发展的领域的透明度。</font>
</div>


<hr>
<div id="paper22"> <b>22. XiaoiceSing: A High-Quality and Integrated Singing Voice Synthesis  System</b>  <a href="https://arxiv.org/pdf/2006.06261" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Lu%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peiling Lu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Wu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jie Wu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Luan%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jian Luan</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Tan%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xu Tan</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhou%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Li Zhou</a><br>
<font size="3">
 Abstract: This paper presents XiaoiceSing, a high-quality singing voice synthesis system which employs an integrated network for spectrum, F0 and duration modeling. We follow the main architecture of FastSpeech while proposing some singing-specific design: 1) Besides phoneme ID and position encoding, features from musical score (e.g.note pitch and length) are also added. 2) To attenuate off-key issues, we add a residual connection in F0 prediction. 3) In addition to the duration loss of each phoneme, the duration of all the phonemes in a musical note is accumulated to calculate the syllable duration loss for rhythm enhancement. Experiment results show that XiaoiceSing outperforms the baseline system of convolutional neural networks by 1.44 MOS on sound quality, 1.18 on pronunciation accuracy and 1.38 on naturalness respectively. In two A/B tests, the proposed F0 and duration modeling methods achieve 97.3% and 84.3% preference rate over baseline respectively, which demonstrates the overwhelming advantages of XiaoiceSing.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文呈现XiaoiceSing，高品质的歌声合成系统，其采用一个集成的网络用于频谱，F0和持续时间的建模。我们遵循FastSpeech的主要架构，同时提出一些歌唱专用设计：1）除了音素ID和位置编码，从乐谱（e.g.note节距和长度特征）也被加入。 2）为了削弱关的关键问题，我们加入F0预测的剩余连接。 3）除了每个音素的时间损失，在音符的所有音素的持续时间累计计算节奏增强音节持续时间损失。实验结果表明，XiaoiceSing 1.44 MOS音质，分别为1.18发音的准确性和1.38上自然优于卷积神经网络的基线系统。在两个A / B的测试中，所提出的F0和持续时间的建模方法实现分别超过基线的偏好率97.3％和84.3％，这表明XiaoiceSing的压倒性优势。</font>
</div>


<hr>
<div id="paper23"> <b>23. Large-Scale Adversarial Training for Vision-and-Language Representation  Learning</b>  <a href="https://arxiv.org/pdf/2006.06195" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhe Gan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yen-Chun Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Linjie Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chen Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Cheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingjing Liu</a><br>
<font size="3">
 Abstract: We present VILLA, the first known effort on large-scale adversarial training for vision-and-language (V+L) representation learning. VILLA consists of two training stages: (i) task-agnostic adversarial pre-training; followed by (ii) task-specific adversarial finetuning. Instead of adding adversarial perturbations on image pixels and textual tokens, we propose to perform adversarial training in the embedding space of each modality. To enable large-scale training, we adopt the "free" adversarial training strategy, and combine it with KL-divergence-based regularization to promote higher invariance in the embedding space. We apply VILLA to current best-performing V+L models, and achieve new state of the art on a wide range of tasks, including Visual Question Answering, Visual Commonsense Reasoning, Image-Text Retrieval, Referring Expression Comprehension, Visual Entailment, and NLVR2.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们目前VILLA，对视力和语言（V + L）表示学习的大规模对抗训练的第一个已知的努力。别墅由两个训练阶段：（一）任务无关的对抗前的培训;接着（ⅱ）任务特异性对抗细化和微调。相反，在图像像素和文本标记加入敌对的扰动，我们建议在每个模式的嵌入空间进行对抗性训练。为了使大规模培训，我们采用了“免费”的对抗性训练策略，并与基于KL散度正规化结合起来，以促进嵌入空间较高的不变性。我们应用VILLA目前表现最好的V + L型，并在广泛的任务实现新的艺术状态，包括Visual答疑，视觉常识推理，图片，文本检索，参考表述的理解，视觉蕴涵和NLVR2 。</font>
</div>


<hr>
<div id="paper24"> <b>24. PeopleMap: Visualization Tool for Mapping Out Researchers using Natural  Language Processing</b>  <a href="https://arxiv.org/pdf/2006.06105" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Saad-Falcon%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jon Saad-Falcon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shaikh%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Omar Shaikh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z+J" target="_blank" rel="noopener" style="color:#0000EE;">Zijie J. Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wright%2C+A+P" target="_blank" rel="noopener" style="color:#0000EE;">Austin P. Wright</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Richardson%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sasha Richardson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chau%2C+D+H" target="_blank" rel="noopener" style="color:#0000EE;">Duen Horng Chau</a><br>
<font size="3">
 Abstract: Discovering research expertise at institutions can be a difficult task. Manually curated university directories easily become out of date and they often lack the information necessary for understanding a researcher's interests and past work, making it harder to explore the diversity of research at an institution and identify research talents. This results in lost opportunities for both internal and external entities to discover new connections and nurture research collaboration. To solve this problem, we have developed PeopleMap, the first interactive, open-source, web-based tool that visually "maps out" researchers based on their research interests and publications by leveraging embeddings generated by natural language processing (NLP) techniques. PeopleMap provides a new engaging way for institutions to summarize their research talents and for people to discover new connections. The platform is developed with ease-of-use and sustainability in mind. Using only researchers' Google Scholar profiles as input, PeopleMap can be readily adopted by any institution using its publicly-accessible repository and detailed documentation.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在机构发现研究的专业知识可以是一个艰巨的任务。手工辅助大学目录很容易成为过时的，他们往往缺乏必要的理解研究人员的兴趣和过去的工作，使其更难探索研究的多样性，在一个机构，并确定研究人才的信息。这导致失去机会，内部和外部的实体能发现新的连接和培育的研究合作。为了解决这个问题，我们已经开发PeopleMap，即第一个互动，开放源码的，基于Web的工具，在视觉上“映射出”，表示可以利用自然语言处理（NLP）技术生成的嵌入根据自己的研究兴趣和出版物的研究人员。 PeopleMap提供机构来概括他们的研究人才，并为人们发现新连接的新的吸引人的方式。该平台具有的易于使用的头脑和可持续性发展。只使用研究者谷歌学术的配置文件作为输入，PeopleMap可以很容易地通过其公开访问的存储库和详细的文档任何机构采纳。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>PROCJX
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://procjx.github.io/2020/06/12/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-06-12/" title="【arxiv论文】 Computation and Language 2020-06-12">https://procjx.github.io/2020/06/12/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-06-12/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">

        
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2020/06/11/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-06-11/" rel="next" title="【arxiv论文】 Computer Vision and Pattern Recognition 2020-06-11">
                  <i class="fa fa-chevron-left"></i> 【arxiv论文】 Computer Vision and Pattern Recognition 2020-06-11
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/2020/06/12/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-06-12/" rel="prev" title="【arxiv论文】 Computer Vision and Pattern Recognition 2020-06-12">
                  【arxiv论文】 Computer Vision and Pattern Recognition 2020-06-12 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>

        
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang -->
<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9197824246"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="gitalk-container"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#目录"><span class="nav-number">1.</span> <span class="nav-text">目录</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#摘要"><span class="nav-number">2.</span> <span class="nav-text">摘要</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="PROCJX"
    src="/images/procjx.png">
  <p class="site-author-name" itemprop="name">PROCJX</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">324</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">71</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/procjx" title="GitHub &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;procjx" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:procjx@gmail.com" title="E-Mail &amp;rarr; mailto:procjx@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>


<!--
      
        <script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
        <script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
        <div class="widget-wrap">
            <h3 class="widget-title">标签云</h3>
            <div id="myCanvasContainer" class="widget tagcloud">
                <canvas width="250" height="250" id="resCanvas" style="width=100%">
                    <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AAAI/" rel="tag">AAAI</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ACL/" rel="tag">ACL</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Accepted-Papers/" rel="tag">Accepted Papers</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ArXiv/" rel="tag">ArXiv</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/" rel="tag">BERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS20SI/" rel="tag">CS20SI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS224d/" rel="tag">CS224d</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/" rel="tag">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CVPR/" rel="tag">CVPR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Context/" rel="tag">Context</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cross-Lingual/" rel="tag">Cross Lingual</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dialog-System/" rel="tag">Dialog System</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse/" rel="tag">Discourse</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Ranking/" rel="tag">Discourse Ranking</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Structure/" rel="tag">Discourse Structure</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Document-NMT/" rel="tag">Document NMT</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EMNLP/" rel="tag">EMNLP</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Extractive/" rel="tag">Extractive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICLR/" rel="tag">ICLR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICML/" rel="tag">ICML</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IJCAI/" rel="tag">IJCAI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Inter-Sentence/" rel="tag">Inter-Sentence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Keyphrase-Generation/" rel="tag">Keyphrase Generation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NAACL/" rel="tag">NAACL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIPS/" rel="tag">NIPS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NMT/" rel="tag">NMT</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Relation-Extraction/" rel="tag">Neural Relation Extraction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RST/" rel="tag">RST</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Relation-Constraints/" rel="tag">Relation Constraints</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Summarization/" rel="tag">Summarization</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Translation/" rel="tag">Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Word-Translation/" rel="tag">Word Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/alias/" rel="tag">alias</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/git/" rel="tag">git</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pip/" rel="tag">pip</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/screen/" rel="tag">screen</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/" rel="tag">shell</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tgz/" rel="tag">tgz</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tts/" rel="tag">tts</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1/" rel="tag">冒泡</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/" rel="tag">冒泡排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%99%E4%BD%9C%E5%8A%A9%E6%89%8B/" rel="tag">写作助手</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8E%8B%E7%BC%A9/" rel="tag">压缩</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/" rel="tag">发送邮件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%88%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">合并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%8E%E5%8F%B0/" rel="tag">后台</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/" rel="tag">基数排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F/" rel="tag">希尔排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6/" rel="tag">归并</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">归并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/" rel="tag">快速排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F/" rel="tag">批量</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4/" rel="tag">批量删除</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F/" rel="tag">排序</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5/" rel="tag">插入</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/" rel="tag">插入排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%99%E7%A8%8B/" rel="tag">教程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/" rel="tag">斐波那契数列</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9D%80%E6%AD%BB%E8%BF%9B%E7%A8%8B/" rel="tag">杀死进程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B1%89%E8%AF%BA%E5%A1%94/" rel="tag">汉诺塔</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%A7%A3%E5%8E%8B/" rel="tag">解压</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%B0%B7%E6%AD%8C%E7%BF%BB%E8%AF%91/" rel="tag">谷歌翻译</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BF%AD%E4%BB%A3%E5%9B%9E%E7%BF%BB/" rel="tag">迭代回翻</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9/" rel="tag">选择</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/" rel="tag">选择排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%99%84%E4%BB%B6/" rel="tag">附件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3/" rel="tag">非监督</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%A2%86%E5%9F%9F%E9%80%82%E5%BA%94/" rel="tag">领域适应</a><span class="tag-list-count">1</span></li></ul>
                </canvas>
            </div>
        </div>
        
-->
        
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-hengfu -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9879871597"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-chuizhi -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="1662238719"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-zhengfangxing -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="6699421902"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PROCJX</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.4.2
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>












        
      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  <script src="/js/local-search.js"></script>













  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '2286ab64f5194d9d79ce',
      clientSecret: 'f912492bec2391664b40478f50f2f943376768d6',
      repo: 'procjx.github.io',
      owner: 'procjx',
      admin: ['procjx'],
      id: '4501ff0f01123cfc6ed445e284a2b35c',
        language: 'zh-CN',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
