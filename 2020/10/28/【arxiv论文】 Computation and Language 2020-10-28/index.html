<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/procjx.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/procjxfavicon32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/procjxfavicon16x16.ico">
  <link rel="mask-icon" href="/images/procjx.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.4.2',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

<!-- Google Adsense -->
<!--
<script async src="//pagead2.googlesyndication.com/
pagead/js/adsbygoogle.js"></script>
<script>
(adsbygoogle = window.adsbygoogle || []).push({
google_ad_client: "pub-1179774715076800",
enable_page_level_ads: true
});
</script>
-->

<script data-ad-client="ca-pub-1179774715076800" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>


<meta name="google-site-verification" content="cEiGwg0T8Rj5msmuEcGYZTh5nnf05EhCXy0gp2Ml5BI" />
<meta name="baidu-site-verification" content="noSKHe8MJs" />

  <meta name="description" content="目录  1. Fast Interleaved Bidirectional Sequence Generation [PDF] 摘要  2. It&amp;apos;s All in the Name: A Character Based Approach To Infer Religion [PDF] 摘要  3. Evaluating Gender Bias in Speech Translation [PDF">
<meta property="og:type" content="article">
<meta property="og:title" content="【arxiv论文】 Computation and Language 2020-10-28">
<meta property="og:url" content="https:&#x2F;&#x2F;procjx.github.io&#x2F;2020&#x2F;10&#x2F;28&#x2F;%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-10-28&#x2F;index.html">
<meta property="og:site_name" content="PROCJX&#39;s BLOGS">
<meta property="og:description" content="目录  1. Fast Interleaved Bidirectional Sequence Generation [PDF] 摘要  2. It&amp;apos;s All in the Name: A Character Based Approach To Infer Religion [PDF] 摘要  3. Evaluating Gender Bias in Speech Translation [PDF">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https:&#x2F;&#x2F;procjx.github.io&#x2F;images&#x2F;cl-2020-10-28.jpg">
<meta property="og:updated_time" content="2020-10-28T12:07:42.390Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;procjx.github.io&#x2F;images&#x2F;cl-2020-10-28.jpg">

<link rel="canonical" href="https://procjx.github.io/2020/10/28/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-10-28/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>【arxiv论文】 Computation and Language 2020-10-28 | PROCJX's BLOGS</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">PROCJX's BLOGS</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">WITH LOVE OF WORLD</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-fw fa-download"></i>资源</a>

  </li>
        <li class="menu-item menu-item-arxiv">

    <a href="/arxiv/" rel="section"><i class="fa fa-fw fa-file-pdf-o"></i>arxiv论文</a>

  </li>
        <li class="menu-item menu-item-deadline">

    <a href="/deadline/" rel="section"><i class="fa fa-fw fa-calendar"></i>会议截稿</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://procjx.github.io/2020/10/28/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-10-28/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/procjx.png">
      <meta itemprop="name" content="PROCJX">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PROCJX's BLOGS">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          【arxiv论文】 Computation and Language 2020-10-28
        </h2>

        <div class="post-meta">
        
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-10-28 11:33:19 / 修改时间：20:07:42" itemprop="dateCreated datePublished" datetime="2020-10-28T11:33:19+08:00">2020-10-28</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/arxiv/" itemprop="url" rel="index">
                    <span itemprop="name">arxiv</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/arxiv/CL/" itemprop="url" rel="index">
                    <span itemprop="name">CL</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
              <span>3.8k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
              <span>6 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="/images/cl-2020-10-28.jpg" alt></p><h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Fast Interleaved Bidirectional Sequence Generation <a href="https://arxiv.org/pdf/2010.14481" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> It's All in the Name: A Character Based Approach To Infer Religion <a href="https://arxiv.org/pdf/2010.14479" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Evaluating Gender Bias in Speech Translation <a href="https://arxiv.org/pdf/2010.14465" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Discovering and Interpreting Conceptual Biases in Online Communities <a href="https://arxiv.org/pdf/2010.14448" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Differentiable Open-Ended Commonsense Reasoning <a href="https://arxiv.org/pdf/2010.14439" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Listener's Social Identity Matters in Personalised Response Generation <a href="https://arxiv.org/pdf/2010.14342" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Multitask Training with Text Data for End-to-End Speech Recognition <a href="https://arxiv.org/pdf/2010.14318" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Cross-lingual Machine Reading Comprehension with Language Branch  Knowledge Distillation <a href="https://arxiv.org/pdf/2010.14271" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Improving Reinforcement Learning for Neural Relation Extraction with  Hierarchical Memory Extractor <a href="https://arxiv.org/pdf/2010.14255" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Multi-XScience: A Large-scale Dataset for Extreme Multi-document  Summarization of Scientific Articles <a href="https://arxiv.org/pdf/2010.14235" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Global Sentiment Analysis Of COVID-19 Tweets Over Time <a href="https://arxiv.org/pdf/2010.14234" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Event Detection: Gate Diversity and Syntactic Importance Scoresfor Graph  Convolution Neural Networks <a href="https://arxiv.org/pdf/2010.14123" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Emotion recognition by fusing time synchronous and time asynchronous  representations <a href="https://arxiv.org/pdf/2010.14102" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Multi-Domain Dialogue State Tracking -- A Purely Transformer-Based  Generative Approach <a href="https://arxiv.org/pdf/2010.14061" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> To BERT or Not to BERT: Comparing Task-specific and Task-agnostic  Semi-Supervised Approaches for Sequence Tagging <a href="https://arxiv.org/pdf/2010.14042" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Volctrans Parallel Corpus Filtering System for WMT 2020 <a href="https://arxiv.org/pdf/2010.14029" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Speech SIMCLR: Combining Contrastive and Reconstruction Objective for  Self-supervised Speech Representation Learning <a href="https://arxiv.org/pdf/2010.13991" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Interpretation of NLP models through input marginalization <a href="https://arxiv.org/pdf/2010.13984" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Predict and Use Latent Patterns for Short-Text Conversation <a href="https://arxiv.org/pdf/2010.13982" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Reading Between the Lines: Exploring Infilling in Visual Narratives <a href="https://arxiv.org/pdf/2010.13944" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Improving Limited Labeled Dialogue State Tracking with Self-Supervision <a href="https://arxiv.org/pdf/2010.13920" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Probing Task-Oriented Dialogue Representation from Language Models <a href="https://arxiv.org/pdf/2010.13912" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Improved Neural Language Model Fusion for Streaming Recurrent Neural  Network Transducer <a href="https://arxiv.org/pdf/2010.13878" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Word Frequency Does Not Predict Grammatical Knowledge in Language Models <a href="https://arxiv.org/pdf/2010.13870" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Data Troubles in Sentence Level Confidence Estimation for Machine  Translation <a href="https://arxiv.org/pdf/2010.13856" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> Semi-Supervised Spoken Language Understanding via Self-Supervised Speech  and Language Model Pretraining <a href="https://arxiv.org/pdf/2010.13826" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> PowerTransformer: Unsupervised Controllable Revision for Biased Language  Correction <a href="https://arxiv.org/pdf/2010.13816" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> Is it Great or Terrible? Preserving Sentiment in Neural Machine  Translation of Arabic Reviews <a href="https://arxiv.org/pdf/2010.13814" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Dynamic Boundary Time Warping for Sub-sequence Matching with Few  Examples <a href="https://arxiv.org/pdf/2010.14464" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> Align-Refine: Non-Autoregressive Speech Recognition via Iterative  Realignment <a href="https://arxiv.org/pdf/2010.14233" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> Co-attentional Transformers for Story-Based Video Understanding <a href="https://arxiv.org/pdf/2010.14104" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> VisualHints: A Visual-Lingual Environment for Multimodal Reinforcement  Learning <a href="https://arxiv.org/pdf/2010.13839" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>



<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Fast Interleaved Bidirectional Sequence Generation</b>  <a href="https://arxiv.org/pdf/2010.14481" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Biao Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Titov%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ivan Titov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sennrich%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rico Sennrich</a><br>
<font size="3">
 Abstract: Independence assumptions during sequence generation can speed up inference, but parallel generation of highly inter-dependent tokens comes at a cost in quality. Instead of assuming independence between neighbouring tokens (semi-autoregressive decoding, SA), we take inspiration from bidirectional sequence generation and introduce a decoder that generates target words from the left-to-right and right-to-left directions simultaneously. We show that we can easily convert a standard architecture for unidirectional decoding into a bidirectional decoder by simply interleaving the two directions and adapting the word positions and self-attention masks. Our interleaved bidirectional decoder (IBDecoder) retains the model simplicity and training efficiency of the standard Transformer, and on five machine translation tasks and two document summarization tasks, achieves a decoding speedup of ~2X compared to autoregressive decoding with comparable quality. Notably, it outperforms left-to-right SA because the independence assumptions in IBDecoder are more felicitous. To achieve even higher speedups, we explore hybrid models where we either simultaneously predict multiple neighbouring tokens per direction, or perform multi-directional decoding by partitioning the target sequence. These methods achieve speedups to 4X-11X across different tasks at the cost of <1 bleu or <0.5 rouge (on average). source code is released at this https url. < font>
<br>
<font size="2" style="line-height:30px;">
摘要：序列生成过程中独立性的假设可以加快推论，但并行生成高度相互依存的令牌来在质量成本。代替相邻标记（半自回归解码，SA）之间假定的独立性，我们从双向序列生成的灵感和介绍，从生成目标字的解码器中的左到右和从右到左的方向上同时。我们表明，我们可以很容易地转换成一个标准的架构，单向解码成双向解码器通过简单地交织两个方向和调整字位置和自我关注口罩。我们的交织的双向解码器（IBDecoder）保留了标准变压器的模型简化和训练的效率，并在五个机器翻译任务和两个文档文摘任务，达到〜2X的解码加速相比自回归与可比质量进行解码。值得注意的是，它优于左到右SA因为IBDecoder独立性假设更为恰当。为了达到甚至更高的加速比，我们探索混合模型，我们同时地预测每个方向多个相邻的令牌，或者通过分割与靶序列进行多向解码。这些方法在<1 bleu的成本或<0.5 rouge（平均）实现跨越不同的任务的加速到4x-11x。源代码在此https url释放。< font>
</1></font></1></font></div>


<hr>
<div id="paper2"> <b>2. It's All in the Name: A Character Based Approach To Infer Religion</b>  <a href="https://arxiv.org/pdf/2010.14479" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chaturvedi%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rochana Chaturvedi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chaturvedi%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sugat Chaturvedi</a><br>
<font size="3">
 Abstract: Demographic inference from text has received a surge of attention in the field of natural language processing in the last decade. In this paper, we use personal names to infer religion in South Asia - where religion is a salient social division, and yet, disaggregated data on it remains scarce. Existing work predicts religion using dictionary based method, and therefore, can not classify unseen names. We use character based models which learn character patterns and, therefore, can classify unseen names as well with high accuracy. These models are also much faster and can easily be scaled to large data sets. We improve our classifier by combining the name of an individual with that of their parent/spouse and achieve remarkably high accuracy. Finally, we trace the classification decisions of a convolutional neural network model using layer-wise relevance propagation which can explain the predictions of complex non-linear classifiers and circumvent their purported black box nature. We show how character patterns learned by the classifier are rooted in the linguistic origins of names.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：从文本人口推断已收到在过去十年的关注，自然语言处理领域的激增。在本文中，我们使用人名南亚推断宗教 - 这里的宗教是一个突出的社会分工，然而，分列数据上它仍然是稀缺的。现有的工作预测使用基于字典的方法宗教，因此，看不见的名称不能进行分类。我们使用基于角色模型，其学习字符图案，因此，可以看不见的名称，以及高精度分类。这些模型也更快，可以很容易地扩展到大型数据集。我们通过一个人的名字与他们的父母/配偶的结合提高我们的分类，达到非常高的精度。最后，我们跟踪使用逐层传播的相关性可以解释复杂的非线性分类的预测和规避他们的本意是黑盒性质卷积神经网络模型的分类决定。我们展示了如何通过分类学字符模式植根于地名的语言起源。</font>
</div>


<hr>
<div id="paper3"> <b>3. Evaluating Gender Bias in Speech Translation</b>  <a href="https://arxiv.org/pdf/2010.14465" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Costa-juss%C3%A0%2C+M+R" target="_blank" rel="noopener" style="color:#0000EE;">Marta R. Costa-jussà</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Basta%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christine Basta</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=G%C3%A1llego%2C+G+I" target="_blank" rel="noopener" style="color:#0000EE;">Gerard I. Gállego</a><br>
<font size="3">
 Abstract: The scientific community is more and more aware of the necessity to embrace pluralism and consistently represent major and minor social groups. In this direction, there is an urgent need to provide evaluation sets and protocols to measure existing biases in our automatic systems. This paper introduces WinoST, a new freely available challenge set for evaluating gender bias in speech translation. WinoST is the speech version of WinoMT which is an MT challenge set and both follow an evaluation protocol to measure gender accuracy. Using a state-of-the-art end-to-end speech translation system, we report the gender bias evaluation on 4 language pairs, and we show that gender accuracy in speech translation is more than 23% lower than in MT.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：科学界越来越意识到有必要拥抱多元化，始终代表主要和次要的社会群体。在这个方向，迫切需要提供评估组和协议来衡量我们的自动系统存在偏见。本文介绍WinoST，在语音翻译评估性别偏见，一个新的免费提供的挑战集。 WinoST是WinoMT的语音版本，这是一个挑战，MT组和都遵循一个评价协议来衡量性别准确性。用一个国家的最先进的终端到终端的语音翻译系统，我们报告的4种语言对性别偏见的评价，我们显示了语音翻译，性别精确度超过23％，比MT降低。</font>
</div>


<hr>
<div id="paper4"> <b>4. Discovering and Interpreting Conceptual Biases in Online Communities</b>  <a href="https://arxiv.org/pdf/2010.14448" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ferrer-Aran%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xavier Ferrer-Aran</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=van+Nuenen%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tom van Nuenen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Criado%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Natalia Criado</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Such%2C+J+M" target="_blank" rel="noopener" style="color:#0000EE;">Jose M. Such</a><br>
<font size="3">
 Abstract: Language carries implicit human biases, functioning both as a reflection and a perpetuation of stereotypes that people carry with them. Recently, ML-based NLP methods such as word embeddings have been shown to learn such language biases with striking accuracy. This capability of word embeddings has been successfully exploited as a tool to quantify and study human biases. However, previous studies only consider a predefined set of conceptual biases to attest (e.g., whether gender is more or less associated with particular jobs), or just discover biased words without helping to understand their meaning at the conceptual level. As such, these approaches are either unable to find conceptual biases that have not been defined in advance, or the biases they find are difficult to interpret and study. This makes existing approaches unsuitable to discover and interpret biases in online communities, as such communities may carry different biases than those in mainstream culture. This paper proposes a general, data-driven approach to automatically discover and help interpret conceptual biases encoded in word embeddings. We apply this approach to study the conceptual biases present in the language used in online communities and experimentally show the validity and stability of our method.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：语言承载隐人的偏见，既充当一个反射和成见的延续，人们随身携带。近来，已经展示了基于ML-NLP方法，如字的嵌入学会与惊人的准确性这样的语言偏见。字的嵌入的这种能力已被成功利用，以量化和研究人类偏见的工具。然而，以往的研究只考虑一组预定义的概念偏见来证明（例如，性别是否与特定的工作或多或少的关联），或者只是发现偏见的话，而不帮助理解在概念上它们的含义。因此，这些方法要么无法找到事先没有被定义概念的偏见，或者他们发现偏见难以解释和研究。这使得现有的方法不适合于发现和在线社区解释偏见，因为这些社区可以携带不同的偏见比在主流文化。本文提出了一种通用的，数据驱动的方法来自动发现和帮助解释字的嵌入编码的概念的偏见。我们将这种方法用于研究概念的偏见存在于网络社区所使用的语言，并通过实验证明了该方法的有效性和稳定性。</font>
</div>


<hr>
<div id="paper5"> <b>5. Differentiable Open-Ended Commonsense Reasoning</b>  <a href="https://arxiv.org/pdf/2010.14439" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+B+Y" target="_blank" rel="noopener" style="color:#0000EE;">Bill Yuchen Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haitian Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dhingra%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bhuwan Dhingra</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zaheer%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Manzil Zaheer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiang Ren</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cohen%2C+W+W" target="_blank" rel="noopener" style="color:#0000EE;">William W. Cohen</a><br>
<font size="3">
 Abstract: Current commonsense reasoning research mainly focuses on developing models that use commonsense knowledge to answer multiple-choice questions. However, systems designed to answer multiple-choice questions may not be useful in applications that do not provide a small list of possible candidate answers to choose from. As a step towards making commonsense reasoning research more realistic, we propose to study open-ended commonsense reasoning (OpenCSR) - the task of answering a commonsense question without any pre-defined choices, using as a resource only a corpus of commonsense facts written in natural language. The task is challenging due to a much larger decision space, and because many commonsense questions require multi-hop reasoning. We propose an efficient differentiable model for multi-hop reasoning over knowledge facts, named DrFact. We evaluate our approach on a collection of re-formatted, open-ended versions of popular tests targeting commonsense reasoning, and show that our approach outperforms strong baseline methods by a large margin.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：目前常识推理研究主要集中在开发模式，使用常识知识解答选择题。然而，旨在解答选择题的系统可能无法在不提供可能的候选答案可以选择一个小单子应用。作为致力于使常识推理的研究更切合实际的步骤，我们建议研究开放式的常识推理（OpenCSR） - 回答常识性的问题，没有任何预先定义的选项，使用作为一种资源只写在常识性的事实的语料库的任务自然语言。任务是由于更大的决策空间有挑战性，因为许多常识性的问题，需要多跳推理。我们提出了多跳推理对知识的事实，高效的微模型，命名DrFact。我们评估我们在重新格式化，开放式版本的流行的测试目标常识推理的征收方式，并表明我们的方法优于大幅度强基线的方法。</font>
</div>


<hr>
<div id="paper6"> <b>6. Listener's Social Identity Matters in Personalised Response Generation</b>  <a href="https://arxiv.org/pdf/2010.14342" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guanyi Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yinhe Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yupei Du</a><br>
<font size="3">
 Abstract: Personalised response generation enables generating human-like responses by means of assigning the generator a social identity. However, pragmatics theory suggests that human beings adjust the way of speaking based on not only who they are but also whom they are talking to. In other words, when modelling personalised dialogues, it might be favourable if we also take the listener's social identity into consideration. To validate this idea, we use gender as a typical example of a social variable to investigate how the listener's identity influences the language used in Chinese dialogues on social media. Also, we build personalised generators. The experiment results demonstrate that the listener's identity indeed matters in the language use of responses and that the response generator can capture such differences in language use. More interestingly, by additionally modelling the listener's identity, the personalised response generator performs better in its own identity.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：个性化响应产生能够产生人样通过分配所述发电机的社交身份的手段的响应。然而，语用学理论认为，人类调整上讲基础上，他们不仅是谁的方式也为之他们聊天。换句话说，造型个性化的对话的时候，它可能是有利的，如果我们也把听者的社会身份考虑。为了验证这个想法，我们用性别作为一个社会变量的一个典型例子，调查听众的身份如何影响中国的对话中使用社交媒体的语言。此外，我们建立个性化的发电机。实验结果表明，听者的身份确实是在语言使用的反应和响应生成器可以捕捉语言运用这种差异很重要。更有趣的是，通过额外模拟听者的身份，个性化反应生成在执行自己的身份更好。</font>
</div>


<hr>
<div id="paper7"> <b>7. Multitask Training with Text Data for End-to-End Speech Recognition</b>  <a href="https://arxiv.org/pdf/2010.14318" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peidong Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sainath%2C+T+N" target="_blank" rel="noopener" style="color:#0000EE;">Tara N. Sainath</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Weiss%2C+R+J" target="_blank" rel="noopener" style="color:#0000EE;">Ron J. Weiss</a><br>
<font size="3">
 Abstract: We propose a multitask training method for attention-based end-to-end speech recognition models to better incorporate language level information. We regularize the decoder in a sequence-to-sequence architecture by multitask training it on both the speech recognition task and a next-token prediction language modeling task. Trained on either the 100 hour subset of LibriSpeech or the full 960 hour dataset, the proposed method leads to an 11% relative performance improvement over the baseline and is comparable to language model shallow fusion, without requiring an additional neural network during decoding. Analyses of sample output sentences and the word error rate on rare words demonstrate that the proposed method can incorporate language level information effectively.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了基于注意机制的端至端的语音识别模式，以更好地将语言水平信息的多任务训练方法。我们通过多任务的正规化解码器在一个序列到序列架构训练它的语音识别任务，下一个令牌预测语言建模任务两者。受过训练的在任LibriSpeech的100小时子集或全部960小时数据集，所提出的方法导致超过基线11％的相对性能的改进和相当于语言模型浅融合，而不解码期间需要额外的神经网络。样本输出语句和生僻字的字错误率的分析表明，该方法能有效地将语言水平的信息。</font>
</div>


<hr>
<div id="paper8"> <b>8. Cross-lingual Machine Reading Comprehension with Language Branch  Knowledge Distillation</b>  <a href="https://arxiv.org/pdf/2010.14271" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junhao Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shou%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Linjun Shou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pei%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jian Pei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming Gong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Min Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daxin Jiang</a><br>
<font size="3">
 Abstract: Cross-lingual Machine Reading Comprehension (CLMRC) remains a challenging problem due to the lack of large-scale annotated datasets in low-source languages, such as Arabic, Hindi, and Vietnamese. Many previous approaches use translation data by translating from a rich-source language, such as English, to low-source languages as auxiliary supervision. However, how to effectively leverage translation data and reduce the impact of noise introduced by translation remains onerous. In this paper, we tackle this challenge and enhance the cross-lingual transferring performance by a novel augmentation approach named Language Branch Machine Reading Comprehension (LBMRC). A language branch is a group of passages in one single language paired with questions in all target languages. We train multiple machine reading comprehension (MRC) models proficient in individual language based on LBMRC. Then, we devise a multilingual distillation approach to amalgamate knowledge from multiple language branch models to a single model for all target languages. Combining the LBMRC and multilingual distillation can be more robust to the data noises, therefore, improving the model's cross-lingual ability. Meanwhile, the produced single multilingual model is applicable to all target languages, which saves the cost of training, inference, and maintenance for multiple models. Extensive experiments on two CLMRC benchmarks clearly show the effectiveness of our proposed method.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：跨语种机器阅读理解（CLMRC）仍然是一个具有挑战性的问题，由于在低源语言，如阿拉伯语，印地文和越南缺乏大型注释的数据集。许多以前的方法通过翻译用翻译的数据从一个丰富的源语言，如英语，低源语言作为辅助监督。然而，如何有效地利用翻译数据，并降低噪音推出的由翻译的影响依然繁重。在本文中，我们应对这种挑战，增强名为语支机阅读理解（LBMRC）一种新型的增强方法的跨语言传输性能。语言分支是一组中的所有目标语言问题配对在一个单一的语言通道。我们训练多机阅读理解（MRC）模型的基础上LBMRC个人的语言精通。然后，我们设计一个多语种的蒸馏方法从多语言分支模型合并将知识，为所有目标语言的单一模式。组合LBMRC和多种语言蒸馏可以更健壮的数据的噪声，因此，改进了模型的跨语种能力。同时，生产单一的多语言模型适用于所有目标语言，从而节省了培训，推理和维护多个型号的成本。两个CLMRC基准大量的实验清楚地表明我们提出的方法的有效性。</font>
</div>


<hr>
<div id="paper9"> <b>9. Improving Reinforcement Learning for Neural Relation Extraction with  Hierarchical Memory Extractor</b>  <a href="https://arxiv.org/pdf/2010.14255" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianing Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Su%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chong Su</a><br>
<font size="3">
 Abstract: Distant supervision relation extraction (DSRE) is an efficient method to extract semantic relations on a large-scale heuristic labeling corpus. However, it usually brings in a massive noisy data. In order to alleviate this problem, many recent approaches adopt reinforcement learning (RL), which aims to select correct data autonomously before relation classification. Although these RL methods outperform conventional multi-instance learning-based methods, there are still two neglected problems: 1) the existing RL methods ignore the feedback of noisy data, 2) the reduction of training corpus exacerbates long-tail problem. In this paper, we propose a novel framework to solve the two problems mentioned above. Firstly, we design a novel reward function to obtain feedback from both correct and noisy data. In addition, we use implicit relations information to improve RL. Secondly, we propose the hierarchical memory extractor (HME), which utilizes the gating mechanism to share the semantics from correlative instances between data-rich and data-poor classes. Moreover, we define a hierarchical weighted ranking loss function to implement top-down search processing. Extensive experiments conducted on the widely used NYT dataset show significant improvement over state-of-the-art baseline methods.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：远程监管关系抽取（DSRE）是一种有效的方法以提取关于大规模启发式标记语料库的语义关系。然而，它通常带来了一个巨大的噪声数据。为了缓解这一问题，最近的许多方法采用强化学习（RL），目的是关系分类之前，自主地选择正确的数据，。虽然这些方法RL优于传统的多实例基于学习的方法，还有两个被忽视的问题：1）现有的RL方法忽略噪声数据的反馈，2）训练语料库加剧长尾问题的减少。在本文中，我们提出了一个新的框架，以解决上述两个问题。首先，我们设计了一个新的回报函数，从正确和噪声的数据获得反馈。此外，我们使用的隐含关系的信息，以提高RL。其次，我们提出了分层存储提取（HME），它利用门控机制，以分享数据丰富，数据贫乏类之间关联实例的语义。此外，我们定义了一个分层加权排序损失函数来实现自上而下的搜索处理。广泛使用的数据集纽约时报进行了大量的实验表明，在国家的最先进的基线方法显著改善。</font>
</div>


<hr>
<div id="paper10"> <b>10. Multi-XScience: A Large-scale Dataset for Extreme Multi-document  Summarization of Scientific Articles</b>  <a href="https://arxiv.org/pdf/2010.14235" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yao Lu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yue Dong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Charlin%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Laurent Charlin</a><br>
<font size="3">
 Abstract: Multi-document summarization is a challenging task for which there exists little large-scale datasets. We propose Multi-XScience, a large-scale multi-document summarization dataset created from scientific articles. Multi-XScience introduces a challenging multi-document summarization task: writing the related-work section of a paper based on its abstract and the articles it references. Our work is inspired by extreme summarization, a dataset construction protocol that favours abstractive modeling approaches. Descriptive statistics and empirical results---using several state-of-the-art models trained on the Multi-XScience dataset---reveal that Multi-XScience is well suited for abstractive models.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：多文档文摘是其中存在小的大型数据集的一个具有挑战性的任务。我们建议多XScience，从科学文章创造了一个大型的多文档文摘数据集。多XScience引入了一个具有挑战性的多文档文摘任务：编写基于它的抽象和文章引用了一个文件的相关作业区间。我们的工作是由极端的概括的启发，有利于抽象建模的数据集构建方案接近。使用的培训上多XScience数据集的几个国家的最先进的机型描述性统计和实证研究结果---表明，多XScience非常适合抽象模型。</font>
</div>


<hr>
<div id="paper11"> <b>11. Global Sentiment Analysis Of COVID-19 Tweets Over Time</b>  <a href="https://arxiv.org/pdf/2010.14234" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Mansoor%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Muvazima Mansoor</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gurumurthy%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kirthika Gurumurthy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=U%2C+A+R" target="_blank" rel="noopener" style="color:#0000EE;">Anantharam R U</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Prasad%2C+V+R+B" target="_blank" rel="noopener" style="color:#0000EE;">V R Badri Prasad</a><br>
<font size="3">
 Abstract: The Coronavirus pandemic has affected the normal course of life. People around the world have taken to social media to express their opinions and general emotions regarding this phenomenon that has taken over the world by storm. The social networking site, Twitter showed an unprecedented increase in tweets related to the novel Coronavirus in a very short span of time. This paper presents the global sentiment analysis of tweets related to Coronavirus and how the sentiment of people in different countries has changed over time. Furthermore, to determine the impact of Coronavirus on daily aspects of life, tweets related to Work From Home (WFH) and Online Learning were scraped and the change in sentiment over time was observed. In addition, various Machine Learning models such as Long Short Term Memory (LSTM) and Artificial Neural Networks (ANN) were implemented for sentiment classification and their accuracies were determined. Exploratory data analysis was also performed for a dataset providing information about the number of confirmed cases on a per-day basis in a few of the worst-hit countries to provide a comparison between the change in sentiment with the change in cases since the start of this pandemic till June 2020.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：冠状病毒大流行已经影响到了正常的生活过程。世界各地的人们所采取的社会化媒体来表达自己的意见和情绪普遍对此现象已经席卷了全世界。社交网站，微博显示，在一段很短的跨度有关新型冠状病毒的鸣叫了前所未有的提高。本文列出了与冠状病毒，以及如何的人在不同的国家的情绪发生了变化随着时间的推移鸣叫的全球情感分析。此外，以确定冠状病毒对日常的生活方面的影响，涉及到在家工作（WFH）和在线学习鸣叫刮掉，观察一段时间内情绪的变化。此外，各种机器学习模型，如长短期记忆（LSTM）和人工神经网络（ANN）分别实施了情感分类并确定其精确度。也为数据集在几个重灾区国家提供在每一天的基础上对确诊病例数的信息进行探索性数据分析，以提供自从开始在情绪与变化的情况下的变化之间的比较这一流行病直到2020年6月。</font>
</div>


<hr>
<div id="paper12"> <b>12. Event Detection: Gate Diversity and Syntactic Importance Scoresfor Graph  Convolution Neural Networks</b>  <a href="https://arxiv.org/pdf/2010.14123" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+V+D" target="_blank" rel="noopener" style="color:#0000EE;">Viet Dac Lai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+T+N" target="_blank" rel="noopener" style="color:#0000EE;">Tuan Ngo Nguyen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+T+H" target="_blank" rel="noopener" style="color:#0000EE;">Thien Huu Nguyen</a><br>
<font size="3">
 Abstract: Recent studies on event detection (ED) haveshown that the syntactic dependency graph canbe employed in graph convolution neural net-works (GCN) to achieve state-of-the-art per-formance. However, the computation of thehidden vectors in such graph-based models isagnostic to the trigger candidate words, po-tentially leaving irrelevant information for thetrigger candidate for event prediction. In addi-tion, the current models for ED fail to exploitthe overall contextual importance scores of thewords, which can be obtained via the depen-dency tree, to boost the performance. In thisstudy, we propose a novel gating mechanismto filter noisy information in the hidden vec-tors of the GCN models for ED based on theinformation from the trigger candidate. Wealso introduce novel mechanisms to achievethe contextual diversity for the gates and theimportance score consistency for the graphsand models in ED. The experiments show thatthe proposed model achieves state-of-the-artperformance on two ED datasets   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：事件检测最近的研究（ED）haveshown的语法结构图热点可以用采用图形卷积神经网工程（GCN），以实现国家的最先进的每formance。然而，在这种基于图形的模型isagnostic于触发候选词thehidden矢量的计算，PO-tentially离开thetrigger候选事件预测无关信息。在ADDI-重刑，目前型号为ED不能thewords的exploitthe整体的上下文重要性得分，这可以通过依赖新生 -  dency树获得，以提高性能。在thisstudy，我们提出了基于从触发候选人theinformation对于ED的GCN车型的隐藏VEC-职责范围的新颖门mechanismto过滤嘈杂的信息。 Wealso引入新的机制，为门和theimportance评分一致性在ED的graphsand车型achievethe背景的多样性。实验结果表明thatthe提出的模型实现了两个ED数据集的国家的最artperformance</font>
</div>


<hr>
<div id="paper13"> <b>13. Emotion recognition by fusing time synchronous and time asynchronous  representations</b>  <a href="https://arxiv.org/pdf/2010.14102" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wen Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chao Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Woodland%2C+P+C" target="_blank" rel="noopener" style="color:#0000EE;">Philip C. Woodland</a><br>
<font size="3">
 Abstract: In this paper, a novel two-branch neural network model structure is proposed for multimodal emotion recognition, which consists of a time synchronous branch (TSB) and a time asynchronous branch (TAB). To capture correlations between each word and its acoustic realisation, the TSB combines speech and text modalities at each input window frame and then does pooling across time to form a single embedding vector. The TAB, by contrast, provides cross-utterance information by integrating sentence text embeddings from a number of context utterances into another embedding vector. The final emotion classification uses both the TSB and the TAB embeddings. Experimental results on the IEMOCAP dataset demonstrate that the two-branch structure achieves state-of-the-art results in 4-way classification with all common test setups. When using automatic speech recognition (ASR) output instead of manually transcribed reference text, it is shown that the cross-utterance information considerably improves the robustness against ASR errors. Furthermore, by incorporating an extra class for all the other emotions, the final 5-way classification system with ASR hypotheses can be viewed as a prototype for more realistic emotion recognition systems.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，一种新颖的两分支的神经网络模型的结构提出了一种用于多模态情感识别，其由时间同步分支（TSB）和时间异步分支（TAB）的。每个单词和其声之间实现相关性的捕获，所述TSB结合语音和文本模式在每个输入窗框，然后不跨越时间汇集以形成单个嵌入矢量。的TAB，相比之下，通过从多个上下文话语句整合的嵌入文本到另一个嵌入矢量提供跨发声信息。最终的情感类别同时使用TSB和TAB的嵌入。在IEMOCAP实验结果数据集表明，两分支结构实现状态的最先进的结果在4路分类与所有普通的测试设置。当使用自动语音识别（ASR）输出，而不是手动转录参考文本中，示出的是跨发声信息显着地改善对ASR的错误的鲁棒性。此外，通过将所有其他的情绪一个额外的类，具有ASR假设最后的5路分类系统可以被看作是一个原型更真实的情感识别系统。</font>
</div>


<hr>
<div id="paper14"> <b>14. Multi-Domain Dialogue State Tracking -- A Purely Transformer-Based  Generative Approach</b>  <a href="https://arxiv.org/pdf/2010.14061" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yan Zeng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nie%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jian-Yun Nie</a><br>
<font size="3">
 Abstract: We investigate the problem of multi-domain Dialogue State Tracking (DST) with open vocabulary. Existing approaches exploit BERT encoder and copy-based RNN decoder, where the encoder first predicts the state operation, and then the decoder generates new slot values. However, in this stacked encoder-decoder structure, the operation prediction objective only affects the BERT encoder and the value generation objective mainly affects the RNN decoder. In this paper, we propose a purely Transformer-based framework that uses BERT as both encoder and decoder. In so doing, the operation prediction objective and the value generation objective can jointly optimize our model for DST. At the decoding step, we re-use the hidden states of the encoder in the self-attention mechanism of the corresponding decoder layer to construct a flat model structure for effective parameter updating. Experimental results show that our approach substantially outperforms the existing state-of-the-art framework, and it also achieves very competitive performance to the best ontology-based approaches.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们调查多域对话状态跟踪（DST）的开放词汇的问题。现有的方法利用BERT编码器和基于复制RNN解码器，其中编码器第一预测的状态的操作，然后将解码器产生新的槽值。然而，在该堆叠的编码器 - 解码器的结构，操作预测目标仅影响BERT编码器和值生成目标主要影响RNN解码器。在本文中，我们提出了一个纯粹基于变压器的框架，使用BERT既是编码器和解码器。这样一来，运转预测目标和值生成目标可以联合优化我们的DST模型。在解码步骤中，我们重新使用编码器的隐蔽状态在相应的解码器层的自注意机制构建一个平面模型结构进行有效的参数更新。实验结果表明，该方法显着优于现有的国家的最先进的框架，而且还实现了非常有竞争力的性能，以最好的基于本体的方法。</font>
</div>


<hr>
<div id="paper15"> <b>15. To BERT or Not to BERT: Comparing Task-specific and Task-agnostic  Semi-Supervised Approaches for Sequence Tagging</b>  <a href="https://arxiv.org/pdf/2010.14042" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bhattacharjee%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kasturi Bhattacharjee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ballesteros%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Miguel Ballesteros</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Anubhai%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rishita Anubhai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Muresan%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Smaranda Muresan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jie Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ladhak%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Faisal Ladhak</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Al-Onaizan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yaser Al-Onaizan</a><br>
<font size="3">
 Abstract: Leveraging large amounts of unlabeled data using Transformer-like architectures, like BERT, has gained popularity in recent times owing to their effectiveness in learning general representations that can then be further fine-tuned for downstream tasks to much success. However, training these models can be costly both from an economic and environmental standpoint. In this work, we investigate how to effectively use unlabeled data: by exploring the task-specific semi-supervised approach, Cross-View Training (CVT) and comparing it with task-agnostic BERT in multiple settings that include domain and task relevant English data. CVT uses a much lighter model architecture and we show that it achieves similar performance to BERT on a set of sequence tagging tasks, with lesser financial and environmental impact.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：利用使用变压器样的架构，比如说BERT，已经得到普及在最近一个时期，由于其有效性在学习一般性的描述，然后可将大量的未标记数据的进一步微调下游任务很成功。然而，训练这些模型可能是昂贵无论从经济和环保的角度来看。在这项工作中，我们研究如何有效地使用无标签的数据：通过探索任务特定的半监督方法，交叉查看培训（CVT），并将其与多种设置，包括域和任务相关的英文资料的任务无关的BERT比较。 CVT采用的是轻得多模型架构，我们表明它实现了性能类似于BERT一组序列标注任务，用较少的资金和对环境的影响。</font>
</div>


<hr>
<div id="paper16"> <b>16. Volctrans Parallel Corpus Filtering System for WMT 2020</b>  <a href="https://arxiv.org/pdf/2010.14029" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Runxin Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhi%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhuo Zhi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mingxuan Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei Li</a><br>
<font size="3">
 Abstract: In this paper, we describe our submissions to the WMT20 shared task on parallel corpus filtering and alignment for low-resource conditions. The task requires the participants to align potential parallel sentence pairs out of the given document pairs, and score them so that low-quality pairs can be filtered. Our system, Volctrans, is made of two modules, i.e., a mining module and a scoring module. Based on the word alignment model, the mining module adopts an iterative mining strategy to extract latent parallel sentences. In the scoring module, an XLM-based scorer provides scores, followed by reranking mechanisms and ensemble. Our submissions outperform the baseline by 3.x/2.x and 2.x/2.x for km-en and ps-en on From Scratch/Fine-Tune conditions, which is the highest among all submissions.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们描述了我们陈词，平行语料库筛选和比对低资源条件WMT20共享任务。任务要求参与者对准潜在的平行句对出给定文件对，和他们得分，使低质量对可以被过滤。我们的系统中，Volctrans，由两个模块，即，挖掘模块和计分模块。基于字对齐模式，挖掘模块采用迭代挖掘策略，以提取潜在的并行语句。在计分模块，基于XLM射手提供分数，随后再排序机制和合奏。我们提交由跑赢大市3.X / 2.X和2.x / x为KM-EN和PS-EN基线上的划痕/微调的条件，这是所有提交中最高的。</font>
</div>


<hr>
<div id="paper17"> <b>17. Speech SIMCLR: Combining Contrastive and Reconstruction Objective for  Self-supervised Speech Representation Learning</b>  <a href="https://arxiv.org/pdf/2010.13991" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dongwei Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wubo Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Miao Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruixiong Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zou%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Zou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kun Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiangang Li</a><br>
<font size="3">
 Abstract: Self-supervised visual pretraining has shown significant progress recently. Among those methods, SimCLR greatly advanced the state of the art in self-supervised and semi-supervised learning on ImageNet. The input feature representations for speech and visual tasks are both continuous, so it is natural to consider applying similar objective on speech representation learning. In this paper, we propose Speech SimCLR, a new self-supervised objective for speech representation learning. During training, Speech SimCLR applies augmentation on raw speech and its spectrogram. Its objective is the combination of contrastive loss that maximizes agreement between differently augmented samples in the latent space and reconstruction loss of input representation. The proposed method achieved competitive results on speech emotion recognition and speech recognition. When used as feature extractor, our best model achieved 5.89% word error rate on LibriSpeech test-clean set using LibriSpeech 960 hours as pretraining data and LibriSpeech train-clean-100 set as fine-tuning data, which is the lowest error rate obtained in this setup to the best of our knowledge.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自监督视觉训练前近来显示出显著的进展。在这些方法中，SimCLR大大推进了ImageNet艺术的自我监督和半监督学习的状态。对于语音和视觉任务的输入特征表示是连续的，所以很自然地考虑对言论表示学习应用类似的目标。在本文中，我们提出了语音SimCLR，语音表示学习一个新的自我监督的目标。在培训过程中，语音SimCLR适用于原始语音和频谱增强。其目标是最大化在输入表示的潜在空间和重建损失增强不同样品之间的协议对比损耗的组合。该方法实现了对语音情感识别和语音识别的竞争结果。当作为特征提取器使用的，我们的最佳模型利用LibriSpeech960小时作为预训练数据和LibriSpeech列车清洁-100组作为微调数据，这是在所获得的最低误差率达到5.89％的字错误率上LibriSpeech测试清洁组这种设置到最佳的认识。</font>
</div>


<hr>
<div id="paper18"> <b>18. Interpretation of NLP models through input marginalization</b>  <a href="https://arxiv.org/pdf/2010.13984" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Siwon Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yi%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jihun Yi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eunji Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yoon%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sungroh Yoon</a><br>
<font size="3">
 Abstract: To demystify the "black box" property of deep neural networks for natural language processing (NLP), several methods have been proposed to interpret their predictions by measuring the change in prediction probability after erasing each token of an input. Since existing methods replace each token with a predefined value (i.e., zero), the resulting sentence lies out of the training data distribution, yielding misleading interpretations. In this study, we raise the out-of-distribution problem induced by the existing interpretation methods and present a remedy; we propose to marginalize each token out. We interpret various NLP models trained for sentiment analysis and natural language inference using the proposed method.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：为神秘面纱自然语言处理（NLP）深层神经网络的“黑盒子”的属性，几种方法已经提出通过清除每个令牌的输入后测量预测概率的变化来解释他们的预测。由于现有方法与预定义的值（即，零）替换每个令牌，所得到的句子所在出训练数据的分布，产生误导性的解释。在这项研究中，我们提出由现有的解释方法和现在的补救措施引起的乱分配问题;我们建议边缘化每个标记出来。我们解释使用该方法训练情绪分析和自然语言推理各种NLP模型。</font>
</div>


<hr>
<div id="paper19"> <b>19. Predict and Use Latent Patterns for Short-Text Conversation</b>  <a href="https://arxiv.org/pdf/2010.13982" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hung-Ting Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu-Chieh Chao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Ta-Hsuan Chao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei-Yun Ma</a><br>
<font size="3">
 Abstract: Many neural network models nowadays have achieved promising performances in Chit-chat settings. The majority of them rely on an encoder for understanding the post and a decoder for generating the response. Without given assigned semantics, the models lack the fine-grained control over responses as the semantic mapping between posts and responses is hidden on the fly within the end-to-end manners. Some previous works utilize sampled latent words as a controllable semantic form to drive the generated response around the work, but few works attempt to use more complex semantic forms to guide the generation. In this paper, we propose to use more detailed semantic forms, including latent responses and part-of-speech sequences sampled from the corresponding distributions, as the controllable semantics to guide the generation. Our experimental results show that the richer semantics are not only able to provide informative and diverse responses, but also increase the overall performance of response quality, including fluency and coherence.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：许多神经网络模型现在已经实现承诺在闲聊设置表演。它们中的大多数依赖于编码器，用于理解柱和用于产生响应的解码器。如果没有给出分配语义模型缺乏对反应的细粒度控制，岗位与反应之间的语义映射是隐藏在终端到终端的方式中的苍蝇。以前的一些作品利用采样潜词作为一个可控的语义形式，带动周边的工作所产生的响应，但作品很少尝试使用更复杂的语义的形式来引导产生。在本文中，我们提出使用更详细的语义形式，包括潜响应和从相应的分布采样的部分的语音序列，所述可控语义引导的产生。我们的实验结果表明，丰富的语义不仅能提供丰富和多样化的反应，但也增加了响应的质量，包括流畅性和连贯性的整体性能。</font>
</div>


<hr>
<div id="paper20"> <b>20. Reading Between the Lines: Exploring Infilling in Visual Narratives</b>  <a href="https://arxiv.org/pdf/2010.13944" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chandu%2C+K+R" target="_blank" rel="noopener" style="color:#0000EE;">Khyathi Raghavi Chandu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruo-Ping Dong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Black%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alan Black</a><br>
<font size="3">
 Abstract: Generating long form narratives such as stories and procedures from multiple modalities has been a long standing dream for artificial intelligence. In this regard, there is often crucial subtext that is derived from the surrounding contexts. The general seq2seq training methods render the models shorthanded while attempting to bridge the gap between these neighbouring contexts. In this paper, we tackle this problem by using \textit{infilling} techniques involving prediction of missing steps in a narrative while generating textual descriptions from a sequence of images. We also present a new large scale \textit{visual procedure telling} (ViPT) dataset with a total of 46,200 procedures and around 340k pairwise images and textual descriptions that is rich in such contextual dependencies. Generating steps using infilling technique demonstrates the effectiveness in visual procedures with more coherent texts. We conclusively show a METEOR score of 27.51 on procedures which is higher than the state-of-the-art on visual storytelling. We also demonstrate the effects of interposing new text with missing images during inference. The code and the dataset will be publicly available at this https URL.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：生成长叙事形式，如从多个模式的故事和程序一直是人工智能一个长期的梦想。在这方面，往往是从周围环境中得到的重要的潜台词。一般seq2seq培训方式呈现在试图弥合这些周边环境之间的差距缺兵少将的车型。在本文中，我们通过使用涉及的丢失预测步骤以叙述，同时从图像序列生成的文本描述\ textit {充填}技术解决这个问题。我们还提出了一种新的大规模\ textit {视觉过程告诉}（ViPT）数据集，共有46,200程序和340K左右成对的图像和文字描述富含这样的上下文相关性。生成使用充填技术的步骤演示了用更一致的文本视觉程序的有效性。我们得出结论表明27.51的程序流星得分比视觉讲故事的国家的最先进的高。我们还演示了用推理过程中丢失的图像插入新的文本的效果。代码和数据集将公开可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper21"> <b>21. Improving Limited Labeled Dialogue State Tracking with Self-Supervision</b>  <a href="https://arxiv.org/pdf/2010.13920" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chien-Sheng Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hoi%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Steven Hoi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Caiming Xiong</a><br>
<font size="3">
 Abstract: Existing dialogue state tracking (DST) models require plenty of labeled data. However, collecting high-quality labels is costly, especially when the number of domains increases. In this paper, we address a practical DST problem that is rarely discussed, i.e., learning efficiently with limited labeled data. We present and investigate two self-supervised objectives: preserving latent consistency and modeling conversational behavior. We encourage a DST model to have consistent latent distributions given a perturbed input, making it more robust to an unseen scenario. We also add an auxiliary utterance generation task, modeling a potential correlation between conversational behavior and dialogue states. The experimental results show that our proposed self-supervised signals can improve joint goal accuracy by 8.95\% when only 1\% labeled data is used on the MultiWOZ dataset. We can achieve an additional 1.76\% improvement if some unlabeled data is jointly trained as semi-supervised learning. We analyze and visualize how our proposed self-supervised signals help the DST task and hope to stimulate future data-efficient DST research.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现有的对话状态跟踪（DST）模型需要大量的标签数据。然而，收集高质量的标签是昂贵的，结构域的增加，特别是当数。在本文中，我们解决很少讨论的，即，具有有限的标记数据高效地学习实际DST问题。我们提出并研究两个自监管目标：保护潜在的一致性和建模会话的行为。我们鼓励DST模式对给定扰动输入一致的潜在分布，使其更加坚固，以看不见的场景。我们还添加了辅助话语生成任务，造型对话的行为和对话状态之间的潜在关系。实验结果表明，我们提出的自我监督的信号可以由8.95 \％时，只有1个\％标记数据在MultiWOZ数据集用于改善关节目标的准确性。我们可以实现一个额外的1.76 \％的改善，如果一些标签数据被联合训练成半监督学习。我们分析和可视化我们提出的自我监督的信号是如何帮助DST任务和希望，激发未来的数据高效DST研究。</font>
</div>


<hr>
<div id="paper22"> <b>22. Probing Task-Oriented Dialogue Representation from Language Models</b>  <a href="https://arxiv.org/pdf/2010.13912" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chien-Sheng Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Caiming Xiong</a><br>
<font size="3">
 Abstract: This paper investigates pre-trained language models to find out which model intrinsically carries the most informative representation for task-oriented dialogue tasks. We approach the problem from two aspects: supervised classifier probe and unsupervised mutual information probe. We fine-tune a feed-forward layer as the classifier probe on top of a fixed pre-trained language model with annotated labels in a supervised way. Meanwhile, we propose an unsupervised mutual information probe to evaluate the mutual dependence between a real clustering and a representation clustering. The goals of this empirical paper are to 1) investigate probing techniques, especially from the unsupervised mutual information aspect, 2) provide guidelines of pre-trained language model selection for the dialogue research community, 3) find insights of pre-training factors for dialogue application that may be the key to success.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文探讨预先训练语言模型，以找出哪些模型本质上带有面向任务的对话任务最翔实表示。监督分类探测和无人监督的互信息探测器：我们从两个方面来解决这个问题。我们微调前馈层与受监督的方式标注的标签固定预先训练语言模型的顶部分类探头。同时，我们提出了一种无监督的相互信息的探针来评估一个真正的聚类和表示群集之间的相互关系。这种经验本文的目标是1）调查探测技术，特别是从监督的互信息方面，2）对话研究界提供预先训练语言模型选择的指导方针，3）找到对话前培训因素的见解应用程序，可能是成功的关键。</font>
</div>


<hr>
<div id="paper23"> <b>23. Improved Neural Language Model Fusion for Streaming Recurrent Neural  Network Transducer</b>  <a href="https://arxiv.org/pdf/2010.13878" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Suyoun Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shangguan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuan Shangguan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mahadeokar%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jay Mahadeokar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bruguier%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Antoine Bruguier</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fuegen%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christian Fuegen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Seltzer%2C+M+L" target="_blank" rel="noopener" style="color:#0000EE;">Michael L. Seltzer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Le%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Duc Le</a><br>
<font size="3">
 Abstract: Recurrent Neural Network Transducer (RNN-T), like most end-to-end speech recognition model architectures, has an implicit neural network language model (NNLM) and cannot easily leverage unpaired text data during training. Previous work has proposed various fusion methods to incorporate external NNLMs into end-to-end ASR to address this weakness. In this paper, we propose extensions to these techniques that allow RNN-T to exploit external NNLMs during both training and inference time, resulting in 13-18% relative Word Error Rate improvement on Librispeech compared to strong baselines. Furthermore, our methods do not incur extra algorithmic latency and allow for flexible plug-and-play of different NNLMs without re-training. We also share in-depth analysis to better understand the benefits of the different NNLM fusion methods. Our work provides a reliable technique for leveraging unpaired text data to significantly improve RNN-T while keeping the system streamable, flexible, and lightweight.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：递归神经网络传感器（RNN-T），最喜欢的终端到终端的语音识别模型架构，有一个隐含的神经网络语言模型（NNLM）和培训过程中不能轻易利用非成对的文本数据。以前的工作已提出了各种融合方法，以将外部NNLMs到终端到终端的ASR来解决这个弱点。在本文中，我们提出扩展这些技术，让RNN-T期间训练和推理时间利用外部NNLMs，从而对Librispeech 13-18％的相对词错误率的改善比较强的基线。此外，我们的方法都是不收取额外的算法延迟和允许灵活的不同NNLMs没有再培训的插件和播放。我们也分享了深入的分析，以更好地了解不同NNLM融合方法的好处。我们的工作提供了利用未配对的文本数据显著提高RNN-T，同时保持系统可流，灵活，轻便的可靠技术。</font>
</div>


<hr>
<div id="paper24"> <b>24. Word Frequency Does Not Predict Grammatical Knowledge in Language Models</b>  <a href="https://arxiv.org/pdf/2010.13870" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Charles Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sie%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ryan Sie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tedeschi%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nico Tedeschi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bergen%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Leon Bergen</a><br>
<font size="3">
 Abstract: Neural language models learn, to varying degrees of accuracy, the grammatical properties of natural languages. In this work, we investigate whether there are systematic sources of variation in the language models' accuracy. Focusing on subject-verb agreement and reflexive anaphora, we find that certain nouns are systematically understood better than others, an effect which is robust across grammatical tasks and different language models. Surprisingly, we find that across four orders of magnitude, corpus frequency is unrelated to a noun's performance on grammatical tasks. Finally, we find that a novel noun's grammatical properties can be few-shot learned from various types of training data. The results present a paradox: there should be less variation in grammatical performance than is actually observed.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：神经语言模型学习，在不同程度上的精度，自然语言的语法特性。在这项工作中，我们调查是否有语言模型的精度变化的系统资源。专注于主谓一致和反思照应，我们发现某些名词进行了系统的理解比别人好，这是整个语法任务和不同的语言模型强大的效果。出人意料的是，我们发现，横跨四个数量级，语料库频率无关的名词对语法任务中的表现。最后，我们发现一个新的名词的语法属性可以少拍从各类训练数据的教训。结果呈现矛盾：应该有比实际观察到的语法性能变化较少。</font>
</div>


<hr>
<div id="paper25"> <b>25. Data Troubles in Sentence Level Confidence Estimation for Machine  Translation</b>  <a href="https://arxiv.org/pdf/2010.13856" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chelba%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Ciprian Chelba</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junpei Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yuezhang" target="_blank" rel="noopener" style="color:#0000EE;">Yuezhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kazawa%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hideto Kazawa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Klingner%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jeff Klingner</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mengmeng Niu</a><br>
<font size="3">
 Abstract: The paper investigates the feasibility of confidence estimation for neural machine translation models operating at the high end of the performance spectrum. As a side product of the data annotation process necessary for building such models we propose sentence level accuracy $SACC$ as a simple, self-explanatory evaluation metric for quality of translation. Experiments on two different annotator pools, one comprised of non-expert (crowd-sourced) and one of expert (professional) translators show that $SACC$ can vary greatly depending on the translation proficiency of the annotators, despite the fact that both pools are about equally reliable according to Krippendorff's alpha metric; the relatively low values of inter-annotator agreement confirm the expectation that sentence-level binary labeling $good$ / $needs\ work$ for translation out of context is very hard. For an English-Spanish translation model operating at $SACC = 0.89$ according to a non-expert annotator pool we can derive a confidence estimate that labels 0.5-0.6 of the $good$ translations in an "in-domain" test set with 0.95 Precision. Switching to an expert annotator pool decreases $SACC$ dramatically: $0.61$ for English-Spanish, measured on the exact same data as above. This forces us to lower the CE model operating point to 0.9 Precision while labeling correctly about 0.20-0.25 of the $good$ translations in the data. We find surprising the extent to which CE depends on the level of proficiency of the annotator pool used for labeling the data. This leads to an important recommendation we wish to make when tackling CE modeling in practice: it is critical to match the end-user expectation for translation quality in the desired domain with the demands of annotators assigning binary quality labels to CE training data.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文研究的信心估计在性能范围的高端操作的神经机器翻译模型的可行性。根据需要建立这样的模型数据注释过程的副产物，我们提出了句子层面的准确性$ SACC $作为翻译质量的简单，不言自明的评价指标。在两个不同的注释库，一个由非专业的（人群来源）和专家的一个实验（专业）翻译显示，即$ SACC $是多种多样的，这取决于注释的翻译能力，尽管事实上，这两个池根据克里彭多夫的alpha度量大约同等可靠的; -注释间协议确认的相对低值的预期句子级别的二进制标签$好/ $需求\ $工作翻译断章取义是很辛苦。根据一个非专家注释池在$ SACC的英语 - 西班牙语翻译模型操作= 0.89 $，我们可以得出一个信心，估计标签$ $好翻译0.5-0.6在“中域”测试集0.95精确。切换到专家注释池减小$ SACC $显着：英语，西班牙语$ 0.61 $，在完全相同的数据与上述测量。这迫使我们的CE模式操作点降低到0.9精度，同时正确标注有关的数据$ $好翻译0.20-0.25。我们发现令人吃惊，其CE取决于用于标记数据的注释池的熟练水平的程度。这就导致了一个重要的建议，我们希望在实践中解决CE建模时做出：关键是要匹配与注释分配二进制质量标签CE训练数据的需求所需的域名翻译质量的最终用户的期望。</font>
</div>


<hr>
<div id="paper26"> <b>26. Semi-Supervised Spoken Language Understanding via Self-Supervised Speech  and Language Model Pretraining</b>  <a href="https://arxiv.org/pdf/2010.13826" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cheng-I Lai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chuang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yung-Sung Chuang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hung-Yi Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shang-Wen Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Glass%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">James Glass</a><br>
<font size="3">
 Abstract: Much recent work on Spoken Language Understanding (SLU) is limited in at least one of three ways: models were trained on oracle text input and neglected ASR errors, models were trained to predict only intents without the slot values, or models were trained on a large amount of in-house data. In this paper, we propose a clean and general framework to learn semantics directly from speech with semi-supervision from transcribed or untranscribed speech to address these issues. Our framework is built upon pretrained end-to-end (E2E) ASR and self-supervised language models, such as BERT, and fine-tuned on a limited amount of target SLU data. We study two semi-supervised settings for the ASR component: supervised pretraining on transcribed speech, and unsupervised pretraining by replacing the ASR encoder with self-supervised speech representations, such as wav2vec. In parallel, we identify two essential criteria for evaluating SLU models: environmental noise-robustness and E2E semantics evaluation. Experiments on ATIS show that our SLU framework with speech as input can perform on par with those using oracle text as input in semantics understanding, even though environmental noise is present and a limited amount of labeled semantics data is available for training.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：口语理解（SLU）最近的许多工作是有限的三种方式中的至少一个：模型接受了关于Oracle Text的输入和被忽视的ASR错误，模型进行训练以预测仅意图没有插槽值，或模型进行了培训在大量内部数据。在本文中，我们提出了一个干净，总体框架，直接从语音学习的语义与转录或非转录的语音解决这些问题的半监督。我们的框架是在预训练的端至端内置（E2E）ASR和自我监督语言模型，如BERT，和微调目标SLU有限的数据量。我们研究了ASR组件两个半圆监督设置：监督下与自我监督的讲话表示，如wav2vec更换编码器ASR训练前的讲话转录，且无人监管的训练前。与此同时，我们确定评估模型SLU两个基本标准：环境噪声的鲁棒性和端到端的语义评价。在ATIS实验表明，我们的语音作为输入SLU框架可以媲美那些使用Oracle文本语义理解输入执行，即使环境噪声存在，并且语义标记有限的数据量可供训练。</font>
</div>


<hr>
<div id="paper27"> <b>27. PowerTransformer: Unsupervised Controllable Revision for Biased Language  Correction</b>  <a href="https://arxiv.org/pdf/2010.13816" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinyao Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sap%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maarten Sap</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rashkin%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hannah Rashkin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yejin Choi</a><br>
<font size="3">
 Abstract: Unconscious biases continue to be prevalent in modern text and media, calling for algorithms that can assist writers with bias correction. For example, a female character in a story is often portrayed as passive and powerless ("She daydreams about being a doctor") while a man is portrayed as more proactive and powerful ("He pursues his dream of being a doctor"). We formulate *Controllable Debiasing*, a new revision task that aims to rewrite a given text to correct the implicit and potentially undesirable bias in character portrayals. We then introduce PowerTransformer as an approach that debiases text through the lens of connotation frames (Sap et al., 2017), which encode pragmatic knowledge of implied power dynamics with respect to verb predicates. One key challenge of our task is the lack of parallel corpora. To address this challenge, we adopt an unsupervised approach using auxiliary supervision with related tasks such as paraphrasing and self-supervision based on a reconstruction loss, building on pretrained language models. Through comprehensive experiments based on automatic and human evaluations, we demonstrate that our approach outperforms ablations and existing methods from related tasks. Furthermore, we demonstrate the use of PowerTransformer as a step toward mitigating the well-documented gender bias in character portrayal in movie scripts.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：无意识的偏见继续在现代社会文本和媒体进行普及，要求，可以帮助作家与偏差修正算法。例如，一个女性角色在故事经常被描绘为被动和无能为力的（“她浮想大约是一个医生”），而男子被描绘成更加主动和强大的（“他努力追求自己当医生的梦想”）。我们制定*可控去除偏差*，一个新的修订任务，旨在改写给定文本纠正人物描写的隐性和潜在的不良倾向。然后，我们介绍PowerTransformer作为通过内涵帧的透镜debiases文本的方法（SAP等人，2017），这暗示功率动力学的语用知识编码相对于动词谓词。我们的任务的一个关键挑战是缺乏平行语料库的。为了应对这一挑战，我们使用辅助监管与相关的任务，如基于重建的损失的释义和自我监督，建立在预先训练语言模型采用无监督的做法。通过基于自动与人评价综合性实验，我们证明了我们的方法比消融和相关任务的现有方法。此外，我们展示了使用PowerTransformer作为对缓解性格的写照证据充分的性别偏见在电影剧本的一个步骤。</font>
</div>


<hr>
<div id="paper28"> <b>28. Is it Great or Terrible? Preserving Sentiment in Neural Machine  Translation of Arabic Reviews</b>  <a href="https://arxiv.org/pdf/2010.13814" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Saadany%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hadeel Saadany</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Orasan%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Constantin Orasan</a><br>
<font size="3">
 Abstract: Since the advent of Neural Machine Translation (NMT) approaches there has been a tremendous improvement in the quality of automatic translation. However, NMT output still lacks accuracy in some low-resource languages and sometimes makes major errors that need extensive post-editing. This is particularly noticeable with texts that do not follow common lexico-grammatical standards, such as user generated content (UGC). In this paper we investigate the challenges involved in translating book reviews from Arabic into English, with particular focus on the errors that lead to incorrect translation of sentiment polarity. Our study points to the special characteristics of Arabic UGC, examines the sentiment transfer errors made by Google Translate of Arabic UGC to English, analyzes why the problem occurs, and proposes an error typology specific of the translation of Arabic UGC. Our analysis shows that the output of online translation tools of Arabic UGC can either fail to transfer the sentiment at all by producing a neutral target text, or completely flips the sentiment polarity of the target word or phrase and hence delivers a wrong affect message. We address this problem by fine-tuning an NMT model with respect to sentiment polarity showing that this approach can significantly help with correcting sentiment errors detected in the online translation of Arabic UGC.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：由于神经机器翻译（NMT）的出现接近出现了自动翻译的质量巨大的进步。然而，NMT输出仍然缺乏一些低资源语言准确性，有时使得需要大量的后期编辑重大失误。这是与不遵循通用词汇语法标准，例如用户生成的内容（UGC）的文本格外引人注目。在本文中，我们调查的误差涉及从阿拉伯语翻译书评成英文的挑战，特别是重点对铅的情感极性的不正确翻译。我们的研究指出，阿拉伯UGC的特殊性，检查由谷歌提出的情绪传递错误阿拉伯语UGC的翻译成英文，分析为什么会出现这个问题，并提出了阿拉伯语UGC的翻译错误类型学具体。我们的分析表明，阿拉伯UGC的在线翻译工具的输出可以失败的情绪都通过产生中性目标文本，或者完全转移翻转的目标词或短语的情感极性，因此提供了一个错误的影响的信息。我们通过微调解决这一问题的NMT模型相对于情感极性显示，这种方法可以显著与纠正错误的情绪在帮助阿拉伯UGC的在线翻译检测。</font>
</div>


<hr>
<div id="paper29"> <b>29. Dynamic Boundary Time Warping for Sub-sequence Matching with Few  Examples</b>  <a href="https://arxiv.org/pdf/2010.14464" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Borchmann%2C+%C5%81" target="_blank" rel="noopener" style="color:#0000EE;">Łukasz Borchmann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jurkiewicz%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dawid Jurkiewicz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Grali%C5%84ski%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Filip Graliński</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=G%C3%B3recki%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tomasz Górecki</a><br>
<font size="3">
 Abstract: The paper presents a novel method of finding a fragment in a long temporal sequence similar to the set of shorter sequences. We are the first to propose an algorithm for such a search that does not rely on computing the average sequence from query examples. Instead, we use query examples as is, utilizing all of them simultaneously. The introduced method based on the Dynamic Time Warping (DTW) technique is suited explicitly for few-shot query-by-example retrieval tasks. We evaluate it on two different few-shot problems from the field of Natural Language Processing. The results show it either outperforms baselines and previous approaches or achieves comparable results when a low number of examples is available.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：介绍以类似于组较短的序列的长的时间顺序查找片段的新方法。我们是第一个提出的算法，使得不依赖于计算从查询样品的平均序列的搜索。相反，我们使用的查询例子如，同时利用所有的人。基于动态时间规整（DTW）技术所提出的方法是明确适用于少数次查询通过例如检索任务。我们评估它从自然语言处理领域的两种不同的几个拍的问题。结果表明，要么性能优于基准和以前的方法或达到类似的结果时的例子低数量是可用的。</font>
</div>


<hr>
<div id="paper30"> <b>30. Align-Refine: Non-Autoregressive Speech Recognition via Iterative  Realignment</b>  <a href="https://arxiv.org/pdf/2010.14233" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Chi%2C+E+A" target="_blank" rel="noopener" style="color:#0000EE;">Ethan A. Chi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Salazar%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Julian Salazar</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Kirchhoff%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Katrin Kirchhoff</a><br>
<font size="3">
 Abstract: Non-autoregressive models greatly improve decoding speed over typical sequence-to-sequence models, but suffer from degraded performance. Infilling and iterative refinement models make up some of this gap by editing the outputs of a non-autoregressive model, but are constrained in the edits that they can make. We propose iterative realignment, where refinements occur over latent alignments rather than output sequence space. We demonstrate this in speech recognition with Align-Refine, an end-to-end Transformer-based model which refines connectionist temporal classification (CTC) alignments to allow length-changing insertions and deletions. Align-Refine outperforms Imputer and Mask-CTC, matching an autoregressive baseline on WSJ at 1/14th the real-time factor and attaining a LibriSpeech test-other WER of 9.0% without an LM. Our model is strong even in one iteration with a shallower decoder.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：非自回归模型极大地提高了典型顺序对序列模型的解码速度，但是从性能下降的困扰。充填和迭代优化模型弥补了一些这方面的差距由编辑非自回归模型的输出，但在编辑的限制，他们可以做。我们建议迭代调整，其中的改进发生在潜在的路线，而不是输出序列空间。我们在ALIGN-提纯，端至端基于变压器的模型的语音识别证明这一点，其提炼联结颞分类（CTC）的比对，以允许长度改变的插入和缺失。 ALIGN-精确性能优于Imputer和面膜-CTC，以1 /第十四匹配上WSJ自回归基线实时因子和实现9.0％LibriSpeech测试其他WER没有LM。我们的模型是强大的，即使在一个迭代在较浅的解码器。</font>
</div>


<hr>
<div id="paper31"> <b>31. Co-attentional Transformers for Story-Based Video Understanding</b>  <a href="https://arxiv.org/pdf/2010.14104" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bebensee%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Björn Bebensee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Byoung-Tak Zhang</a><br>
<font size="3">
 Abstract: Inspired by recent trends in vision and language learning, we explore applications of attention mechanisms for visio-lingual fusion within an application to story-based video understanding. Like other video-based QA tasks, video story understanding requires agents to grasp complex temporal dependencies. However, as it focuses on the narrative aspect of video it also requires understanding of the interactions between different characters, as well as their actions and their motivations. We propose a novel co-attentional transformer model to better capture long-term dependencies seen in visual stories such as dramas and measure its performance on the video question answering task. We evaluate our approach on the recently introduced DramaQA dataset which features character-centered video story understanding questions. Our model outperforms the baseline model by 8 percentage points overall, at least 4.95 and up to 12.8 percentage points on all difficulty levels and manages to beat the winner of the DramaQA challenge.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：通过视觉和语言学习的最新趋势的启发，我们的应用程序，以故事为基础的视频理解范围内探索Visio的语言融合的重视机制的应用程序。像其他基于视频的QA任务，视频故事的理解要求代理商掌握复杂的时间依赖性。然而，因为它专注于视频的叙述方面也需要不同的角色，以及他们的行动和动机之间的相互作用的理解。我们提出了一个新颖的共注意力变压器模型在视觉故事，看到更好的捕捉长期相关性，如戏剧，并测量其上的视频答疑任务性能。我们评估在最近推出DramaQA数据集为特色以角色为中心视频故事的理解问题，我们的做法。我们的模型了8个百分点，优于基准模型整体而言，至少4.95和高达所有困难水平12.8个百分点，并设法击败DramaQA挑战的赢家。</font>
</div>


<hr>
<div id="paper32"> <b>32. VisualHints: A Visual-Lingual Environment for Multimodal Reinforcement  Learning</b>  <a href="https://arxiv.org/pdf/2010.13839" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Carta%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Carta</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chaudhury%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Subhajit Chaudhury</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Talamadupula%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kartik Talamadupula</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tatsubori%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michiaki Tatsubori</a><br>
<font size="3">
 Abstract: We present VisualHints, a novel environment for multimodal reinforcement learning (RL) involving text-based interactions along with visual hints (obtained from the environment). Real-life problems often demand that agents interact with the environment using both natural language information and visual perception towards solving a goal. However, most traditional RL environments either solve pure vision-based tasks like Atari games or video-based robotic manipulation; or entirely use natural language as a mode of interaction, like Text-based games and dialog systems. In this work, we aim to bridge this gap and unify these two approaches in a single environment for multimodal RL. We introduce an extension of the TextWorld cooking environment with the addition of visual clues interspersed throughout the environment. The goal is to force an RL agent to use both text and visual features to predict natural language action commands for solving the final task of cooking a meal. We enable variations and difficulties in our environment to emulate various interactive real-world scenarios. We present a baseline multimodal agent for solving such problems using CNN-based feature extraction from visual hints and LSTMs for textual feature extraction. We believe that our proposed visual-lingual environment will facilitate novel problem settings for the RL community.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们目前VisualHints，多式联运强化学习（RL）涉及与视觉提示（从环境中获得）以及基于文本的交互的新环境。现实生活中的问题往往要求代理商与同时使用自然语言的信息和对解决一个目标视觉感知环境互动。然而，大多数传统的RL环境无论是解决纯基于视觉的任务，如雅达利游戏或基于视频的机器人操作;或完全使用自然语言交互的模式，如基于文本的游戏和对话系统。在这项工作中，我们的目标是弥合这一差距，在多式联运RL单一环境统一这两种方法。我们引进与另外在整个环境中穿插视觉线索的TextWorld烹饪环境的扩展。我们的目标是迫使RL剂同时使用文字和视觉特征来预测自然语言动作命令解决做饭的最后一项任务。我们能变化和困难在我们的环境来模拟各种交互式的现实世界的情景。我们提出了一个基线代理多式联运解决利用视觉线索和LSTMs的文本特征提取基于CNN特征提取等问题。我们相信，我们提出的视觉语言环境将有利于为RL社会新问题的设置。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！封面为论文标题词云图！</font></p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>PROCJX
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://procjx.github.io/2020/10/28/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-10-28/" title="【arxiv论文】 Computation and Language 2020-10-28">https://procjx.github.io/2020/10/28/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-10-28/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">

        
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2020/10/27/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-10-27/" rel="next" title="【arxiv论文】 Computer Vision and Pattern Recognition 2020-10-27">
                  <i class="fa fa-chevron-left"></i> 【arxiv论文】 Computer Vision and Pattern Recognition 2020-10-27
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/2020/10/28/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-10-28/" rel="prev" title="【arxiv论文】 Computer Vision and Pattern Recognition 2020-10-28">
                  【arxiv论文】 Computer Vision and Pattern Recognition 2020-10-28 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>

        
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang -->
<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9197824246"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="gitalk-container"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#目录"><span class="nav-number">1.</span> <span class="nav-text">目录</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#摘要"><span class="nav-number">2.</span> <span class="nav-text">摘要</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="PROCJX"
    src="/images/procjx.png">
  <p class="site-author-name" itemprop="name">PROCJX</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">444</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">71</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/procjx" title="GitHub &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;procjx" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:procjx@gmail.com" title="E-Mail &amp;rarr; mailto:procjx@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>


<!--
      
        <script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
        <script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
        <div class="widget-wrap">
            <h3 class="widget-title">标签云</h3>
            <div id="myCanvasContainer" class="widget tagcloud">
                <canvas width="250" height="250" id="resCanvas" style="width=100%">
                    <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AAAI/" rel="tag">AAAI</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ACL/" rel="tag">ACL</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Accepted-Papers/" rel="tag">Accepted Papers</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ArXiv/" rel="tag">ArXiv</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/" rel="tag">BERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS20SI/" rel="tag">CS20SI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS224d/" rel="tag">CS224d</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/" rel="tag">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CVPR/" rel="tag">CVPR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Context/" rel="tag">Context</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cross-Lingual/" rel="tag">Cross Lingual</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dialog-System/" rel="tag">Dialog System</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse/" rel="tag">Discourse</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Ranking/" rel="tag">Discourse Ranking</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Structure/" rel="tag">Discourse Structure</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Document-NMT/" rel="tag">Document NMT</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EMNLP/" rel="tag">EMNLP</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Extractive/" rel="tag">Extractive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICLR/" rel="tag">ICLR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICML/" rel="tag">ICML</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IJCAI/" rel="tag">IJCAI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Inter-Sentence/" rel="tag">Inter-Sentence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Keyphrase-Generation/" rel="tag">Keyphrase Generation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NAACL/" rel="tag">NAACL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIPS/" rel="tag">NIPS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NMT/" rel="tag">NMT</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Relation-Extraction/" rel="tag">Neural Relation Extraction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RST/" rel="tag">RST</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Relation-Constraints/" rel="tag">Relation Constraints</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Summarization/" rel="tag">Summarization</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Translation/" rel="tag">Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Word-Translation/" rel="tag">Word Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/alias/" rel="tag">alias</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/git/" rel="tag">git</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pip/" rel="tag">pip</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/screen/" rel="tag">screen</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/" rel="tag">shell</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tgz/" rel="tag">tgz</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tts/" rel="tag">tts</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1/" rel="tag">冒泡</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/" rel="tag">冒泡排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%99%E4%BD%9C%E5%8A%A9%E6%89%8B/" rel="tag">写作助手</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8E%8B%E7%BC%A9/" rel="tag">压缩</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/" rel="tag">发送邮件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%88%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">合并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%8E%E5%8F%B0/" rel="tag">后台</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/" rel="tag">基数排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F/" rel="tag">希尔排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6/" rel="tag">归并</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">归并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/" rel="tag">快速排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F/" rel="tag">批量</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4/" rel="tag">批量删除</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F/" rel="tag">排序</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5/" rel="tag">插入</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/" rel="tag">插入排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%99%E7%A8%8B/" rel="tag">教程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/" rel="tag">斐波那契数列</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9D%80%E6%AD%BB%E8%BF%9B%E7%A8%8B/" rel="tag">杀死进程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B1%89%E8%AF%BA%E5%A1%94/" rel="tag">汉诺塔</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%A7%A3%E5%8E%8B/" rel="tag">解压</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%B0%B7%E6%AD%8C%E7%BF%BB%E8%AF%91/" rel="tag">谷歌翻译</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BF%AD%E4%BB%A3%E5%9B%9E%E7%BF%BB/" rel="tag">迭代回翻</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9/" rel="tag">选择</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/" rel="tag">选择排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%99%84%E4%BB%B6/" rel="tag">附件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3/" rel="tag">非监督</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%A2%86%E5%9F%9F%E9%80%82%E5%BA%94/" rel="tag">领域适应</a><span class="tag-list-count">1</span></li></ul>
                </canvas>
            </div>
        </div>
        
-->
        
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-hengfu -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9879871597"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-chuizhi -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="1662238719"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-zhengfangxing -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="6699421902"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PROCJX</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.4.2
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>












        
      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  <script src="/js/local-search.js"></script>













  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '2286ab64f5194d9d79ce',
      clientSecret: 'f912492bec2391664b40478f50f2f943376768d6',
      repo: 'procjx.github.io',
      owner: 'procjx',
      admin: ['procjx'],
      id: '7d9ae7b6c90401be5accd79f45a556f2',
        language: 'zh-CN',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
