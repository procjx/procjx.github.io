<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/procjx.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/procjxfavicon32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/procjxfavicon16x16.ico">
  <link rel="mask-icon" href="/images/procjx.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.4.2',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

<!-- Google Adsense -->
<!--
<script async src="//pagead2.googlesyndication.com/
pagead/js/adsbygoogle.js"></script>
<script>
(adsbygoogle = window.adsbygoogle || []).push({
google_ad_client: "pub-1179774715076800",
enable_page_level_ads: true
});
</script>
-->

<script data-ad-client="ca-pub-1179774715076800" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>


<meta name="google-site-verification" content="cEiGwg0T8Rj5msmuEcGYZTh5nnf05EhCXy0gp2Ml5BI" />
<meta name="baidu-site-verification" content="noSKHe8MJs" />

  <meta name="description" content="目录  1. Semi-supervised Medical Image Segmentation through Dual-task Consistency [PDF] 摘要  2. Plant Diseases recognition on images using Convolutional Neural  Networks: A Systematic Review [PDF] 摘要  3.">
<meta property="og:type" content="article">
<meta property="og:title" content="【arxiv论文】 Computer Vision and Pattern Recognition 2020-09-10">
<meta property="og:url" content="https:&#x2F;&#x2F;procjx.github.io&#x2F;2020&#x2F;09&#x2F;10&#x2F;%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-09-10&#x2F;index.html">
<meta property="og:site_name" content="PROCJX&#39;s BLOGS">
<meta property="og:description" content="目录  1. Semi-supervised Medical Image Segmentation through Dual-task Consistency [PDF] 摘要  2. Plant Diseases recognition on images using Convolutional Neural  Networks: A Systematic Review [PDF] 摘要  3.">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https:&#x2F;&#x2F;procjx.github.io&#x2F;images&#x2F;cv-2020-09-10.jpg">
<meta property="og:updated_time" content="2020-09-10T05:00:05.477Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;procjx.github.io&#x2F;images&#x2F;cv-2020-09-10.jpg">

<link rel="canonical" href="https://procjx.github.io/2020/09/10/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-09-10/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-09-10 | PROCJX's BLOGS</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">PROCJX's BLOGS</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">WITH LOVE OF WORLD</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-fw fa-download"></i>资源</a>

  </li>
        <li class="menu-item menu-item-arxiv">

    <a href="/arxiv/" rel="section"><i class="fa fa-fw fa-file-pdf-o"></i>arxiv论文</a>

  </li>
        <li class="menu-item menu-item-deadline">

    <a href="/deadline/" rel="section"><i class="fa fa-fw fa-calendar"></i>会议截稿</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://procjx.github.io/2020/09/10/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-09-10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/procjx.png">
      <meta itemprop="name" content="PROCJX">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PROCJX's BLOGS">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          【arxiv论文】 Computer Vision and Pattern Recognition 2020-09-10
        </h2>

        <div class="post-meta">
        
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-09-10 12:35:17 / 修改时间：13:00:05" itemprop="dateCreated datePublished" datetime="2020-09-10T12:35:17+08:00">2020-09-10</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/arxiv/" itemprop="url" rel="index">
                    <span itemprop="name">arxiv</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/arxiv/CV/" itemprop="url" rel="index">
                    <span itemprop="name">CV</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
              <span>46k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
              <span>1:16</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="/images/cv-2020-09-10.jpg" alt></p><h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Semi-supervised Medical Image Segmentation through Dual-task Consistency <a href="https://arxiv.org/pdf/2009.04448" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Plant Diseases recognition on images using Convolutional Neural  Networks: A Systematic Review <a href="https://arxiv.org/pdf/2009.04365" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> HSFM-$Σ$nn: Combining a Feedforward Motion Prediction Network and  Covariance Prediction <a href="https://arxiv.org/pdf/2009.04299" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Online trajectory recovery from offline handwritten Japanese kanji  characters <a href="https://arxiv.org/pdf/2009.04284" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Unsupervised Part Discovery by Unsupervised Disentanglement <a href="https://arxiv.org/pdf/2009.04264" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Binarized Neural Architecture Search for Efficient Object Recognition <a href="https://arxiv.org/pdf/2009.04247" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Temporal Attribute-Appearance Learning Network for Video-based Person  Re-Identification <a href="https://arxiv.org/pdf/2009.04181" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> MU-GAN: Facial Attribute Editing based on Multi-attention Mechanism <a href="https://arxiv.org/pdf/2009.04177" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Diversified Mutual Learning for Deep Metric Learning <a href="https://arxiv.org/pdf/2009.04170" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> One-shot Text Field Labeling using Attention and Belief Propagation for  Structure Information Extraction <a href="https://arxiv.org/pdf/2009.04153" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Real-time Plant Health Assessment Via Implementing Cloud-based Scalable  Transfer Learning On AWS DeepLens <a href="https://arxiv.org/pdf/2009.04110" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Deep Metric Learning Meets Deep Clustering: An Novel Unsupervised  Approach for Feature Embedding <a href="https://arxiv.org/pdf/2009.04091" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> View-consistent 4D Light Field Depth Estimation <a href="https://arxiv.org/pdf/2009.04065" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Improved Trainable Calibration Method for Neural Networks on Medical  Imaging Classification <a href="https://arxiv.org/pdf/2009.04057" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Unconstrained Text Detection in Manga: a New Dataset and Baseline <a href="https://arxiv.org/pdf/2009.04042" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Modeling Wildfire Perimeter Evolution using Deep Neural Networks <a href="https://arxiv.org/pdf/2009.03977" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Joint Pose and Shape Estimation of Vehicles from LiDAR Data <a href="https://arxiv.org/pdf/2009.03964" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Towards Unique and Informative Captioning of Images <a href="https://arxiv.org/pdf/2009.03949" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Map-Adaptive Goal-Based Trajectory Prediction <a href="https://arxiv.org/pdf/2009.04450" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> not-so-BigGAN: Generating High-Fidelity Images on a Small Compute Budget <a href="https://arxiv.org/pdf/2009.04433" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Cephalogram Synthesis and Landmark Detection in Dental Cone-Beam CT  Systems <a href="https://arxiv.org/pdf/2009.04420" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> NTGAN: Learning Blind Image Denoising without Clean Reference <a href="https://arxiv.org/pdf/2009.04286" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Small-floating Target Detection in Sea Clutter via Visual Feature  Classifying in the Time-Doppler Spectra <a href="https://arxiv.org/pdf/2009.04185" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Revealing Lung Affections from CTs. A Comparative Analysis of Various  Deep Learning Approaches for Dealing with Volumetric Data <a href="https://arxiv.org/pdf/2009.04160" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Single Image Super-Resolution for Domain-Specific Ultra-Low Bandwidth  Image Transmission <a href="https://arxiv.org/pdf/2009.04127" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> Generalizing Complex/Hyper-complex Convolutions to Vector Map  Convolutions <a href="https://arxiv.org/pdf/2009.04083" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Learning joint segmentation of tissues and brain lesions from  task-specific hetero-modal domain-shifted datasets <a href="https://arxiv.org/pdf/2009.04009" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> Fuzzy Unique Image Transformation: Defense Against Adversarial Attacks  On Deep COVID-19 Models <a href="https://arxiv.org/pdf/2009.04004" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Tangent Space Based Alternating Projections for Nonnegative Low Rank  Matrix Approximation <a href="https://arxiv.org/pdf/2009.03998" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Semi-supervised Medical Image Segmentation through Dual-task Consistency</b>  <a href="https://arxiv.org/pdf/2009.04448" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiangde Luo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jieneng Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yinan Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guotai Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shaoting Zhang</a><br>
<font size="3">
 Abstract: Deep learning-based semi-supervised learning (SSL) algorithms have led to promising results in medical images segmentation and can alleviate doctors' expensive annotations by leveraging unlabeled data. However, most of the existing SSL algorithms in literature tend to regularize the model training by perturbing networks and/or data. Observing that multi/dual-task learning attends to various levels of information which have inherent prediction perturbation, we ask the question in this work: can we explicitly build task-level regularization rather than implicitly constructing networks- and/or data-level perturbation-and-transformation for SSL? To answer this question, we propose a novel dual-task-consistency semi-supervised framework for the first time. Concretely, we use a dual-task deep network that jointly predicts a pixel-wise segmentation map and a geometry-aware level set representation of the target. The level set representation is converted to an approximated segmentation map through a differentiable task transform layer. Simultaneously, we introduce a dual-task consistency regularization between the level set-derived segmentation maps and directly predicted segmentation maps for both labeled and unlabeled data. Extensive experiments on two public datasets show that our method can largely improve the performance by incorporating the unlabeled data. Meanwhile, our framework outperforms the state-of-the-art semi-supervised medical image segmentation methods. Code is available at: this https URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深学习型半监督学习（SSL）算法导致承诺在医学图像分割结果，可以通过利用未标记数据减轻医生的昂贵的注解。然而，大多数的文献中现有的SSL算法往往通过扰动网络和/或数据来规范模型训练。观察到多/双任务学习照顾到其具有固有预测扰动的各种信息的水平，我们要求在这项工作中的问题：我们可以明确地建立任务级正规化，而不是隐含构建网络 - 和/或数据级perturbation-并转化为SSL？要回答这个问题，我们提出了第一次新的双任务一致性半监督框架。具体而言，我们使用了双任务深厚的网络，共同预测逐像素分割图和目标的几何感知水平集表示。水平集表示通过微的任务转换成的近似分段图变换层。同时，我们引入水平集衍生的分割的地图之间的双任务一致性正规化和直接预测的分割映射两个标记和未标记的数据。在两个公共数据集大量的实验表明，该方法可以在很大程度上通过将无标签数据提高性能。同时，我们的框架优于状态的最先进的半监督医学图像分割的方法。代码，请访问：此HTTPS URL</font>
</div>


<hr>
<div id="paper2"> <b>2. Plant Diseases recognition on images using Convolutional Neural  Networks: A Systematic Review</b>  <a href="https://arxiv.org/pdf/2009.04365" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Abade%2C+A+S" target="_blank" rel="noopener" style="color:#0000EE;">Andre S. Abade</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ferreira%2C+P+A" target="_blank" rel="noopener" style="color:#0000EE;">Paulo Afonso Ferreira</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=de+Barros+Vidal%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Flavio de Barros Vidal</a><br>
<font size="3">
 Abstract: Plant diseases are considered one of the main factors influencing food production and minimize losses in production, and it is essential that crop diseases have fast detection and recognition. The recent expansion of deep learning methods has found its application in plant disease detection, offering a robust tool with highly accurate results. In this context, this work presents a systematic review of the literature that aims to identify the state of the art of the use of convolutional neural networks(CNN) in the process of identification and classification of plant diseases, delimiting trends, and indicating gaps. In this sense, we present 121 papers selected in the last ten years with different approaches to treat aspects related to disease detection, characteristics of the data set, the crops and pathogens investigated. From the results of the systematic review, it is possible to understand the innovative trends regarding the use of CNNs in the identification of plant diseases and to identify the gaps that need the attention of the research community.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：植物疾病被认为是影响粮食生产的主要因素之一，并尽量减少生产损失，这是至关重要的作物病害具有快速检测与识别。最近的深度学习方法扩张已经发现其在植物病害检测中的应用，提供高度精确的结果的强大的工具。在这方面，这个工作提出的文献，其目的是确定在本领域中的识别和植物病害，限定趋势的分类，和指示间隙的过程中使用的卷积神经网络的（CNN）的状态的系统评价。在这个意义上说，在过去的十年里有不同的方法来与疾病检测治疗方面我们选择了目前121篇论文，数据集的特点，作物和病原体调查。从系统评价的结果，这是可以理解的创新趋势关于植物病害的识别采用细胞神经网络，并确定需要研究界的关注的差距。</font>
</div>


<hr>
<div id="paper3"> <b>3. HSFM-$Σ$nn: Combining a Feedforward Motion Prediction Network and  Covariance Prediction</b>  <a href="https://arxiv.org/pdf/2009.04299" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Postnikov%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">A. Postnikov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gamayunov%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">A. Gamayunov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ferrer%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">G. Ferrer</a><br>
<font size="3">
 Abstract: In this paper, we propose a new method for motion prediction: HSFM-$\Sigma$nn. Our proposed method combines two different approaches: a feedforward network whose layers are model-based transition functions using the HSFM and a Neural Network (NN), on each of these layers, for covariance prediction. We will compare our method with classical methods for covariance estimation showing their limitations. We will also compare with a learning-based approach, social-LSTM, showing that our method is more precise and efficient.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出了运动预测的新方法：HSFM  -  $ \ $西格玛NN。我们提出的方法结合了两种不同的方法：前馈网络的其层是使用HSFM和一个神经网络（NN）基于模型的转移函数，对这些层的每用于协方差的预测。我们将比较与展示自己的局限性协方差估计的经典方法我们的方法。我们还将与以学习为主的方针，社会LSTM比较，表明我们的方法是更精确和高效。</font>
</div>


<hr>
<div id="paper4"> <b>4. Online trajectory recovery from offline handwritten Japanese kanji  characters</b>  <a href="https://arxiv.org/pdf/2009.04284" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+H+T" target="_blank" rel="noopener" style="color:#0000EE;">Hung Tuan Nguyen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nakamura%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tsubasa Nakamura</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+C+T" target="_blank" rel="noopener" style="color:#0000EE;">Cuong Tuan Nguyen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nakagawa%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Masaki Nakagawa</a><br>
<font size="3">
 Abstract: In general, it is straightforward to render an offline handwriting image from an online handwriting pattern. However, it is challenging to reconstruct an online handwriting pattern given an offline handwriting image, especially for multiple-stroke character as Japanese kanji. The multiple-stroke character requires not only point coordinates but also stroke orders whose difficulty is exponential growth by the number of strokes. Besides, several crossed and touch points might increase the difficulty of the recovered task. We propose a deep neural network-based method to solve the recovered task using a large online handwriting database. Our proposed model has two main components: Convolutional Neural Network-based encoder and Long Short-Term Memory Network-based decoder with an attention layer. The encoder focuses on feature extraction while the decoder refers to the extracted features and generates the time-sequences of coordinates. We also demonstrate the effect of the attention layer to guide the decoder during the reconstruction. We evaluate the performance of the proposed method by both visual verification and handwritten character recognition. Although the visual verification reveals some problems, the recognition experiments demonstrate the effect of trajectory recovery in improving the accuracy of offline handwritten character recognition when online recognition for the recovered trajectories are combined.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在一般情况下，它是直接从在线手写模式使脱机手写图像。但是，它是具有挑战性的重建给予脱机手写图像的手写在线模式，特别是对于多笔画字符的日语汉字。在多笔画字符不仅需要点坐标也笔顺，其困难是由笔画数呈指数增长。此外，一些交叉和接触点可能会增加恢复任务的难度。我们提出了一个深刻的基于神经网络的方法来解决使用大型在线手写数据库恢复的任务。我们提出的模型有两个主要组件：基于网络的卷积神经编码器和基于网络的长短期记忆解码器与关注层。编码器集中于特征提取而解码器是指所提取的特征，并产生坐标的时间序列。我们也证明了关注层的作用重建过程中引导解码器。我们评估了该方法的双方视觉验证和手写字符识别的性能。虽然视觉验证发现一些问题，识别实验证明轨迹复苏的提高脱机手写字符识别的准确度时，为恢复的轨迹在线识别相结合的效果。</font>
</div>


<hr>
<div id="paper5"> <b>5. Unsupervised Part Discovery by Unsupervised Disentanglement</b>  <a href="https://arxiv.org/pdf/2009.04264" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Braun%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sandro Braun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Esser%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Patrick Esser</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ommer%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Björn Ommer</a><br>
<font size="3">
 Abstract: We address the problem of discovering part segmentations of articulated objects without supervision. In contrast to keypoints, part segmentations provide information about part localizations on the level of individual pixels. Capturing both locations and semantics, they are an attractive target for supervised learning approaches. However, large annotation costs limit the scalability of supervised algorithms to other object categories than humans. Unsupervised approaches potentially allow to use much more data at a lower cost. Most existing unsupervised approaches focus on learning abstract representations to be refined with supervision into the final representation. Our approach leverages a generative model consisting of two disentangled representations for an object's shape and appearance and a latent variable for the part segmentation. From a single image, the trained model infers a semantic part segmentation map. In experiments, we compare our approach to previous state-of-the-art approaches and observe significant gains in segmentation accuracy and shape consistency. Our work demonstrates the feasibility to discover semantic part segmentations without supervision.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们发现解决铰接式对象的一部分分割不受监督的问题。与此相反，以关键点，部分分割提供有关各像素的电平部分的本地化信息。捕获这两个地点和语义，它们是监督学习有吸引力的目标接近。然而，大注释成本限制监督算法到其他对象类别比人类的可扩展性。无监督办法可能允许以较低的成本使用更多的数据。大多数现有的无监督方法的重点是学习抽象的表述与监理到最后的表现加以完善。我们的方法利用由一个对象的形状和外观以及部分细分潜在变量2所解开表示的生成模型。从一个单一的形象，训练的模型推断语义部分分割图。在实验中，我们我们的做法比较以前的方法的国家的最先进的，并观察分割精度和形状一致性显著的收益。我们的工作表明的可行性，以发现语义部分的分割没有监督。</font>
</div>


<hr>
<div id="paper6"> <b>6. Binarized Neural Architecture Search for Efficient Object Recognition</b>  <a href="https://arxiv.org/pdf/2009.04247" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hanlin Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhuo%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Li'an Zhuo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Baochang Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiawu Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianzhuang Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rongrong Ji</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Doermann%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Doermann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guodong Guo</a><br>
<font size="3">
 Abstract: Traditional neural architecture search (NAS) has a significant impact in computer vision by automatically designing network architectures for various tasks. In this paper, binarized neural architecture search (BNAS), with a search space of binarized convolutions, is introduced to produce extremely compressed models to reduce huge computational cost on embedded devices for edge computing. The BNAS calculation is more challenging than NAS due to the learning inefficiency caused by optimization requirements and the huge architecture space, and the performance loss when handling the wild data in various computing applications. To address these issues, we introduce operation space reduction and channel sampling into BNAS to significantly reduce the cost of searching. This is accomplished through a performance-based strategy that is robust to wild data, which is further used to abandon less potential operations. Furthermore, we introduce the Upper Confidence Bound (UCB) to solve 1-bit BNAS. Two optimization methods for binarized neural networks are used to validate the effectiveness of our BNAS. Extensive experiments demonstrate that the proposed BNAS achieves a comparable performance to NAS on both CIFAR and ImageNet databases. An accuracy of $96.53\%$ vs. $97.22\%$ is achieved on the CIFAR-10 dataset, but with a significantly compressed model, and a $40\%$ faster search than the state-of-the-art PC-DARTS. On the wild face recognition task, our binarized models achieve a performance similar to their corresponding full-precision models.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：传统的神经结构搜索（NAS）具有自动设计网络架构完成各项任务，在计算机视觉显著的影响。在本文中，二值化的神经结构搜索（BNAS），与二值化卷积的搜索空间，引入产生极高的压缩模式，以减少对边缘计算嵌入式设备巨大的计算成本。该BNAS计算比NAS更加具有挑战性，因为造成优化要求的学习效率和巨大的建筑空间，以及处理各种计算应用野生数据时的性能损失。为了解决这些问题，我们引入的操作空间减少和通道的采样到BNAS以显著降低搜索成本。这是通过具有较强的抗野生的数据，将其进一步使用，放弃的可能性更小操作的基于性能的策略来实现的。此外，我们引入上置信限（UCB）来解决1位BNAS。对于二值化神经网络的两种优化方法来验证我们BNAS的有效性。大量的实验表明，该BNAS实现了相当的性能到NAS上都CIFAR和ImageNet数据库。的96.53 $ \％$与$ 97.22 \％$的精度在CIFAR-10数据集实现，但有显著压缩模型和$ 40 \％$比国家的最先进的PC-DARTS更快的搜索。对野生人脸识别的任务，我们的二值化模型实现类似于其对应的全精度的模型性能。</font>
</div>


<hr>
<div id="paper7"> <b>7. Temporal Attribute-Appearance Learning Network for Video-based Person  Re-Identification</b>  <a href="https://arxiv.org/pdf/2009.04181" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiawei Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xierong Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zha%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zheng-Jun Zha</a><br>
<font size="3">
 Abstract: Video-based person re-identification aims to match a specific pedestrian in surveillance videos across different time and locations. Human attributes and appearance are complementary to each other, both of them contribute to pedestrian matching. In this work, we propose a novel Temporal Attribute-Appearance Learning Network (TALNet) for video-based person re-identification. TALNet simultaneously exploits human attributes and appearance to learn comprehensive and effective pedestrian representations from videos. It explores hard visual attention and temporal-semantic context for attributes, and spatial-temporal dependencies among body parts for appearance, to boost the learning of them. Specifically, an attribute branch network is proposed with a spatial attention block and a temporal-semantic context block for learning robust attribute representation. The spatial attention block focuses the network on corresponding regions within video frames related to each attribute, the temporal-semantic context block learns both the temporal context for each attribute across video frames and the semantic context among attributes in each video frame. The appearance branch network is designed to learn effective appearance representation from both whole body and body parts with spatial-temporal dependencies among them. TALNet leverages the complementation between attribute and appearance representations, and jointly optimizes them by multi-task learning fashion. Moreover, we annotate ID-level attributes for each pedestrian in the two commonly used video datasets. Extensive experiments on these datasets, have verified the superiority of TALNet over state-of-the-art methods.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于视频的人重新鉴定的目的，以匹配在不同时间和地点的监视录像特定的行人。人属性和外观都彼此互补，两者向行人匹配。在这项工作中，我们提出了一个新颖的时间属性 - 外观学习网络（TALNet）基于视频的人重新鉴定。 TALNet同时利用人类的属性和外观从视频中学习全面有效的行人表示。它探讨了硬视觉注意力和属性颞语义上下文，和身体部位的外观之间的时空相关性，以提高他们的学习。具体而言，属性分支网络，提出具有空间注意力块和用于学习强健属性表示的颞语义语境块。空间注意力集中块在网络上对应与每个属性的视频帧中的区域，所述时间 - 语义语境块获悉既用于跨视频帧的每个属性和每个视频帧属性之间的语义语境的时间上下文。外观分支网络的设计从两个全身和身体部位与它们之间的时空依赖性学习有效的外观表现。 TALNet利用属性和外观表述的互补，共同通过多任务学习的方式优化它们。此外，我们注释ID级在两个常用的视频数据集的每个行人属性。对这些数据集大量的实验，已经证实TALNet超过国家的最先进方法的优越性。</font>
</div>


<hr>
<div id="paper8"> <b>8. MU-GAN: Facial Attribute Editing based on Multi-attention Mechanism</b>  <a href="https://arxiv.org/pdf/2009.04177" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Ke Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Su%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yukun Su</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiwang Guo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liang Qi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhenbing Zhao</a><br>
<font size="3">
 Abstract: Facial attribute editing has mainly two objectives: 1) translating image from a source domain to a target one, and 2) only changing the facial regions related to a target attribute and preserving the attribute-excluding details. In this work, we propose a Multi-attention U-Net-based Generative Adversarial Network (MU-GAN). First, we replace a classic convolutional encoder-decoder with a symmetric U-Net-like structure in a generator, and then apply an additive attention mechanism to build attention-based U-Net connections for adaptively transferring encoder representations to complement a decoder with attribute-excluding detail and enhance attribute editing ability. Second, a self-attention mechanism is incorporated into convolutional layers for modeling long-range and multi-level dependencies across image regions. experimental results indicate that our method is capable of balancing attribute editing ability and details preservation ability, and can decouple the correlation among attributes. It outperforms the state-of-the-art methods in terms of attribute manipulation accuracy and image quality.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：面部属性编辑主要有两个目的：1）从源域转换的图像的目标之一，和2）仅改变相关的目标属性的面部区域和保存属性不包括的信息。在这项工作中，我们提出了一个基于掌中宽带，多关注剖成对抗性网络（MU-GAN）。首先，我们更换一个经典的卷积编码器，解码器，U型网状对称结构的发电机，然后应用的添加剂注意机制，以诚为本注意构建掌中宽带连接用于自适应传输编码表示，以补充与属性的解码器-excluding细节增强属性编辑能力。其次，自关注机构被并入到卷积层跨越图像区域建模远距离和多级的依赖关系。实验结果表明，我们的方法是能够平衡属性的编辑能力和细节保护能力，并且可以断开属性之间的相关性。它优于在属性操作的精度和图像质量而言国家的最先进的方法。</font>
</div>


<hr>
<div id="paper9"> <b>9. Diversified Mutual Learning for Deep Metric Learning</b>  <a href="https://arxiv.org/pdf/2009.04170" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Park%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wonpyo Park</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wonjae Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=You%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kihyun You</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minsu Cho</a><br>
<font size="3">
 Abstract: Mutual learning is an ensemble training strategy to improve generalization by transferring individual knowledge to each other while simultaneously training multiple models. In this work, we propose an effective mutual learning method for deep metric learning, called Diversified Mutual Metric Learning, which enhances embedding models with diversified mutual learning. We transfer relational knowledge for deep metric learning by leveraging three kinds of diversities in mutual learning: (1) model diversity from different initializations of models, (2) temporal diversity from different frequencies of parameter update, and (3) view diversity from different augmentations of inputs. Our method is particularly adequate for inductive transfer learning at the lack of large-scale data, where the embedding model is initialized with a pretrained model and then fine-tuned on a target dataset. Extensive experiments show that our method significantly improves individual models as well as their ensemble. Finally, the proposed method with a conventional triplet loss achieves the state-of-the-art performance of Recall@1 on standard datasets: 69.9 on CUB-200-2011 and 89.1 on CARS-196.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：相互学习是个体知识传递给对方，同时多种训练模式来提高泛化合奏培训战略。在这项工作中，我们提出了深刻的度量学习的有效相互学习方法，称为多元互助度量学习，增强多元化相互学习嵌入模型。 （1）从型号不同的初始化模式的多样性，（2）从参数更新的频率不同时间分集，和（3）从不同扩充观点的多样性：我们通过利用3种相互学习多样性的转让对深度量学习的关系知识输入。我们的方法是特别足以感应传输在缺乏大规模数据的，其中所述嵌入模型初始化为预训练的模型和一个目标数据集然后微调学习。大量的实验表明，我们的方法显著提高个别型号以及它们的合奏。最后，所提出的方法与常规的三重态损耗达到召回的标准数据集的状态的最先进的性能@ 1：69.9上CUB-200-2011和89.1上CARS-196。</font>
</div>


<hr>
<div id="paper10"> <b>10. One-shot Text Field Labeling using Attention and Belief Propagation for  Structure Information Extraction</b>  <a href="https://arxiv.org/pdf/2009.04153" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mengli Cheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minghui Qiu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xing Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Lin</a><br>
<font size="3">
 Abstract: Structured information extraction from document images usually consists of three steps: text detection, text recognition, and text field labeling. While text detection and text recognition have been heavily studied and improved a lot in literature, text field labeling is less explored and still faces many challenges. Existing learning based methods for text labeling task usually require a large amount of labeled examples to train a specific model for each type of document. However, collecting large amounts of document images and labeling them is difficult and sometimes impossible due to privacy issues. Deploying separate models for each type of document also consumes a lot of resources. Facing these challenges, we explore one-shot learning for the text field labeling task. Existing one-shot learning methods for the task are mostly rule-based and have difficulty in labeling fields in crowded regions with few landmarks and fields consisting of multiple separate text regions. To alleviate these problems, we proposed a novel deep end-to-end trainable approach for one-shot text field labeling, which makes use of attention mechanism to transfer the layout information between document images. We further applied conditional random field on the transferred layout information for the refinement of field labeling. We collected and annotated a real-world one-shot field labeling dataset with a large variety of document types and conducted extensive experiments to examine the effectiveness of the proposed model. To stimulate research in this direction, the collected dataset and the one-shot model will be released1.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：从文档图像结构化信息提取通常包括三个步骤：文本检测，文字识别，和文本字段标记。虽然文本检测和文字识别已经大量研究和改进了很多文献，文本字段标签较少探索，仍面临诸多挑战。文本标签制作任务的现有的学习基础的方法通常需要大量的标识样本培养出具体型号为每种类型的文件。然而，收集了大量的文本图像和标签他们是困难，有时甚至不可能的，因为隐私问题。部署单独的模型为每种类型的文件也消耗了大量的资源。面对这些挑战，我们探索一次性学习的文本字段标签制作任务。大多是基于规则的任务现有一次性学习方法，并在与几个标志性建筑和领域由多个单独的文本区域的拥挤区域标记领域的难度。为了缓解这些问题，我们提出了一次性文本字段标签，这使得使用中注意的机制传递文档图像之间的布局信息的新的深底到终端的可训练的方法。我们在现场贴标签的细化转移布局信息进一步应用条件随机场。我们收集并注明真实世界的一次性场标签数据集与大量的文档类型，并进行了大量的实验研究了该模型的有效性。为了促进研究在这个方向上，所收集的数据集和单稳态模式将released1。</font>
</div>


<hr>
<div id="paper11"> <b>11. Real-time Plant Health Assessment Via Implementing Cloud-based Scalable  Transfer Learning On AWS DeepLens</b>  <a href="https://arxiv.org/pdf/2009.04110" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Khan%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Asim Khan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nawaz%2C+U" target="_blank" rel="noopener" style="color:#0000EE;">Umair Nawaz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ulhaq%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anwaar Ulhaq</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Robinson%2C+R+W" target="_blank" rel="noopener" style="color:#0000EE;">Randall W. Robinson</a><br>
<font size="3">
 Abstract: In the Agriculture sector, control of plant leaf diseases is crucial as it influences the quality and production of plant species with an impact on the economy of any country. Therefore, automated identification and classification of plant leaf disease at an early stage is essential to reduce economic loss and to conserve the specific species. Previously, to detect and classify plant leaf disease, various Machine Learning models have been proposed; however, they lack usability due to hardware incompatibility, limited scalability and inefficiency in practical usage. Our proposed DeepLens Classification and Detection Model (DCDM) approach deal with such limitations by introducing automated detection and classification of the leaf diseases in fruits (apple, grapes, peach and strawberry) and vegetables (potato and tomato) via scalable transfer learning on AWS SageMaker and importing it on AWS DeepLens for real-time practical usability. Cloud integration provides scalability and ubiquitous access to our approach. Our experiments on extensive image data set of healthy and unhealthy leaves of fruits and vegetables showed an accuracy of 98.78% with a real-time diagnosis of plant leaves diseases. We used forty thousand images for the training of deep learning model and then evaluated it on ten thousand images. The process of testing an image for disease diagnosis and classification using AWS DeepLens on average took 0.349s, providing disease information to the user in less than a second.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在农业部门，植物叶部病害的控制是至关重要的，因为它会影响质量和生产植物物种与任何国家的经济产生影响。因此，在早期阶段自动识别和植物叶疾病的分类是必不可少的，以减少经济损失，并保护的特定物种。此前，用于检测和分类植物叶病，各种机器学习模型已被提出;然而，他们缺乏实用性由于硬件不兼容，有限的可扩展性和效率低下的实际应用。我们提出的DeepLens分类检测模型（DCDM）的方式处理这样的限制由在水果（苹果，葡萄，桃和草莓），蔬菜上AWS SageMaker引入叶疾病的自动检测和分类经由可伸缩的迁移学习（马铃薯和番茄）和进口它AWS DeepLens实时实际可用性。云集成提供了可扩展性和我们的做法普遍接入。我们对水果和蔬菜的健康和不健康的树叶大量图像数据集的实验表明的98.78％的准确度与植物叶子疾病的实时诊断。我们用四万图像进行深度学习模型的训练，然后评估它对于1万倍的图像。使用平均AWS DeepLens测试对于疾病的诊断和分类的图像的过程花费0.349s，在不到一秒钟的疾病提供信息给用户。</font>
</div>


<hr>
<div id="paper12"> <b>12. Deep Metric Learning Meets Deep Clustering: An Novel Unsupervised  Approach for Feature Embedding</b>  <a href="https://arxiv.org/pdf/2009.04091" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+B+X" target="_blank" rel="noopener" style="color:#0000EE;">Binh X. Nguyen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+B+D" target="_blank" rel="noopener" style="color:#0000EE;">Binh D. Nguyen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Carneiro%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gustavo Carneiro</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tjiputra%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Erman Tjiputra</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+Q+D" target="_blank" rel="noopener" style="color:#0000EE;">Quang D. Tran</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Do%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thanh-Toan Do</a><br>
<font size="3">
 Abstract: Unsupervised Deep Distance Metric Learning (UDML) aims to learn sample similarities in the embedding space from an unlabeled dataset. Traditional UDML methods usually use the triplet loss or pairwise loss which requires the mining of positive and negative samples w.r.t. anchor data points. This is, however, challenging in an unsupervised setting as the label information is not available. In this paper, we propose a new UDML method that overcomes that challenge. In particular, we propose to use a deep clustering loss to learn centroids, i.e., pseudo labels, that represent semantic classes. During learning, these centroids are also used to reconstruct the input samples. It hence ensures the representativeness of centroids - each centroid represents visually similar samples. Therefore, the centroids give information about positive (visually similar) and negative (visually dissimilar) samples. Based on pseudo labels, we propose a novel unsupervised metric loss which enforces the positive concentration and negative separation of samples in the embedding space. Experimental results on benchmarking datasets show that the proposed approach outperforms other UDML methods.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：无监督深度距离度量学习（UDML）旨在从一个未标记的数据集学习中嵌入空间样品相似之处。传统UDML方法通常使用需要阳性和阴性样品的采矿三重损失或成对损失w.r.t.锚数据点。这，然而，在无人监督的设置作为标签信息挑战不可用。在本文中，我们提出了一种新的方法UDML克服这一挑战。特别是，我们建议使用一个深集群丧失学习重心，即伪标签，代表的语义类。在学习中，这些质心也用于重建输入样本。它因此确保质心的代表性 - 每个质心表示视觉上相似的样品。因此，质心给出关于正（视觉上相似的）和负极（视觉上不相似的）采样的信息。基于伪标签，我们提出了一种新的无监督度量损失，这强制正浓度和嵌入空间样本的负分离。标杆数据集实验结果表明，该方法优于其他UDML方法。</font>
</div>


<hr>
<div id="paper13"> <b>13. View-consistent 4D Light Field Depth Estimation</b>  <a href="https://arxiv.org/pdf/2009.04065" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Khan%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Numair Khan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+M+H" target="_blank" rel="noopener" style="color:#0000EE;">Min H. Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tompkin%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">James Tompkin</a><br>
<font size="3">
 Abstract: We propose a method to compute depth maps for every sub-aperture image in a light field in a view consistent way. Previous light field depth estimation methods typically estimate a depth map only for the central sub-aperture view, and struggle with view consistent estimation. Our method precisely defines depth edges via EPIs, then we diffuse these edges spatially within the central view. These depth estimates are then propagated to all other views in an occlusion-aware way. Finally, disoccluded regions are completed by diffusion in EPI space. Our method runs efficiently with respect to both other classical and deep learning-based approaches, and achieves competitive quantitative metrics and qualitative performance on both synthetic and real-world light fields   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们建议在光场来计算深度贴图为每个子孔径图像的方法的观点是一致的方式。先前的光场的深度估计方法通常仅估计为中央子孔径视图的深度图，并用斗争观点一致估计。我们的方法精确地经由环境绩效指标定义深度边缘，那么我们中央视图内的空间扩散这些边缘。然后，这些深度估计会传播到闭塞感知方式，所有其他视图。最后，disoccluded区域由在EPI空间扩散完成。我们的方法相对于其他两种古典与深学习型方式高效运行，实现对合成和真实世界的光场竞争性定量指标和定性性能</font>
</div>


<hr>
<div id="paper14"> <b>14. Improved Trainable Calibration Method for Neural Networks on Medical  Imaging Classification</b>  <a href="https://arxiv.org/pdf/2009.04057" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gongbo Liang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoqin Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jacobs%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nathan Jacobs</a><br>
<font size="3">
 Abstract: Recent works have shown that deep neural networks can achieve super-human performance in a wide range of image classification tasks in the medical imaging domain. However, these works have primarily focused on classification accuracy, ignoring the important role of uncertainty quantification. Empirically, neural networks are often miscalibrated and overconfident in their predictions. This miscalibration could be problematic in any automatic decision-making system, but we focus on the medical field in which neural network miscalibration has the potential to lead to significant treatment errors. We propose a novel calibration approach that maintains the overall classification accuracy while significantly improving model calibration. The proposed approach is based on expected calibration error, which is a common metric for quantifying miscalibration. Our approach can be easily integrated into any classification task as an auxiliary loss term, thus not requiring an explicit training round for calibration. We show that our approach reduces calibration error significantly across various architectures and datasets.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近的工作表明，深层神经网络可以在医疗成像领域广泛的图像分类任务实现超人类的表现。然而，这些作品主要集中于分类的准确性，忽视不确定性量化的重要作用。根据经验，神经网络常常误校准，并在他们的预测过于自信。这种失准可能在任何自动决策体系有问题，但我们专注于在神经网络的失准有导致显著治疗误区的潜在医疗领域。我们建议维持整体分类准确度，同时显著改善模型校准一个新的标定方法。所提出的方法是基于预期的校准误差，这是量化失准的共同指标。我们的方法可以很容易地集成到任何分类任务作为辅助损耗项，因此不需要校准明确的训练轮。我们证明了我们的方法在不同的体系结构和数据集显著减少了校准误差。</font>
</div>


<hr>
<div id="paper15"> <b>15. Unconstrained Text Detection in Manga: a New Dataset and Baseline</b>  <a href="https://arxiv.org/pdf/2009.04042" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Del+Gobbo%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Julián Del Gobbo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Herrera%2C+R+M" target="_blank" rel="noopener" style="color:#0000EE;">Rosana Matuk Herrera</a><br>
<font size="3">
 Abstract: The detection and recognition of unconstrained text is an open problem in research. Text in comic books has unusual styles that raise many challenges for text detection. This work aims to binarize text in a comic genre with highly sophisticated text styles: Japanese manga. To overcome the lack of a manga dataset with text annotations at a pixel level, we create our own. To improve the evaluation and search of an optimal model, in addition to standard metrics in binarization, we implement other special metrics. Using these resources, we designed and evaluated a deep network model, outperforming current methods for text binarization in manga in most metrics.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：检测与识别不受约束的文本是在研究一个开放的问题。在漫画书文字有着不同寻常的风格，提高对文本检测许多挑战。这项工作旨在以二进制化文本的体裁漫画与高度复杂的文本样式：日本漫画。为了克服缺少文本注释的漫画集的在像素级，我们创造我们自己。为了提高评估和寻找一个最佳的模式，除了在二值化标准的指标，我们实行其他特别指标。利用这些资源，我们设计并评估了深刻的网络模型，超越了大多数指标在漫画文本二值化目前的方法。</font>
</div>


<hr>
<div id="paper16"> <b>16. Modeling Wildfire Perimeter Evolution using Deep Neural Networks</b>  <a href="https://arxiv.org/pdf/2009.03977" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Green%2C+M+E" target="_blank" rel="noopener" style="color:#0000EE;">Maxfield E. Green</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kaiser%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Karl Kaiser</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shenton%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nat Shenton</a><br>
<font size="3">
 Abstract: With the increased size and frequency of wildfire eventsworldwide, accurate real-time prediction of evolving wildfirefronts is a crucial component of firefighting efforts and for-est management practices. We propose a wildfire spreadingmodel that predicts the evolution of the wildfire perimeter in24 hour periods. The fire spreading simulation is based ona deep convolutional neural network (CNN) that is trainedon remotely sensed atmospheric and environmental time se-ries data. We show that the model is able to learn wildfirespreading dynamics from real historic data sets from a seriesof wildfires in the Western Sierra Nevada Mountains in Cal-ifornia. We validate the model on a previously unseen wild-fire and produce realistic results that significantly outperformhistoric alternatives with validation accuracies ranging from78% 98%   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着增加的大小和野火eventsworldwide的频率，不断发展wildfirefronts的准确的实时预测是消防工作的重要组成部分和-EST管理实践。我们提出了一个野火spreadingmodel，预测野火周边IN24小时周期的演变。火势蔓延模拟基于奥纳是trainedon遥感大气和环境的时间SE-RIES数据深卷积神经网络（CNN）。我们表明，该模型能够学到真实的历史数据集wildfirespreading动力从西内华达山脉seriesof野火在加州，ifornia。我们验证一个前所未见的野火模型，并产生实际结果与验证精度显著outperformhistoric替代范围from78％98％</font>
</div>


<hr>
<div id="paper17"> <b>17. Joint Pose and Shape Estimation of Vehicles from LiDAR Data</b>  <a href="https://arxiv.org/pdf/2009.03964" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Goforth%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hunter Goforth</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoyan Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Happold%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Happold</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lucey%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simon Lucey</a><br>
<font size="3">
 Abstract: We address the problem of estimating the pose and shape of vehicles from LiDAR scans, a common problem faced by the autonomous vehicle community. Recent work has tended to address pose and shape estimation separately in isolation, despite the inherent connection between the two. We investigate a method of jointly estimating shape and pose where a single encoding is learned from which shape and pose may be decoded in an efficient yet effective manner. We additionally introduce a novel joint pose and shape loss, and show that this joint training method produces better results than independently-trained pose and shape estimators. We evaluate our method on both synthetic data and real-world data, and show superior performance against a state-of-the-art baseline.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：针对估算激光雷达扫描，所面临的自主汽车社会面临的共同问题的姿态和车辆形状的问题。最近的工作已经趋于地址姿势和孤立的单独形状估计，尽管两者之间的内在联系。我们研究推定共同形状的方法和姿势其中单个编码了解到从形状和姿势可以高效而有效的方式进行解码。我们还引进了新的联合姿势和体形的损失，并表明该联合训练方法产生比单独训练的姿势和体形估计更好的结果。我们评估我们的模拟数据和真实数据的方法，并显示出对国家的最先进的基准性能优越。</font>
</div>


<hr>
<div id="paper18"> <b>18. Towards Unique and Informative Captioning of Images</b>  <a href="https://arxiv.org/pdf/2009.03949" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zeyu Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Berthy Feng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Narasimhan%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Karthik Narasimhan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Russakovsky%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Olga Russakovsky</a><br>
<font size="3">
 Abstract: Despite considerable progress, state of the art image captioning models produce generic captions, leaving out important image details. Furthermore, these systems may even misrepresent the image in order to produce a simpler caption consisting of common concepts. In this paper, we first analyze both modern captioning systems and evaluation metrics through empirical experiments to quantify these phenomena. We find that modern captioning systems return higher likelihoods for incorrect distractor sentences compared to ground truth captions, and that evaluation metrics like SPICE can be 'topped' using simple captioning systems relying on object detectors. Inspired by these observations, we design a new metric (SPICE-U) by introducing a notion of uniqueness over the concepts generated in a caption. We show that SPICE-U is better correlated with human judgements compared to SPICE, and effectively captures notions of diversity and descriptiveness. Finally, we also demonstrate a general technique to improve any existing captioning model -- by using mutual information as a re-ranking objective during decoding. Empirically, this results in more unique and informative captions, and improves three different state-of-the-art models on SPICE-U as well as average score over existing metrics.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管有相当大的进步，艺术形象字幕车型的状态产生通用字幕，而忽略了重要的图像细节。此外，这些系统可能甚至歪曲以便产生更简单的字幕组成的共同概念的图像。在本文中，我们首先分析既现代字幕系统和评价指标，通过实证实验来量化这些现象。我们发现，现代的字幕系统相对于地面实况字幕不正确牵张句子返回高可能性，像SPICE该评价指标可以是“突破”使用简单的字幕系统依靠对象探测器。由这些观察结果的鼓舞，我们通过引入独特性在以上的字幕生成的概念的概念设计一个新的度量（SPICE-U）。我们证明了SPICE-U是更好地相比，SPICE人判断，多样性和描述性的有效捕获概念相关。最后，我们还展示一个通用的技术来提高现有的字幕模型 - 利用互信息作为重新排名解码过程中的目标。根据经验，这会导致更多的独特和翔实的字幕，并提高了SPICE-U三个不同的国家的最先进的车型，以及超过现有指标平均分。</font>
</div>


<hr>
<div id="paper19"> <b>19. Map-Adaptive Goal-Based Trajectory Prediction</b>  <a href="https://arxiv.org/pdf/2009.04450" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lingyao Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Su%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Po-Hsun Su</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hoang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jerrick Hoang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Haynes%2C+G+C" target="_blank" rel="noopener" style="color:#0000EE;">Galen Clark Haynes</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marchetti-Bowick%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Micol Marchetti-Bowick</a><br>
<font size="3">
 Abstract: We present a new method for multi-modal, long-term vehicle trajectory prediction. Our approach relies on using lane centerlines captured in rich maps of the environment to generate a set of proposed goal paths for each vehicle. Using these paths -- which are generated at run time and therefore dynamically adapt to the scene -- as spatial anchors, we predict a set of goal-based trajectories along with a categorical distribution over the goals. This approach allows us to directly model the goal-directed behavior of traffic actors, which unlocks the potential for more accurate long-term prediction. Our experimental results on both a large-scale internal driving dataset and on the public nuScenes dataset show that our model outperforms state-of-the-art approaches for vehicle trajectory prediction over a 6-second horizon. We also empirically demonstrate that our model is better able to generalize to road scenes from a completely new city than existing methods.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种新的方法用于多模态的，长期的车辆轨迹预测。我们的方法依赖于使用中丰富环境的地图捕捉车道中心线，以生成一组每辆车提出的目标路径。使用这些路径 - 这是在运行时产生的，因此动态适应场景 - 作为空间锚，我们预计有超过目标的一个类别分布沿一套基于目标的轨迹。这种方法允许我们直接模型的交通参与者的目标导向行为，这样就打开了更准确的长期预测的潜力。我们两个大规模的内在驱动数据集，并在公共nuScenes数据集上，我们的模型优于国家的最先进的车辆轨迹预测在6秒地平线接近实验结果。我们也经验表明，我们的模型能够更好地从一个全新的城市比现有的方法推广到道路场景。</font>
</div>


<hr>
<div id="paper20"> <b>20. not-so-BigGAN: Generating High-Fidelity Images on a Small Compute Budget</b>  <a href="https://arxiv.org/pdf/2009.04433" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Han%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Seungwook Han</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Srivastava%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Akash Srivastava</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Hurwitz%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cole Hurwitz</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Sattigeri%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Prasanna Sattigeri</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Cox%2C+D+D" target="_blank" rel="noopener" style="color:#0000EE;">David D. Cox</a><br>
<font size="3">
 Abstract: BigGAN is the state-of-the-art in high-resolution image generation, successfully leveraging advancements in scalable computing and theoretical understanding of generative adversarial methods to set new records in conditional image generation. A major part of BigGAN's success is due to its use of large mini-batch sizes during training in high dimensions. While effective, this technique requires an incredible amount of compute resources and/or time (256 TPU-v3 Cores), putting the model out of reach for the larger research community. In this paper, we present not-so-BigGAN, a simple and scalable framework for training deep generative models on high-dimensional natural images. Instead of modelling the image in pixel space like in BigGAN, not-so-BigGAN uses wavelet transformations to bypass the curse of dimensionality, reducing the overall compute requirement significantly. Through extensive empirical evaluation, we demonstrate that for a fixed compute budget, not-so-BigGAN converges several times faster than BigGAN, reaching competitive image quality with an order of magnitude lower compute budget (4 Telsa-V100 GPUs).   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：BigGAN是国家的最先进的高分辨率图像生成，可扩展计算和生成性的对抗方法理论的理解成功地撬动进步来设置条件图像生成新的记录。 BigGAN成功的一个重要组成部分是由于其在高维训练中使用大小批量的大小。尽管有效，但这种技术需要的计算资源和/或时间（256 TPU-V3核心）的数量惊人，把模型遥不可及了较大的研究团体。在本文中，我们提出不那么BigGAN，对高维自然影像培养深厚生成模型简单，可扩展的框架。代替在像素空间的图像建模像BigGAN的，不那么BigGAN使用小波变换来旁路维数灾难，显著减少了总计算要求。通过大量的实证评价，我们证明了一个固定的计算预算，不那么BigGAN收敛数倍于BigGAN快，达到了有竞争力的图像质量与较低的幅度计算预算（4特斯拉-V100的GPU）的顺序。</font>
</div>


<hr>
<div id="paper21"> <b>21. Cephalogram Synthesis and Landmark Detection in Dental Cone-Beam CT  Systems</b>  <a href="https://arxiv.org/pdf/2009.04420" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Huang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yixing Huang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Fan%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fuxin Fan</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Syben%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christopher Syben</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Roser%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Philipp Roser</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Mill%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Leonid Mill</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Maier%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andreas Maier</a><br>
<font size="3">
 Abstract: Due to the lack of standardized 3D cephalometric analytic methodology, 2D cephalograms synthesized from 3D cone-beam computed tomography (CBCT) volumes are widely used for cephalometric analysis in dental CBCT systems. However, compared with conventional X-ray film based cephalograms, such synthetic cephalograms lack image contrast and resolution. In addition, the radiation dose during the scan for 3D reconstruction causes potential health risks. In this work, we propose a sigmoid-based intensity transform that uses the nonlinear optical property of X-ray films to increase image contrast of synthetic cephalograms. To improve image resolution, super resolution deep learning techniques are investigated. For low dose purpose, the pixel-to-pixel generative adversarial network (pix2pixGAN) is proposed for 2D cephalogram synthesis directly from two CBCT projections. For landmark detection in the synthetic cephalograms, an efficient automatic landmark detection method using the combination of LeNet-5 and ResNet50 is proposed. Our experiments demonstrate the efficacy of pix2pixGAN in 2D cephalogram synthesis, achieving an average peak signal-to-noise ratio (PSNR) value of 33.8 with reference to the cephalograms synthesized from 3D CBCT volumes. Pix2pixGAN also achieves the best performance in super resolution, achieving an average PSNR value of 32.5 without the introduction of checkerboard or jagging artifacts. Our proposed automatic landmark detection method achieves 86.7% successful detection rate in the 2 mm clinical acceptable range on the ISBI Test1 data, which is comparable to the state-of-the-art methods. The method trained on conventional cephalograms can be directly applied to landmark detection in the synthetic cephalograms, achieving 93.0% and 80.7% successful detection rate in 4 mm precision range for synthetic cephalograms from 3D volumes and 2D projections respectively.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：由于缺乏标准化3D头部测量分析的方法学，2D测颅从三维锥形束计算机断层摄影合成（CBCT）卷被广泛地用于在牙科CBCT系统测量分析。然而，与传统的X射线胶片基于测颅相比，这种合成测颅缺乏图像对比度和分辨率。此外，扫描三维重建过程中的辐射剂量会导致潜在的健康风险。在这项工作中，我们提出基于乙状结肠强度变换使用X射线胶片的非线性光学性质，以增加合成测颅的图像对比度。为了提高图像分辨率，超分辨率深学习技术进行了研究。对于低剂量的目的，像素到像素的生成对抗网络（pix2pixGAN）提出了一种用于2D测颅X直接从两个CBCT突起合成。对于标志检测在合成测颅，使用LeNet-5和ResNet50的组合的有效自动标志检测方法，提出了我们的实验证明pix2pixGAN在2D测颅X合成的功效，参照从三维CBCT卷合成的测颅实现的平均峰值信噪比（PSNR）的33.8的值。 Pix2pixGAN也实现了超分辨率的最佳性能，无需引入棋盘或锯齿文物达到32.5的平均PSNR值。我们提出的自动标志检测方法实现了对ISBI测试1数据，这与国家的最先进的方法2毫米临床上可接受的范围为86.7％成功检测率。上训练常规测颅的方法可直接应用于标志检测在合成测颅，实现93.0％和80.7％的成功检测率在4毫米精度范围从分别3D体积和2D投影合成的测颅。</font>
</div>


<hr>
<div id="paper22"> <b>22. NTGAN: Learning Blind Image Denoising without Clean Reference</b>  <a href="https://arxiv.org/pdf/2009.04286" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhao%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rui Zhao</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Lun%2C+D+P+K" target="_blank" rel="noopener" style="color:#0000EE;">Daniel P.K. Lun</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Lam%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kin-Man Lam</a><br>
<font size="3">
 Abstract: Recent studies on learning-based image denoising have achieved promising performance on various noise reduction tasks. Most of these deep denoisers are trained either under the supervision of clean references, or unsupervised on synthetic noise. The assumption with the synthetic noise leads to poor generalization when facing real photographs. To address this issue, we propose a novel deep unsupervised image-denoising method by regarding the noise reduction task as a special case of the noise transference task. Learning noise transference enables the network to acquire the denoising ability by only observing the corrupted samples. The results on real-world denoising benchmarks demonstrate that our proposed method achieves state-of-the-art performance on removing realistic noises, making it a potential solution to practical noise reduction problems.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于学习图像去噪最近的研究已经取得了各种降噪任务有前途的性能。大多数这些深denoisers都干净引用的监督下，无论是受过训练的，或非监督上合成的噪音。用合成的噪音导致泛化差的假设面临真正的照片时。为了解决这个问题，我们建议对于降噪的任务，因为噪声转移任务的特殊情况，一种新型的无监督的深层图像去噪方法。学习噪声转移使网络仅观察变质取样获取去噪能力。现实世界的去噪基准测试结果表明，我们提出的方法实现对消除噪音的现实，使其成为一个潜在的解决方案，以实际降噪问题的国家的最先进的性能。</font>
</div>


<hr>
<div id="paper23"> <b>23. Small-floating Target Detection in Sea Clutter via Visual Feature  Classifying in the Time-Doppler Spectra</b>  <a href="https://arxiv.org/pdf/2009.04185" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhou%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yi Zhou</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Cui%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yin Cui</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Xu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoke Xu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Suo%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jidong Suo</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoming Liu</a><br>
<font size="3">
 Abstract: It is challenging to detect small-floating object in the sea clutter for a surface radar. In this paper, we have observed that the backscatters from the target brake the continuity of the underlying motion of the sea surface in the time-Doppler spectra (TDS) images. Following this visual clue, we exploit the local binary pattern (LBP) to measure the variations of texture in the TDS images. It is shown that the radar returns containing target and those only having clutter are separable in the feature space of LBP. An unsupervised one-class support vector machine (SVM) is then utilized to detect the deviation of the LBP histogram of the clutter. The outiler of the detector is classified as the target. In the real-life IPIX radar data sets, our visual feature based detector shows favorable detection rate compared to other three existing approaches.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：这是具有挑战性的检测海杂波的表面雷达小浮动对象。在本文中，我们已经观察到，从目标制动的反向散射海面的在时多普勒频谱（TDS）的图像的基础运动的连续性。在此之后的视觉线索，我们利用了局部二元模式（LBP）来测量TDS图像纹理的变化。结果表明，含有靶和那些只具有杂波的雷达回波在LBP的特征空间分离。然后无监督一个类支持向量机（SVM）是用来检测所述杂波的LBP直方图的偏差。检测器的outiler被分类为目标。在现实生活中的IPIX雷达数据集，我们的视觉特征基于探测器显示良好的检出率相对于其他三个现有的方法。</font>
</div>


<hr>
<div id="paper24"> <b>24. Revealing Lung Affections from CTs. A Comparative Analysis of Various  Deep Learning Approaches for Dealing with Volumetric Data</b>  <a href="https://arxiv.org/pdf/2009.04160" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Miron%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Radu Miron</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Moisii%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cosmin Moisii</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Breaban%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mihaela Breaban</a><br>
<font size="3">
 Abstract: The paper presents and comparatively analyses several deep learning approaches to automatically detect tuberculosis related lesions in lung CTs, in the context of the ImageClef 2020 Tuberculosis task. Three classes of methods, different with respect to the way the volumetric data is given as input to neural network-based classifiers are discussed and evaluated. All these come with a rich experimental analysis comprising a variety of neural network architectures, various segmentation algorithms and data augmentation schemes. The reported work belongs to the SenticLab.UAIC team, which obtained the best results in the competition.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文介绍和比较分析了几种深的学习方法来自动检测肺CT的肺结核病灶相关，在ImageClef 2020结核病任务的上下文。三个类的方法，相对于所述体积数据被给定为输入到基于神经网络的分类器的方式不同的讨论和评价。所有这些都配有丰富的实验分析，包括各种神经网络体系结构，各个分割算法和数据扩张方案。报告的工作属于SenticLab.UAIC队，获得比赛的最好成绩。</font>
</div>


<hr>
<div id="paper25"> <b>25. Single Image Super-Resolution for Domain-Specific Ultra-Low Bandwidth  Image Transmission</b>  <a href="https://arxiv.org/pdf/2009.04127" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Christensen%2C+J+H" target="_blank" rel="noopener" style="color:#0000EE;">Jesper Haahr Christensen</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Mogensen%2C+L+V" target="_blank" rel="noopener" style="color:#0000EE;">Lars Valdemar Mogensen</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Ravn%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Ole Ravn</a><br>
<font size="3">
 Abstract: Low-bandwidth communication, such as underwater acoustic communication, is limited by best-case data rates of 30--50 kbit/s. This renders such channels unusable or inefficient at best for single image, video, or other bandwidth-demanding sensor-data transmission. To combat data-transmission bottlenecks, we consider practical use-cases within the maritime domain and investigate the prospect of Single Image Super-Resolution methodologies. This is investigated on a large, diverse dataset obtained during years of trawl fishing where cameras have been placed in the fishing nets. We propose down-sampling images to a low-resolution low-size version of about 1 kB that satisfies underwater acoustic bandwidth requirements for even several frames per second. A neural network is then trained to perform up-sampling, trying to reconstruct the original image. We aim to investigate the quality of reconstructed images and prospects for such methods in practical use-cases in general. Our focus in this work is solely on learning to reconstruct the high-resolution images on "real-world" data. We show that our method achieves better perceptual quality and superior reconstruction than generic bicubic up-sampling and motivates further work in this area for underwater applications.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：低带宽通信，例如水声通信，由30--50千比特的最佳情况的数据速率限制/秒。这使得这样的信道不可用的或低效率充其量为单个图像，视频或其他带宽要求传感器数据的传输。打击数据传输的瓶颈，我们认为海洋领域内的实际使用情况，并调查单幅图像的超分辨率方法的前景。这是对在多年拖网捕鱼时，其中摄像机被放置在渔网获得大量的，多样化的数据集调查。我们建议下采样图像的约1 KB的低分辨率低尺寸版本，满足水下每秒甚至几帧声带宽需求。然后，训练神经网络来执行上采样，试图重构原始图像。我们的目标是调查重建图像和前景的用于一般实际使用情况等方法的质量。我们在此工作的重点仅仅是学习到的“真实世界”的数据重构高分辨率图像。我们证明了我们的方法获得更好的感知质量和卓越的重建比在这方面的通用双三次上采样和激励进一步工作水下应用。</font>
</div>


<hr>
<div id="paper26"> <b>26. Generalizing Complex/Hyper-complex Convolutions to Vector Map  Convolutions</b>  <a href="https://arxiv.org/pdf/2009.04083" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gaudet%2C+C+J" target="_blank" rel="noopener" style="color:#0000EE;">Chase J Gaudet</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Maida%2C+A+S" target="_blank" rel="noopener" style="color:#0000EE;">Anthony S Maida</a><br>
<font size="3">
 Abstract: We show that the core reasons that complex and hypercomplex valued neural networks offer improvements over their real-valued counterparts is the weight sharing mechanism and treating multidimensional data as a single entity. Their algebra linearly combines the dimensions, making each dimension related to the others. However, both are constrained to a set number of dimensions, two for complex and four for quaternions. Here we introduce novel vector map convolutions which capture both of these properties provided by complex/hypercomplex convolutions, while dropping the unnatural dimensionality constraints they impose. This is achieved by introducing a system that mimics the unique linear combination of input dimensions, such as the Hamilton product for quaternions. We perform three experiments to show that these novel vector map convolutions seem to capture all the benefits of complex and hyper-complex networks, such as their ability to capture internal latent relations, while avoiding the dimensionality restriction.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们发现，核心原因复杂和超复数重视神经网络提供的改进在他们的实值是同行中的权重共享机制和治疗多维数据作为一个单一的实体。他们的线性代数结合的尺寸，使得相关的其他每个维度。但是，这两个被限制为维度的一组数字，两个用于复杂和四个为四元数。在这里，我们引入新颖的矢量地图卷积其中两个由复杂/超复数卷积提供这些性质的捕获，同时丢弃所述非天然维约束它们强加。这是通过引入一个系统来实现的，模仿输入的尺寸，如汉密尔顿产物为四元数的独特线性组合。我们进行三个实验表明，这些新的矢量地图回旋似乎捕获复杂和超复杂网络的所有优势，如他们捕捉潜在的内部关系，同时也避免了维数限制的能力。</font>
</div>


<hr>
<div id="paper27"> <b>27. Learning joint segmentation of tissues and brain lesions from  task-specific hetero-modal domain-shifted datasets</b>  <a href="https://arxiv.org/pdf/2009.04009" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Dorent%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Reuben Dorent</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Booth%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Booth</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Li%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenqi Li</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Sudre%2C+C+H" target="_blank" rel="noopener" style="color:#0000EE;">Carole H. Sudre</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Kafiabadi%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sina Kafiabadi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Cardoso%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jorge Cardoso</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Ourselin%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sebastien Ourselin</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Vercauteren%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tom Vercauteren</a><br>
<font size="3">
 Abstract: Brain tissue segmentation from multimodal MRI is a key building block of many neuroimaging analysis pipelines. Established tissue segmentation approaches have, however, not been developed to cope with large anatomical changes resulting from pathology, such as white matter lesions or tumours, and often fail in these cases. In the meantime, with the advent of deep neural networks (DNNs), segmentation of brain lesions has matured significantly. However, few existing approaches allow for the joint segmentation of normal tissue and brain lesions. Developing a DNN for such a joint task is currently hampered by the fact that annotated datasets typically address only one specific task and rely on task-specific imaging protocols including a task-specific set of imaging modalities. In this work, we propose a novel approach to build a joint tissue and lesion segmentation model from aggregated task-specific hetero-modal domain-shifted and partially-annotated datasets. Starting from a variational formulation of the joint problem, we show how the expected risk can be decomposed and optimised empirically. We exploit an upper bound of the risk to deal with heterogeneous imaging modalities across datasets. To deal with potential domain shift, we integrated and tested three conventional techniques based on data augmentation, adversarial learning and pseudo-healthy generation. For each individual task, our joint approach reaches comparable performance to task-specific and fully-supervised models. The proposed framework is assessed on two different types of brain lesions: White matter lesions and gliomas. In the latter case, lacking a joint ground-truth for quantitative assessment purposes, we propose and use a novel clinically-relevant qualitative assessment methodology.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：从多式联运MRI脑组织分割是许多神经影像学分析管道的重要组成部分。成立组织分割方法已经，但是，尚未被开发，以应对从病理产生大的解剖结构变化，如白质病变或肿瘤，并且经常在这些情况下失败。在此期间，具有深厚的神经网络（DNNs）的问世，脑部病变的分割已显著成熟。然而，现有的几个方案允许正常组织和脑部病变的关节分割。对于这样一个联合工作制定DNN当前的事实，数据集注释通常地址只有一个特定的任务，依赖于特定任务的成像协议，包括成像方式的特定任务集阻碍。在这项工作中，我们提出构建从聚集的特定任务的杂模态域移和部分标注数据集的联合组织和病变分割模型的新方法。从关节问题的变分列式开始，我们展示了预期风险如何可以分解和经验进行了优化。我们利用一个上限，以应对跨异构数据集的成像方式的风险。为了应对潜在领域转移，我们集成和基于数据增强，对抗学习和伪健康的一代测试了三种常规技术。对于每一个单独的任务，我们联合的方式达到相当的性能，以任务的具体和全面监督模式。拟议的框架评估在两个不同类型的脑病变：脑白质病变和神经胶质瘤。在后一种情况下，缺乏联合地面实况进行定量评估的目的，我们提出并使用了一种新的临床相关的定性评估方法。</font>
</div>


<hr>
<div id="paper28"> <b>28. Fuzzy Unique Image Transformation: Defense Against Adversarial Attacks  On Deep COVID-19 Models</b>  <a href="https://arxiv.org/pdf/2009.04004" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Tripathi%2C+A+M" target="_blank" rel="noopener" style="color:#0000EE;">Achyut Mani Tripathi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Mishra%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ashish Mishra</a><br>
<font size="3">
 Abstract: Early identification of COVID-19 using a deep model trained on Chest X-Ray and CT images has gained considerable attention from researchers to speed up the process of identification of active COVID-19 cases. These deep models act as an aid to hospitals that suffer from the unavailability of specialists or radiologists, specifically in remote areas. Various deep models have been proposed to detect the COVID-19 cases, but few works have been performed to prevent the deep models against adversarial attacks capable of fooling the deep model by using a small perturbation in image pixels. This paper presents an evaluation of the performance of deep COVID-19 models against adversarial attacks. Also, it proposes an efficient yet effective Fuzzy Unique Image Transformation (FUIT) technique that downsamples the image pixels into an interval. The images obtained after the FUIT transformation are further utilized for training the secure deep model that preserves high accuracy of the diagnosis of COVID-19 cases and provides reliable defense against the adversarial attacks. The experiments and results show the proposed model prevents the deep model against the six adversarial attacks and maintains high accuracy to classify the COVID-19 cases from the Chest X-Ray image and CT image Datasets. The results also recommend that a careful inspection is required before practically applying the deep models to diagnose the COVID-19 cases.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：COVID-19的早期识别使用训练的胸部X射线和CT图像的深层模型已经获得了相当大的关注，从研究人员加快的积极COVID-19案件识别过程。这些深层次的模型作为一个辅助，从专家或放射科医师的可用性遭受医院，特别是在偏远地区。已经提出了各种深模型来检测COVID-19的情况下，但很少工程已进行，以防止深模型对能够通过图像的像素用一个小的扰动愚弄深层模型的敌对攻击。本文介绍了深COVID-19模型的性能对敌对攻击的评估。此外，它提出了下采样的图像像素到时间间隔的有效而有效的模糊图像的唯一变换（FUIT）技术。在FUIT改造后获得的图像被进一步用于培训，保留的COVID-19例诊断准确度高，提供了对敌对攻击防御可靠安全的深层模型。该实验结果表明，该模型对防止六种对抗攻击的深层模型，并保持高精确度从胸部X射线图像和CT图像数据集的COVID-19个案分类。结果还建议仔细检查实际适用的深层模型诊断COVID-19案件之前需要。</font>
</div>


<hr>
<div id="paper29"> <b>29. Tangent Space Based Alternating Projections for Nonnegative Low Rank  Matrix Approximation</b>  <a href="https://arxiv.org/pdf/2009.03998" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guangjing Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ng%2C+M+K" target="_blank" rel="noopener" style="color:#0000EE;">Michael K. Ng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tai-Xiang Jiang</a><br>
<font size="3">
 Abstract: In this paper, we develop a new alternating projection method to compute nonnegative low rank matrix approximation for nonnegative matrices. In the nonnegative low rank matrix approximation method, the projection onto the manifold of fixed rank matrices can be expensive as the singular value decomposition is required. We propose to use the tangent space of the point in the manifold to approximate the projection onto the manifold in order to reduce the computational cost. We show that the sequence generated by the alternating projections onto the tangent spaces of the fixed rank matrices manifold and the nonnegative matrix manifold, converge linearly to a point in the intersection of the two manifolds where the convergent point is sufficiently close to optimal solutions. This convergence result based inexact projection onto the manifold is new and is not studied in the literature. Numerical examples in data clustering, pattern recognition and hyperspectral data analysis are given to demonstrate that the performance of the proposed method is better than that of nonnegative matrix factorization methods in terms of computational time and accuracy.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们开发了一个新的交替投影方法计算非负低阶矩阵近似非负矩阵。在非负低秩矩阵的近似方法，因为需要奇异值分解投射到固定秩矩阵的歧管可以是昂贵的。我们建议使用点的切线空间的歧管到投影逼近到歧管上，以减少计算成本。我们表明，由交流突起到固定秩的正切空间中产生的序列矩阵歧管和所述非负矩阵歧管，线性收敛到在两个歧管的交叉的点处会聚点足够接近最优解。此收敛结果基于不精确投影到歧管上是新的，在文献中没有研究。在数据聚类，模式识别和高光谱数据分析数值实施例是为了证明所提出的方法的性能比的中的计算时间和精度方面非负矩阵因式分解方法更好。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！封面为论文标题词云图！</font></p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>PROCJX
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://procjx.github.io/2020/09/10/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-09-10/" title="【arxiv论文】 Computer Vision and Pattern Recognition 2020-09-10">https://procjx.github.io/2020/09/10/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-09-10/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">

        
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2020/09/10/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-09-10/" rel="next" title="【arxiv论文】 Computation and Language 2020-09-10">
                  <i class="fa fa-chevron-left"></i> 【arxiv论文】 Computation and Language 2020-09-10
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/2020/09/14/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-09-14/" rel="prev" title="【arxiv论文】 Computation and Language 2020-09-14">
                  【arxiv论文】 Computation and Language 2020-09-14 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>

        
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang -->
<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9197824246"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="gitalk-container"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#目录"><span class="nav-number">1.</span> <span class="nav-text">目录</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#摘要"><span class="nav-number">2.</span> <span class="nav-text">摘要</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="PROCJX"
    src="/images/procjx.png">
  <p class="site-author-name" itemprop="name">PROCJX</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">334</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">71</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/procjx" title="GitHub &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;procjx" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:procjx@gmail.com" title="E-Mail &amp;rarr; mailto:procjx@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>


<!--
      
        <script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
        <script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
        <div class="widget-wrap">
            <h3 class="widget-title">标签云</h3>
            <div id="myCanvasContainer" class="widget tagcloud">
                <canvas width="250" height="250" id="resCanvas" style="width=100%">
                    <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AAAI/" rel="tag">AAAI</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ACL/" rel="tag">ACL</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Accepted-Papers/" rel="tag">Accepted Papers</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ArXiv/" rel="tag">ArXiv</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/" rel="tag">BERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS20SI/" rel="tag">CS20SI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS224d/" rel="tag">CS224d</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/" rel="tag">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CVPR/" rel="tag">CVPR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Context/" rel="tag">Context</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cross-Lingual/" rel="tag">Cross Lingual</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dialog-System/" rel="tag">Dialog System</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse/" rel="tag">Discourse</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Ranking/" rel="tag">Discourse Ranking</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Structure/" rel="tag">Discourse Structure</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Document-NMT/" rel="tag">Document NMT</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EMNLP/" rel="tag">EMNLP</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Extractive/" rel="tag">Extractive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICLR/" rel="tag">ICLR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICML/" rel="tag">ICML</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IJCAI/" rel="tag">IJCAI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Inter-Sentence/" rel="tag">Inter-Sentence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Keyphrase-Generation/" rel="tag">Keyphrase Generation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NAACL/" rel="tag">NAACL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIPS/" rel="tag">NIPS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NMT/" rel="tag">NMT</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Relation-Extraction/" rel="tag">Neural Relation Extraction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RST/" rel="tag">RST</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Relation-Constraints/" rel="tag">Relation Constraints</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Summarization/" rel="tag">Summarization</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Translation/" rel="tag">Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Word-Translation/" rel="tag">Word Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/alias/" rel="tag">alias</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/git/" rel="tag">git</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pip/" rel="tag">pip</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/screen/" rel="tag">screen</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/" rel="tag">shell</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tgz/" rel="tag">tgz</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tts/" rel="tag">tts</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1/" rel="tag">冒泡</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/" rel="tag">冒泡排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%99%E4%BD%9C%E5%8A%A9%E6%89%8B/" rel="tag">写作助手</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8E%8B%E7%BC%A9/" rel="tag">压缩</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/" rel="tag">发送邮件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%88%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">合并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%8E%E5%8F%B0/" rel="tag">后台</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/" rel="tag">基数排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F/" rel="tag">希尔排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6/" rel="tag">归并</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">归并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/" rel="tag">快速排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F/" rel="tag">批量</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4/" rel="tag">批量删除</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F/" rel="tag">排序</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5/" rel="tag">插入</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/" rel="tag">插入排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%99%E7%A8%8B/" rel="tag">教程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/" rel="tag">斐波那契数列</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9D%80%E6%AD%BB%E8%BF%9B%E7%A8%8B/" rel="tag">杀死进程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B1%89%E8%AF%BA%E5%A1%94/" rel="tag">汉诺塔</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%A7%A3%E5%8E%8B/" rel="tag">解压</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%B0%B7%E6%AD%8C%E7%BF%BB%E8%AF%91/" rel="tag">谷歌翻译</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BF%AD%E4%BB%A3%E5%9B%9E%E7%BF%BB/" rel="tag">迭代回翻</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9/" rel="tag">选择</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/" rel="tag">选择排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%99%84%E4%BB%B6/" rel="tag">附件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3/" rel="tag">非监督</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%A2%86%E5%9F%9F%E9%80%82%E5%BA%94/" rel="tag">领域适应</a><span class="tag-list-count">1</span></li></ul>
                </canvas>
            </div>
        </div>
        
-->
        
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-hengfu -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9879871597"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-chuizhi -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="1662238719"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-zhengfangxing -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="6699421902"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PROCJX</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.4.2
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>












        
      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  <script src="/js/local-search.js"></script>













  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '2286ab64f5194d9d79ce',
      clientSecret: 'f912492bec2391664b40478f50f2f943376768d6',
      repo: 'procjx.github.io',
      owner: 'procjx',
      admin: ['procjx'],
      id: 'ae47ee9d658cf3345753e0d8588745c6',
        language: 'zh-CN',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
