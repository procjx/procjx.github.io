<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/procjx.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/procjxfavicon32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/procjxfavicon16x16.ico">
  <link rel="mask-icon" href="/images/procjx.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.4.2',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

<!-- Google Adsense -->
<!--
<script async src="//pagead2.googlesyndication.com/
pagead/js/adsbygoogle.js"></script>
<script>
(adsbygoogle = window.adsbygoogle || []).push({
google_ad_client: "pub-1179774715076800",
enable_page_level_ads: true
});
</script>
-->

<script data-ad-client="ca-pub-1179774715076800" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>


<meta name="google-site-verification" content="cEiGwg0T8Rj5msmuEcGYZTh5nnf05EhCXy0gp2Ml5BI" />
<meta name="baidu-site-verification" content="noSKHe8MJs" />

  <meta name="description" content="目录  1. Intraoperative Liver Surface Completion with Graph Convolutional VAE [PDF] 摘要  2. Understanding and Exploiting Dependent Variables with Deep Metric  Learning [PDF] 摘要  3. VisCode: Embedding Inf">
<meta property="og:type" content="article">
<meta property="og:title" content="【arxiv论文】 Computer Vision and Pattern Recognition 2020-09-09">
<meta property="og:url" content="https:&#x2F;&#x2F;procjx.github.io&#x2F;2020&#x2F;09&#x2F;09&#x2F;%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-09-09&#x2F;index.html">
<meta property="og:site_name" content="PROCJX&#39;s BLOGS">
<meta property="og:description" content="目录  1. Intraoperative Liver Surface Completion with Graph Convolutional VAE [PDF] 摘要  2. Understanding and Exploiting Dependent Variables with Deep Metric  Learning [PDF] 摘要  3. VisCode: Embedding Inf">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https:&#x2F;&#x2F;procjx.github.io&#x2F;images&#x2F;cv-2020-09-09.jpg">
<meta property="og:updated_time" content="2020-09-14T07:46:26.509Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;procjx.github.io&#x2F;images&#x2F;cv-2020-09-09.jpg">

<link rel="canonical" href="https://procjx.github.io/2020/09/09/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-09-09/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-09-09 | PROCJX's BLOGS</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">PROCJX's BLOGS</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">WITH LOVE OF WORLD</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-fw fa-download"></i>资源</a>

  </li>
        <li class="menu-item menu-item-arxiv">

    <a href="/arxiv/" rel="section"><i class="fa fa-fw fa-file-pdf-o"></i>arxiv论文</a>

  </li>
        <li class="menu-item menu-item-deadline">

    <a href="/deadline/" rel="section"><i class="fa fa-fw fa-calendar"></i>会议截稿</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://procjx.github.io/2020/09/09/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-09-09/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/procjx.png">
      <meta itemprop="name" content="PROCJX">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PROCJX's BLOGS">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          【arxiv论文】 Computer Vision and Pattern Recognition 2020-09-09
        </h2>

        <div class="post-meta">
        
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-09-09 12:34:29" itemprop="dateCreated datePublished" datetime="2020-09-09T12:34:29+08:00">2020-09-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-09-14 15:46:26" itemprop="dateModified" datetime="2020-09-14T15:46:26+08:00">2020-09-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/arxiv/" itemprop="url" rel="index">
                    <span itemprop="name">arxiv</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/arxiv/CV/" itemprop="url" rel="index">
                    <span itemprop="name">CV</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
              <span>42k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
              <span>1:11</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="/images/cv-2020-09-09.jpg" alt></p><h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Intraoperative Liver Surface Completion with Graph Convolutional VAE <a href="https://arxiv.org/pdf/2009.03871" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Understanding and Exploiting Dependent Variables with Deep Metric  Learning <a href="https://arxiv.org/pdf/2009.03820" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> VisCode: Embedding Information in Visualization Images using  Encoder-Decoder Network <a href="https://arxiv.org/pdf/2009.03817" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Understanding Compositional Structures in Art Historical Images using  Pose and Gaze Priors <a href="https://arxiv.org/pdf/2009.03807" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Analysis and Prediction of Deforming 3D Shapes using Oriented Bounding  Boxes and LSTM Autoencoders <a href="https://arxiv.org/pdf/2009.03782" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Adversarial Machine Learning in Image Classification: A Survey Towards  the Defender's Perspective <a href="https://arxiv.org/pdf/2009.03728" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Convolutional Neural Networks for Automatic Detection of Artifacts from  Independent Components Represented in Scalp Topographies of EEG Signals <a href="https://arxiv.org/pdf/2009.03696" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Rain rendering for evaluating and improving robustness to bad weather <a href="https://arxiv.org/pdf/2009.03683" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> A Self-Supervised Gait Encoding Approach with Locality-Awareness for 3D  Skeleton Based Person Re-Identification <a href="https://arxiv.org/pdf/2009.03671" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Region Comparison Network for Interpretable Few-shot Image  Classification <a href="https://arxiv.org/pdf/2009.03558" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Few-Shot Hyperspectral Image Classification With Unknown Classes Using  Multitask Deep Learning <a href="https://arxiv.org/pdf/2009.03508" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> A Residual Solver and Its Unfolding Neural Network for Total Variation  Regularized Models <a href="https://arxiv.org/pdf/2009.03477" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> LaSOT: A High-quality Large-scale Single Object Tracking Benchmark <a href="https://arxiv.org/pdf/2009.03465" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> ePointDA: An End-to-End Simulation-to-Real Domain Adaptation Framework  for LiDAR Point Cloud Segmentation <a href="https://arxiv.org/pdf/2009.03456" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Convolution Neural Networks for diagnosing colon and lung cancer  histopathological images <a href="https://arxiv.org/pdf/2009.03878" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> TanhSoft -- a family of activation functions combining Tanh and Softplus <a href="https://arxiv.org/pdf/2009.03863" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Self-Supervised Scale Recovery for Monocular Depth and Egomotion  Estimation <a href="https://arxiv.org/pdf/2009.03787" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Deep Cyclic Generative Adversarial Residual Convolutional Networks for  Real Image Super-Resolution <a href="https://arxiv.org/pdf/2009.03693" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> GPU-based Self-Organizing Maps for Post-Labeled Few-Shot Unsupervised  Learning <a href="https://arxiv.org/pdf/2009.03665" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Learning more expressive joint distributions in multimodal variational  methods <a href="https://arxiv.org/pdf/2009.03651" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Imbalanced Continual Learning with Partitioning Reservoir Sampling <a href="https://arxiv.org/pdf/2009.03632" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Horus: Using Sensor Fusion to Combine Infrastructure and On-board  Sensing to Improve Autonomous Vehicle Safety <a href="https://arxiv.org/pdf/2009.03458" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Adversarial attacks on deep learning models for fatty liver disease  classification by modification of ultrasound image reconstruction method <a href="https://arxiv.org/pdf/2009.03364" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Going deeper with brain morphometry using neural networks <a href="https://arxiv.org/pdf/2009.03303" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Sensors, Safety Models and A System-Level Approach to Safe and Scalable  Automated Vehicles <a href="https://arxiv.org/pdf/2009.03301" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Intraoperative Liver Surface Completion with Graph Convolutional VAE</b>  <a href="https://arxiv.org/pdf/2009.03871" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Foti%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simone Foti</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Koo%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bongjin Koo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dowrick%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Dowrick</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ramalhinho%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Joao Ramalhinho</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Allam%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Moustafa Allam</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Davidson%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Brian Davidson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Stoyanov%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Danail Stoyanov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Clarkson%2C+M+J" target="_blank" rel="noopener" style="color:#0000EE;">Matthew J. Clarkson</a><br>
<font size="3">
 Abstract: In this work we propose a method based on geometric deep learning to predict the complete surface of the liver, given a partial point cloud of the organ obtained during the surgical laparoscopic procedure. We introduce a new data augmentation technique that randomly perturbs shapes in their frequency domain to compensate the limited size of our dataset. The core of our method is a variational autoencoder (VAE) that is trained to learn a latent space for complete shapes of the liver. At inference time, the generative part of the model is embedded in an optimisation procedure where the latent representation is iteratively updated to generate a model that matches the intraoperative partial point cloud. The effect of this optimisation is a progressive non-rigid deformation of the initially generated shape. Our method is qualitatively evaluated on real data and quantitatively evaluated on synthetic data. We compared with a state-of-the-art rigid registration algorithm, that our method outperformed in visible areas.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这项工作中，我们提出了一种基于几何深度学习来预测肝脏的整个表面的方法，考虑到腹腔镜手术过程中获取器官的局部点云。我们推出了新的数据增强技术，随机扰动形状在频率域，以弥补我们的数据集的大小限制。我们的方法的核心是变的自动编码（VAE）被训练来学习对肝脏的完整形状的潜在空间。在推理时，该模型的生成部分嵌入其中的优化过程，其中潜表示被迭代地更新，以生成术中局部点云相匹配的模型英寸这种优化的效果是在最初被生成的形状的渐进非刚性变形。我们的方法是定性评价实际的数据以及对合成数据定量评价。我们有一个国家的最先进的刚性配准算法相比，我们的方法在可见区域跑赢。</font>
</div>


<hr>
<div id="paper2"> <b>2. Understanding and Exploiting Dependent Variables with Deep Metric  Learning</b>  <a href="https://arxiv.org/pdf/2009.03820" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Mahony%2C+N+O" target="_blank" rel="noopener" style="color:#0000EE;">Niall O' Mahony</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Campbell%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sean Campbell</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Carvalho%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anderson Carvalho</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Krpalkova%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lenka Krpalkova</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Velasco-Hernandez%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gustavo Velasco-Hernandez</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Riordan%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniel Riordan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Walsh%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Joseph Walsh</a><br>
<font size="3">
 Abstract: Deep Metric Learning (DML) approaches learn to represent inputs to a lower-dimensional latent space such that the distance between representations in this space corresponds with a predefined notion of similarity. This paper investigates how the mapping element of DML may be exploited in situations where the salient features in arbitrary classification problems vary over time or due to changing underlying variables. Examples of such variable features include seasonal and time-of-day variations in outdoor scenes in place recognition tasks for autonomous navigation and age/gender variations in human/animal subjects in classification tasks for medical/ethological studies. Through the use of visualisation tools for observing the distribution of DML representations per each query variable for which prior information is available, the influence of each variable on the classification task may be better understood. Based on these relationships, prior information on these salient background variables may be exploited at the inference stage of the DML approach by using a clustering algorithm to improve classification performance. This research proposes such a methodology establishing the saliency of query background variables and formulating clustering algorithms for better separating latent-space representations at run-time. The paper also discusses online management strategies to preserve the quality and diversity of data and the representation of each class in the gallery of embeddings in the DML approach. We also discuss latent works towards understanding the relevance of underlying/multiple variables with DML.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深度量学习（DML）接近学习来表示输入到较低维潜在空间，使得在该空间中以对应于相似性的预定义的概念表示之间的距离。本文研究如何DML的映射元件可以在任意分类问题的显着特征的情况下被利用随着时间的推移或由于改变基础变量而变化。这样的可变功能的例子包括在室外场景到位识别任务的自主导航和年龄/人/动物科目分类任务的医疗/行为学研究性别差异的季节性日分时和变化。通过使用可视化工具，用于观察每每个查询变量这之前信息，请DML表示的分布，在分类任务中每个变量的影响，可以更好地理解。基于这些关系，这些突出的背景变量先验信息，可以在DML方法的推论阶段使用的聚类算法，提高分类性能的发挥。这项研究提出了这样的方法建立查询背景变量的显着性和制定聚类算法在运行时更好的分离潜空间表示。本文还讨论了在线管理策略，以保持数据的质量和多样性，在DML方法的嵌入的画廊的每个类的代表。我们还讨论了对理解与DML底层/多个变量的相关性潜在的作品。</font>
</div>


<hr>
<div id="paper3"> <b>3. VisCode: Embedding Information in Visualization Images using  Encoder-Decoder Network</b>  <a href="https://arxiv.org/pdf/2009.03817" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peiying Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chenhui Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Changbo Wang</a><br>
<font size="3">
 Abstract: We present an approach called VisCode for embedding information into visualization images. This technology can implicitly embed data information specified by the user into a visualization while ensuring that the encoded visualization image is not distorted. The VisCode framework is based on a deep neural network. We propose to use visualization images and QR codes data as training data and design a robust deep encoder-decoder network. The designed model considers the salient features of visualization images to reduce the explicit visual loss caused by encoding. To further support large-scale encoding and decoding, we consider the characteristics of information visualization and propose a saliency-based QR code layout algorithm. We present a variety of practical applications of VisCode in the context of information visualization and conduct a comprehensive evaluation of the perceptual quality of encoding, decoding success rate, anti-attack capability, time performance, etc. The evaluation results demonstrate the effectiveness of VisCode.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出呼吁VisCode用于把信息嵌入到可视化图像的方法。该技术可以隐式地嵌入由用户指定到一个可视化，同时确保编码的可视化图像不失真数据的信息。该VisCode框架是基于深层神经网络。我们建议使用可视化图像和QR码的数据作为训练数据而设计的强大的深编码器，解码器网络。所设计的模型考虑可视化的图像的显着特征，以减少所造成的编码明确视觉丧失。为了进一步支持大型编码和解码，我们考虑信息可视化的特点，提出了一个基于显着-QR码布局算法。我们提出的各种信息可视化的背景下VisCode的实际应用并进行编码的感知质量进行综合评价，解码成功率，抗攻击能力，及时性等评价结果表明VisCode的有效性。</font>
</div>


<hr>
<div id="paper4"> <b>4. Understanding Compositional Structures in Art Historical Images using  Pose and Gaze Priors</b>  <a href="https://arxiv.org/pdf/2009.03807" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Madhu%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Prathmesh Madhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marquart%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tilman Marquart</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kosti%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ronak Kosti</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bell%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peter Bell</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Maier%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andreas Maier</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Christlein%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vincent Christlein</a><br>
<font size="3">
 Abstract: Image compositions as a tool for analysis of artworks is of extreme significance for art historians. These compositions are useful in analyzing the interactions in an image to study artists and their artworks. Max Imdahl in his work called Ikonik, along with other prominent art historians of the 20th century, underlined the aesthetic and semantic importance of the structural composition of an image. Understanding underlying compositional structures within images is challenging and a time consuming task. Generating these structures automatically using computer vision techniques (1) can help art historians towards their sophisticated analysis by saving lot of time; providing an overview and access to huge image repositories and (2) also provide an important step towards an understanding of man made imagery by machines. In this work, we attempt to automate this process using the existing state of the art machine learning techniques, without involving any form of training. Our approach, inspired by Max Imdahl's pioneering work, focuses on two central themes of image composition: (a) detection of action regions and action lines of the artwork; and (b) pose-based segmentation of foreground and background. Currently, our approach works for artworks comprising of protagonists (persons) in an image. In order to validate our approach qualitatively and quantitatively, we conduct a user study involving experts and non-experts. The outcome of the study highly correlates with our approach and also demonstrates its domain-agnostic capability. We have open-sourced the code at this https URL.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：作为艺术品的分析的工具的图像的组合物是用于艺术历史学家极端重要性。这些组合物在分析图像中的相互作用研究的艺术家和他们的作品是有用的。马克斯·达尔在他叫Ikonik，与20世纪的其他著名的艺术史学家一起工作，强调图像的结构组成的审美和语义重要性。图像中了解基本的组成结构是具有挑战性和耗时的任务。产生这些结构自动利用计算机视觉技术（1）可以帮助艺术史学家对通过节省大量的时间其复杂的分析;提供一个概要，并获得巨大的图像库和（2）还提供了对通过机器的人造图像的理解的重要一步。在这项工作中，我们试图利用本领域的机器学习技术的现有状态来自动完成这一过程，而不涉及任何形式的培训。我们的方法，由马克斯·达尔的开拓性工作的启发，着眼于图像合成的两个中心的主题：动作区域和艺术品的动作线（a）的检测;和（b）基于姿势-前景和背景的分割。目前，我们的方法也适用于图像中包含主角（人）的作品。为了定性和定量验证我们的方法，我们进行涉及专家和非专家用户研究。这项研究的结果与我们的做法高度相关，同时也表明其领域无关的能力。我们开源这个HTTPS URL的代码。</font>
</div>


<hr>
<div id="paper5"> <b>5. Analysis and Prediction of Deforming 3D Shapes using Oriented Bounding  Boxes and LSTM Autoencoders</b>  <a href="https://arxiv.org/pdf/2009.03782" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hahner%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sara Hahner</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Iza-Teran%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rodrigo Iza-Teran</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Garcke%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jochen Garcke</a><br>
<font size="3">
 Abstract: For sequences of complex 3D shapes in time we present a general approach to detect patterns for their analysis and to predict the deformation by making use of structural components of the complex shape. We incorporate long short-term memory (LSTM) layers into an autoencoder to create low dimensional representations that allow the detection of patterns in the data and additionally detect the temporal dynamics in the deformation behavior. This is achieved with two decoders, one for reconstruction and one for prediction of future time steps of the sequence. In a preprocessing step the components of the studied object are converted to oriented bounding boxes which capture the impact of plastic deformation and allow reducing the dimensionality of the data describing the structure. The architecture is tested on the results of 196 car crash simulations of a model with 133 different components, where material properties are varied. In the latent representation we can detect patterns in the plastic deformation for the different components. The predicted bounding boxes give an estimate of the final simulation result and their quality is improved in comparison to different baselines.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在时间复杂的三维形状的序列，我们提出以检测模式为他们的分析和通过利用复杂形状的结构部件的预测变形的通用方法。我们结合长短期存储器（LSTM）层成自动编码以创建低维表示，其允许在所述数据的模式的检测，并且另外检测在变形行为时间动态。这是通过两个解码器，一个用于重建和一个用于序列的未来时间步预测来实现的。在预处理步骤中所研究的对象的成分被转化成定向包围盒，其捕获的塑性变形的影响，并允许减小描述结构中的数据的维数。该架构上的133个不同的组件，其中，材料特性变化的模型的196个汽车碰撞模拟的结果进行测试。在潜表示，我们可以在用于不同部件的塑性变形检测图案。预测的边界框得到的最终模拟结果的估计值和它们的质量相比，不同的基线的改善。</font>
</div>


<hr>
<div id="paper6"> <b>6. Adversarial Machine Learning in Image Classification: A Survey Towards  the Defender's Perspective</b>  <a href="https://arxiv.org/pdf/2009.03728" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Machado%2C+G+R" target="_blank" rel="noopener" style="color:#0000EE;">Gabriel Resende Machado</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Silva%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eugênio Silva</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Goldschmidt%2C+R+R" target="_blank" rel="noopener" style="color:#0000EE;">Ronaldo Ribeiro Goldschmidt</a><br>
<font size="3">
 Abstract: Deep Learning algorithms have achieved the state-of-the-art performance for Image Classification and have been used even in security-critical applications, such as biometric recognition systems and self-driving cars. However, recent works have shown those algorithms, which can even surpass the human capabilities, are vulnerable to adversarial examples. In Computer Vision, adversarial examples are images containing subtle perturbations generated by malicious optimization algorithms in order to fool classifiers. As an attempt to mitigate these vulnerabilities, numerous countermeasures have been constantly proposed in literature. Nevertheless, devising an efficient defense mechanism has proven to be a difficult task, since many approaches have already shown to be ineffective to adaptive attackers. Thus, this self-containing paper aims to provide all readerships with a review of the latest research progress on Adversarial Machine Learning in Image Classification, however with a defender's perspective. Here, novel taxonomies for categorizing adversarial attacks and defenses are introduced and discussions about the existence of adversarial examples are provided. Further, in contrast to exisiting surveys, it is also given relevant guidance that should be taken into consideration by researchers when devising and evaluating defenses. Finally, based on the reviewed literature, it is discussed some promising paths for future research.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深学习算法都取得了国家的最先进的性能图像分类，并已在安全关键应用，如生物特征识别系统和自动驾驶汽车，甚至使用。然而，最近的作品显示，那些算法，甚至可以超过人的能力，很容易受到对抗性的例子。在计算机视觉，对抗性的例子是含有以愚弄分类器被恶意优化算法生成细微扰动图象。作为一种尝试，以缓解这些漏洞，许多措施已不断文献中提出。然而，制定一个有效的防御机制已被证明是一项艰巨的任务，因为许多方法已经证明是无效的自适应攻击者。因此，这种含自本文旨在提供所有的读者群具有对对抗性机器学习在图像分类中的最新研究进展进行审查，但是有一个后卫的角度。在这里，归类敌对攻击和防御小说分类进行了介绍和提供有关的对抗性例子存在的讨论。此外，相比于exisiting调查，也给出应该由研究人员设计和评估防御时，可以考虑相关的指导。最后，基于该评审的文献，它讨论了今后的研究方向有前途的路径。</font>
</div>


<hr>
<div id="paper7"> <b>7. Convolutional Neural Networks for Automatic Detection of Artifacts from  Independent Components Represented in Scalp Topographies of EEG Signals</b>  <a href="https://arxiv.org/pdf/2009.03696" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Placidi%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Giuseppe Placidi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cinque%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Luigi Cinque</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Polsinelli%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matteo Polsinelli</a><br>
<font size="3">
 Abstract: Electroencephalography (EEG) measures the electrical brain activity in real-time by using sensors placed on the scalp. Artifacts, due to eye movements and blink, muscular/cardiac activity and generic electrical disturbances, have to be recognized and eliminated to allow a correct interpretation of the useful brain signals (UBS) of EEG. Independent Component Analysis (ICA) is effective to split the signal into independent components (ICs) whose re-projections on 2D scalp topographies (images), also called topoplots, allow to recognize/separate artifacts and by UBS. Until now, IC topoplot analysis, a gold standard in EEG, has been carried on visually by human experts and, hence, not usable in automatic, fast-response EEG. We present a completely automatic and effective framework for EEG artifact recognition by IC topoplots, based on 2D Convolutional Neural Networks (CNNs), capable to divide topoplots in 4 classes: 3 types of artifacts and UBS. The framework setup is described and results are presented, discussed and compared with those obtained by other competitive strategies. Experiments, carried on public EEG datasets, have shown an overall accuracy of above 98%, employing 1.4 sec on a standard PC to classify 32 topoplots, that is to drive an EEG system of 32 sensors. Though not real-time, the proposed framework is efficient enough to be used in fast-response EEG-based Brain-Computer Interfaces (BCI) and faster than other automatic methods based on ICs.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：脑电图（EEG）措施，通过放置在头皮上的传感器实时的脑电活动。工件，由于眼睛运动和眨眼，肌肉/心脏活动和通用电气干扰，必须被识别和消除，以允许EEG的有用脑信号（UBS）的正确解释。独立分量分析（ICA）是有效的分裂信号分成独立组件（集成电路），其上的2D头皮形貌（图像），也称为topoplots，重新投影允许识别/分离伪像和通过UBS。到现在为止，IC topoplot分析，在EEG黄金标准，已经在视觉上由人类专家进行的，因此，在自动，快速响应的EEG不能使用。我们提出了通过IC topoplots，基于二维卷积神经网络（细胞神经网络），能够分topoplots在4类EEG假象识别完全自动和有效的框架：3种类型的伪像和UBS的。该框架设置被描述和结果被呈现，讨论并与其他的竞争战略获得的那些进行比较。实验中，在公共数据集EEG进行，都显示出98％以上的总体准确度，采用在标准PC上1.4秒至32个topoplots分类，即驱动的32个传感器的EEG系统。虽然不是实时的，所提出的框架是足够有效的基于EEG的快速响应脑 - 机接口（BCI）中使用，比基于IC的其他自动方法更快。</font>
</div>


<hr>
<div id="paper8"> <b>8. Rain rendering for evaluating and improving robustness to bad weather</b>  <a href="https://arxiv.org/pdf/2009.03683" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tremblay%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maxime Tremblay</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Halder%2C+S+S" target="_blank" rel="noopener" style="color:#0000EE;">Shirsendu Sukanta Halder</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=de+Charette%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Raoul de Charette</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lalonde%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jean-François Lalonde</a><br>
<font size="3">
 Abstract: Rain fills the atmosphere with water particles, which breaks the common assumption that light travels unaltered from the scene to the camera. While it is well-known that rain affects computer vision algorithms, quantifying its impact is difficult. In this context, we present a rain rendering pipeline that enables the systematic evaluation of common computer vision algorithms to controlled amounts of rain. We present three different ways to add synthetic rain to existing images datasets: completely physic-based; completely data-driven; and a combination of both. The physic-based rain augmentation combines a physical particle simulator and accurate rain photometric modeling. We validate our rendering methods with a user study, demonstrating our rain is judged as much as 73% more realistic than the state-of-theart. Using our generated rain-augmented KITTI, Cityscapes, and nuScenes datasets, we conduct a thorough evaluation of object detection, semantic segmentation, and depth estimation algorithms and show that their performance decreases in degraded weather, on the order of 15% for object detection, 60% for semantic segmentation, and 6-fold increase in depth estimation error. Finetuning on our augmented synthetic data results in improvements of 21% on object detection, 37% on semantic segmentation, and 8% on depth estimation.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：雨填补了大气中的水粒子，它打破了常见的假设是光速从现场摄像机不变。虽然这是众所周知的，雨会影响计算机视觉算法，量化其影响是困难的。在此背景下，我们提出了一个雨渲染管线，使普通计算机视觉算法的系统评价，以控制量的雨。我们提出三种不同的方法来合成雨水添加到现有的图像数据集：完全物理为基础的;完全数据驱动的;和两者的组合。基于物理雨增强结合了物理粒子模拟器和准确雨光度建模。我们验证了我们与用户研究呈现方法，证明了我们雨判断高达73％，比国家的theart更加逼真。使用我们生成的降雨增加了的KITTI，风情，和nuScenes数据集，我们进行目标检测，语义分割和深度估计算法进行全面评估，并表明其性能退化的天气物体检测减小，15％左右， 60％的语义分割，并且在深度估计误差增加6倍。在对物体检测21％，在语义分割37％，并且在深度估计8％改进微调对我们的增强合成数据的结果。</font>
</div>


<hr>
<div id="paper9"> <b>9. A Self-Supervised Gait Encoding Approach with Locality-Awareness for 3D  Skeleton Based Person Re-Identification</b>  <a href="https://arxiv.org/pdf/2009.03671" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Rao%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haocong Rao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Siqi Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiping Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mingkui Tan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yi Guo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Cheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bin Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinwang Liu</a><br>
<font size="3">
 Abstract: Person re-identification (Re-ID) via gait features within 3D skeleton sequences is a newly-emerging topic with several advantages. Existing solutions either rely on hand-crafted descriptors or supervised gait representation learning. This paper proposes a self-supervised gait encoding approach that can leverage unlabeled skeleton data to learn gait representations for person Re-ID. Specifically, we first create self-supervision by learning to reconstruct unlabeled skeleton sequences reversely, which involves richer high-level semantics to obtain better gait representations. Other pretext tasks are also explored to further improve self-supervised learning. Second, inspired by the fact that motion's continuity endows adjacent skeletons in one skeleton sequence and temporally consecutive skeleton sequences with higher correlations (referred as locality in 3D skeleton data), we propose a locality-aware attention mechanism and a locality-aware contrastive learning scheme, which aim to preserve locality-awareness on intra-sequence level and inter-sequence level respectively during self-supervised learning. Last, with context vectors learned by our locality-aware attention mechanism and contrastive learning scheme, a novel feature named Constrastive Attention-based Gait Encodings (CAGEs) is designed to represent gait effectively. Empirical evaluations show that our approach significantly outperforms skeleton-based counterparts by 15-40% Rank-1 accuracy, and it even achieves superior performance to numerous multi-modal methods with extra RGB or depth information. Our codes are available at this https URL.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人重新鉴定（再ID）通过步态3D骨架序列中的特征是具有若干优点的新兴主题。现有的解决方案无论是依靠手工制作的描述符或监督步态表示学习。本文提出了一种自我监督的步态编码的方法，可以利用未标记的骨架数据来学习的人重新编号步态表示。具体而言，我们首先通过学习反向重构未标记骨架序列，其中包括更丰富的高层次语义来获得更好的步态表示创建自检。其他借口任务进行了探讨，以进一步提高自我监督学习。其次，通过该运动的连续性赋予在一个骨架序列中的相邻骨架和具有较高的相关性（以下简称在三维骨架数据局部性）时间上连续的骨架序列的事实启发，我们提出了一种局部性感知注意机制和局部性感知对比学习方案，其目的自监督学习过程中保持分别在序列内水平和序列间水平局部性意识。最后，与我们的本地感知注意机制和对比学习方案，名为Constrastive基于注意力步态编码（笼）一个新的特征了解到情境矢量旨在有效地代表步态。实证评估表明15-40％排名-1的精度，我们的方法比骨架为基础显著同行，它甚至实现了卓越的性能众多的多模态方法有额外的RGB或深度信息。我们的代码可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper10"> <b>10. Region Comparison Network for Interpretable Few-shot Image  Classification</b>  <a href="https://arxiv.org/pdf/2009.03558" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xue%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhiyu Xue</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lixin Duan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wen Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lin Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiebo Luo</a><br>
<font size="3">
 Abstract: While deep learning has been successfully applied to many real-world computer vision tasks, training robust classifiers usually requires a large amount of well-labeled data. However, the annotation is often expensive and time-consuming. Few-shot image classification has thus been proposed to effectively use only a limited number of labeled examples to train models for new classes. Recent works based on transferable metric learning methods have achieved promising classification performance through learning the similarity between the features of samples from the query and support sets. However, rare of them explicitly considers the model interpretability, which can actually be revealed during the training phase. For that, in this work, we propose a metric learning based method named Region Comparison Network (RCN), which is able to reveal how few-shot learning works as in a neural network as well as to find out specific regions that are related to each other in images coming from the query and support sets. Moreover, we also present a visualization strategy named Region Activation Mapping (RAM) to intuitively explain what our method has learned by visualizing intermediate variables in our network. We also present a new way to generalize the interpretability from the level of tasks to categories, which can also be viewed as a method to find the prototypical parts for supporting the final decision of our RCN. Extensive experiments on four benchmark datasets clearly show the effectiveness of our method over existing baselines.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管深度学习已经成功地应用于许多现实世界的计算机视觉任务，培养健壮的分类通常需要大量的良好标记的数据。然而，注释往往是昂贵和费时。几个镜头图像分类也因此提出了有效的利用只标识样本数量有限的训练模式为新类。根据转让度量学习方法的最新作品都取得通过学习从查询和支持组样品的特征之间的相似性有前途的分类性能。然而，难得他们明确地认为模型解释性，这实际上可以在训练阶段显露出来。为此，在这项工作中，我们提出了一个名为区域比较网络（RCN）度量学习为基础的方法，这是能够揭示如何少拍学习作品的神经网络，以及找出相关的特定区域对方从查询和支持台来的图像。此外，我们还提出一个名为地区激活映射（RAM）可视化战略，以直观地解释我们的方法已经在我们的网络可视化中间变量教训。我们还提出来概括从任务的等级类别，这也可以看作是找到原型零件支持我们的RCN最终决定的方法可解释性一条新的途径。四个基准数据集大量的实验清楚地表明我们对现有基准方法的有效性。</font>
</div>


<hr>
<div id="paper11"> <b>11. Few-Shot Hyperspectral Image Classification With Unknown Classes Using  Multitask Deep Learning</b>  <a href="https://arxiv.org/pdf/2009.03508" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shengjie Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qian Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liangpei Zhang</a><br>
<font size="3">
 Abstract: Current hyperspectral image classification assumes that a predefined classification system is closed and complete, and there are no unknown or novel classes in the unseen data. However, this assumption may be too strict for the real world. Often, novel classes are overlooked when the classification system is constructed. The closed nature forces a model to assign a label given a new sample and may lead to overestimation of known land covers (e.g., crop area). To tackle this issue, we propose a multitask deep learning method that simultaneously conducts classification and reconstruction in the open world (named MDL4OW) where unknown classes may exist. The reconstructed data are compared with the original data; those failing to be reconstructed are considered unknown, based on the assumption that they are not well represented in the latent features due to the lack of labels. A threshold needs to be defined to separate the unknown and known classes; we propose two strategies based on the extreme value theory for few-shot and many-shot scenarios. The proposed method was tested on real-world hyperspectral images; state-of-the-art results were achieved, e.g., improving the overall accuracy by 4.94% for the Salinas data. By considering the existence of unknown classes in the open world, our method achieved more accurate hyperspectral image classification, especially under the few-shot context.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：当前高光谱图像分类假设一个预定的分类系统是封闭的和完整的，并有在看不见的数据没有未知的或新的类。然而，这种假设可能过于严格的现实世界。通常，分类系统被构建时新颖的类被忽视。封闭性力模型来分配标签赋予了新的样本，并可能导致称为土地覆盖（例如，作物区）的高估。为了解决这个问题，我们提出了一个多任务深学习方法，能同时进行在未知的类可能存在开放的世界（名为MDL4OW）的分类和重建。重建的数据与原始数据进行比较;那些未能重建被认为是未知的基础上，他们没有得到很好的潜在功能，由于缺少标签所代表的假设。阈值需要被定义到未知和已知类别分开;我们提出了基于极值理论几拍，很多次情况下有两种策略。该方法是对真实世界的高光谱图像检测;状态的最先进的结果实现的，例如，改进由4.94％的整体精度为萨利纳斯数据。通过考虑未知的班在开放的世界的存在，我们的方法来实现更精确的高光谱影像分类，特别是在少数次上下文。</font>
</div>


<hr>
<div id="paper12"> <b>12. A Residual Solver and Its Unfolding Neural Network for Total Variation  Regularized Models</b>  <a href="https://arxiv.org/pdf/2009.03477" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuanhao Gong</a><br>
<font size="3">
 Abstract: This paper proposes to solve the Total Variation regularized models by finding the residual between the input and the unknown optimal solution. After analyzing a previous method, we developed a new iterative algorithm, named as Residual Solver, which implicitly solves the model in gradient domain. We theoretically prove the uniqueness of the gradient field in our algorithm. We further numerically confirm that the residual solver can reach the same global optimal solutions as the classical method on 500 natural images. Moreover, we unfold our iterative algorithm into a convolution neural network (named as Residual Solver Network). This network is unsupervised and can be considered as an "enhanced version" of our iterative algorithm. Finally, both the proposed algorithm and neural network are successfully applied on several problems to demonstrate their effectiveness and efficiency, including image smoothing, denoising, and biomedical image reconstruction. The proposed network is general and can be applied to solve other total variation regularized models.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了解决全变差通过查找输入和未知的最佳解决方案之间的残余正规化模型。分析以前的方法后，我们开发了一种新的迭代算法，命名为剩余求解器，它隐解决了梯度域模型。从理论上证明了梯度场的独特性在我们的算法。我们进一步数值确认剩余求解器作为500个自然图像的经典方法达到相同的全局最优解。此外，我们开展我们的迭代算法为卷积神经网络（称为残余求解网络）。这个网络是无监督，可以被视为我们的迭代算法的“加强版”。最后，无论是算法和神经网络应用成功的几个问题来证明其有效性和效率，包括图像平滑，去噪和生物医学图像重建。所提出的网络是通用的并且可应用于解决其他全变差正则化的模型。</font>
</div>


<hr>
<div id="paper13"> <b>13. LaSOT: A High-quality Large-scale Single Object Tracking Benchmark</b>  <a href="https://arxiv.org/pdf/2009.03465" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Heng Fan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hexin Bai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liting Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fan Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chu%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peng Chu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Ge Deng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sijia Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Harshit" target="_blank" rel="noopener" style="color:#0000EE;">Harshit</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mingzhen Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Juehuan Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yong Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liao%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chunyuan Liao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lin Yuan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ling%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haibin Ling</a><br>
<font size="3">
 Abstract: Despite great recent advances in visual tracking, its further development, including both algorithm design and evaluation, is limited due to lack of dedicated large-scale benchmarks. To address this problem, we present LaSOT, a high-quality Large-scale Single Object Tracking benchmark. LaSOT contains a diverse selection of 85 object classes, and offers 1,550 totaling more than 3.87 million frames. Each video frame is carefully and manually annotated with a bounding box. This makes LaSOT, to our knowledge, the largest densely annotated tracking benchmark. Our goal in releasing LaSOT is to provide a dedicated high quality platform for both training and evaluation of trackers. The average video length of LaSOT is around 2,500 frames, where each video contains various challenge factors that exist in real world video footage,such as the targets disappearing and re-appearing. These longer video lengths allow for the assessment of long-term trackers. To take advantage of the close connection between visual appearance and natural language, we provide language specification for each video in LaSOT. We believe such additions will allow for future research to use linguistic features to improve tracking. Two protocols, full-overlap and one-shot, are designated for flexible assessment of trackers. We extensively evaluate 48 baseline trackers on LaSOT with in-depth analysis, and results reveal that there still exists significant room for improvement. The complete benchmark, tracking results as well as analysis are available at this https URL.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管最近在视觉跟踪，其进一步的发展，包括算法设计和评估重大进展，由于缺乏专门的大规模基准的限制。为了解决这个问题，我们目前LaSOT，高品质的大型单目标跟踪基准。 LaSOT包含85对象类和总额超过387万帧报价1,550多元化的选择。每个视频帧被仔细地和手动注释与边界框。这使得LaSOT，据我们所知，最大的密集注释跟踪基准。我们在释放LaSOT目标是提供一种用于跟踪的训练和评估专用的高品质平台。 LaSOT的平均视频长度为约2500架，其中每个视频包含了存在于现实世界的录像，如目标消失和重新出现的各种挑战因素。这些较长的视频长度允许长期跟踪评估。要利用视觉外观和自然语言之间的密切联系的优势，我们提供了在LaSOT每个视频语言规范。我们认为，这种增加将允许未来的研究用语言特征，以提高跟踪。两个协议，全重叠和一杆，被指定用于跟踪的灵活评估。我们广泛的评估对LaSOT 48个基线跟踪与深入分析，并且结果显示，仍然存在改进的余地显著。完整的基准，跟踪结果，以及分析可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper14"> <b>14. ePointDA: An End-to-End Simulation-to-Real Domain Adaptation Framework  for LiDAR Point Cloud Segmentation</b>  <a href="https://arxiv.org/pdf/2009.03456" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sicheng Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yezhen Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bichen Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pengfei Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Darrell%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Trevor Darrell</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Keutzer%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kurt Keutzer</a><br>
<font size="3">
 Abstract: Due to its robust and precise distance measurements, LiDAR plays an important role in scene understanding for autonomous driving. Training deep neural networks (DNNs) on LiDAR data requires large-scale point-wise annotations, which are time-consuming and expensive to obtain. Instead, simulation-to-real domain adaptation (SRDA) trains a DNN using unlimited synthetic data with automatically generated labels and transfers the learned model to real scenarios. Existing SRDA methods for LiDAR point cloud segmentation mainly employ a multi-stage pipeline and focus on feature-level alignment. They require prior knowledge of real-world statistics and ignore the pixel-level dropout noise gap and the spatial feature gap between different domains. In this paper, we propose a novel end-to-end framework, named ePointDA, to address the above issues. Specifically, ePointDA consists of three components: self-supervised dropout noise rendering, statistics-invariant and spatially-adaptive feature alignment, and transferable segmentation learning. The joint optimization enables ePointDA to bridge the domain shift at the pixel-level by explicitly rendering dropout noise for synthetic LiDAR and at the feature-level by spatially aligning the features between different domains, without requiring the real-world statistics. Extensive experiments adapting from synthetic GTA-LiDAR to real KITTI and SemanticKITTI demonstrate the superiority of ePointDA for LiDAR point cloud segmentation.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：由于它的强大和精确的距离测量，激光雷达起着场景理解为自主驾驶着重要作用。上LiDAR数据训练深神经网络（DNNs）需要大规模的逐点注解，这是耗时且昂贵，获得。相反，模拟到现实领域适应性（SRDA），使用自动生成的标签和转让的学习模型真实场景无限合成数据将训练DNN。激光雷达点云分割现有SRDA方法主要采用多级流水线，专注于功能级排列。他们需要的真实世界的统计先验知识而忽略了像素级的辍学噪音差距和不同的域之间的空间特征差距。在本文中，我们提出了一个新颖的终端到终端的框架，名为ePointDA，以解决上述问题。具体而言，ePointDA由三个部分组成：自监督降噪声渲染，统计不变和空间自适应特征对准，并转让分割学习。联合优化使ePointDA通过明确地呈现降噪音综合激光雷达，并在通过空间对准不同的域之间的功能特征级弥合在像素级域转移，而不需要真实世界的统计数据。大量的实验由合成GTA激光雷达适应实际KITTI和SemanticKITTI展示ePointDA的激光雷达点云分割的优越性。</font>
</div>


<hr>
<div id="paper15"> <b>15. Convolution Neural Networks for diagnosing colon and lung cancer  histopathological images</b>  <a href="https://arxiv.org/pdf/2009.03878" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Mangal%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sanidhya Mangal</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Chaurasia%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aanchal Chaurasia</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Khajanchi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ayush Khajanchi</a><br>
<font size="3">
 Abstract: Lung and Colon cancer are one of the leading causes of mortality and morbidity in adults. Histopathological diagnosis is one of the key components to discern cancer type. The aim of the present research is to propose a computer aided diagnosis system for diagnosing squamous cell carcinomas and adenocarcinomas of lung as well as adenocarcinomas of colon using convolutional neural networks by evaluating the digital pathology images for these cancers. Hereby, rendering artificial intelligence as useful technology in the near future. A total of 2500 digital images were acquired from LC25000 dataset containing 5000 images for each class. A shallow neural network architecture was used classify the histopathological slides into squamous cell carcinomas, adenocarcinomas and benign for the lung. Similar model was used to classify adenocarcinomas and benign for colon. The diagnostic accuracy of more than 97% and 96% was recorded for lung and colon respectively.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：肺癌和结肠癌是成人死亡率和发病率的主要原因之一。病理组织学诊断是关键部件辨别癌症类型中的一个。本研究的目的是提出一种计算机辅助诊断系统，用于通过评估对于这些癌症的数字病理图像使用卷积神经网络诊断鳞状细胞癌和肺腺癌以及结肠腺癌。在此，使人工智能作为有用的技术在不久的将来。总共2500个的数字图像从含有用于每个类别5000倍的图像数据集LC25000获取。一个浅的神经网络结构被用于分类的组织病理学滑入鳞状细胞癌，腺癌和良性的肺部。类似的模型，用来区分腺癌和良性结肠。的97％以上和96％的诊断准确性分别记录肺癌和结肠癌。</font>
</div>


<hr>
<div id="paper16"> <b>16. TanhSoft -- a family of activation functions combining Tanh and Softplus</b>  <a href="https://arxiv.org/pdf/2009.03863" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Biswas%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Koushik Biswas</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sandeep Kumar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Banerjee%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shilpak Banerjee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pandey%2C+A+K" target="_blank" rel="noopener" style="color:#0000EE;">Ashish Kumar Pandey</a><br>
<font size="3">
 Abstract: Deep learning at its core, contains functions that are composition of a linear transformation with a non-linear function known as activation function. In past few years, there is an increasing interest in construction of novel activation functions resulting in better learning. In this work, we propose a family of novel activation functions, namely TanhSoft, with four undetermined hyper-parameters of the form tanh and tune these hyper-parameters to obtain activation functions which are shown to outperform several well known activation functions. For instance, replacing ReLU with xtanh(0.6e^x)improves top-1 classification accuracy on CIFAR-10 by 0.46% for DenseNet-169 and 0.7% for Inception-v3 while with tanh(0.87x)ln(1 +e^x) top-1 classification accuracy on CIFAR-100 improves by 1.24% for DenseNet-169 and 2.57% for SimpleNet model.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：其核心深学习，包含与已知为活化功能的非线性函数的线性变换的组合物的功能。在过去的几年里，有正在施工中产生更好的学习新的激活功能的越来越大的兴趣。在这项工作中，我们提出了一个家族的新激活的功能，即TanhSoft，与以下形式的tanh四个未定超参数和调整这些超参数，以获得被证明优于几种公知激活函数激活功能。例如，对于xtanh替换RELU（0.6E ^ x）的0.46％提高顶部-1分类精度上CIFAR-10 DenseNet-169和用于启-V3 0.7％，而用的tanh（0.87x）LN（1个+ E ^ x）的上CIFAR-100顶1的分类精度提高了1.24％为DenseNet-169和用于SimpleNet模型2.57％。</font>
</div>


<hr>
<div id="paper17"> <b>17. Self-Supervised Scale Recovery for Monocular Depth and Egomotion  Estimation</b>  <a href="https://arxiv.org/pdf/2009.03787" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wagstaff%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Brandon Wagstaff</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kelly%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jonathan Kelly</a><br>
<font size="3">
 Abstract: The self-supervised loss formulation for jointly training depth and egomotion neural networks with monocular images is well studied and has demonstrated state-of-the-art accuracy. One of the main limitations of this approach, however, is that the depth and egomotion estimates are only determined up to an unknown scale. In this paper, we present a novel \textit{scale recovery loss} that enforces consistency between a known camera height and the estimated camera height, generating metric (scaled) depth and egomotion predictions. % We show that our proposed method is competitive with other scale recovery techniques (i.e., pose supervision and stereo left/right consistency constraints). Further, we demonstrate how our method facilitates network retraining within new environments, whereas other scale-resolving approaches are incapable of doing so. Notably, our egomotion network is able to produce more accurate estimates than a similar method that only recovers scale at test time.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：共同培养深度和自我运动神经网络与单目图像的自监督损失制剂充分的研究，并已证实状态的最先进的精度。其中一个这种方法的主要限制，然而，就是在深度和自身运动估计只确定了一个未知的规模。在本文中，我们提出了一个新颖\ textit {规模恢复损耗}，一个公知的摄像机高度和推定摄像机高度，产生度量（缩放）深度和自我运动预测之间强制实施的一致性。 ％我们证明了我们提出的方法与其他规模恢复技术（即，姿势监督和立体声左/右一致性约束）的竞争力。此外，我们证明我们的方法如何促进中新环境的网络再培训，而其他规模，解决的方法是不能这样做的。值得注意的是，我们的自身运动网络能够产生更准确的估计比仅在测试的时间内恢复规模类似的方法。</font>
</div>


<hr>
<div id="paper18"> <b>18. Deep Cyclic Generative Adversarial Residual Convolutional Networks for  Real Image Super-Resolution</b>  <a href="https://arxiv.org/pdf/2009.03693" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Umer%2C+R+M" target="_blank" rel="noopener" style="color:#0000EE;">Rao Muhammad Umer</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Micheloni%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christian Micheloni</a><br>
<font size="3">
 Abstract: Recent deep learning based single image super-resolution (SISR) methods mostly train their models in a clean data domain where the low-resolution (LR) and the high-resolution (HR) images come from noise-free settings (same domain) due to the bicubic down-sampling assumption. However, such degradation process is not available in real-world settings. We consider a deep cyclic network structure to maintain the domain consistency between the LR and HR data distributions, which is inspired by the recent success of CycleGAN in the image-to-image translation applications. We propose the Super-Resolution Residual Cyclic Generative Adversarial Network (SRResCycGAN) by training with a generative adversarial network (GAN) framework for the LR to HR domain translation in an end-to-end manner. We demonstrate our proposed approach in the quantitative and qualitative experiments that generalize well to the real image super-resolution and it is easy to deploy for the mobile/embedded devices. In addition, our SR results on the AIM 2020 Real Image SR Challenge datasets demonstrate that the proposed SR approach achieves comparable results as the other state-of-art methods.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近的深度学习基于单幅图​​像超分辨率（SISR）方法主要是培养他们的模型在一个干净的数据域在低分辨率（LR）和高分辨率（HR）图像来自无噪音的设置（同一个域）由于双三次下采样的假设。然而，这样的降解过程是不是在现实世界中的可用设置。我们认为深循环的网络结构，以保持LR和HR数据分布，这是由CycleGAN的图像 - 图像转换应用，最近成功的启发之间的域一致性。我们通过培训的LR到HR域翻译的端至端的方式生成对抗网络（GAN）框架提出了超分辨率残循环剖成对抗性网络（SRResCycGAN）。我们证明了我们提出的方法在推广以及对真实影像超分辨率的定量和定性实验，它是易于部署的移动/嵌入式设备。此外，我们对AIM 2020真实影像SR SR结果挑战数据集表明，该SR方法实现了类似的结果与其他国家的技术方法。</font>
</div>


<hr>
<div id="paper19"> <b>19. GPU-based Self-Organizing Maps for Post-Labeled Few-Shot Unsupervised  Learning</b>  <a href="https://arxiv.org/pdf/2009.03665" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Khacef%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lyes Khacef</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gripon%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vincent Gripon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Miramond%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Benoit Miramond</a><br>
<font size="3">
 Abstract: Few-shot classification is a challenge in machine learning where the goal is to train a classifier using a very limited number of labeled examples. This scenario is likely to occur frequently in real life, for example when data acquisition or labeling is expensive. In this work, we consider the problem of post-labeled few-shot unsupervised learning, a classification task where representations are learned in an unsupervised fashion, to be later labeled using very few annotated examples. We argue that this problem is very likely to occur on the edge, when the embedded device directly acquires the data, and the expert needed to perform labeling cannot be prompted often. To address this problem, we consider an algorithm consisting of the concatenation of transfer learning with clustering using Self-Organizing Maps (SOMs). We introduce a TensorFlow-based implementation to speed-up the process in multi-core CPUs and GPUs. Finally, we demonstrate the effectiveness of the method using standard off-the-shelf few-shot classification benchmarks.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：很少拍分类是机器学习是一个挑战，其目的是使用的标识样本数量非常有限训练分类。这种情况是有可能发生的数据采集或标签是很昂贵的频繁发生在现实生活中，例如。在这项工作中，我们考虑后标记几拍无监督学习，分类任务，其中表示在无监督的方式了解到的问题，用非常少的注释例子在后面标记。我们认为，这一问题很可能在边缘，出现在嵌入式设备直接获取数据，并进行标注所需要的专家不能常提示。为了解决这个问题，我们考虑的算法，包括迁移学习的级联使用自组织映射（SOM网络）聚类。我们引入多核CPU和GPU基于TensorFlow的实现的加速过程。最后，我们证明了使用标准的现成的货架为数不多的镜头分类基准测试方法的有效性。</font>
</div>


<hr>
<div id="paper20"> <b>20. Learning more expressive joint distributions in multimodal variational  methods</b>  <a href="https://arxiv.org/pdf/2009.03651" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Nedelkoski%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sasho Nedelkoski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bogojeski%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mihail Bogojeski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kao%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Odej Kao</a><br>
<font size="3">
 Abstract: Data often are formed of multiple modalities, which jointly describe the observed phenomena. Modeling the joint distribution of multimodal data requires larger expressive power to capture high-level concepts and provide better data representations. However, multimodal generative models based on variational inference are limited due to the lack of flexibility of the approximate posterior, which is obtained by searching within a known parametric family of distributions. We introduce a method that improves the representational capacity of multimodal variational methods using normalizing flows. It approximates the joint posterior with a simple parametric distribution and subsequently transforms into a more complex one. Through several experiments, we demonstrate that the model improves on state-of-the-art multimodal methods based on variational inference on various computer vision tasks such as colorization, edge and mask detection, and weakly supervised learning. We also show that learning more powerful approximate joint distributions improves the quality of the generated samples. The code of our model is publicly available at this https URL.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：数据经常多个模态，它们共同描述了观察到的现象的形成。造型多模数据的联合分布需要较大的表现力，以捕捉高层次的概念，并提供更好的数据表示。然而，基于变推理多峰生成模型是有限的，由于缺乏的近似后验，这是由分布的已知参数家庭内进行搜索而获得的灵活性。我们介绍，可以改进使用正火流多峰变分法代表能力的方法。它近似于一个简单的参数分布的联合后验，并随后变换成更复杂的一个。经过多次实验，我们证明，该模型在国家的最先进的多模态方法改进了基于各种计算机视觉任务，如着色，边缘和面罩检测，和弱监督学习变的推论。我们还表明，学习更强大的近似联合分布提高了所产生样品的质量。我们模型的代码是公开的，在此HTTPS URL。</font>
</div>


<hr>
<div id="paper21"> <b>21. Imbalanced Continual Learning with Partitioning Reservoir Sampling</b>  <a href="https://arxiv.org/pdf/2009.03632" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+C+D" target="_blank" rel="noopener" style="color:#0000EE;">Chris Dongjoo Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jeong%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinseo Jeong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gunhee Kim</a><br>
<font size="3">
 Abstract: Continual learning from a sequential stream of data is a crucial challenge for machine learning research. Most studies have been conducted on this topic under the single-label classification setting along with an assumption of balanced label distribution. This work expands this research horizon towards multi-label classification. In doing so, we identify unanticipated adversity innately existent in many multi-label datasets, the long-tailed distribution. We jointly address the two independently solved problems, Catastropic Forgetting and the long-tailed label distribution by first empirically showing a new challenge of destructive forgetting of the minority concepts on the tail. Then, we curate two benchmark datasets, COCOseq and NUS-WIDEseq, that allow the study of both intra- and inter-task imbalances. Lastly, we propose a new sampling strategy for replay-based approach named Partitioning Reservoir Sampling (PRS), which allows the model to maintain a balanced knowledge of both head and tail classes. We publicly release the dataset and the code in our project page.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：从数据的连续流持续学习是机器学习研究的一个重大挑战。大多数研究关于这一主题的单标签分类设置下与均衡标签分发的假设一起被进行。这项工作扩大对多标签分类这项研究的视野。在此过程中，我们确定意外的逆境中许多多标签数据集，长尾分布天生存在。我们共同应对两个独立解决的问题，Catastropic遗忘和第一经验显示的尾巴上少数概念破坏性遗忘了新的挑战长尾标签分发。于是，我们策划2个基准数据集，COCOseq和NUS-WIDEseq，允许内和任务间失衡的研究。最后，我们提出了一个名为分区水库采样（PRS）的重播的方法，它允许模型保持头部和尾部类的平衡知识新的抽样策略。我们公开发布的数据集，并在我们的项目页面的代码。</font>
</div>


<hr>
<div id="paper22"> <b>22. Horus: Using Sensor Fusion to Combine Infrastructure and On-board  Sensing to Improve Autonomous Vehicle Safety</b>  <a href="https://arxiv.org/pdf/2009.03458" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Seshan%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sanjay Seshan</a><br>
<font size="3">
 Abstract: Studies predict that demand for autonomous vehicles will increase tenfold between 2019 and 2026. However, recent high-profile accidents have significantly impacted consumer confidence in this technology. The cause for many of these accidents can be traced back to the inability of these vehicles to correctly sense the impending danger. In response, manufacturers have been improving the already extensive on-vehicle sensor packages to ensure that the system always has access to the data necessary to ensure safe navigation. However, these sensor packages only provide a view from the vehicle's perspective and, as a result, autonomous vehicles still require frequent human intervention to ensure safety. To address this issue, I developed a system, called Horus, that combines on-vehicle and infrastructure-based sensors to provide a more complete view of the environment, including areas not visible from the vehicle. I built a small-scale experimental testbed as a proof of concept. My measurements of the impact of sensor failures showed that even short outages (1 second) at slow speeds (25 km/hr scaled velocity) prevents vehicles that rely on on-vehicle sensors from navigating properly. My experiments also showed that Horus dramatically improves driving safety and that the sensor fusion algorithm selected plays a significant role in the quality of the navigation. With just a pair of infrastructure sensors, Horus could tolerate sensors that fail 40% of the time and still navigate safely. These results are a promising first step towards safer autonomous vehicles.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：研究预测，对自主车的需求将增加十倍之间的2019和2026在这个技术然而，最近高调事故时有显著影响消费者信心。对于很多这些事故发生的原因可追溯到无力这些车辆的正确感知即将发生的危险。对此，制造商一直在改进已经广泛车载传感器封装，以确保系统始终获得必要的，以确保安全航行数据。然而，这些传感器包只提供从车辆的立体图和，因此，自主车仍需要频繁的人工干预，以确保安全。为了解决这个问题，我开发了一个系统，称为荷鲁斯，即在车辆和基础设施为基础的传感器相结合，提供了环境的更全面的了解，包括从车辆不可见区域。我建立了一个小规模的实验测试平台为概念的证明。我的传感器故障的影响测量结果表明，即使短时间中断，在依赖于车载传感器从导航正常低速（25公里/小时速度缩放）防止车辆（1秒）。我的实验还表明，荷鲁斯极大地提高了驾驶的安全性和所选择的传感器融合算法起到导航的质量显著的作用。只需对基础设施的传感器，荷鲁斯可以容忍失败时40％的传感器和导航仍然安全。这些结果是转向更加安全的自主车有前途的第一步。</font>
</div>


<hr>
<div id="paper23"> <b>23. Adversarial attacks on deep learning models for fatty liver disease  classification by modification of ultrasound image reconstruction method</b>  <a href="https://arxiv.org/pdf/2009.03364" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Byra%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michal Byra</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Styczynski%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Grzegorz Styczynski</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Szmigielski%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cezary Szmigielski</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Kalinowski%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Piotr Kalinowski</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Michalowski%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lukasz Michalowski</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Paluszkiewicz%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rafal Paluszkiewicz</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Ziarkiewicz-Wroblewska%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bogna Ziarkiewicz-Wroblewska</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Zieniewicz%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Krzysztof Zieniewicz</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Nowicki%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrzej Nowicki</a><br>
<font size="3">
 Abstract: Convolutional neural networks (CNNs) have achieved remarkable success in medical image analysis tasks. In ultrasound (US) imaging, CNNs have been applied to object classification, image reconstruction and tissue characterization. However, CNNs can be vulnerable to adversarial attacks, even small perturbations applied to input data may significantly affect model performance and result in wrong output. In this work, we devise a novel adversarial attack, specific to ultrasound (US) imaging. US images are reconstructed based on radio-frequency signals. Since the appearance of US images depends on the applied image reconstruction method, we explore the possibility of fooling deep learning model by perturbing US B-mode image reconstruction method. We apply zeroth order optimization to find small perturbations of image reconstruction parameters, related to attenuation compensation and amplitude compression, which can result in wrong output. We illustrate our approach using a deep learning model developed for fatty liver disease diagnosis, where the proposed adversarial attack achieved success rate of 48%.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：卷积神经网络（细胞神经网络）都实现了医学图像分析任务显着成效。在超声（US）成像，细胞神经网络已经被应用到对象分类，图像重建和组织表征。然而，细胞神经网络容易受到攻击的对抗，哪怕是很小的扰动应用于输入数据可能显著影响的输出错误模型的性能和结果。在这项工作中，我们设计了一种新的对抗攻击，具体到超声（US）成像。美国图像是基于射频信号重建。由于US图像的外观取决于所施加的图像重建方法，我们通过扰动US B模式图像重建方法探索嘴硬深学习模型的可能性。我们采用零阶优化找到的图像重建参数，涉及到的衰减补偿和振幅压缩，这可能会导致错误的输出小扰动。我们说明了使用脂肪肝疾病的诊断，其中所提出的对抗性攻击达到了48％的成功率建立了深厚的学习模式，我们的做法。</font>
</div>


<hr>
<div id="paper24"> <b>24. Going deeper with brain morphometry using neural networks</b>  <a href="https://arxiv.org/pdf/2009.03303" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Cruz%2C+R+S" target="_blank" rel="noopener" style="color:#0000EE;">Rodrigo Santa Cruz</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Lebrat%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Léo Lebrat</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Bourgeat%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pierrick Bourgeat</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Dor%C3%A9%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vincent Doré</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Dowling%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jason Dowling</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Fripp%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jurgen Fripp</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Fookes%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Clinton Fookes</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Salvado%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Olivier Salvado</a><br>
<font size="3">
 Abstract: Brain morphometry from magnetic resonance imaging (MRI) is a consolidated biomarker for many neurodegenerative diseases. Recent advances in this domain indicate that deep convolutional neural networks can infer morphometric measurements within a few seconds. Nevertheless, the accuracy of the devised model for insightful bio-markers (mean curvature and thickness) remains unsatisfactory. In this paper, we propose a more accurate and efficient neural network model for brain morphometry named HerstonNet. More specifically, we develop a 3D ResNet-based neural network to learn rich features directly from MRI, design a multi-scale regression scheme by predicting morphometric measures at feature maps of different resolutions, and leverage a robust optimization method to avoid poor quality minima and reduce the prediction variance. As a result, HerstonNet improves the existing approach by 24.30% in terms of intraclass correlation coefficient (agreement measure) to FreeSurfer silver-standards while maintaining a competitive run-time.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：磁共振成像（MRI）脑形态学是许多神经变性疾病的综合生物标志物。在这一领域的最新进展表明，深卷积神经网络可以在几秒钟之内推断形态测量。然而，对于有见地的生物标志物（平均曲率和厚度）所设计的模型的准确性仍然不能令人满意。在本文中，我们提出了一个名为HerstonNet大脑形态更准确，高效的神经网络模型。更具体地讲，我们开发了一个基于RESNET-3D的神经网络直接从MRI学习丰富的功能，通过在不同分辨率的特征图预测形态测量设计多尺度回归方案，并利用强大的优化方法，以避免质量极小差，减少预测方差。其结果是，HerstonNet同时保持有竞争力的运行时提高了24.30％在组内相关系数（协议度量）到FreeSurfer银标准方面的现有的方法。</font>
</div>


<hr>
<div id="paper25"> <b>25. Sensors, Safety Models and A System-Level Approach to Safe and Scalable  Automated Vehicles</b>  <a href="https://arxiv.org/pdf/2009.03301" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Weast%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jack Weast</a><br>
<font size="3">
 Abstract: When considering the accuracy of sensors in an automated vehicle (AV), it is not sufficient to evaluate the performance of any given sensor in isolation. Rather, the performance of any individual sensor must be considered in the context of the overall system design. Techniques like redundancy and different sensing modalities can reduce the chances of a sensing failure. Additionally, the use of safety models is essential to understanding whether any particular sensing failure is relevant. Only when the entire system design is taken into account can one properly understand the meaning of safety-relevant sensing failures in an AV. In this paper, we will consider what should actually constitute a sensing failure, how safety models play an important role in mitigating potential failures, how a system-level approach to safety will deliver a safe and scalable AV, and what an acceptable sensing failure rate should be considering the full picture of an AV's architecture.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：当考虑在自动车辆（AV）的传感器的准确度，它是不够的，以评估隔离任何给定的传感器的性能。相反，任何个别传感器的性能，必须在整个系统的设计的范围内加以考虑。像冗余和不同的传感方式技术可以减少传感失败的机会。此外，使用安全模型是理解任何特定的传感故障是相关的基本。只有当整个系统的设计是考虑到可以在一个正确的理解在AV安全相关的检测失败的含义。在本文中，我们会考虑什么其实应该构成传感故障，安全模型如何减缓潜在的故障，如何在系统级的安全方法将提供一个安全和可扩展的AV发挥了重要作用，什么可接受的传感故障率应考虑的AV架构的全貌。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！封面为论文标题词云图！</font></p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>PROCJX
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://procjx.github.io/2020/09/09/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-09-09/" title="【arxiv论文】 Computer Vision and Pattern Recognition 2020-09-09">https://procjx.github.io/2020/09/09/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-09-09/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">

        
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2020/09/09/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-09-09/" rel="next" title="【arxiv论文】 Computation and Language 2020-09-09">
                  <i class="fa fa-chevron-left"></i> 【arxiv论文】 Computation and Language 2020-09-09
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/2020/09/10/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-09-10/" rel="prev" title="【arxiv论文】 Computation and Language 2020-09-10">
                  【arxiv论文】 Computation and Language 2020-09-10 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>

        
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang -->
<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9197824246"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="gitalk-container"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#目录"><span class="nav-number">1.</span> <span class="nav-text">目录</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#摘要"><span class="nav-number">2.</span> <span class="nav-text">摘要</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="PROCJX"
    src="/images/procjx.png">
  <p class="site-author-name" itemprop="name">PROCJX</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">352</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">71</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/procjx" title="GitHub &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;procjx" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:procjx@gmail.com" title="E-Mail &amp;rarr; mailto:procjx@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>


<!--
      
        <script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
        <script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
        <div class="widget-wrap">
            <h3 class="widget-title">标签云</h3>
            <div id="myCanvasContainer" class="widget tagcloud">
                <canvas width="250" height="250" id="resCanvas" style="width=100%">
                    <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AAAI/" rel="tag">AAAI</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ACL/" rel="tag">ACL</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Accepted-Papers/" rel="tag">Accepted Papers</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ArXiv/" rel="tag">ArXiv</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/" rel="tag">BERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS20SI/" rel="tag">CS20SI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS224d/" rel="tag">CS224d</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/" rel="tag">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CVPR/" rel="tag">CVPR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Context/" rel="tag">Context</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cross-Lingual/" rel="tag">Cross Lingual</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dialog-System/" rel="tag">Dialog System</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse/" rel="tag">Discourse</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Ranking/" rel="tag">Discourse Ranking</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Structure/" rel="tag">Discourse Structure</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Document-NMT/" rel="tag">Document NMT</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EMNLP/" rel="tag">EMNLP</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Extractive/" rel="tag">Extractive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICLR/" rel="tag">ICLR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICML/" rel="tag">ICML</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IJCAI/" rel="tag">IJCAI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Inter-Sentence/" rel="tag">Inter-Sentence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Keyphrase-Generation/" rel="tag">Keyphrase Generation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NAACL/" rel="tag">NAACL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIPS/" rel="tag">NIPS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NMT/" rel="tag">NMT</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Relation-Extraction/" rel="tag">Neural Relation Extraction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RST/" rel="tag">RST</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Relation-Constraints/" rel="tag">Relation Constraints</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Summarization/" rel="tag">Summarization</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Translation/" rel="tag">Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Word-Translation/" rel="tag">Word Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/alias/" rel="tag">alias</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/git/" rel="tag">git</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pip/" rel="tag">pip</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/screen/" rel="tag">screen</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/" rel="tag">shell</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tgz/" rel="tag">tgz</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tts/" rel="tag">tts</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1/" rel="tag">冒泡</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/" rel="tag">冒泡排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%99%E4%BD%9C%E5%8A%A9%E6%89%8B/" rel="tag">写作助手</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8E%8B%E7%BC%A9/" rel="tag">压缩</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/" rel="tag">发送邮件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%88%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">合并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%8E%E5%8F%B0/" rel="tag">后台</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/" rel="tag">基数排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F/" rel="tag">希尔排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6/" rel="tag">归并</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">归并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/" rel="tag">快速排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F/" rel="tag">批量</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4/" rel="tag">批量删除</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F/" rel="tag">排序</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5/" rel="tag">插入</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/" rel="tag">插入排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%99%E7%A8%8B/" rel="tag">教程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/" rel="tag">斐波那契数列</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9D%80%E6%AD%BB%E8%BF%9B%E7%A8%8B/" rel="tag">杀死进程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B1%89%E8%AF%BA%E5%A1%94/" rel="tag">汉诺塔</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%A7%A3%E5%8E%8B/" rel="tag">解压</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%B0%B7%E6%AD%8C%E7%BF%BB%E8%AF%91/" rel="tag">谷歌翻译</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BF%AD%E4%BB%A3%E5%9B%9E%E7%BF%BB/" rel="tag">迭代回翻</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9/" rel="tag">选择</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/" rel="tag">选择排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%99%84%E4%BB%B6/" rel="tag">附件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3/" rel="tag">非监督</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%A2%86%E5%9F%9F%E9%80%82%E5%BA%94/" rel="tag">领域适应</a><span class="tag-list-count">1</span></li></ul>
                </canvas>
            </div>
        </div>
        
-->
        
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-hengfu -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9879871597"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-chuizhi -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="1662238719"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-zhengfangxing -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="6699421902"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PROCJX</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.4.2
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>












        
      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  <script src="/js/local-search.js"></script>













  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '2286ab64f5194d9d79ce',
      clientSecret: 'f912492bec2391664b40478f50f2f943376768d6',
      repo: 'procjx.github.io',
      owner: 'procjx',
      admin: ['procjx'],
      id: 'cc030631b031d7b4edce469e80c95da8',
        language: 'zh-CN',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
