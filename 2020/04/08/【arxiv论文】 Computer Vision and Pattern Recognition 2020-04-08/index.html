<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/procjx.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/procjxfavicon32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/procjxfavicon16x16.ico">
  <link rel="mask-icon" href="/images/procjx.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.4.2',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

<!-- Google Adsense -->
<!--
<script async src="//pagead2.googlesyndication.com/
pagead/js/adsbygoogle.js"></script>
<script>
(adsbygoogle = window.adsbygoogle || []).push({
google_ad_client: "pub-1179774715076800",
enable_page_level_ads: true
});
</script>
-->

<script data-ad-client="ca-pub-1179774715076800" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>


<meta name="google-site-verification" content="cEiGwg0T8Rj5msmuEcGYZTh5nnf05EhCXy0gp2Ml5BI" />
<meta name="baidu-site-verification" content="noSKHe8MJs" />

  <meta name="description" content="目录  1. Feature Pyramid Grids [PDF] 摘要  2. Event Based, Near Eye Gaze Tracking Beyond 10,000Hz [PDF] 摘要  3. Disp R-CNN: Stereo 3D Object Detection via Shape Prior Guided Instance  Disparity Estimation">
<meta property="og:type" content="article">
<meta property="og:title" content="【arxiv论文】 Computer Vision and Pattern Recognition 2020-04-08">
<meta property="og:url" content="https:&#x2F;&#x2F;procjx.github.io&#x2F;2020&#x2F;04&#x2F;08&#x2F;%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-04-08&#x2F;index.html">
<meta property="og:site_name" content="PROCJX&#39;s BLOGS">
<meta property="og:description" content="目录  1. Feature Pyramid Grids [PDF] 摘要  2. Event Based, Near Eye Gaze Tracking Beyond 10,000Hz [PDF] 摘要  3. Disp R-CNN: Stereo 3D Object Detection via Shape Prior Guided Instance  Disparity Estimation">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2020-04-08T05:00:04.032Z">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://procjx.github.io/2020/04/08/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-04-08/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-04-08 | PROCJX's BLOGS</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">PROCJX's BLOGS</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">WITH LOVE OF WORLD</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-fw fa-download"></i>资源</a>

  </li>
        <li class="menu-item menu-item-arxiv">

    <a href="/arxiv/" rel="section"><i class="fa fa-fw fa-file-pdf-o"></i>arxiv论文</a>

  </li>
        <li class="menu-item menu-item-deadline">

    <a href="/deadline/" rel="section"><i class="fa fa-fw fa-calendar"></i>会议截稿</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://procjx.github.io/2020/04/08/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-04-08/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/procjx.png">
      <meta itemprop="name" content="PROCJX">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PROCJX's BLOGS">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          【arxiv论文】 Computer Vision and Pattern Recognition 2020-04-08
        </h2>

        <div class="post-meta">
        
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-04-08 12:45:53 / 修改时间：13:00:04" itemprop="dateCreated datePublished" datetime="2020-04-08T12:45:53+08:00">2020-04-08</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/arxiv/" itemprop="url" rel="index">
                    <span itemprop="name">arxiv</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/arxiv/CV/" itemprop="url" rel="index">
                    <span itemprop="name">CV</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
              <span>39k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
              <span>1:05</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Feature Pyramid Grids <a href="https://arxiv.org/pdf/2004.03580" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Event Based, Near Eye Gaze Tracking Beyond 10,000Hz <a href="https://arxiv.org/pdf/2004.03577" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Disp R-CNN: Stereo 3D Object Detection via Shape Prior Guided Instance  Disparity Estimation <a href="https://arxiv.org/pdf/2004.03572" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Temporal Pyramid Network for Action Recognition <a href="https://arxiv.org/pdf/2004.03548" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Unsupervised Person Re-identification via Softened Similarity Learning <a href="https://arxiv.org/pdf/2004.03547" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Dense Regression Network for Video Grounding <a href="https://arxiv.org/pdf/2004.03545" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Deep Multi-Shot Network for modelling Appearance Similarity in  Multi-Person Tracking applications <a href="https://arxiv.org/pdf/2004.03531" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Bayesian aggregation improves traditional single image crop  classification approaches <a href="https://arxiv.org/pdf/2004.03468" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Hierarchical Image Classification using Entailment Cone Embeddings <a href="https://arxiv.org/pdf/2004.03459" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Learning Formation of Physically-Based Face Attributes <a href="https://arxiv.org/pdf/2004.03458" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Simultaneous Learning from Human Pose and Object Cues for Real-Time  Activity Recognition <a href="https://arxiv.org/pdf/2004.03453" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Strategies for Robust Image Classification <a href="https://arxiv.org/pdf/2004.03452" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> RSS-Net: Weakly-Supervised Multi-Class Semantic Segmentation with FMCW  Radar <a href="https://arxiv.org/pdf/2004.03451" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Nonparametric Data Analysis on the Space of Perceived Colors <a href="https://arxiv.org/pdf/2004.03402" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> MNEW: Multi-domain Neighborhood Embedding and Weighting for Sparse Point  Clouds Segmentation <a href="https://arxiv.org/pdf/2004.03401" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Differential 3D Facial Recognition: Adding 3D to Your State-of-the-Art  2D Method <a href="https://arxiv.org/pdf/2004.03385" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Attribution in Scale and Space <a href="https://arxiv.org/pdf/2004.03383" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Error-Corrected Margin-Based Deep Cross-Modal Hashing for Facial Image  Retrieval <a href="https://arxiv.org/pdf/2004.03378" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Composition of Saliency Metrics for Channel Pruning with a Myopic Oracle <a href="https://arxiv.org/pdf/2004.03376" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Robust Self-Supervised Convolutional Neural Network for Subspace  Clustering and Classification <a href="https://arxiv.org/pdf/2004.03375" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Improving BPSO-based feature selection applied to offline WI handwritten  signature verification through overfitting control <a href="https://arxiv.org/pdf/2004.03373" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> A white-box analysis on the writer-independent dichotomy transformation  applied to offline handwritten signature verification <a href="https://arxiv.org/pdf/2004.03370" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Knife and Threat Detectors <a href="https://arxiv.org/pdf/2004.03366" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> A Machine Learning Based Framework for the Smart Healthcare Monitoring <a href="https://arxiv.org/pdf/2004.03360" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Deep learning approaches in food recognition <a href="https://arxiv.org/pdf/2004.03357" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> Inclusive GAN: Improving Data and Minority Coverage in Generative Models <a href="https://arxiv.org/pdf/2004.03355" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> An Image Labeling Tool and Agricultural Dataset for Deep Learning <a href="https://arxiv.org/pdf/2004.03351" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> Automatic Generation of Chinese Handwriting via Fonts Style  Representation Learning <a href="https://arxiv.org/pdf/2004.03339" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Multiform Fonts-to-Fonts Translation via Style and Content Disentangled  Representations of Chinese Character <a href="https://arxiv.org/pdf/2004.03338" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> An End-to-End Approach for Recognition of Modern and Historical  Handwritten Numeral Strings <a href="https://arxiv.org/pdf/2004.03337" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> Predict the model of a camera <a href="https://arxiv.org/pdf/2004.03336" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> FusedProp: Towards Efficient Training of Generative Adversarial Networks <a href="https://arxiv.org/pdf/2004.03335" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> Streaming Networks: Increase Noise Robustness and Filter Diversity via  Hard-wired and Input-induced Sparsity <a href="https://arxiv.org/pdf/2004.03334" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
<div id="title34">
<b>34.</b> Cascaded Refinement Network for Point Cloud Completion <a href="https://arxiv.org/pdf/2004.03327" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper34" style="color:#0000EE;">摘要</a><br></div>
<div id="title35">
<b>35.</b> Towards Efficient Unconstrained Palmprint Recognition via Deep  Distillation Hashing <a href="https://arxiv.org/pdf/2004.03303" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper35" style="color:#0000EE;">摘要</a><br></div>
<div id="title36">
<b>36.</b> Pyramid Focusing Network for mutation prediction and classification in  CT images <a href="https://arxiv.org/pdf/2004.03302" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper36" style="color:#0000EE;">摘要</a><br></div>
<div id="title37">
<b>37.</b> Super-resolution of clinical CT volumes with modified CycleGAN using  micro CT volumes <a href="https://arxiv.org/pdf/2004.03272" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper37" style="color:#0000EE;">摘要</a><br></div>
<div id="title38">
<b>38.</b> SC4D: A Sparse 4D Convolutional Network for Skeleton-Based Action  Recognition <a href="https://arxiv.org/pdf/2004.03259" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper38" style="color:#0000EE;">摘要</a><br></div>
<div id="title39">
<b>39.</b> Hierarchical Opacity Propagation for Image Matting <a href="https://arxiv.org/pdf/2004.03249" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper39" style="color:#0000EE;">摘要</a><br></div>
<div id="title40">
<b>40.</b> Motion-supervised Co-Part Segmentation <a href="https://arxiv.org/pdf/2004.03234" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper40" style="color:#0000EE;">摘要</a><br></div>
<div id="title41">
<b>41.</b> Utilising Prior Knowledge for Visual Navigation: Distil and Adapt <a href="https://arxiv.org/pdf/2004.03222" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper41" style="color:#0000EE;">摘要</a><br></div>
<div id="title42">
<b>42.</b> Neural Image Inpainting Guided with Descriptive Text <a href="https://arxiv.org/pdf/2004.03212" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper42" style="color:#0000EE;">摘要</a><br></div>
<div id="title43">
<b>43.</b> Multi-Task Learning via Co-Attentive Sharing for Pedestrian Attribute  Recognition <a href="https://arxiv.org/pdf/2004.03164" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper43" style="color:#0000EE;">摘要</a><br></div>
<div id="title44">
<b>44.</b> Real-time Classification from Short Event-Camera Streams using  Input-filtering Neural ODEs <a href="https://arxiv.org/pdf/2004.03156" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper44" style="color:#0000EE;">摘要</a><br></div>
<div id="title45">
<b>45.</b> Adaptive Multiscale Illumination-Invariant Feature Representation for  Undersampled Face Recognition <a href="https://arxiv.org/pdf/2004.03153" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper45" style="color:#0000EE;">摘要</a><br></div>
<div id="title46">
<b>46.</b> Predicting Camera Viewpoint Improves Cross-dataset Generalization for 3D  Human Pose Estimation <a href="https://arxiv.org/pdf/2004.03143" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper46" style="color:#0000EE;">摘要</a><br></div>
<div id="title47">
<b>47.</b> Human Motion Transfer from Poses in the Wild <a href="https://arxiv.org/pdf/2004.03142" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper47" style="color:#0000EE;">摘要</a><br></div>
<div id="title48">
<b>48.</b> Toward Fine-grained Facial Expression Manipulation <a href="https://arxiv.org/pdf/2004.03132" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper48" style="color:#0000EE;">摘要</a><br></div>
<div id="title49">
<b>49.</b> Generative Adversarial Zero-shot Learning via Knowledge Graphs <a href="https://arxiv.org/pdf/2004.03109" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper49" style="color:#0000EE;">摘要</a><br></div>
<div id="title50">
<b>50.</b> End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection <a href="https://arxiv.org/pdf/2004.03080" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper50" style="color:#0000EE;">摘要</a><br></div>
<div id="title51">
<b>51.</b> A Method for Curation of Web-Scraped Face Image Datasets <a href="https://arxiv.org/pdf/2004.03074" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper51" style="color:#0000EE;">摘要</a><br></div>
<div id="title52">
<b>52.</b> MGGR: MultiModal-Guided Gaze Redirection with Coarse-to-Fine Learning <a href="https://arxiv.org/pdf/2004.03064" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper52" style="color:#0000EE;">摘要</a><br></div>
<div id="title53">
<b>53.</b> Depth Sensing Beyond LiDAR Range <a href="https://arxiv.org/pdf/2004.03048" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper53" style="color:#0000EE;">摘要</a><br></div>
<div id="title54">
<b>54.</b> Manifold-driven Attention Maps for Weakly Supervised Segmentation <a href="https://arxiv.org/pdf/2004.03046" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper54" style="color:#0000EE;">摘要</a><br></div>
<div id="title55">
<b>55.</b> When, Where, and What? A New Dataset for Anomaly Detection in Driving  Videos <a href="https://arxiv.org/pdf/2004.03044" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper55" style="color:#0000EE;">摘要</a><br></div>
<div id="title56">
<b>56.</b> Learning Generative Models of Shape Handles <a href="https://arxiv.org/pdf/2004.03028" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper56" style="color:#0000EE;">摘要</a><br></div>
<div id="title57">
<b>57.</b> Field-Level Crop Type Classification with k Nearest Neighbors: A  Baseline for a New Kenya Smallholder Dataset <a href="https://arxiv.org/pdf/2004.03023" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper57" style="color:#0000EE;">摘要</a><br></div>
<div id="title58">
<b>58.</b> Adaptive Fractional Dilated Convolution Network for Image Aesthetics  Assessment <a href="https://arxiv.org/pdf/2004.03015" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper58" style="color:#0000EE;">摘要</a><br></div>
<div id="title59">
<b>59.</b> LUVLi Face Alignment: Estimating Landmarks' Location, Uncertainty, and  Visibility Likelihood <a href="https://arxiv.org/pdf/2004.02980" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper59" style="color:#0000EE;">摘要</a><br></div>
<div id="title60">
<b>60.</b> Deblurring using Analysis-Synthesis Networks Pair <a href="https://arxiv.org/pdf/2004.02956" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper60" style="color:#0000EE;">摘要</a><br></div>
<div id="title61">
<b>61.</b> Objectness-Aware One-Shot Semantic Segmentation <a href="https://arxiv.org/pdf/2004.02945" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper61" style="color:#0000EE;">摘要</a><br></div>
<div id="title62">
<b>62.</b> Fingerprint Presentation Attack Detection: A Sensor and Material  Agnostic Approach <a href="https://arxiv.org/pdf/2004.02941" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper62" style="color:#0000EE;">摘要</a><br></div>
<div id="title63">
<b>63.</b> Efficient Scale Estimation Methods using Lightweight Deep Convolutional  Neural Networks for Visual Tracking <a href="https://arxiv.org/pdf/2004.02933" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper63" style="color:#0000EE;">摘要</a><br></div>
<div id="title64">
<b>64.</b> Beyond Background-Aware Correlation Filters: Adaptive Context Modeling  by Hand-Crafted and Deep RGB Features for Visual Tracking <a href="https://arxiv.org/pdf/2004.02932" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper64" style="color:#0000EE;">摘要</a><br></div>
<div id="title65">
<b>65.</b> Empirical Upper Bound, Error Diagnosis and Invariance Analysis of Modern  Object Detectors <a href="https://arxiv.org/pdf/2004.02877" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper65" style="color:#0000EE;">摘要</a><br></div>
<div id="title66">
<b>66.</b> U-Net Using Stacked Dilated Convolutions for Medical Image Segmentation <a href="https://arxiv.org/pdf/2004.03466" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper66" style="color:#0000EE;">摘要</a><br></div>
<div id="title67">
<b>67.</b> Learning to Accelerate Decomposition for Multi-Directional 3D Printing <a href="https://arxiv.org/pdf/2004.03450" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper67" style="color:#0000EE;">摘要</a><br></div>
<div id="title68">
<b>68.</b> Deep Open Space Segmentation using Automotive Radar <a href="https://arxiv.org/pdf/2004.03449" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper68" style="color:#0000EE;">摘要</a><br></div>
<div id="title69">
<b>69.</b> Harmony-Search and Otsu based System for Coronavirus Disease (COVID-19)  Detection using Lung CT Scan Images <a href="https://arxiv.org/pdf/2004.03431" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper69" style="color:#0000EE;">摘要</a><br></div>
<div id="title70">
<b>70.</b> Automated Smartphone based System for Diagnosis of Diabetic Retinopathy <a href="https://arxiv.org/pdf/2004.03408" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper70" style="color:#0000EE;">摘要</a><br></div>
<div id="title71">
<b>71.</b> Deep Learning on Chest X-ray Images to Detect and Evaluate Pneumonia  Cases at the Era of COVID-19 <a href="https://arxiv.org/pdf/2004.03399" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper71" style="color:#0000EE;">摘要</a><br></div>
<div id="title72">
<b>72.</b> Complete CVDL Methodology for Investigating Hydrodynamic Instabilities <a href="https://arxiv.org/pdf/2004.03374" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper72" style="color:#0000EE;">摘要</a><br></div>
<div id="title73">
<b>73.</b> Convolutional Neural Networks based automated segmentation and labelling  of the lumbar spine X-ray <a href="https://arxiv.org/pdf/2004.03364" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper73" style="color:#0000EE;">摘要</a><br></div>
<div id="title74">
<b>74.</b> Binary Neural Networks: A Survey <a href="https://arxiv.org/pdf/2004.03333" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper74" style="color:#0000EE;">摘要</a><br></div>
<div id="title75">
<b>75.</b> Two-Stage Resampling for Convolutional Neural Network Training in the  Imbalanced Colorectal Cancer Image Classification <a href="https://arxiv.org/pdf/2004.03332" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper75" style="color:#0000EE;">摘要</a><br></div>
<div id="title76">
<b>76.</b> Teacher-Class Network: A Neural Network Compression Mechanism <a href="https://arxiv.org/pdf/2004.03281" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper76" style="color:#0000EE;">摘要</a><br></div>
<div id="title77">
<b>77.</b> Autoencoders for Unsupervised Anomaly Segmentation in Brain MR Images: A  Comparative Study <a href="https://arxiv.org/pdf/2004.03271" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper77" style="color:#0000EE;">摘要</a><br></div>
<div id="title78">
<b>78.</b> Inspector Gadget: A Data Programming-based Labeling System for  Industrial Images <a href="https://arxiv.org/pdf/2004.03264" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper78" style="color:#0000EE;">摘要</a><br></div>
<div id="title79">
<b>79.</b> Iconify: Converting Photographs into Icons <a href="https://arxiv.org/pdf/2004.03179" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper79" style="color:#0000EE;">摘要</a><br></div>
<div id="title80">
<b>80.</b> Deep Attentive Generative Adversarial Network for Photo-Realistic Image  De-Quantization <a href="https://arxiv.org/pdf/2004.03150" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper80" style="color:#0000EE;">摘要</a><br></div>
<div id="title81">
<b>81.</b> Plug-and-play ISTA converges with kernel denoisers <a href="https://arxiv.org/pdf/2004.03145" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper81" style="color:#0000EE;">摘要</a><br></div>
<div id="title82">
<b>82.</b> Generalized Label Enhancement with Sample Correlations <a href="https://arxiv.org/pdf/2004.03104" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper82" style="color:#0000EE;">摘要</a><br></div>
<div id="title83">
<b>83.</b> COVID-Xpert: An AI Powered Population Screening of COVID-19 Cases Using  Chest Radiography Images <a href="https://arxiv.org/pdf/2004.03042" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper83" style="color:#0000EE;">摘要</a><br></div>
<div id="title84">
<b>84.</b> Dense Steerable Filter CNNs for Exploiting Rotational Symmetry in  Histology Images <a href="https://arxiv.org/pdf/2004.03037" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper84" style="color:#0000EE;">摘要</a><br></div>
<div id="title85">
<b>85.</b> Evolving Normalization-Activation Layers <a href="https://arxiv.org/pdf/2004.02967" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper85" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>

<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Feature Pyramid Grids</b>  <a href="https://arxiv.org/pdf/2004.03580" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kai Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuhang Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Loy%2C+C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chen Change Loy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dahua Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feichtenhofer%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christoph Feichtenhofer</a><br>
<font size="3">
Abstract: Feature pyramid networks have been widely adopted in the object detection literature to improve feature representations for better handling of variations in scale. In this paper, we present Feature Pyramid Grids (FPG), a deep multi-pathway feature pyramid, that represents the feature scale-space as a regular grid of parallel bottom-up pathways which are fused by multi-directional lateral connections. FPG can improve single-pathway feature pyramid networks by significantly increasing its performance at similar computation cost, highlighting importance of deep pyramid representations. In addition to its general and uniform structure, over complicated structures that have been found with neural architecture search, it also compares favorably against such approaches without relying on search. We hope that FPG with its uniform and effective nature can serve as a strong component for future work in object recognition. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：功能金字塔网络在目标物检测文献中被广泛采用，以提高功能表示为更好地处理规模的变化。在本文中，我们本特征金字塔网格（FPG），深多途径特征金字塔，表示所述特征的尺度空间作为并行自下而上途径的规则网格其通过多方向横向连接稠合。 FPG可以通过类似的计算成本显著提高其性能，突出深金字塔表示的重要性提高单通路功能金字塔网络。除了它一般和结构均匀，已经发现与神经结构搜索过于复杂的结构，它也毫不逊色反对这种方法进行了比较，而不依赖于搜索。我们希望能与它的统一和有效本性FPG可以作为在物体识别今后工作的强有力的成分。</font>
</div>


<hr>
<div id="paper2"> <b>2. Event Based, Near Eye Gaze Tracking Beyond 10,000Hz</b>  <a href="https://arxiv.org/pdf/2004.03577" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Angelopoulos%2C+A+N" target="_blank" rel="noopener" style="color:#0000EE;">Anastasios N. Angelopoulos</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Martel%2C+J+N+P" target="_blank" rel="noopener" style="color:#0000EE;">Julien N.P. Martel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kohli%2C+A+P+S" target="_blank" rel="noopener" style="color:#0000EE;">Amit P.S. Kohli</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Conradt%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jorg Conradt</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wetzstein%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gordon Wetzstein</a><br>
<font size="3">
Abstract: Fast and accurate eye tracking is crucial for many applications. Current camera-based eye tracking systems, however, are fundamentally limited by their bandwidth, forcing a tradeoff between image resolution and framerate, i.e. between latency and update rate. Here, we propose a hybrid frame-event-based near-eye gaze tracking system offering update rates beyond 10,000 Hz with an accuracy that matches that of high-end desktop-mounted commercial eye trackers when evaluated in the same conditions. Our system builds on emerging event cameras that simultaneously acquire regularly sampled frames and adaptively sampled events. We develop an online 2D pupil fitting method that updates a parametric model every one or few events. Moreover, we propose a polynomial regressor for estimating the gaze vector from the parametric pupil model in real time. Using the first hybrid frame-event gaze dataset, which will be made public, we demonstrate that our system achieves accuracies of 0.45 degrees - 1.75 degrees for fields of view ranging from 45 degrees to 98 degrees. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：快速准确的眼动追踪对许多应用是至关重要的。当前基于相机的眼睛跟踪系统，但是，从根本上可以通过带宽的限制，迫使延迟和更新速率之间的图像分辨率和帧率，即之间的折衷。在这里，我们提出了一种基于帧事件混合动力车近眼睛注视跟踪系统提供的更新速率超过10000赫兹，其精确度在相同条件下评估时高端的比赛桌面安装商业眼动仪。我们的系统建立在新兴的事件摄像机可同时获得定期采样帧和自适应采样事件。我们开发了一个在线2D瞳拟合方法，更新的参数模型的每一个或几个事件。此外，我们提出了一个多项式回归从实时参数瞳孔模型估计的注视矢量。使用所述第一混合帧事件注视数据集，这将被公开，我们证明，我们的系统实现了0.45度的精度 -  1.75度用于测距从45度到98度的视场。</font>
</div>


<hr>
<div id="paper3"> <b>3. Disp R-CNN: Stereo 3D Object Detection via Shape Prior Guided Instance  Disparity Estimation</b>  <a href="https://arxiv.org/pdf/2004.03572" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiaming Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Linghao Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yiming Xie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Siyu Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qinhong Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaowei Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hujun Bao</a><br>
<font size="3">
Abstract: In this paper, we propose a novel system named Disp R-CNN for 3D object detection from stereo images. Many recent works solve this problem by first recovering a point cloud with disparity estimation and then apply a 3D detector. The disparity map is computed for the entire image, which is costly and fails to leverage category-specific prior. In contrast, we design an instance disparity estimation network (iDispNet) that predicts disparity only for pixels on objects of interest and learns a category-specific shape prior for more accurate disparity estimation. To address the challenge from scarcity of disparity annotation in training, we propose to use a statistical shape model to generate dense disparity pseudo-ground-truth without the need of LiDAR point clouds, which makes our system more widely applicable. Experiments on the KITTI dataset show that, even when LiDAR ground-truth is not available at training time, Disp R-CNN achieves competitive performance and outperforms previous state-of-the-art methods by 20% in terms of average precision. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出了名为详细显示R-CNN用于从立体图像的3D对象检测的新颖系统。最近的许多作品首先恢复与差异估算点云解决这个问题，然后应用3D探测器。视差图来计算针对整个图像，这是昂贵的和以前未能类别特异性的杠杆作用。与此相反，我们设计出只对感兴趣的对象像素预测差距和学习之前更精确的视差估计一类特定形状的实例视差估计网络（iDispNet）。为了解决在训练差距注释稀缺的挑战，我们建议使用一个统计形状模型来生成稠密视差伪地面实况，而不需要激光雷达点云的，这使得我们的系统更加广泛适用。在KITTI实验数据集上，即使在激光雷达地面实况不可用在训练时间，详细显示R-CNN实现了有竞争力的性能，远远超过前国家的最先进的方法，通过20％的平均准确率方面。</font>
</div>


<hr>
<div id="paper4"> <b>4. Temporal Pyramid Network for Action Recognition</b>  <a href="https://arxiv.org/pdf/2004.03548" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Ceyuan Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yinghao Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianping Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Dai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bolei Zhou</a><br>
<font size="3">
Abstract: Visual tempo characterizes the dynamics and the temporal scale of an action. Modeling such visual tempos of different actions facilitates their recognition. Previous works often capture the visual tempo through sampling raw videos at multiple rates and constructing an input-level frame pyramid, which usually requires a costly multi-branch network to handle. In this work we propose a generic Temporal Pyramid Network (TPN) at the feature-level, which can be flexibly integrated into 2D or 3D backbone networks in a plug-and-play manner. Two essential components of TPN, the source of features and the fusion of features, form a feature hierarchy for the backbone so that it can capture action instances at various tempos. TPN also shows consistent improvements over other challenging baselines on several action recognition datasets. Specifically, when equipped with TPN, the 3D ResNet-50 with dense sampling obtains a 2% gain on the validation set of Kinetics-400. A further analysis also reveals that TPN gains most of its improvements on action classes that have large variances in their visual tempos, validating the effectiveness of TPN. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：视觉节奏特点的动态和行动的时间尺度。造型各异的动作这样的视觉节奏有利于他们的认可。以前的作品往往通过多个速率采样原始视频和构建输入级帧的金字塔，这通常需要昂贵的多分支网络来处理捕获的视觉节奏。在这项工作中，我们提出了一个通用的时空金字塔网（TPN）在功能层面，它可以在一个插件和播放方式灵活地集成到2D或3D骨干网。 TPN的两个重要组成部分，功能的来源和功能融合，形成骨干功能层次，以便它可以在不同的节拍捕捉动作的情况。 TPN还显示了几个动作识别的数据集在其它极具挑战性基线持续改善。具体地，当配备有TPN，所述3D RESNET-50与密集采样获得关于验证集合动力学-400的2％的增益。进一步的分析还表明，TPN获得其大部分的改进对那些在他们的视觉节奏变化很大的，验证TPN的有效性动作类。</font>
</div>


<hr>
<div id="paper5"> <b>5. Unsupervised Person Re-identification via Softened Similarity Learning</b>  <a href="https://arxiv.org/pdf/2004.03547" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yutian Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lingxi Xie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chenggang Yan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Tian</a><br>
<font size="3">
Abstract: Person re-identification (re-ID) is an important topic in computer vision. This paper studies the unsupervised setting of re-ID, which does not require any labeled information and thus is freely deployed to new scenarios. There are very few studies under this setting, and one of the best approach till now used iterative clustering and classification, so that unlabeled images are clustered into pseudo classes for a classifier to get trained, and the updated features are used for clustering and so on. This approach suffers two problems, namely, the difficulty of determining the number of clusters, and the hard quantization loss in clustering. In this paper, we follow the iterative training mechanism but discard clustering, since it incurs loss from hard quantization, yet its only product, image-level similarity, can be easily replaced by pairwise computation and a softened classification task. With these improvements, our approach becomes more elegant and is more robust to hyper-parameter changes. Experiments on two image-based and video-based datasets demonstrate state-of-the-art performance under the unsupervised re-ID setting. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人重新鉴定（重新-ID）是计算机视觉中的一个重要课题。本文研究再ID的无监督的设置，它不需要任何标签的信息，从而自由地部署新的场景。有在此设置下很少研究，至今使用迭代聚类和分类的最佳方法之一，使未标记的图像被聚集成伪类分类得到培训，更新的功能是用于群集等。这种方法受到两个问题，即确定集群的数量的难度，并在集群硬量化损失。在本文中，我们遵循的迭代训练机制，但丢弃群集，因为它会带来硬量化损失，但其唯一的产品，图像层次相似，可以很容易地通过逐计算和软化的分类任务所取代。通过这些改进，我们的方法变得更优雅，更稳健的超参数变化。上两个基于视频图像为基础的和数据集实验表明无监督重新ID设定下状态的最先进的性能。</font>
</div>


<hr>
<div id="paper6"> <b>6. Dense Regression Network for Video Grounding</b>  <a href="https://arxiv.org/pdf/2004.03545" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Runhao Zeng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haoming Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenbing Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peihao Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mingkui Tan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chuang Gan</a><br>
<font size="3">
Abstract: We address the problem of video grounding from natural language queries. The key challenge in this task is that one training video might only contain a few annotated starting/ending frames that can be used as positive examples for model training. Most conventional approaches directly train a binary classifier using such imbalance data, thus achieving inferior results. The key idea of this paper is to use the distances between the frame within the ground truth and the starting (ending) frame as dense supervisions to improve the video grounding accuracy. Specifically, we design a novel dense regression network (DRN) to regress the distances from each frame to the starting (ending) frame of the video segment described by the query. We also propose a simple but effective IoU regression head module to explicitly consider the localization quality of the grounding results (i.e., the IoU between the predicted location and the ground truth). Experimental results show that our approach significantly outperforms state-of-the-arts on three datasets (i.e., Charades-STA, ActivityNet-Captions, and TACoS). </font>
<br>
<font size="2" style="line-height:30px;">
摘要：从地址的自然语言查询视频接地的问题。此任务中的关键挑战是，一个培训视频可能只包含可作为模型训练正例数注释开始/结束帧。最传统的方法使用这样的不平衡数据，从而实现差的结果直接训练二元分类器。本文的核心思想是使用真实地面内的画面，并开始（结束）帧密集监督，以提高视频接地精度之间的距离。具体来说，我们设计了一个新的密回归网络（DRN），以从每一帧的距离回归到由查询所描述的视频段的开始（结束）帧。我们还提出了一个简单而有效的欠条回归头模组，明确地考虑接地结果的本地化质量（即，预测位置和地面实况之间的IOU）。实验结果表明，我们的方法显著优于国家的最艺术的三个数据集（即字谜-STA，ActivityNet，字幕，和玉米饼）。</font>
</div>


<hr>
<div id="paper7"> <b>7. Deep Multi-Shot Network for modelling Appearance Similarity in  Multi-Person Tracking applications</b>  <a href="https://arxiv.org/pdf/2004.03531" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=G%C3%B3mez-Silva%2C+M+J" target="_blank" rel="noopener" style="color:#0000EE;">María J. Gómez-Silva</a><br>
<font size="3">
Abstract: The automatization of Multi-Object Tracking becomes a demanding task in real unconstrained scenarios, where the algorithms have to deal with crowds, crossing people, occlusions, disappearances and the presence of visually similar individuals. In those circumstances, the data association between the incoming detections and their corresponding identities could miss some tracks or produce identity switches. In order to reduce these tracking errors, and even their propagation in further frames, this article presents a Deep Multi-Shot neural model for measuring the Degree of Appearance Similarity (MS-DoAS) between person observations. This model provides temporal consistency to the individuals' appearance representation, and provides an affinity metric to perform frame-by-frame data association, allowing online tracking. The model has been deliberately trained to be able to manage the presence of previous identity switches and missed observations in the handled tracks. With that purpose, a novel data generation tool has been designed to create training tracklets that simulate such situations. The model has demonstrated a high capacity to discern when a new observation corresponds to a certain track, achieving a classification accuracy of 97\% in a hard test that simulates tracks with previous mistakes. Moreover, the tracking efficiency of the model in a Surveillance application has been demonstrated by integrating that into the frame-by-frame association of a Tracking-by-Detection algorithm. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：多目标跟踪的自动化成为现实不受约束的情况，这里的算法必须处理的人群，过路的人，闭塞，失踪和视觉上相似的个体存在一个艰巨的任务。在这些情况下，输入的检测和它们对应的身份之间的关联的数据可能会错过一些轨道或出示身份开关。为了减少这些跟踪误差，甚至他们在进一步帧传输，本文提出了一种人的意见之间的测量外观的相似度（MS-DOAS）深连拍神经网络模型。该模型提供了对个体的外观表示时间一致性，并且提供度量来执行帧接一帧数据关联，允许在线跟踪的亲和力。该模型被刻意培养成能够管理在处理的磁道此前的身份交换机和错过意见的存在。有了这个目的，一种新型的数据生成工具的设计是为了创建一个模拟这种情况下训练tracklets。该模型已显示出高容量以辨别何时一个新的观察对应于某一轨道，在硬测试模拟与先前的错误的轨道实现的97 \％的分类准确度。此外，在监视应用程序中的模型的跟踪效率已经通过集成证明成跟踪逐检测算法的帧接一帧的关联。</font>
</div>


<hr>
<div id="paper8"> <b>8. Bayesian aggregation improves traditional single image crop  classification approaches</b>  <a href="https://arxiv.org/pdf/2004.03468" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Matvienko%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ivan Matvienko</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gasanov%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mikhail Gasanov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Petrovskaia%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anna Petrovskaia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jana%2C+R+B" target="_blank" rel="noopener" style="color:#0000EE;">Raghavendra Belur Jana</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pukalchik%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maria Pukalchik</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Oseledets%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ivan Oseledets</a><br>
<font size="3">
Abstract: Machine learning (ML) methods and neural networks (NN) are widely implemented for crop types recognition and classification based on satellite images. However, most of these studies use several multi-temporal images which could be inapplicable for cloudy regions. We present a comparison between the classical ML approaches and U-Net NN for classifying crops with a single satellite image. The results show the advantages of using field-wise classification over pixel-wise approach. We first used a Bayesian aggregation for field-wise classification and improved on 1.5% results between majority voting aggregation. The best result for single satellite image crop classification is achieved for gradient boosting with an overall accuracy of 77.4% and macro F1-score 0.66. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：机器学习（ML）方法和神经网络（NN）基于卫星图像作物种类识别和分类的广泛实施。然而，这些研究大多使用的可能是不适用于阴天地区几个多时相影像。我们提出的经典ML之间的比较接近和U-Net的NN用于作物与单个卫星图像分类。结果表明，使用现场明智的分类在逐象素方法的优点。我们首先用于现场明智的分类的贝叶斯聚集和多数表决聚集之间1.5％的结果改善。对于单个卫星图像作物分类最好的结果是对于具有77.4％的总体准确度和宏观梯度升压实现F1-得分0.66。</font>
</div>


<hr>
<div id="paper9"> <b>9. Hierarchical Image Classification using Entailment Cone Embeddings</b>  <a href="https://arxiv.org/pdf/2004.03459" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Dhall%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ankit Dhall</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Makarova%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anastasia Makarova</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ganea%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Octavian Ganea</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pavllo%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dario Pavllo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Greeff%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Greeff</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Krause%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andreas Krause</a><br>
<font size="3">
Abstract: Image classification has been studied extensively, but there has been limited work in using unconventional, external guidance other than traditional image-label pairs for training. We present a set of methods for leveraging information about the semantic hierarchy embedded in class labels. We first inject label-hierarchy knowledge into an arbitrary CNN-based classifier and empirically show that availability of such external semantic information in conjunction with the visual semantics from images boosts overall performance. Taking a step further in this direction, we model more explicitly the label-label and label-image interactions using order-preserving embeddings governed by both Euclidean and hyperbolic geometries, prevalent in natural language, and tailor them to hierarchical image classification and representation learning. We empirically validate all the models on the hierarchical ETHEC dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：图像分类已被广泛研究，但一直有限的工作，使用比传统的训练图像标签对其他非常规，外部指导。我们提出了一套方法利用有关嵌入类标签语义层级的信息。我们首先注入标签层次知识转化为任意基于CNN分类器和经验表明在与图像提升整体性能的视觉语义结合这样的外部语义信息的可用性。进一步先走一步朝着这个方向，我们的模型更明确的标签，标签和使用的由欧几里德和双曲几何形状，在自然语言普遍管辖保序的嵌入标签图像交互，并定制他们的分层图像分类和代表学习。我们经验验证所有的分层ETHEC数据集模型。</font>
</div>


<hr>
<div id="paper10"> <b>10. Learning Formation of Physically-Based Face Attributes</b>  <a href="https://arxiv.org/pdf/2004.03458" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruilong Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bladin%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Karl Bladin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yajie Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chinara%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chinmay Chinara</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ingraham%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Owen Ingraham</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiang%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pengda Xiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinglei Ren</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Prasad%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pratusha Prasad</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kishore%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bipin Kishore</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Xing</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Li</a><br>
<font size="3">
Abstract: Based on a combined data set of 4000 high resolution facial scans, we introduce a non-linear morphable face model, capable of producing multifarious face geometry of pore-level resolution, coupled with material attributes for use in physically-based rendering. We aim to maximize the variety of identities, while increasing the robustness of correspondence between unique components, including middle-frequency geometry, albedo maps, specular intensity maps and high-frequency displacement details. Our deep learning based generative model learns to correlate albedo and geometry, which ensures the anatomical correctness of the generated assets. We demonstrate potential use of our generative model for novel identity generation, model fitting, interpolation, animation, high fidelity data visualization, and low-to-high resolution data domain transferring. We hope the release of this generative model will encourage further cooperation between all graphics, vision, and data focused professionals, while demonstrating the cumulative value of every individual's complete biometric profile. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于4000次高分辨率面部扫描的组合数据集，我们引入一个非线性形变脸部模型，能够产生孔隙级别分辨率的繁杂面的几何形状，再加上用于基于物理的渲染使用材料的属性。我们的目标是最大化各种身份，同时增加独特组件，包括中等频率的几何形状，反照率地图，镜面强度地图和高频位移的信息之间的对应关系的鲁棒性。我们深厚的学习基础生成模型获悉关联反照率和几何形状，保证了资产产生的解剖正确性。我们演示了新的身份生成，模型拟合，插值，动画，高保真数据可视化，以及低到高的分辨率的数据传输领域的潜在应用我们的生成模型的。我们希望这生成模型的发布将鼓励所有的图形，视觉之间的进一步合作和数据集中的专业人士，同时展现每个人的生物特征完整轮廓的累计值。</font>
</div>


<hr>
<div id="paper11"> <b>11. Simultaneous Learning from Human Pose and Object Cues for Real-Time  Activity Recognition</b>  <a href="https://arxiv.org/pdf/2004.03453" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Reily%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Brian Reily</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qingzhao Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Reardon%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christopher Reardon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Zhang</a><br>
<font size="3">
Abstract: Real-time human activity recognition plays an essential role in real-world human-centered robotics applications, such as assisted living and human-robot collaboration. Although previous methods based on skeletal data to encode human poses showed promising results on real-time activity recognition, they lacked the capability to consider the context provided by objects within the scene and in use by the humans, which can provide a further discriminant between human activity categories. In this paper, we propose a novel approach to real-time human activity recognition, through simultaneously learning from observations of both human poses and objects involved in the human activity. We formulate human activity recognition as a joint optimization problem under a unified mathematical framework, which uses a regression-like loss function to integrate human pose and object cues and defines structured sparsity-inducing norms to identify discriminative body joints and object attributes. To evaluate our method, we perform extensive experiments on two benchmark datasets and a physical robot in a home assistance setting. Experimental results have shown that our method outperforms previous methods and obtains real-time performance for human activity recognition with a processing speed of 10^4 Hz. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：实时人类活动的识别发挥在现实世界的人类为中心的机器人应用，如辅助生活和人类与机器人合作的重要作用。虽然基于骨骼数据编码人体姿势以前的方法显示出期望的实时行为识别的结果，他们缺乏考虑人类的场景中，并使用由对象提供的背景下，它可以提供人之间的进一步判别能力活动类别。在本文中，我们提出了一种新的方法来实时人类活动的识别，通过从两个人的姿势的同时观察和学习对象参与了人类活动。我们制定人类活动的认可，在统一的数学框架，它采用了回归般的损失函数整合结构化稀疏诱导规范，以确定辨别身体关节和对象属性人体姿势和对象的线索和定义的联合优化问题。为了评估我们的方法，我们在两个基准数据集，并在家庭援助设置物理机器人进行大量的实验。实验结果已经表明，我们的方法优于先前的方法和获得用于人类的活动识别与10 ^ 4赫兹的处理速度的实时性能。</font>
</div>


<hr>
<div id="paper12"> <b>12. Strategies for Robust Image Classification</b>  <a href="https://arxiv.org/pdf/2004.03452" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Stock%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jason Stock</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dolan%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andy Dolan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cavey%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tom Cavey</a><br>
<font size="3">
Abstract: In this work we evaluate the impact of digitally altered images on the performance of artificial neural networks. We explore factors that negatively affect the ability of an image classification model to produce consistent and accurate results. A model's ability to classify is negatively influenced by alterations to images as a result of digital abnormalities or changes in the physical environment. The focus of this paper is to discover and replicate scenarios that modify the appearance of an image and evaluate them on state-of-the-art machine learning models. Our contributions present various training techniques that enhance a model's ability to generalize and improve robustness against these alterations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这项工作中，我们评估数字图像改变的人工神经网络的性能的影响。我们探索负面影响图像分类模型的产生一致和准确的结果的能力因素。模型的分类能力产生负面影响，改变对图像作为数字异常或物理环境变化的结果。本文的重点是发现和修改图像的外观，并评估他们在国家的最先进的机器学习模型复制方案。我们的贡献呈现各种培训技术，增强模型的概括，并提高对这些变化的鲁棒性的能力。</font>
</div>


<hr>
<div id="paper13"> <b>13. RSS-Net: Weakly-Supervised Multi-Class Semantic Segmentation with FMCW  Radar</b>  <a href="https://arxiv.org/pdf/2004.03451" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kaul%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Prannay Kaul</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=De+Martini%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniele De Martini</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gadd%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matthew Gadd</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Newman%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Paul Newman</a><br>
<font size="3">
Abstract: This paper presents an efficient annotation procedure and an application thereof to end-to-end, rich semantic segmentation of the sensed environment using FMCW scanning radar. We advocate radar over the traditional sensors used for this task as it operates at longer ranges and is substantially more robust to adverse weather and illumination conditions. We avoid laborious manual labelling by exploiting the largest radar-focused urban autonomy dataset collected to date, correlating radar scans with RGB cameras and LiDAR sensors, for which semantic segmentation is an already consolidated procedure. The training procedure leverages a state-of-the-art natural image segmentation system which is publicly available and as such, in contrast to previous approaches, allows for the production of copious labels for the radar stream by incorporating four camera and two LiDAR streams. Additionally, the losses are computed taking into account labels to the radar sensor horizon by accumulating LiDAR returns along a pose-chain ahead and behind of the current vehicle position. Finally, we present the network with multi-channel radar scan inputs in order to deal with ephemeral and dynamic scene objects. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种有效的注释过程以及它们的使用FMCW雷达扫描感测到的环境的端至端，富含语义分割的应用程序。我们主张雷达在用于此任务的传统的传感器，因为它工作在较长的范围内，并且基本上更稳健不利的天气和照明条件。我们避免费力的手工贴标通过利用迄今收集的最大的雷达为重点的城市自治数据集，雷达扫描与RGB照相机和激光雷达传感器，其语义分割是一个已经整合过程相关。训练过程利用一个国家的最先进的自然图像分割系统，其可公开获得的，因此，相对于先前的方法，允许生产丰富标签雷达流的通过将4个相机和两个激光雷达流。此外，该损耗是通过沿着向前的姿势链和后面车辆的当前位置的累积激光雷达返回计算要考虑标签雷达传感器的视野。最后，我们提出用多通道雷达扫描输入的网络，以应对短暂的和动态场景中的对象。</font>
</div>


<hr>
<div id="paper14"> <b>14. Nonparametric Data Analysis on the Space of Perceived Colors</b>  <a href="https://arxiv.org/pdf/2004.03402" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Patrangenaru%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vic Patrangenaru</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yifang Deng</a><br>
<font size="3">
Abstract: Moving around in a 3D world, requires the visual system of a living individual to rely on three channels of image recognition, which is done through three types of retinal cones. Newton, Grasmann, Helmholz and Schr$\ddot{o}$dinger laid down the basic assumptions needed to understand colored vision. Such concepts were furthered by Resnikoff, who imagined the space of perceived colors as a 3D homogeneous space. This article is concerned with perceived colors regarded as random objects on a Resnikoff 3D homogeneous space model. Two applications to color differentiation in machine vision are illustrated for the proposed statistical methodology, applied to the Euclidean model for perceived colors. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在3D世界中四处移动，需要有生命的个体的视觉系统依靠图像识别，这是通过三种视锥的做的三个通道。牛顿，Grasmann，亥姆霍兹和薛定谔$ \ {DDOTØ} $薛定谔放下来了解彩色愿景所需的基本假设。这些概念是由Resnikoff，谁想到感知颜色的空间作为3D齐性空间进一步发展。本文关注的是作为一个Resnikoff 3D均匀空间模型随机物体感知颜色。两个应用程序，以颜色区分的机器视觉针对所提出的统计方法，应用到感知颜色的欧几里得模型所示。</font>
</div>


<hr>
<div id="paper15"> <b>15. MNEW: Multi-domain Neighborhood Embedding and Weighting for Sparse Point  Clouds Segmentation</b>  <a href="https://arxiv.org/pdf/2004.03401" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Izzat%2C+I+H" target="_blank" rel="noopener" style="color:#0000EE;">Izzat H. Izzat</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sanling Song</a><br>
<font size="3">
Abstract: Point clouds have been widely adopted in 3D semantic scene understanding. However, point clouds for typical tasks such as 3D shape segmentation or indoor scenario parsing are much denser than outdoor LiDAR sweeps for the application of autonomous driving perception. Due to the spatial property disparity, many successful methods designed for dense point clouds behave depreciated effectiveness on the sparse data. In this paper, we focus on the semantic segmentation task of sparse outdoor point clouds. We propose a new method called MNEW, including multi-domain neighborhood embedding, and attention weighting based on their geometry distance, feature similarity, and neighborhood sparsity. The network architecture inherits PointNet which directly process point clouds to capture pointwise details and global semantics, and is improved by involving multi-scale local neighborhoods in static geometry domain and dynamic feature space. The distance/similarity attention and sparsity-adapted weighting mechanism of MNEW enable its capability for a wide range of data sparsity distribution. With experiments conducted on virtual and real KITTI semantic datasets, MNEW achieves the top performance for sparse point clouds, which is important to the application of LiDAR-based automated driving perception. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：点云已经在3D场景中的语义理解被广泛采用。然而，对于典型的任务，例如3D形状分割或室内场景解析点云密得多比室外激光雷达扫描的自动驾驶感觉的应用。由于空间的财产差距，专为密集的点云许多成功的方法的行为对稀疏数据贬值的有效性。在本文中，我们专注于户外稀疏点云的语义分割任务。我们提出了一个名为MNEW新方法，其中包括多域附近嵌入，并基于其几何距离关注权重，功能相似，和邻里稀疏。网络架构继承PointNet直接处理点云来捕获逐点细节和全局语义，并通过涉及静态几何域和动态特征空间多尺度地方邻里提高。的距离和/相似性的关注和MNEW的稀疏性适应加权机制使其能够对于宽范围的数据的稀疏分布的能力。随着虚拟和实际KITTI语义数据集进行的实验中，MNEW实现为稀疏点云的最佳性能，这是基于激光雷达的自动驾驶知觉的应用是重要的。</font>
</div>


<hr>
<div id="paper16"> <b>16. Differential 3D Facial Recognition: Adding 3D to Your State-of-the-Art  2D Method</b>  <a href="https://arxiv.org/pdf/2004.03385" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Di+Martino%2C+J+M" target="_blank" rel="noopener" style="color:#0000EE;">J. Matias Di Martino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Suzacq%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fernando Suzacq</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Delbracio%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mauricio Delbracio</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiang Qiu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sapiro%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guillermo Sapiro</a><br>
<font size="3">
Abstract: Active illumination is a prominent complement to enhance 2D face recognition and make it more robust, e.g., to spoofing attacks and low-light conditions. In the present work we show that it is possible to adopt active illumination to enhance state-of-the-art 2D face recognition approaches with 3D features, while bypassing the complicated task of 3D reconstruction. The key idea is to project over the test face a high spatial frequency pattern, which allows us to simultaneously recover real 3D information plus a standard 2D facial image. Therefore, state-of-the-art 2D face recognition solution can be transparently applied, while from the high frequency component of the input image, complementary 3D facial features are extracted. Experimental results on ND-2006 dataset show that the proposed ideas can significantly boost face recognition performance and dramatically improve the robustness to spoofing attacks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：主动照明是一个突出的补充，以提高2D脸部识别，并使其更加坚固，例如，以欺骗和低光照条件下。另外，在本工作中，我们表明，可以采用有源照明以增强状态的最先进的2D人脸识别接近与3D功能，同时绕过3D重建的复杂任务。其核心思想是项目在测试面高空间频率模式，这使我们能够同时恢复真正的3D信息加上一个标准的2D脸部图像。因此，国家的最先进的2D人脸识别溶液可透明地施加，而从输入图像的高频分量被提取的互补3D面部特征。对ND-2006数据集的实验结果表明所提出的想法可以显著提升面部识别性能，并显着提高了鲁棒性欺骗攻击。</font>
</div>


<hr>
<div id="paper17"> <b>17. Attribution in Scale and Space</b>  <a href="https://arxiv.org/pdf/2004.03383" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shawn Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Venugopalan%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Subashini Venugopalan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sundararajan%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mukund Sundararajan</a><br>
<font size="3">
Abstract: We study the attribution problem [28] for deep networks applied to perception tasks. For vision tasks, attribution techniques attribute the prediction of a network to the pixels of the input image. We propose a new technique called \emph{Blur Integrated Gradients}. This technique has several advantages over other methods. First, it can tell at what scale a network recognizes an object. It produces scores in the scale/frequency dimension, that we find captures interesting phenomena. Second, it satisfies the scale-space axioms [14], which imply that it employs perturbations that are free of artifact. We therefore produce explanations that are cleaner and consistent with the operation of deep networks. Third, it eliminates the need for a 'baseline' parameter for Integrated Gradients [31] for perception tasks. This is desirable because the choice of baseline has a significant effect on the explanations. We compare the proposed technique against previous techniques and demonstrate application on three tasks: ImageNet object recognition, Diabetic Retinopathy prediction, and AudioSet audio event identification. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们研究的归属问题[28]应用到感知任务深层网络。对于视觉任务，归属技术属性的网络与输入图像的像素的预测。我们提出了一个所谓的新技术\ {EMPH综合模糊渐变}。这种技术比其他方法的几个优点。首先，它可以在什么规模的网络识别对象告诉。它产生的分数，在规模/频率维度，我们发现捕捉有趣的现象。其次，它满足尺度空间公理[14]，这意味着它采用了是无伪影的扰动。因此，我们生产出更清洁和深层网络的操作一致的解释。第三，它消除了对综合梯度[31]一个“基准”参数感知任务的需要。这是可取的，因为基线的选择对一个解释效果显著。我们比较反对先前的技术所提出的技术和展示三个任务应用程序：ImageNet物体识别，糖尿病视网膜病变的预测，并AudioSet音频事件识别。</font>
</div>


<hr>
<div id="paper18"> <b>18. Error-Corrected Margin-Based Deep Cross-Modal Hashing for Facial Image  Retrieval</b>  <a href="https://arxiv.org/pdf/2004.03378" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Taherkhani%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fariborz Taherkhani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Talreja%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Veeru Talreja</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Valenti%2C+M+C" target="_blank" rel="noopener" style="color:#0000EE;">Matthew C. Valenti</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nasrabadi%2C+N+M" target="_blank" rel="noopener" style="color:#0000EE;">Nasser M. Nasrabadi</a><br>
<font size="3">
Abstract: Cross-modal hashing facilitates mapping of heterogeneous multimedia data into a common Hamming space, which can beutilized for fast and flexible retrieval across different modalities. In this paper, we propose a novel cross-modal hashingarchitecture-deep neural decoder cross-modal hashing (DNDCMH), which uses a binary vector specifying the presence of certainfacial attributes as an input query to retrieve relevant face images from a database. The DNDCMH network consists of two separatecomponents: an attribute-based deep cross-modal hashing (ADCMH) module, which uses a margin (m)-based loss function toefficiently learn compact binary codes to preserve similarity between modalities in the Hamming space, and a neural error correctingdecoder (NECD), which is an error correcting decoder implemented with a neural network. The goal of NECD network in DNDCMH isto error correct the hash codes generated by ADCMH to improve the retrieval efficiency. The NECD network is trained such that it hasan error correcting capability greater than or equal to the margin (m) of the margin-based loss function. This results in NECD cancorrect the corrupted hash codes generated by ADCMH up to the Hamming distance of m. We have evaluated and comparedDNDCMH with state-of-the-art cross-modal hashing methods on standard datasets to demonstrate the superiority of our method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：跨模态散列有助于不同种类的多媒体数据的映射到一个共同的汉明空间，其可以为beutilized跨不同模态快速灵活的检索。在本文中，我们提出了一种新颖的跨通道hashingarchitecture深神经解码器跨通道散列（DNDCMH），它使用双元载体，指定作为输入查询以从数据库中检索相关的面部图像certainfacial属性的存在。所述DNDCMH网络由两个separatecomponents的：基于属性的深跨通道散列（ADCMH）模块，它使用一个余量（M）系损失函数toefficiently学习紧凑二进制码保存模式之间的相似性中的汉明空间，和一个神经误差correctingdecoder（NECD），这是用神经网络来实现纠错解码器。在这一领域的空白DNDCMH NECD误差网络的目标纠正ADCMH产生，以提高检索效率的散列码。所述NECD网络进行训练，使得其哈桑纠错能力大于或等于基于容限的损失函数的裕度（M）。这导致NECD cancorrect由ADCMH产生高达m的汉明距离被破坏的散列码。我们已评估并comparedDNDCMH与标准数据集的国家的最先进的跨模态散列方法来证明我们的方法的优越性。</font>
</div>


<hr>
<div id="paper19"> <b>19. Composition of Saliency Metrics for Channel Pruning with a Myopic Oracle</b>  <a href="https://arxiv.org/pdf/2004.03376" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Persand%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kaveena Persand</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Anderson%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrew Anderson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gregg%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Gregg</a><br>
<font size="3">
Abstract: The computation and memory needed for Convolutional Neural Network (CNN) inference can be reduced by pruning weights from the trained network. Pruning is guided by a pruning saliency, which heuristically approximates the change in the loss function associated with the removal of specific weights. Many pruning signals have been proposed, but the performance of each heuristic depends on the particular trained network. This leaves the data scientist with a difficult choice. When using any one saliency metric for the entire pruning process, we run the risk of the metric assumptions being invalidated, leading to poor decisions being made by the metric. Ideally we could combine the best aspects of different saliency metrics. However, despite an extensive literature review, we are unable to find any prior work on composing different saliency metrics. The chief difficulty lies in combining the numerical output of different saliency metrics, which are not directly comparable. We propose a method to compose several primitive pruning saliencies, to exploit the cases where each saliency measure does well. Our experiments show that the composition of saliencies avoids many poor pruning choices identified by individual saliencies. In most cases our method finds better selections than even the best individual pruning saliency. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：可以通过从训练的网络修剪权重来降低所需要的卷积神经网络（CNN）推论的计算和存储。修剪由修剪显着性，这启发式近似于具有去除特定的权重相关联的损失函数的变化引导。许多修剪信号已经被提出，但每个启发式的性能取决于特定训练的网络上。这使得困难的抉择数据科学家。当使用度量整个清理过程中的任何一个显着，我们冒着被无效的度量假设的风险，从而导致决策失误由指标作出。理想情况下，我们可以结合不同的显着性指标最好的方面。然而，尽管大量的文献综述，我们无法找到组成不同的显着性指标任​​何以前的工作。主要困难在于组合不同的显着性的指标，这是不直接进行比较的数字输出。我们提出了一个方法来编写几个原始的修剪凸极，以利用每个显着特征度量做得很好的情况下。我们的实验表明，凸极的组成可避免由个别凸极发现了许多修剪糟糕的选择。在大多数情况下，我们的方法发现甚至比最佳个体修剪显着更好的选择。</font>
</div>


<hr>
<div id="paper20"> <b>20. Robust Self-Supervised Convolutional Neural Network for Subspace  Clustering and Classification</b>  <a href="https://arxiv.org/pdf/2004.03375" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sitnik%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dario Sitnik</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kopriva%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ivica Kopriva</a><br>
<font size="3">
Abstract: Insufficient capability of existing subspace clustering methods to handle data coming from nonlinear manifolds, data corruptions, and out-of-sample data hinders their applicability to address real-world clustering and classification problems. This paper proposes the robust formulation of the self-supervised convolutional subspace clustering network ($S^2$ConvSCN) that incorporates the fully connected (FC) layer and, thus, it is capable for handling out-of-sample data by classifying them using a softmax classifier. $S^2$ConvSCN clusters data coming from nonlinear manifolds by learning the linear self-representation model in the feature space. Robustness to data corruptions is achieved by using the correntropy induced metric (CIM) of the error. Furthermore, the block-diagonal (BD) structure of the representation matrix is enforced explicitly through BD regularization. In a truly unsupervised training environment, Robust $S^2$ConvSCN outperforms its baseline version by a significant amount for both seen and unseen data on four well-known datasets. Arguably, such an ablation study has not been reported before. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现有的子空间聚类方法来处理数据，从非线性歧管，数据损坏到来能力不足，外的样本数据阻碍了其适用于解决现实世界的聚类和分类问题。本文提出的自监督卷积子空间聚类网络的鲁棒制剂（$ S ^ 2 $ ConvSCN）结合了完全连接（FC）层，因此，它能够用于通过其分类处理外的样本数据用Softmax分类。 $ S ^ 2点$ ConvSCN簇的数据通过学习在特征空间中的线性自表示模型从非线性歧管来。鲁棒性数据损坏通过使用错误的诱导度量（CIM）的correntropy实现。此外，表示矩阵的块对角（BD）结构是通过BD正规化明确地执行。在一个真正的无监督的训练环境，乐百氏$ S ^ 2 $ ConvSCN由四个著名的数据集都看到和看不到的数据显著量优于其基线版本。可以说，这样的消融研究尚未见报道。</font>
</div>


<hr>
<div id="paper21"> <b>21. Improving BPSO-based feature selection applied to offline WI handwritten  signature verification through overfitting control</b>  <a href="https://arxiv.org/pdf/2004.03373" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Souza%2C+V+L+F" target="_blank" rel="noopener" style="color:#0000EE;">Victor L. F. Souza</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Oliveira%2C+A+L+I" target="_blank" rel="noopener" style="color:#0000EE;">Adriano L. I. Oliveira</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cruz%2C+R+M+O" target="_blank" rel="noopener" style="color:#0000EE;">Rafael M. O. Cruz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sabourin%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Robert Sabourin</a><br>
<font size="3">
Abstract: This paper investigates the presence of overfitting when using Binary Particle Swarm Optimization (BPSO) to perform the feature selection in a context of Handwritten Signature Verification (HSV). SigNet is a state of the art Deep CNN model for feature representation in the HSV context and contains 2048 dimensions. Some of these dimensions may include redundant information in the dissimilarity representation space generated by the dichotomy transformation (DT) used by the writer-independent (WI) approach. The analysis is carried out on the GPDS-960 dataset. Experiments demonstrate that the proposed method is able to control overfitting during the search for the most discriminant representation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文研究使用二进制粒子群优化（BPSO）在手写签名验证（HSV）的上下文中执行特征选择时过度拟合的存在。图章是本领域深CNN模型在HSV上下文特征的表示的状态，并含有2048和尺寸。一些这些尺寸可包括在通过由写入器无关的（WI）的方法中使用的二分法变换（DT）中产生的相异度表示空间冗余信息。该分析是在GPDS-960数据集进行。实验表明，该方法能够控制搜索最判别表示期间的过度拟合。</font>
</div>


<hr>
<div id="paper22"> <b>22. A white-box analysis on the writer-independent dichotomy transformation  applied to offline handwritten signature verification</b>  <a href="https://arxiv.org/pdf/2004.03370" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Souza%2C+V+L+F" target="_blank" rel="noopener" style="color:#0000EE;">Victor L. F. Souza</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Oliveira%2C+A+L+I" target="_blank" rel="noopener" style="color:#0000EE;">Adriano L. I. Oliveira</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cruz%2C+R+M+O" target="_blank" rel="noopener" style="color:#0000EE;">Rafael M. O. Cruz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sabourin%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Robert Sabourin</a><br>
<font size="3">
Abstract: High number of writers, small number of training samples per writer with high intra-class variability and heavily imbalanced class distributions are among the challenges and difficulties of the offline Handwritten Signature Verification (HSV) problem. A good alternative to tackle these issues is to use a writer-independent (WI) framework. In WI systems, a single model is trained to perform signature verification for all writers from a dissimilarity space generated by the dichotomy transformation. Among the advantages of this framework is its scalability to deal with some of these challenges and its ease in managing new writers, and hence of being used in a transfer learning context. In this work, we present a white-box analysis of this approach highlighting how it handles the challenges, the dynamic selection of references through fusion function, and its application for transfer learning. All the analyses are carried out at the instance level using the instance hardness (IH) measure. The experimental results show that, using the IH analysis, we were able to characterize "good" and "bad" quality skilled forgeries as well as the frontier region between positive and negative samples. This enables futures investigations on methods for improving discrimination between genuine signatures and skilled forgeries by considering these characterizations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：大量作家，培养每个作家的样本具有较高的类内变化和严重不平衡类分布少数是脱机手写签名认证（HSV）问题的挑战和困难之中。一个很好的替代解决这些问题的方法是使用一个作家独立（WI）框架。在WI系统中，单个模型被训练以用于从由二分法变换产生的相异度空间中的所有作家执行签名验证。在这个框架的优点是其可扩展性，以应对一些挑战及其管理的新的作家，因此在转移学习环境中使用放心。在这项工作中，我们提出这个方法的白盒分析强调它是如何处理的挑战，通过融合函数引用的动态选择，及其转移学习应用。所有的分析是在使用实例硬度（1H）测量实例级进行。实验结果表明，采用IH分析，我们可以更好地表征“好”与“坏”的高素质技能伪造以及阳性和阴性样品之间的边境地区。这使得对通过考虑这些特性提高真正的签名和伪造熟练区分方法期货调查。</font>
</div>


<hr>
<div id="paper23"> <b>23. Knife and Threat Detectors</b>  <a href="https://arxiv.org/pdf/2004.03366" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Noever%2C+D+A" target="_blank" rel="noopener" style="color:#0000EE;">David A. Noever</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Noever%2C+S+E+M" target="_blank" rel="noopener" style="color:#0000EE;">Sam E. Miller Noever</a><br>
<font size="3">
Abstract: Despite rapid advances in image-based machine learning, the threat identification of a knife wielding attacker has not garnered substantial academic attention. This relative research gap appears less understandable given the high knife assault rate (>100,000 annually) and the increasing availability of public video surveillance to analyze and forensically document. We present three complementary methods for scoring automated threat identification using multiple knife image datasets, each with the goal of narrowing down possible assault intentions while minimizing misidentifying false positives and risky false negatives. To alert an observer to the knife-wielding threat, we test and deploy classification built around MobileNet in a sparse and pruned neural network with a small memory requirement (< 2.2 megabytes) and 95% test accuracy. We secondly train a detection algorithm (MaskRCNN) to segment the hand from the knife in a single image and assign probable certainty to their relative location. This segmentation accomplishes both localization with bounding boxes but also relative positions to infer overhand threats. A final model built on the PoseNet architecture assigns anatomical waypoints or skeletal features to narrow the threat characteristics and reduce misunderstood intentions. We further identify and supplement existing data gaps that might blind a deployed knife threat detector such as collecting innocuous hand and fist images as important negative training sets. When automated on commodity hardware and software solutions one original research contribution is this systematic survey of timely and readily available image-based alerts to task and prioritize crime prevention countermeasures prior to a tragic outcome. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管基于图像的机器学习的迅猛发展，一把刀挥舞攻击者的威胁识别并没有囊括实质性学术界的重视。给予很高的持刀伤人率（> 100,000每年）和公共视频监控的日益普及，分析和取证文档此相关研究的差距似乎更难理解。我们目前使用多刀图像数据集进行打分自动化威胁识别三种互为补充的方法，分别具有缩小可能的攻击意图，同时尽量减少错误识别误报和风险漏报的目标。要提醒的观察员持刀威胁，我们测试和部署分类内置围绕MobileNet在稀疏和修剪神经网络的小内存要求（<2.2兆字节）和95％的测试精度。我们其次在单个图像训练检测算法（maskrcnn）来分割手从刀和可能确定性分配给它们的相对位置。这种分割实现国产化都与边框也相对位置来推断上手的威胁。建立在posenet架构受让人解剖航点或骨骼特征的最终模型缩小的威胁特性，并减少误解的意图。我们进一步确定并有可能瞎部署的刀威胁检测器，例如收集无害一手握拳图像作为重要的负面训练集合补充现有的数据差距。当在商用硬件和软件解决方案的自动化一个原创性研究的贡献是基于图像的及时和随时可用警报任务和之前的悲惨结局重点发展预防犯罪对策的这个系统的调查。< font>
</2.2兆字节）和95％的测试精度。我们其次在单个图像训练检测算法（maskrcnn）来分割手从刀和可能确定性分配给它们的相对位置。这种分割实现国产化都与边框也相对位置来推断上手的威胁。建立在posenet架构受让人解剖航点或骨骼特征的最终模型缩小的威胁特性，并减少误解的意图。我们进一步确定并有可能瞎部署的刀威胁检测器，例如收集无害一手握拳图像作为重要的负面训练集合补充现有的数据差距。当在商用硬件和软件解决方案的自动化一个原创性研究的贡献是基于图像的及时和随时可用警报任务和之前的悲惨结局重点发展预防犯罪对策的这个系统的调查。<></font></div>


<hr>
<div id="paper24"> <b>24. A Machine Learning Based Framework for the Smart Healthcare Monitoring</b>  <a href="https://arxiv.org/pdf/2004.03360" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zahin%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Abrar Zahin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+L+T" target="_blank" rel="noopener" style="color:#0000EE;">Le Thanh Tan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+R+Q" target="_blank" rel="noopener" style="color:#0000EE;">Rose Qingyang Hu</a><br>
<font size="3">
Abstract: In this paper, we propose a novel framework for the smart healthcare system, where we employ the compressed sensing (CS) and the combination of the state-of-the-art machine learning based denoiser as well as the alternating direction of method of multipliers (ADMM) structure. This integration significantly simplifies the software implementation for the lowcomplexity encoder, thanks to the modular structure of ADMM. Furthermore, we focus on detecting fall down actions from image streams. Thus, teh primary purpose of thus study is to reconstruct the image as visibly clear as possible and hence it helps the detection step at the trained classifier. For this efficient smart health monitoring framework, we employ the trained binary convolutional neural network (CNN) classifier for the fall-action classifier, because this scheme is a part of surveillance scenario. In this scenario, we deal with the fallimages, thus, we compress, transmit and reconstruct the fallimages. Experimental results demonstrate the impacts of network parameters and the significant performance gain of the proposal compared to traditional methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出一种用于智能医疗系统，其中我们采用压缩感测（CS）和国家的先进机器的组合的新的框架基于学习的降噪以及方法的交替方向的乘法器（ADMM）结构。这种集成显著简化了lowcomplexity编码器的软件实现，得益于ADMM的模块化结构。此外，我们专注于检测落下从图像流的行为。因此，由此研究TEH主要目的是重建图像作为可见透明尽可能并且因此有助于在经训练的分类的检测步骤。对于这种高效的智能健康监测框架，我们采用秋季行动分类的训练的二进制卷积神经网络（CNN）的分类，因为该方案是监控方案的一部分。在这种情况下，我们应对fallimages，因此，我们压缩，传输和重建fallimages。实验结果表明，网络参数的影响，并与传统方法相比该提案的显著性能增益。</font>
</div>


<hr>
<div id="paper25"> <b>25. Deep learning approaches in food recognition</b>  <a href="https://arxiv.org/pdf/2004.03357" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kiourt%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chairi Kiourt</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pavlidis%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">George Pavlidis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Markantonatou%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stella Markantonatou</a><br>
<font size="3">
Abstract: Automatic image-based food recognition is a particularly challenging task. Traditional image analysis approaches have achieved low classification accuracy in the past, whereas deep learning approaches enabled the identification of food types and their ingredients. The contents of food dishes are typically deformable objects, usually including complex semantics, which makes the task of defining their structure very difficult. Deep learning methods have already shown very promising results in such challenges, so this chapter focuses on the presentation of some popular approaches and techniques applied in image-based food recognition. The three main lines of solutions, namely the design from scratch, the transfer learning and the platform-based approaches, are outlined, particularly for the task at hand, and are tested and compared to reveal the inherent strengths and weaknesses. The chapter is complemented with basic background material, a section devoted to the relevant datasets that are crucial in light of the empirical approaches adopted, and some concluding remarks that underline the future directions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自动基于图像的食品识别是一个特别具有挑战性的任务。传统的图像分析方法已经在过去取得低分类精度，而深度学习的方法使食物的种类及其成分的鉴定。食品菜肴的内容通常是可变形物体，通常包括复杂的语义，这使得确定其结构非常困难的任务。深学习方法已经显示出非常乐观的这种挑战的结果，因此本章重点介绍一些流行的方法和技术演示基于图像识别的食品应用。解决方案，即从头开始设计，传递学习和基于平台的方法的三条主线，概述，特别是对于手头的任务，并进行测试和比较，揭示内在的优势和弱点。该章补充了基本的背景材料，专用于相关数据集，它们采用的办法在经验的光的关键一节，和一些结论性意见是下划线的未来发展方向。</font>
</div>


<hr>
<div id="paper26"> <b>26. Inclusive GAN: Improving Data and Minority Coverage in Generative Models</b>  <a href="https://arxiv.org/pdf/2004.03355" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Ning Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Ke Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peng Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Malik%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jitendra Malik</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Davis%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Larry Davis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fritz%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mario Fritz</a><br>
<font size="3">
Abstract: Generative Adversarial Networks (GANs) have brought about rapid progress towards generating photorealistic images. Yet the equitable allocation of their modeling capacity among subgroups has received less attention, which could lead to potential biases against underrepresented minorities if left uncontrolled. In this work, we first formalize the problem of minority inclusion as one of data coverage, and then propose to improve data coverage by harmonizing adversarial training with reconstructive generation. The experiments show that our method outperforms the existing state-of-the-art methods in terms of data coverage on both seen and unseen data. We develop an extension that allows explicit control over the minority subgroups that the model should ensure to include, and validate its effectiveness at little compromise from the overall performance on the entire dataset. Code, models, and supplemental videos are available at GitHub. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：创成对抗性网络（甘斯）带来了对生成逼真的图像快速进步。然而，他们的造型能力组间公平分配已经受到足够的重视，如果不加以控制可能导致潜在的偏倚对少数族裔。在这项工作中，我们第一次正式列入少数民族数据覆盖的一个问题，然后提出通过协调与重建一代对抗训练，以提高数据覆盖。实验结果表明，我们的方法优于在看见也看不见数据数据覆盖范围方面存在的国家的最先进的方法。我们开发的扩展，允许在少数分组明确的控制，该模型应确保包括，在小妥协从整个数据集的整体性能验证其有效性。代码，模型和补充视频可在GitHub上。</font>
</div>


<hr>
<div id="paper27"> <b>27. An Image Labeling Tool and Agricultural Dataset for Deep Learning</b>  <a href="https://arxiv.org/pdf/2004.03351" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wspanialy%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Patrick Wspanialy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Brooks%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Justin Brooks</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Moussa%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Medhat Moussa</a><br>
<font size="3">
Abstract: We introduce a labeling tool and dataset aimed to facilitate computer vision research in agriculture. The annotation tool introduces novel methods for labeling with a variety of manual, semi-automatic, and fully-automatic tools. The dataset includes original images collected from commercial greenhouses, images from PlantVillage, and images from Google Images. Images were annotated with segmentations for foreground leaf, fruit, and stem instances, and diseased leaf area. Labels were in an extended COCO format. In total the dataset contained 10k tomatoes, 7k leaves, 2k stems, and 2k diseased leaf annotations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：介绍一个标签工具和数据集的目的是要促进农业的计算机视觉研究。注释工具引入了各种手动，半自动，和全自动工具用于标记的新方法。该数据集包括从谷歌图片来自商业大棚采集的原始图像，从图像PlantVillage和图像。图像用分割的前景叶，果注释和干的情况下，和患病叶面积。标签是在扩展COCO格式。在总的数据集包含10K西红柿，7K叶，茎2K和2K病叶注解。</font>
</div>


<hr>
<div id="paper28"> <b>28. Automatic Generation of Chinese Handwriting via Fonts Style  Representation Learning</b>  <a href="https://arxiv.org/pdf/2004.03339" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fenxi Xiao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xia Wu</a><br>
<font size="3">
Abstract: In this paper, we propose and end-to-end deep Chinese font generation system. This system can generate new style fonts by interpolation of latent style-related embeding variables that could achieve smooth transition between different style. Our method is simpler and more effective than other methods, which will help to improve the font design efficiency </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出和终端到终端的深厚的中国字体发电系统。该系统可以通过生成，可以实现不同的风格之间的平滑过渡的潜在风格相关embeding变量的插值新风格的字体。我们的方法是比其他方法，这将有助于改善字体的设计效率，更简单和更有效</font>
</div>


<hr>
<div id="paper29"> <b>29. Multiform Fonts-to-Fonts Translation via Style and Content Disentangled  Representations of Chinese Character</b>  <a href="https://arxiv.org/pdf/2004.03338" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fenxi Xiao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jie Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xia Wu</a><br>
<font size="3">
Abstract: This paper mainly discusses the generation of personalized fonts as the problem of image style transfer. The main purpose of this paper is to design a network framework that can extract and recombine the content and style of the characters. These attempts can be used to synthesize the entire set of fonts with only a small amount of characters. The paper combines various depth networks such as Convolutional Neural Network, Multi-layer Perceptron and Residual Network to find the optimal model to extract the features of the fonts character. The result shows that those characters we have generated is very close to real characters, using Structural Similarity index and Peak Signal-to-Noise Ratio evaluation criterions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文主要论述了个性化字体的一代影像风格转移的问题。本文的主要目的是设计一个网络架构，可以提取和重组字符的内容和风格。这些尝试可以被用于合成整个集的字体的，只有字符的量小。在本文结合各种深度网络，诸如卷积神经网络，多层感知和剩余网络以找到最佳的模型以提取字体的字符的功能。结果表明，我们已经产生这些字符是非常接近真实人物，利用结构相似度指数和峰值信噪比评价标准。</font>
</div>


<hr>
<div id="paper30"> <b>30. An End-to-End Approach for Recognition of Modern and Historical  Handwritten Numeral Strings</b>  <a href="https://arxiv.org/pdf/2004.03337" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hochuli%2C+A+G" target="_blank" rel="noopener" style="color:#0000EE;">Andre G. Hochuli</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Britto%2C+A+S" target="_blank" rel="noopener" style="color:#0000EE;">Alceu S. Britto Jr.</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Barddal%2C+J+P" target="_blank" rel="noopener" style="color:#0000EE;">Jean P. Barddal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Oliveira%2C+L+E+S" target="_blank" rel="noopener" style="color:#0000EE;">Luiz E. S. Oliveira</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sabourin%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Robert Sabourin</a><br>
<font size="3">
Abstract: An end-to-end solution for handwritten numeral string recognition is proposed, in which the numeral string is considered as composed of objects automatically detected and recognized by a YoLo-based model. The main contribution of this paper is to avoid heuristic-based methods for string preprocessing and segmentation, the need for task-oriented classifiers, and also the use of specific constraints related to the string length. A robust experimental protocol based on several numeral string datasets, including one composed of historical documents, has shown that the proposed method is a feasible end-to-end solution for numeral string recognition. Besides, it reduces the complexity of the string recognition task considerably since it drops out classical steps, in special preprocessing, segmentation, and a set of classifiers devoted to strings with a specific length. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：端至端解决方案，手写数字字符串识别提出，其中如由自动检测和基于YOLO模型识别的对象的数字串被考虑。本文的主要贡献是避免串预处理和分割，需要面向任务的分类，并利用相关的字符串长度的具体限制启发式为主的方法。基于几个数字串的数据集，包括一个历史文件组成的健壮的实验方案，已经显示出所提出的方法是一种可行的端至端的解决方案，数字串的识别。此外，它降低了串识别任务的复杂性显着，因为它滴出古典的步骤，在特殊预处理，分段，以及一组专门用于字符串具有特定长度分类的。</font>
</div>


<hr>
<div id="paper31"> <b>31. Predict the model of a camera</b>  <a href="https://arxiv.org/pdf/2004.03336" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Penedo%2C+C+J+D" target="_blank" rel="noopener" style="color:#0000EE;">Ciro Javier Diaz Penedo</a><br>
<font size="3">
Abstract: In this work we address the problem of predicting the model of a camera based on the content of their photographs. We use two set of features, one set consist in properties extracted from a Discrete Wavelet Domain (DWD) obtained by applying a 4 level Fast Wavelet Decomposition of the images, and a second set are Local Binary Patterns (LBP) features from the after filter noise of images. The algorithms used for classification were Logistic regression, K-NN and Artificial Neural Networks </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这项工作中，我们解决预测基础上，他们的照片内容的摄像机的型号的问题。我们使用了两个组特征，一组包括在从通过施加第4级的图像的快速小波分解，以及第二组获得的离散小波变换域（DWD）提取的属性是局部二元模式（LBP）从所述后过滤器设有图像的噪声。用于分类的算法是Logistic回归分析，K-NN和人工神经网络</font>
</div>


<hr>
<div id="paper32"> <b>32. FusedProp: Towards Efficient Training of Generative Adversarial Networks</b>  <a href="https://arxiv.org/pdf/2004.03335" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Polizzi%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zachary Polizzi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tsai%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chuan-Yung Tsai</a><br>
<font size="3">
Abstract: Generative adversarial networks (GANs) are capable of generating strikingly realistic samples but state-of-the-art GANs can be extremely computationally expensive to train. In this paper, we propose the fused propagation (FusedProp) algorithm which can be used to efficiently train the discriminator and the generator of common GANs simultaneously using only one forward and one backward propagation. We show that FusedProp achieves 1.49 times the training speed compared to the conventional training of GANs, although further studies are required to improve its stability. By reporting our preliminary results and open-sourcing our implementation, we hope to accelerate future research on the training of GANs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：剖成对抗网络（甘斯）能够产生显着现实的样本但国家的最先进的甘斯可能是极其昂贵的计算来训练。在本文中，我们提出一种可用于有效地训练鉴别器和共同甘斯的同时只使用一个正向和一个反向传播发电机的熔融传播（FusedProp）算法。我们表明，FusedProp相比，甘斯的常规训练达到训练速度的1.49倍，虽然还需要进一步研究，以提高其稳定性。通过报告我们的初步结果，以及开放式采购项目中，我们希望能够加快未来在甘斯的训练研究。</font>
</div>


<hr>
<div id="paper33"> <b>33. Streaming Networks: Increase Noise Robustness and Filter Diversity via  Hard-wired and Input-induced Sparsity</b>  <a href="https://arxiv.org/pdf/2004.03334" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title33" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tarasenko%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sergey Tarasenko</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Takahashi%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fumihiko Takahashi</a><br>
<font size="3">
Abstract: The CNNs have achieved a state-of-the-art performance in many applications. Recent studies illustrate that CNN's recognition accuracy drops drastically if images are noise corrupted. We focus on the problem of robust recognition accuracy of noise-corrupted images. We introduce a novel network architecture called Streaming Networks. Each stream is taking a certain intensity slice of the original image as an input, and stream parameters are trained independently. We use network capacity, hard-wired and input-induced sparsity as the dimensions for experiments. The results indicate that only the presence of both hard-wired and input-induces sparsity enables robust noisy image recognition. Streaming Nets is the only architecture which has both types of sparsity and exhibits higher robustness to noise. Finally, to illustrate increase in filter diversity we illustrate that a distribution of filter weights of the first conv layer gradually approaches uniform distribution as the degree of hard-wired and domain-induced sparsity and capacities increases. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：细胞神经网络已经实现在许多应用中一个国家的最先进的性能。最近的研究表明，CNN的识别准确率急剧下降，如果图像噪声污染。我们注重的噪声破坏图像的强大的识别准确度的问题。我们引入了一种新的网络架构，称为流媒体网络。每个流走的是原始图像的特定强度切片作为输入，和流​​参数被独立地训练。我们使用的网络容量，硬连线和输入引起的稀疏的尺寸实验。结果表明，只有既硬连线和输入诱导稀疏性的存在使得健壮嘈杂图像识别。流网是具有两种类型的稀疏性和表现出更高的鲁棒性噪声的唯一结构。最后，以示出过滤器的多样性的增加，我们说明的是，第一层CONV的滤波器权重的分布均匀分布逐渐接近作为硬连线和域诱导的稀疏性和能力的程度增加。</font>
</div>


<hr>
<div id="paper34"> <b>34. Cascaded Refinement Network for Point Cloud Completion</b>  <a href="https://arxiv.org/pdf/2004.03327" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title34" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaogang Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ang%2C+M+H" target="_blank" rel="noopener" style="color:#0000EE;">Marcelo H Ang Jr</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+G+H" target="_blank" rel="noopener" style="color:#0000EE;">Gim Hee Lee</a><br>
<font size="3">
Abstract: Point clouds are often sparse and incomplete. Existing shape completion methods are incapable of generating details of objects or learning the complex point distributions. To this end, we propose a cascaded refinement network together with a coarse-to-fine strategy to synthesize the detailed object shapes. Considering the local details of partial input with the global shape information together, we can preserve the existing details in the incomplete point set and generate the missing parts with high fidelity. We also design a patch discriminator that guarantees every local area has the same pattern with the ground truth to learn the complicated point distribution. Quantitative and qualitative experiments on different datasets show that our method achieves superior results compared to existing state-of-the-art approaches on the 3D point cloud completion task. Our source code is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：点云，往往稀疏，不完整的。现有形状完成方法不能产生对象的详细信息或学习复杂的点分布。为此，我们一起提出了一个级联细化网络，由粗到细的策略，合成了详细的目标形状。考虑到与整体形状信息一起输入部分的局部细节，我们可以在不完整的点集保留现有的细节并产生高保真缺少的部分。我们还设计了一个补丁鉴别，保证每一个局部区域与地面实况相同的模式来学习复杂的点分布。对不同的数据集的定量和定性实验表明，相对于现有的最先进的国家，我们的方法实现了优异的业绩接近的三维点云完成任务。我们的源代码可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper35"> <b>35. Towards Efficient Unconstrained Palmprint Recognition via Deep  Distillation Hashing</b>  <a href="https://arxiv.org/pdf/2004.03303" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title35" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huikai Shao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhong%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dexing Zhong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Du%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuefeng Du</a><br>
<font size="3">
Abstract: Deep palmprint recognition has become an emerging issue with great potential for personal authentication on handheld and wearable consumer devices. Previous studies of palmprint recognition are mainly based on constrained datasets collected by dedicated devices in controlled environments, which has to reduce the flexibility and convenience. In addition, general deep palmprint recognition algorithms are often too heavy to meet the real-time requirements of embedded system. In this paper, a new palmprint benchmark is established, which consists of more than 20,000 images collected by 5 brands of smart phones in an unconstrained manner. Each image has been manually labeled with 14 key points for region of interest (ROI) extraction. Further, the approach called Deep Distillation Hashing (DDH) is proposed as benchmark for efficient deep palmprint recognition. Palmprint images are converted to binary codes to improve the efficiency of feature matching. Derived from knowledge distillation, novel distillation loss functions are constructed to compress deep model to further improve the efficiency of feature extraction on light network. Comprehensive experiments are conducted on both constrained and unconstrained palmprint databases. Using DDH, the accuracy of palmprint identification can be increased by up to 11.37%, and the Equal Error Rate (EER) of palmprint verification can be reduced by up to 3.11%. The results indicate the feasibility of our database, and DDH can outperform other baselines to achieve the state-of-the-art performance. The collected dataset and related source codes are publicly available at this http URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深掌纹识别已经成为对手持设备和可穿戴式消费电子设备的个人认证潜力巨大的新兴问题。掌纹识别的以前的研究主要是根据通过在受控环境中的专用设备，其具有降低的灵活性和方便收集约束数据集。此外，一般的深掌纹识别算法往往过于沉重，以满足嵌入式系统的实时性要求。在本文中，一种新的掌纹基准的建立，其中包括5个品牌的智能手机不受约束的方式收集的超过20,000图像。每个图像具有对关注区域（ROI）的提取区域14个的关键点被手动标记。此外，该方法被称为深蒸馏散列（DDH），提出为基准的高效深掌纹识别。掌纹图像被转换为​​二进制码来提高特征匹配的效率。从知识蒸馏而得，新颖蒸馏损失函数被构造成压缩深模型进一步提高的特征提取的光网络上的效率。综合实验两个约束和不受约束的掌纹数据库进行。使用DDH，掌纹识别精度可提高到11.37％，并且可以通过最多减少到3.11％掌纹核查等错误率（EER）。结果表明我们的数据库的可行性，并DDH可以超越其他基准，实现国家的最先进的性能。所收集的数据集和相关的源代码是公开的，在此http网址。</font>
</div>


<hr>
<div id="paper36"> <b>36. Pyramid Focusing Network for mutation prediction and classification in  CT images</b>  <a href="https://arxiv.org/pdf/2004.03302" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title36" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xukun Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenxin Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wen Wu</a><br>
<font size="3">
Abstract: Predicting the mutation status of genes in tumors is of great clinical significance. Recent studies have suggested that certain mutations may be noninvasively predicted by studying image features of the tumors from Computed Tomography (CT) data. Currently, this kind of image feature identification method mainly relies on manual processing to extract generalized image features alone or machine processing without considering the morphological differences of the tumor itself, which makes it difficult to achieve further breakthroughs. In this paper, we propose a pyramid focusing network (PFNet) for mutation prediction and classification based on CT images. Firstly, we use Space Pyramid Pooling to collect semantic cues in feature maps from multiple scales according to the observation that the shape and size of the tumors are varied.Secondly, we improve the loss function based on the consideration that the features required for proper mutation detection are often not obvious in cross-sections of tumor edges, which raises more attention to these hard examples in the network. Finally, we devise a training scheme based on data augmentation to enhance the generalization ability of networks. Extensively verified on clinical gastric CT datasets of 20 testing volumes with 63648 CT images, our method achieves the accuracy of 94.90% in predicting the HER-2 genes mutation status of at the CT image. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：预测肿瘤基因的突变状态具有重要的临床意义。最近的研究已经表明，某些突变可以通过研究从计算机断层摄影（CT）数据肿瘤的图像特征来非侵入性地进行预测。目前，这种图像特征识别方法的主要依赖于手工处理来提取图像的广义特征单独使用或机器处理，而不考虑肿瘤本身，这使得它难以实现进一步突破的形态差异。在本文中，我们提出了一个金字塔聚焦网络（PFNET）基于CT图像突变预测和分类。首先，我们使用空间金字塔池收集线索的语义功能中根据观察，从多尺度地图，肿瘤的形状和大小varied.Secondly，我们提高基于考虑损失函数的需要进行适当的突变特征检测经常在肿瘤的边缘，这引起了更多的关注到网络中的这些硬实例的横截面并不明显。最后，我们设计基于数据增强，以提高网络的泛化能力的培训计划。对20测试卷与63648 CT图像临床胃癌CT数据验证广泛，我们的方法实现了94.90％的准确预测在CT图像的HER-2基因突变状态。</font>
</div>


<hr>
<div id="paper37"> <b>37. Super-resolution of clinical CT volumes with modified CycleGAN using  micro CT volumes</b>  <a href="https://arxiv.org/pdf/2004.03272" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title37" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=ZHENG%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tong ZHENG</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=ODA%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hirohisa ODA</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=MORIYA%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Takayasu MORIYA</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=SUGINO%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Takaaki SUGINO</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=NAKAMURA%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shota NAKAMURA</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=ODA%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Masahiro ODA</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=MORI%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Masaki MORI</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=TAKABATAKE%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hirotsugu TAKABATAKE</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=NATORI%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hiroshi NATORI</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=MORI%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kensaku MORI</a><br>
<font size="3">
Abstract: This paper presents a super-resolution (SR) method with unpaired training dataset of clinical CT and micro CT volumes. For obtaining very detailed information such as cancer invasion from pre-operative clinical CT volumes of lung cancer patients, SR of clinical CT volumes to $\m$}CT level is desired. While most SR methods require paired low- and high- resolution images for training, it is infeasible to obtain paired clinical CT and {\mu}CT volumes. We propose a SR approach based on CycleGAN, which could perform SR on clinical CT into $\mu$CT level. We proposed new loss functions to keep cycle consistency, while training without paired volumes. Experimental results demonstrated that our proposed method successfully performed SR of clinical CT volume of lung cancer patients into $\mu$CT level. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种超分辨率（SR）方法的临床CT和微型CT卷不成对训练数据集。为了获得非常详细的信息，例如来自肺癌患者，临床CT卷SR的手术前的临床CT体积癌症侵入到$ \ M $} CT水平是期望的。虽然大多数SR方法需要配对的低收入和高清晰度的图像进行训练，这是不可行的，获得配对临床CT和{\亩} CT卷。我们提出了一种基于CycleGAN一个SR方法，这可以在临床CT执行SR到$ \ $万亩CT水平。我们提出了新的损失的功能，以保持一致性循环，而没有配对训练量。实验结果表明，我们提出的方法成功地进行临床CT体积的肺癌患者到$ \ $万亩CT水平的SR。</font>
</div>


<hr>
<div id="paper38"> <b>38. SC4D: A Sparse 4D Convolutional Network for Skeleton-Based Action  Recognition</b>  <a href="https://arxiv.org/pdf/2004.03259" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title38" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yifan Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jian Cheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hanqing Lu</a><br>
<font size="3">
Abstract: In this paper, a new perspective is presented for skeleton-based action recognition. Specifically, we regard the skeletal sequence as a spatial-temporal point cloud and voxelize it into a 4-dimensional grid. A novel sparse 4D convolutional network (SC4D) is proposed to directly process the generated 4D grid for high-level perceptions. Without manually designing the hand-crafted transformation rules, it makes better use of the advantages of the convolutional network, resulting in a more concise, general and robust framework for skeletal data. Besides, by processing the space and time simultaneously, it largely keeps the spatial-temporal consistency of the skeletal data, and thus brings better expressiveness. Moreover, with the help of the sparse tensor, it can be efficiently executed with less computations. To verify the superiority of SC4D, extensive experiments are conducted on two challenging datasets, namely, NTU-RGBD and SHREC, where SC4D achieves state-of-the-art performance on both of them. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种新的视角提出了一种基于骨架动作识别。具体而言，我们认为骨架序列作为时空点云和它体素化到4维网格。一种新的疏4D卷积网络（SC4D）提出了直接处理所生成的网格4D为高电平感知。无需手动设计手工制作的转换规则，它可以更好地利用的卷积网络的优势，产生了更加简洁，通用和健壮的框架骨骼数据。此外，通过同时处理所述空间和时间，它在很大程度上保持骨架数据的时空一致性，并因此带来了更好的表现力。而且，随着稀疏张量的帮助下，它可有效地以较少的计算来执行。为了验证SC4D的优越性，广泛的实验工作是两个有挑战性的数据集，分别是台大RGBD和SHREC，其中SC4D达到他们两个国家的最先进的性能进行。</font>
</div>


<hr>
<div id="paper39"> <b>39. Hierarchical Opacity Propagation for Image Matting</b>  <a href="https://arxiv.org/pdf/2004.03249" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title39" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yaoyi Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qingyao Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongtao Lu</a><br>
<font size="3">
Abstract: Natural image matting is a fundamental problem in computational photography and computer vision. Deep neural networks have seen the surge of successful methods in natural image matting in recent years. In contrast to traditional propagation-based matting methods, some top-tier deep image matting approaches tend to perform propagation in the neural network implicitly. A novel structure for more direct alpha matte propagation between pixels is in demand. To this end, this paper presents a hierarchical opacity propagation (HOP) matting method, where the opacity information is propagated in the neighborhood of each point at different semantic levels. The hierarchical structure is based on one global and multiple local propagation blocks. With the HOP structure, every feature point pair in high-resolution feature maps will be connected based on the appearance of input image. We further propose a scale-insensitive positional encoding tailored for image matting to deal with the unfixed size of input image and introduce the random interpolation augmentation into image matting. Extensive experiments and ablation study show that HOP matting is capable of outperforming state-of-the-art matting methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自然图像抠图是计算摄影测量和计算机视觉的一个基本问题。深层神经网络已经看到了在自然图像抠图成功的方法在近年来的激增。与传统的基于传播消光的方法，一些顶级深图像抠图方法往往在神经网络中隐式地进行传播。为像素之间更直接的阿尔法磨砂传播一种新颖的结构是需要的。为此，本文提出了一种分层的不透明度传播（HOP）消光方法，其中不透明度信息在不同的语义等级的每个点附近传播。层次结构是基于一个全局和多个本地传播块。与HOP结构，在高分辨率的特征地图的每个特征点对将基于输入的图像的外观进行连接。我们进一步提出了图像抠图量身定做应对输入图像的大小不固定，并介绍了随机插值增强到图像抠图规模不敏感的位置编码。大量的实验和研究的消融表明，HOP消光能够超越国家的最先进的消光方法。</font>
</div>


<hr>
<div id="paper40"> <b>40. Motion-supervised Co-Part Segmentation</b>  <a href="https://arxiv.org/pdf/2004.03234" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title40" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Siarohin%2A%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aliaksandr Siarohin*</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Roy%2A%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Subhankar Roy*</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lathuili%C3%A8re%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stéphane Lathuilière</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tulyakov%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sergey Tulyakov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ricci%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Elisa Ricci</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sebe%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nicu Sebe</a><br>
<font size="3">
Abstract: Recent co-part segmentation methods mostly operate in a supervised learning setting, which requires a large amount of annotated data for training. To overcome this limitation, we propose a self-supervised deep learning method for co-part segmentation. Differently from previous works, our approach develops the idea that motion information inferred from videos can be leveraged to discover meaningful object parts. To this end, our method relies on pairs of frames sampled from the same video. The network learns to predict part segments together with a representation of the motion between two frames, which permits reconstruction of the target image. Through extensive experimental evaluation on publicly available video sequences we demonstrate that our approach can produce improved segmentation maps with respect to previous self-supervised co-part segmentation approaches. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近的共同部分的分割方法主要是在监督学习环境，这需要大量的训练注解数据进行操作。为了克服这种局限性，我们提出了联合部分分割自我监督的深度学习方法。不同于以往的作品，我们的方法发展的想法，从视频中推断出运动信息可以被利用来发现有意义的对象部分。为此，我们的方法依赖于对来自同一视频采样帧。网络学习有两个帧之间的运动，这允许对象图像的重建的表示预测部段连接在一起。通过对可公开获得的视频序列广泛的实验评估中，我们证明了我们的方法可以产生改进的分割地图相对于以前的自我监督的共同部分的分割方法。</font>
</div>


<hr>
<div id="paper41"> <b>41. Utilising Prior Knowledge for Visual Navigation: Distil and Adapt</b>  <a href="https://arxiv.org/pdf/2004.03222" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title41" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Moghaddam%2C+M+M+K" target="_blank" rel="noopener" style="color:#0000EE;">M. Mahdi Kazemi Moghaddam</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Abbasnejad%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Ehsan Abbasnejad</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Javen Shi</a><br>
<font size="3">
Abstract: We, as humans, can impeccably navigate to localise a target object, even in an unseen environment. We argue that this impressive ability is largely due to incorporation of \emph{prior knowledge} (or experience) and \emph{visual cues}--that current visual navigation approaches lack. In this paper, we propose to use externally learned prior knowledge of object relations, which is integrated to our model via constructing a neural graph. To combine appropriate assessment of the states and the prior (knowledge), we propose to decompose the value function in the actor-critic reinforcement learning algorithm and incorporate the prior in the critic in a novel way that reduces the model complexity and improves model generalisation. Our approach outperforms the current state-of-the-art in AI2THOR visual navigation dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们作为人类，可以抹得导航本地化的目标对象，即使是在一个看不见的环境。我们认为，这令人印象深刻的能力在很大程度上是由于\ {EMPH先验知识纳入}（或经验）和\ {EMPH视觉线索}  - ，目前的可视导航方法缺乏。在本文中，我们建议使用对象的关系，这是通过构建神经图形集成到我们的模型从外部学习先验知识。要结合状态和之前的（知识）的合适的评估，我们建议以分解演员评论家强化学习算法的价值功能和降低模型复杂度，提高模型综合了一种新的方式结合在评论家之前。我们的方法优于当前状态的最先进的在AI2THOR可视导航数据集。</font>
</div>


<hr>
<div id="paper42"> <b>42. Neural Image Inpainting Guided with Descriptive Text</b>  <a href="https://arxiv.org/pdf/2004.03212" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title42" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lisai Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qingcai Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Baotian Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuoran Jiang</a><br>
<font size="3">
Abstract: Neural image inpainting has achieved promising performance in generating semantically plausible content. Most of the recent works mainly focus on inpainting images depending on vision information, while neglecting the semantic information implied in human languages. To acquire more semantically accurate inpainting images, this paper proposes a novel inpainting model named \textit{N}eural \textit{I}mage Inpainting \textit{G}uided with \textit{D}escriptive \textit{T}ext (NIGDT). First, a dual multi-modal attention mechanism is designed to extract the explicit semantic information about corrupted regions. The mechanism is trained to combine the descriptive text and two complementary images through reciprocal attention maps. Second, an image-text matching loss is designed to enforce the model output following the descriptive text. Its goal is to maximize the semantic similarity of the generated image and the text. Finally, experiments are conducted on two open datasets with captions. Experimental results show that the proposed NIGDT model outperforms all compared models on both quantitative and qualitative comparison. The results also demonstrate that the proposed model can generate images consistent with the guidance text, which provides a flexible way for user-guided inpainting. Our systems and code will be released soon. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在产生合理的语义内容神经图像修复已取得看好的表现。最近期的作品主要集中在依赖于视觉信息补绘的图像，而忽略了人类语言所蕴含的语义信息。为了获得更多语义准确修补图像，提出命名一种新颖修复模型\ textit {N} eural \ textit {I}法师图像修复\ textit {G} uided与\ textit {d} escriptive \ textit横置EXT（NIGDT ）。首先，双多模态注意机制的目的是提取约损坏的地区明确的语义信息。该机制被训练为描述性文字，并通过相互关注地图两种互补的图像组合。第二，图像文本匹配损耗被设计为执行以下描述文本模式输出。它的目标是最大限度地生成的图像和文本的语义相似。最后，实验议两个公开数据集与字幕进行。实验结果表明，该NIGDT模型优于在定性和定量比较所有比较的车型。结果还表明，该模型可生成具有指导文本，它为用户提供引导修补灵活的方式是一致的图像。我们的系统和代码将很快被释放。</font>
</div>


<hr>
<div id="paper43"> <b>43. Multi-Task Learning via Co-Attentive Sharing for Pedestrian Attribute  Recognition</b>  <a href="https://arxiv.org/pdf/2004.03164" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title43" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haitian Zeng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ai%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haizhou Ai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhuang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zijie Zhuang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Long Chen</a><br>
<font size="3">
Abstract: Learning to predict multiple attributes of a pedestrian is a multi-task learning problem. To share feature representation between two individual task networks, conventional methods like Cross-Stitch and Sluice network learn a linear combination of features or feature subspaces. However, linear combination rules out the complex interdependency between channels. Moreover, spatial information exchanging is less-considered. In this paper, we propose a novel Co-Attentive Sharing (CAS) module which extracts discriminative channels and spatial regions for more effective feature sharing in multi-task learning. The module consists of three branches, which leverage different channels for between-task feature fusing, attention generation and task-specific feature enhancing, respectively. Experiments on two pedestrian attribute recognition datasets show that our module outperforms the conventional sharing units and achieves superior results compared to the state-of-the-art approaches using many metrics. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：学习预测行人的多个属性是一个多任务学习问题。到两个单独的任务网络之间共享的特征表示，像十字绣和水闸网络常规方法学的特征或特征子空间的线性组合。然而，线性组合排除了信道之间的复杂的相互依赖性。此外，交换空间信息被考虑的以下。在本文中，我们提出一种提取歧视渠道，在多任务学习更有效的特征共享空间区域一个新的合作细心的共享（CAS）模块。该模块包括三个分支，其任务间的功能融合，关注代和任务特定功能的杠杆作用不同渠道分别提高。在两个行人属性识别数据集实验结果表明，我们的模块优于传统的共享单元并且实现优异的结果相比，所述状态的最技术方法使用多个指标。</font>
</div>


<hr>
<div id="paper44"> <b>44. Real-time Classification from Short Event-Camera Streams using  Input-filtering Neural ODEs</b>  <a href="https://arxiv.org/pdf/2004.03156" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title44" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Giannone%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Giorgio Giannone</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Anoosheh%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Asha Anoosheh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Quaglino%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alessio Quaglino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=D%27Oro%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pierluca D'Oro</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gallieri%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marco Gallieri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Masci%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jonathan Masci</a><br>
<font size="3">
Abstract: Event-based cameras are novel, efficient sensors inspired by the human vision system, generating an asynchronous, pixel-wise stream of data. Learning from such data is generally performed through heavy preprocessing and event integration into images. This requires buffering of possibly long sequences and can limit the response time of the inference system. In this work, we instead propose to directly use events from a DVS camera, a stream of intensity changes and their spatial coordinates. This sequence is used as the input for a novel \emph{asynchronous} RNN-like architecture, the Input-filtering Neural ODEs (INODE). This is inspired by the dynamical systems and filtering literature. INODE is an extension of Neural ODEs (NODE) that allows for input signals to be continuously fed to the network, like in filtering. The approach naturally handles batches of time series with irregular time-stamps by implementing a batch forward Euler solver. INODE is trained like a standard RNN, it learns to discriminate short event sequences and to perform event-by-event online inference. We demonstrate our approach on a series of classification tasks, comparing against a set of LSTM baselines. We show that, independently of the camera resolution, INODE can outperform the baselines by a large margin on the ASL task and it's on par with a much larger LSTM for the NCALTECH task. Finally, we show that INODE is accurate even when provided with very few events. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于事件的相机是通过人类视觉系统的启发新颖，高效的传感器，生成数据的异步，逐像素流。从这些数据中学习一般是通过重预处理和事件融入图像进行。这需要可能长序列的缓冲和可以限制推理系统的响应时间。在这项工作中，我们反而建议直接使用从DVS相机，强度变化及其空间坐标的流事件。该序列被用作输入一种新颖\ EMPH {异步} RNN状架构，输入滤波神经常微分方程（INODE）。这是由动力系统和过滤文献启发。 INODE是神经常微分方程（节点），其允许输入信号被连续地供给到所述网络，像滤波的延伸。该方法通过实施一批向欧拉求解自然处理的不规则的时间戳时间序列批次。 INODE被训练像一个标准的RNN，它学会辨别短的事件序列，并执行事件的事件在网上推断。我们证明了我们一系列的分类任务的方法，针对一组LSTM基准的比较。我们表明，独立于照相机的分辨率，索引节点可以通过翔升任务大幅跑赢基准和它看齐的NCALTECH任务大得多LSTM。最后，我们表明，即使极少数事件提供索引节点是准确的。</font>
</div>


<hr>
<div id="paper45"> <b>45. Adaptive Multiscale Illumination-Invariant Feature Representation for  Undersampled Face Recognition</b>  <a href="https://arxiv.org/pdf/2004.03153" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title45" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Changhui Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaobo Lu</a><br>
<font size="3">
Abstract: This paper presents an novel illumination-invariant feature representation approach used to eliminate the varying illumination affection in undersampled face recognition. Firstly, a new illumination level classification technique based on Singular Value Decomposition (SVD) is proposed to judge the illumination level of input image. Secondly, we construct the logarithm edgemaps feature (LEF) based on lambertian model and local near neighbor feature of the face image, applying to local region within multiple scales. Then, the illumination level is referenced to construct the high performance LEF as well realize adaptive fusion for multiple scales LEFs for the face image, performing JLEF-feature. In addition, the constrain operation is used to remove the useless high-frequency interference, disentangling useful facial feature edges and constructing AJLEF-face. Finally, the effects of the our methods and other state-of-the-art algorithms including deep learning methods are tested on Extended Yale B, CMU PIE, AR as well as our Self-build Driver database (SDB). The experimental results demonstrate that the JLEF-feature and AJLEF-face outperform other related approaches for undersampled face recognition under varying illumination. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：提出用于消除欠人脸识别的变化的照明感情的新颖照明不变的特征表示的方法。首先，基于奇异值分解（SVD）以新的照明电平分类技术，提出了以判断输入图像的照明水平。其次，我们构建了对数edgemaps功能（LEF）基于朗伯模型和人脸图像的局部近邻的特征，多尺度范围内适用于局部地区。然后，将照明水平被引用来构建高性能LEF以及实现对多尺度LEFS为脸部图像，在执行JLEF特征自适应融合。此外，该约束操作用于去除无用的高频干扰，解开有用面部特征的边缘和构建AJLEF面。最后，对我们的方法和国家的最先进的其他算法包括深学习方法的效果是在扩展耶鲁B，CMU PIE，AR测试以及我们自建的驱动程序数据库（SDB）。实验结果表明，JLEF特征和AJLEF面优于变化的照明下欠人脸识别其他相关方法。</font>
</div>


<hr>
<div id="paper46"> <b>46. Predicting Camera Viewpoint Improves Cross-dataset Generalization for 3D  Human Pose Estimation</b>  <a href="https://arxiv.org/pdf/2004.03143" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title46" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhe Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shin%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daeyun Shin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fowlkes%2C+C+C" target="_blank" rel="noopener" style="color:#0000EE;">Charless C. Fowlkes</a><br>
<font size="3">
Abstract: Monocular estimation of 3d human pose has attracted increased attention with the availability of large ground-truth motion capture datasets. However, the diversity of training data available is limited and it is not clear to what extent methods generalize outside the specific datasets they are trained on. In this work we carry out a systematic study of the diversity and biases present in specific datasets and its effect on cross-dataset generalization across a compendium of 5 pose datasets. We specifically focus on systematic differences in the distribution of camera viewpoints relative to a body-centered coordinate frame. Based on this observation, we propose an auxiliary task of predicting the camera viewpoint in addition to pose. We find that models trained to jointly predict viewpoint and pose systematically show significantly improved cross-dataset generalization. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：三维人体姿势的单眼估计已吸引了越来越多的关注与大型地面实况动作捕捉数据集的可用性。然而，现有的训练数据的多样性是有限的，目前尚不清楚在何种程度上概括的方法外，他们正在训练特定数据集。在这项工作中，我们开展多样性和偏见存在于特定的数据集的系统研究和跨5个姿态数据集汇编了跨数据集推广效果。我们特别着眼于在照相机视点的分布相对于系统性差异体心坐标框架。基于这一观察，我们提出预测除了姿势摄像机视点的辅助任务。我们发现受过训练，共同的是模型预测的观点，并造成系统表现出显著改善跨数据集的泛化。</font>
</div>


<hr>
<div id="paper47"> <b>47. Human Motion Transfer from Poses in the Wild</b>  <a href="https://arxiv.org/pdf/2004.03142" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title47" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jian Ren</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chai%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Menglei Chai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tulyakov%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sergey Tulyakov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chen Fang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaohui Shen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianchao Yang</a><br>
<font size="3">
Abstract: In this paper, we tackle the problem of human motion transfer, where we synthesize novel motion video for a target person that imitates the movement from a reference video. It is a video-to-video translation task in which the estimated poses are used to bridge two domains. Despite substantial progress on the topic, there exist several problems with the previous methods. First, there is a domain gap between training and testing pose sequences--the model is tested on poses it has not seen during training, such as difficult dancing moves. Furthermore, pose detection errors are inevitable, making the job of the generator harder. Finally, generating realistic pixels from sparse poses is challenging in a single step. To address these challenges, we introduce a novel pose-to-video translation framework for generating high-quality videos that are temporally coherent even for in-the-wild pose sequences unseen during training. We propose a pose augmentation method to minimize the training-test gap, a unified paired and unpaired learning strategy to improve the robustness to detection errors, and two-stage network architecture to achieve superior texture quality. To further boost research on the topic, we build two human motion datasets. Finally, we show the superiority of our approach over the state-of-the-art studies through extensive experiments and evaluations on different datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们解决人类运动传递，在这里我们合成新型移动视频，从参考视频模仿运动的目标的人的问题。这是一个视频到视频翻译任务所估计的姿态被用来桥接两个领域。尽管在话题实质性进展，存在着一些问题与以前的方法。首先，有训练和测试姿势序列之间的间隙域 - 该模型是在其上训练时没有看到姿势，如难以舞蹈动作测试。此外，姿势检测错误是不可避免的，使得发电机的工作更难。最后，从稀疏姿势产生逼真的像素在一个单一的步骤是具有挑战性的。为了应对这些挑战，我们引入了一种新的姿势到视频转换框架生成高品质的视频在时间上一致，即使在最野化训练过程中看不见的姿势序列。我们提出了一个姿势隆胸方法，以尽量减少培训测试间隙，统一成对和不成对学习策略，提高稳健性检测误差，和两级网络架构，从而实现卓越的纹理质量。对问题的进一步提升研究，我们构建两个人体运动数据集。最后，我们将展示我们对国家的最先进的研究方法，通过对不同的数据集广泛的实验和评价的优越性。</font>
</div>


<hr>
<div id="paper48"> <b>48. Toward Fine-grained Facial Expression Manipulation</b>  <a href="https://arxiv.org/pdf/2004.03132" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title48" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ling%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Ling</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xue%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Han Xue</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Li Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuhui Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rong Xie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiao Gu</a><br>
<font size="3">
Abstract: Facial expression manipulation, as an image-to-image translation problem, aims at editing facial expression with a given condition. Previous methods edit an input image under the guidance of a discrete emotion label or absolute condition (e.g., facial action units) to possess the desired expression. However, these methods either suffer from changing condition-irrelevant regions or are inefficient to preserve image quality. In this study, we take these two objectives into consideration and propose a novel conditional GAN model. First, we replace continuous absolute condition with relative condition, specifically, relative action units. With relative action units, the generator learns to only transform regions of interest which are specified by non-zero-valued relative AUs, avoiding estimating the current AUs of input image. Second, our generator is built on U-Net architecture and strengthened by multi-scale feature fusion (MSF) mechanism for high-quality expression editing purpose. Extensive experiments on both quantitative and qualitative evaluation demonstrate the improvements of our proposed approach compared with the state-of-the-art expression editing methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人脸表情的操作，作为图像 - 图像翻译的问题，目的是编辑给定的条件的面部表情。以前的方法编辑离散情感标签或绝对条件（例如，面部动作单元）的指导下的输入图像具有所需的表达。然而，这些方法无论是从变更条件无关的区域遭受或效率低下保持图像质量。在这项研究中，我们以这两个目标考虑，提出了一种新的条件GAN模式。首先，我们与相关条件，具体而言，相对动作单元代替连续绝对条件。与相对动作单元，发电机学会仅变换其通过非零值的AU相对指定感兴趣的区域，从而避免估计输入图像的当前的AU。其次，我们的发电机是建立在掌中宽带架构，并通过多尺度特征融合（MSF）机制，高品质的表达编辑宗旨加强。在定量和定性评估大量的实验证明与国家的最先进的表达编辑方法相比我们所提出的方法的改进。</font>
</div>


<hr>
<div id="paper49"> <b>49. Generative Adversarial Zero-shot Learning via Knowledge Graphs</b>  <a href="https://arxiv.org/pdf/2004.03109" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title49" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Geng%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuxia Geng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiaoyan Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhuo Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhiquan Ye</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zonggang Yuan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yantao Jia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huajun Chen</a><br>
<font size="3">
Abstract: Zero-shot learning (ZSL) is to handle the prediction of those unseen classes that have no labeled training data. Recently, generative methods like Generative Adversarial Networks (GANs) are being widely investigated for ZSL due to their high accuracy, generalization capability and so on. However, the side information of classes used now is limited to text descriptions and attribute annotations, which are in short of semantics of the classes. In this paper, we introduce a new generative ZSL method named KG-GAN by incorporating rich semantics in a knowledge graph (KG) into GANs. Specifically, we build upon Graph Neural Networks and encode KG from two views: class view and attribute view considering the different semantics of KG. With well-learned semantic embeddings for each node (representing a visual category), we leverage GANs to synthesize compelling visual features for unseen classes. According to our evaluation with multiple image classification datasets, KG-GAN can achieve better performance than the state-of-the-art baselines. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：零次学习（ZSL）是处理那些没有标记的训练数据的那些看不见类的预测。最近，像剖成对抗性网络（甘斯）生成方法正在广泛研究的ZSL由于其精度高，泛化能力强等特点。然而，现在使用的类的辅助信息被限制在文字描述和属性的注释，这是在类的语义的短。在本文中，我们引入一个名为KG-GaN通过在知识图（KG）到甘斯结合丰富的语义新生成ZSL方法。具体来说，我们从两个观点建立在图的神经网络和编码KG：类映射视图，并考虑KG的不同语义属性视图。随着每一个节点（代表了一种类型）以及学习的语义的嵌入，我们利用甘斯合成引人注目的视觉特征为看不见的类。根据我们与多个图像分类数据集的评估，KG-GaN可以实现比国家的最先进的基线更好的性能。</font>
</div>


<hr>
<div id="paper50"> <b>50. End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection</b>  <a href="https://arxiv.org/pdf/2004.03080" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title50" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Qian%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rui Qian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Garg%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Divyansh Garg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yan Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=You%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yurong You</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Belongie%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Serge Belongie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hariharan%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bharath Hariharan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Campbell%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mark Campbell</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Weinberger%2C+K+Q" target="_blank" rel="noopener" style="color:#0000EE;">Kilian Q. Weinberger</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei-Lun Chao</a><br>
<font size="3">
Abstract: Reliable and accurate 3D object detection is a necessity for safe autonomous driving. Although LiDAR sensors can provide accurate 3D point cloud estimates of the environment, they are also prohibitively expensive for many settings. Recently, the introduction of pseudo-LiDAR (PL) has led to a drastic reduction in the accuracy gap between methods based on LiDAR sensors and those based on cheap stereo cameras. PL combines state-of-the-art deep neural networks for 3D depth estimation with those for 3D object detection by converting 2D depth map outputs to 3D point cloud inputs. However, so far these two networks have to be trained separately. In this paper, we introduce a new framework based on differentiable Change of Representation (CoR) modules that allow the entire PL pipeline to be trained end-to-end. The resulting framework is compatible with most state-of-the-art networks for both tasks and in combination with PointRCNN improves over PL consistently across all benchmarks -- yielding the highest entry on the KITTI image-based 3D object detection leaderboard at the time of submission. Our code will be made available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：可靠和准确的立体物检测是安全的自动驾驶的必需品。虽然激光雷达传感器可以提供环境的准确的三维点云估计，它们也是许多设置过于昂贵。最近，引入伪激光雷达（PL）的导致了基于激光雷达传感器和方法是那些基于廉价立体摄像机之间的间隙的精度急剧降低。 PL通过转换2D深度图输出到3D点云输入结合了与那些用于3D物体检测三维深度估计状态的最先进的深神经网络。然而，到目前为止，这两个网络必须单独训练。在本文中，我们介绍了基于表象的微变化（COR）模块，允许整个PL管道进行培训结束到终端的新框架。所得到的框架是与大多数国家的最先进的网络，对于这两项任务，并与PointRCNN改善了PL组合兼容一直存在于所有的基准 - 产生在时间上基于图像KITTI立体物检测排行榜最高的入门提交。我们的代码将在这个HTTPS URL提供。</font>
</div>


<hr>
<div id="paper51"> <b>51. A Method for Curation of Web-Scraped Face Image Datasets</b>  <a href="https://arxiv.org/pdf/2004.03074" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title51" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kai Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Albiero%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vítor Albiero</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bowyer%2C+K+W" target="_blank" rel="noopener" style="color:#0000EE;">Kevin W. Bowyer</a><br>
<font size="3">
Abstract: Web-scraped, in-the-wild datasets have become the norm in face recognition research. The numbers of subjects and images acquired in web-scraped datasets are usually very large, with number of images on the millions scale. A variety of issues occur when collecting a dataset in-the-wild, including images with the wrong identity label, duplicate images, duplicate subjects and variation in quality. With the number of images being in the millions, a manual cleaning procedure is not feasible. But fully automated methods used to date result in a less-than-ideal level of clean dataset. We propose a semi-automated method, where the goal is to have a clean dataset for testing face recognition methods, with similar quality across men and women, to support comparison of accuracy across gender. Our approach removes near-duplicate images, merges duplicate subjects, corrects mislabeled images, and removes images outside a defined range of pose and quality. We conduct the curation on the Asian Face Dataset (AFD) and VGGFace2 test dataset. The experiments show that a state-of-the-art method achieves a much higher accuracy on the datasets after they are curated. Finally, we release our cleaned versions of both datasets to the research community. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：网络刮，在最狂野的数据集已经成为人脸识别研究的规范。在Web刮数据集收购对象和图像的数字通常是非常大的，对数以百万计的规模图像数量。在最疯狂收集数据集时，包括在质量与错误的身份标签图像，图像重复，重复的主题和变化会发生各种问题。与图像数百万的存在的数，手动清洁程序是不可行的。但是全自动清洁数据集的低于理想水平迄今使用结果的方法。我们提出了一个半自动化的方法，其目的是有一个干净的数据集用于测试人脸识别方法，并在保持男性和女性类似的质量，跨性别的准确性支持比较。我们的方式消除近乎重复的图像，合并重复的科目，贴错标签的图像进行校正，并定义的姿势和质量的范围之外，移除了图像。我们进行的亚洲面孔数据集（AFD）和VGGFace2测试数据集的策展。实验结果表明，国家的最先进的方法实现上的数据集的更高的精度它们策划后。最后，我们发布清洁两个数据集的版本研究团体。</font>
</div>


<hr>
<div id="paper52"> <b>52. MGGR: MultiModal-Guided Gaze Redirection with Coarse-to-Fine Learning</b>  <a href="https://arxiv.org/pdf/2004.03064" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title52" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingjing Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jichao Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiayuan Fan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sangineto%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Enver Sangineto</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sebe%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nicu Sebe</a><br>
<font size="3">
Abstract: Gaze redirection aims at manipulating a given eye gaze to a desirable direction according to a reference angle and it can be applied to many real life scenarios, such as video-conferencing or taking groups. However, the previous works suffer from two limitations: (1) low-quality generation and (2) low redirection precision. To this end, we propose an innovative MultiModal-Guided Gaze Redirection~(MGGR) framework that fully exploits eye-map images and target angles to adjust a given eye appearance through a designed coarse-to-fine learning. Our contribution is combining the flow-learning and adversarial learning for coarse-to-fine generation. More specifically, the role of the proposed coarse branch with flow field is to rapidly learn the spatial transformation for attaining the warped result with the desired gaze. The proposed fine-grained branch consists of a generator network with conditional residual image learning and a multi-task discriminator to reduce the gap between the warped image and the ground-truth image for recovering the finer texture details. Moreover, we propose leveraging the gazemap for desired angles as an extra guide to further improve the precision of gaze redirection. Extensive experiments on a benchmark dataset show that the proposed method outperforms the state-of-the-art methods in terms of image quality and redirection precision. Further evaluations demonstrate the effectiveness of the proposed coarse-to-fine and gazemap modules. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：凝视重定向目的在操纵一个给定的眼根据参考角度注视到所希望的方向和它可以被应用到许多真实生活场景，诸如视频会议或服用组。 （1）低质量生成和（2）低重定向精度：但是，之前的作品从两个限制的影响。为此，我们提出了一个创新的多模式制导凝视重定向〜（MGGR）框架，充分利用了人眼的地图图像和目标角度来调整通过设计粗到精的学习给定眼的外观。我们的贡献是结合流动学习和对抗性学习粗到细的一代。更具体地，与流场所提出的粗支的作用是迅速学习空间变换为与所需的注视达到翘曲结果。所提出的细粒度支路由发电机网络条件残像学习和多任务鉴别，减少扭曲图像和地面实况图像之间的差距为恢复更精细的纹理细节。此外，我们建议撬动gazemap为所需角度作为一个额外的引导件，以进一步提高注视重定向的精度。上的基准数据集显示，该方法优于在图像质量和精度重定向方面国家的最先进的方法广泛的实验。进一步的评估证明了该粗到细和gazemap模块的有效性。</font>
</div>


<hr>
<div id="paper53"> <b>53. Depth Sensing Beyond LiDAR Range</b>  <a href="https://arxiv.org/pdf/2004.03048" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title53" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kai Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiaxin Xie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Snavely%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Noah Snavely</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qifeng Chen</a><br>
<font size="3">
Abstract: Depth sensing is a critical component of autonomous driving technologies, but today's LiDAR- or stereo camera-based solutions have limited range. We seek to increase the maximum range of self-driving vehicles' depth perception modules for the sake of better safety. To that end, we propose a novel three-camera system that utilizes small field of view cameras. Our system, along with our novel algorithm for computing metric depth, does not require full pre-calibration and can output dense depth maps with practically acceptable accuracy for scenes and objects at long distances not well covered by most commercial LiDARs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深度感知是自主驾驶技术的重要组成部分，但今天的LiDAR-或立体声基于摄像机的解决方案具有有限的范围内。我们努力提高的自驾车车辆的深度感知模块的最大范围内获得更好的安全着想。为此，我们建议采用的视摄像头的小领域的新三摄像系统。我们的系统，我们的新算法计算量度深度一起，不需要与长距离不能很好的大多数商业激光雷达覆盖的场景和物体实际上是可接受的准确性承担全部预校准，并可以输出稠密深度图。</font>
</div>


<hr>
<div id="paper54"> <b>54. Manifold-driven Attention Maps for Weakly Supervised Segmentation</b>  <a href="https://arxiv.org/pdf/2004.03046" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title54" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=V%2C+S+A" target="_blank" rel="noopener" style="color:#0000EE;">Sukesh Adiga V</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dolz%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jose Dolz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lombaert%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Herve Lombaert</a><br>
<font size="3">
Abstract: Segmentation using deep learning has shown promising directions in medical imaging as it aids in the analysis and diagnosis of diseases. Nevertheless, a main drawback of deep models is that they require a large amount of pixel-level labels, which are laborious and expensive to obtain. To mitigate this problem, weakly supervised learning has emerged as an efficient alternative, which employs image-level labels, scribbles, points, or bounding boxes as supervision. Among these, image-level labels are easier to obtain. However, since this type of annotation only contains object category information, the segmentation task under this learning paradigm is a challenging problem. To address this issue, visual salient regions derived from trained classification networks are typically used. Despite their success to identify important regions on classification tasks, these saliency regions only focus on the most discriminant areas of an image, limiting their use in semantic segmentation. In this work, we propose a manifold driven attention-based network to enhance visual salient regions, thereby improving segmentation accuracy in a weakly supervised setting. Our method generates superior attention maps directly during inference without the need of extra computations. We evaluate the benefits of our approach in the task of segmentation using a public benchmark on skin lesion images. Results demonstrate that our method outperforms the state-of-the-art GradCAM by a margin of ~22% in terms of Dice score. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：分割使用深学习已示于医学成像有前途的方向，因为它在分析和诊断疾病爱滋病。然而，深模型的主要缺点是它们需要大量的像素级别的标签，这是费力的和昂贵的获得。为了缓解这一问题，弱监督学习已经成为一种有效的替代方案，它采用影像级标签，涂鸦，点，或边界框的监督。其中，图像层次标签更容易获得。然而，由于这种类型的注释中只包含对象类别的信息，这种学习模式下的细分任务是一个具有挑战性的问题。为了解决这个问题，从训练的分类网络衍生的视觉显着区域通常被使用。尽管他们的成功，以确定对分类任务的重要区域，这些区域的显着只注重图像的最判别区域，限制其在语义分割使用。在这项工作中，我们提出了一种歧管驱动的关注，基于网络，以增强视觉显着区域，从而提高在弱监督设置分割精度。我们的方法产生的推理过程中优越的注意力直接映射，而无需额外计算。我们评估我们使用对皮肤的损伤图像公共基准分割的任务方法的好处。结果表明，我们的方法通过的〜22％的裕度优于国家的最先进的GradCAM在骰子分数方面。</font>
</div>


<hr>
<div id="paper55"> <b>55. When, Where, and What? A New Dataset for Anomaly Detection in Driving  Videos</b>  <a href="https://arxiv.org/pdf/2004.03044" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title55" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Yao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xizi Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mingze Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zelin Pu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Atkins%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Ella Atkins</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Crandall%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Crandall</a><br>
<font size="3">
Abstract: Video anomaly detection (VAD) has been extensively studied. However, research on egocentric traffic videos with dynamic scenes lacks large-scale benchmark datasets as well as effective evaluation metrics. This paper proposes traffic anomaly detection with a \textit{when-where-what} pipeline to detect, localize, and recognize anomalous events from egocentric videos. We introduce a new dataset called Detection of Traffic Anomaly (DoTA) containing 4,677 videos with temporal, spatial, and categorical annotations. A new spatial-temporal area under curve (STAUC) evaluation metric is proposed and used with DoTA. State-of-the-art methods are benchmarked for two VAD-related tasks.Experimental results show STAUC is an effective VAD metric. To our knowledge, DoTA is the largest traffic anomaly dataset to-date and is the first supporting traffic anomaly studies across when-where-what perspectives. Our code and dataset can be found in: this https URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：视频异常检测（VAD）已被广泛研究。然而，与动态场景以自我为中心的交通视频研究缺乏大规模的基准数据集以及有效的评价指标。本文提出了流量异常检测与\ {textit时，在那里，什么}管线探测，定位和识别从以自我为中心的视频异常事件。我们引入所谓的流量异常检测（DOTA）含有4677个与时间，空间和明确的注解视频新的数据集。下曲线（STAUC）评估度量的新的空间 - 时间区域被提出并具有DOTA使用。方法基准两VAD相关tasks.Experimental结果的最先进的国家显示STAUC是一种有效的VAD度量。据我们所知，DOTA是最大的流量异常数据集的最新，也是第一支撑流量异常时，对面，在那里，什么样的观点研究。此HTTPS URL：我们的代码和数据集可以发现</font>
</div>


<hr>
<div id="paper56"> <b>56. Learning Generative Models of Shape Handles</b>  <a href="https://arxiv.org/pdf/2004.03028" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title56" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gadelha%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matheus Gadelha</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gori%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Giorgio Gori</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ceylan%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Duygu Ceylan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mech%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Radomir Mech</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Carr%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nathan Carr</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Boubekeur%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tamy Boubekeur</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rui Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Maji%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Subhransu Maji</a><br>
<font size="3">
Abstract: We present a generative model to synthesize 3D shapes as sets of handles - lightweight proxies that approximate the original 3D shape -- for applications in interactive editing, shape parsing, and building compact 3D representations. Our model can generate handle sets with varying cardinality and different types of handles (Figure 1). Key to our approach is a deep architecture that predicts both the parameters and existence of shape handles, and a novel similarity measure that can easily accommodate different types of handles, such as cuboids or sphere-meshes. We leverage the recent advances in semantic 3D annotation as well as automatic shape summarizing techniques to supervise our approach. We show that the resulting shape representations are intuitive and achieve superior quality than previous state-of-the-art. Finally, we demonstrate how our method can be used in applications such as interactive shape editing, completion, and interpolation, leveraging the latent space learned by our model to guide these tasks. Project page: this http URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种生成模型合成三维形状，套柄 - 轻量级代理是近似原始的3D形状 - 在交互式编辑，形状分析，建设紧凑型3D表示应用程序。我们的模型可以产生手柄套具有变化的基数和不同类型的手柄（图1）。关键是我们的方法是一种深架构，预测双方的参数和形状的手柄的存在，和一种新型的相似性度量，可以很容易适应不同类型的手柄，诸如长方体或球体的网格。我们利用语义标注3D以及自动形状总结技术的最新进展，以监督我们的方法。我们发现，产生的形状表示是直观，实现卓越的质量比以前的国家的最先进的。最后，我们将演示如何我们的方法可以在应用程序中使用，如互动形状编辑，完成和插值，从而利用我们的模型学会引导这些任务的潜在空间。项目页面：这个HTTP URL。</font>
</div>


<hr>
<div id="paper57"> <b>57. Field-Level Crop Type Classification with k Nearest Neighbors: A  Baseline for a New Kenya Smallholder Dataset</b>  <a href="https://arxiv.org/pdf/2004.03023" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title57" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kerner%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hannah Kerner</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nakalembe%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Catherine Nakalembe</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Becker-Reshef%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Inbal Becker-Reshef</a><br>
<font size="3">
Abstract: Accurate crop type maps provide critical information for ensuring food security, yet there has been limited research on crop type classification for smallholder agriculture, particularly in sub-Saharan Africa where risk of food insecurity is highest. Publicly-available ground-truth data such as the newly-released training dataset of crop types in Kenya (Radiant MLHub) are catalyzing this research, but it is important to understand the context of when, where, and how these datasets were obtained when evaluating classification performance and using them as a benchmark across methods. In this paper, we provide context for the new western Kenya dataset which was collected during an atypical 2019 main growing season and demonstrate classification accuracy up to 64% for maize and 70% for cassava using k Nearest Neighbors--a fast, interpretable, and scalable method that can serve as a baseline for future work. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：精确的作物类型映射为确保粮食安全的重要信息，还出现了对作物类型的分类研究有限公司为小农农业，特别是在撒哈拉以南非洲，粮食不安全的风险是最高的。如在肯尼亚（辐射MLHub）作物类型的新推出的训练数据集公开可用的地面实况数据催化该研究，但要明白何时，何地，如何获得这些数据集的情况下是很重要的评估时，分类性能和使用它们作为整个方法的基准。在本文中，我们提供了这是在非典型2019主要生长季节收集到的新的肯尼亚西部数据集的背景下，展示的分类准确率高达64％的玉米和使用k近邻木薯70％ - 一个快速的，可解释的，并可伸缩的方法，可以作为今后工作的基础。</font>
</div>


<hr>
<div id="paper58"> <b>58. Adaptive Fractional Dilated Convolution Network for Image Aesthetics  Assessment</b>  <a href="https://arxiv.org/pdf/2004.03015" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title58" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiuyu Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Ning Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lei%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peng Lei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yi Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianping Fan</a><br>
<font size="3">
Abstract: To leverage deep learning for image aesthetics assessment, one critical but unsolved issue is how to seamlessly incorporate the information of image aspect ratios to learn more robust models. In this paper, an adaptive fractional dilated convolution (AFDC), which is aspect-ratio-embedded, composition-preserving and parameter-free, is developed to tackle this issue natively in convolutional kernel level. Specifically, the fractional dilated kernel is adaptively constructed according to the image aspect ratios, where the interpolation of nearest two integers dilated kernels is used to cope with the misalignment of fractional sampling. Moreover, we provide a concise formulation for mini-batch training and utilize a grouping strategy to reduce computational overhead. As a result, it can be easily implemented by common deep learning libraries and plugged into popular CNN architectures in a computation-efficient manner. Our experimental results demonstrate that our proposed method achieves state-of-the-art performance on image aesthetics assessment over the AVA dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：为了能利用深层学习图像美学评价，一个关键，但没有解决的问题是如何无缝地整合图像宽高比的信息，以了解更多可靠的模型。在本文中，一种自适应分数扩张卷积（AFDC），这是纵横比包埋，组合物保留和无参数，被显影以在卷积内核级本地处理这个问题。具体地讲，分数扩张内核根据图像的纵横比，其中最近的两个整数的插值扩张内核用来应付分数采样的未对准自适应地构成。此外，我们提供小批量训练简洁制定和利用组策略，减少计算开销。其结果是，它可以容易地由普通深学习库实现，并插入到在计算有效的方式流行CNN架构。我们的实验结果表明，我们提出的方法实现对图像的美学评价过AVA数据集的国家的最先进的性能。</font>
</div>


<hr>
<div id="paper59"> <b>59. LUVLi Face Alignment: Estimating Landmarks' Location, Uncertainty, and  Visibility Likelihood</b>  <a href="https://arxiv.org/pdf/2004.02980" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title59" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Abhinav Kumar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marks%2C+T+K" target="_blank" rel="noopener" style="color:#0000EE;">Tim K. Marks</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mou%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenxuan Mou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Ye Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jones%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Jones</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cherian%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anoop Cherian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Koike-Akino%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Toshiaki Koike-Akino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoming Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chen Feng</a><br>
<font size="3">
Abstract: Modern face alignment methods have become quite accurate at predicting the locations of facial landmarks, but they do not typically estimate the uncertainty of their predicted locations nor predict whether landmarks are visible. In this paper, we present a novel framework for jointly predicting landmark locations, associated uncertainties of these predicted locations, and landmark visibilities. We model these as mixed random variables and estimate them using a deep network trained with our proposed Location, Uncertainty, and Visibility Likelihood (LUVLi) loss. In addition, we release an entirely new labeling of a large face alignment dataset with over 19,000 face images in a full range of head poses. Each face is manually labeled with the ground-truth locations of 68 landmarks, with the additional information of whether each landmark is unoccluded, self-occluded (due to extreme head poses), or externally occluded. Not only does our joint estimation yield accurate estimates of the uncertainty of predicted landmark locations, but it also yields state-of-the-art estimates for the landmark locations themselves on multiple standard face alignment datasets. Our method's estimates of the uncertainty of predicted landmark locations could be used to automatically identify input images on which face alignment fails, which can be critical for downstream tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现代的脸比对方法已经在预测脸部显着标记的位置相当准确，但他们通常不会估计其预测位置的不确定性，也没有预测的地标是否可见。在本文中，我们提出了一个新颖的框架共同预测界标位置，这些预测的位置的相关的不确定性，和界标能见度。我们这些建模为混合随机变量，并使用与我们提出的地点，不确定性和能见度似然（LUVLi）亏损培养了深厚的网络估计它们。此外，我们在全方位头部姿势释放出大量的脸比对数据集有超过19,000的人脸图像的全新标识。每一个面是手动与68个的地标地面实况位置标记的，与每一个地标是否是未被遮挡的附加信息，自闭塞（由于极端头的姿势），或从外部封闭。不仅我们的联合估计产量预测的地标位置的不确定性准确的估计，但同时也产生了国家的最先进的估计在多个标准人脸比对数据集的界标位置本身。预测的地标位置的不确定性，我们的方法的估计可以用来自动识别哪个脸比对失败的输入图像，这对于下游的任务至关重要。</font>
</div>


<hr>
<div id="paper60"> <b>60. Deblurring using Analysis-Synthesis Networks Pair</b>  <a href="https://arxiv.org/pdf/2004.02956" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title60" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kaufman%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adam Kaufman</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fattal%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Raanan Fattal</a><br>
<font size="3">
Abstract: Blind image deblurring remains a challenging problem for modern artificial neural networks. Unlike other image restoration problems, deblurring networks fail behind the performance of existing deblurring algorithms in case of uniform and 3D blur models. This follows from the diverse and profound effect that the unknown blur-kernel has on the deblurring operator. We propose a new architecture which breaks the deblurring network into an analysis network which estimates the blur, and a synthesis network that uses this kernel to deblur the image. Unlike existing deblurring networks, this design allows us to explicitly incorporate the blur-kernel in the network's training. In addition, we introduce new cross-correlation layers that allow better blur estimations, as well as unique components that allow the estimate blur to control the action of the synthesis deblurring action. Evaluating the new approach over established benchmark datasets shows its ability to achieve state-of-the-art deblurring accuracy on various tests, as well as offer a major speedup in runtime. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：盲图像去模糊仍然是现代人工神经网络的一个具有挑战性的问题。不像其他的图像恢复的问题，去模糊网络故障的存在去模糊算法统一和3D模糊模型的情况下，表现落后。在此之前，从多样化和深远的影响是未知的模糊内核的去模糊操作。我们提出了一个新的架构，它打破了去模糊网络成估计模糊的分析网络，以及使用该内核图像去模糊的综合网络。与现有的去模糊的网络，这样的设计可以让我们明确地将模糊内核在网络的训练。此外，我们引入允许更好的模糊估算新的互相关的层，以及独特的组件，其允许估计模糊控制合成去模糊的动作的动作。评估新办法在建立标准数据集显示了其实现国家的最先进的去模糊各种实验的准确性，以及提供在运行时可以大大提高速度的能力。</font>
</div>


<hr>
<div id="paper61"> <b>61. Objectness-Aware One-Shot Semantic Segmentation</b>  <a href="https://arxiv.org/pdf/2004.02945" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title61" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yinan Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Price%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Brian Price</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cohen%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Scott Cohen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gurari%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Danna Gurari</a><br>
<font size="3">
Abstract: While deep convolutional neural networks have led to great progress in image semantic segmentation, they typically require collecting a large number of densely-annotated images for training. Moreover, once trained, the model can only make predictions in a pre-defined set of categories. Therefore, few-shot image semantic segmentation has been explored to learn to segment from only a few annotated examples. In this paper, we tackle the challenging one-shot semantic segmentation problem by taking advantage of objectness. In order to capture prior knowledge of object and background, we first train an objectness segmentation module which generalizes well to unseen categories. Then we use the objectness module to predict the objects present in the query image, and train an objectness-aware few-shot segmentation model that takes advantage of both the object information and limited annotations of the unseen category to perform segmentation in the query image. Our method achieves a mIoU score of 57.9% and 22.6% given only one annotated example of an unseen category in PASCAL-5i and COCO-20i, outperforming related baselines overall. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：虽然深卷积神经网络已经导致图像语义分割了长足的进步，它们通常需要收集大量的培训密集标注图像。而且，一旦受过训练的模型只能使一组预定义的类别的预测。因此，很少有镜头图像语义分割已探索从只有几个注释例子学段。在本文中，我们通过采取对象性的优势，解决具有挑战性的单次语义分割问题。为了捕捉对象和背景的先验知识，我们首先训练从而推广很好地看不见的类别的对象性分割模块。然后我们使用对象性模块预测的对象出现在查询图像中，培养的对象性意识的为数不多的镜头分割模式，拍摄对象信息和看不见类的有限的注释两者的优势，以查询图像中进行分割。我们的方法实现了米欧得分的57.9％和22.6％，仅给出一个注释在PASCAL-5i和COCO-20I一个看不见的类实例，跑赢基准相关的整体。</font>
</div>


<hr>
<div id="paper62"> <b>62. Fingerprint Presentation Attack Detection: A Sensor and Material  Agnostic Approach</b>  <a href="https://arxiv.org/pdf/2004.02941" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title62" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Grosz%2C+S+A" target="_blank" rel="noopener" style="color:#0000EE;">Steven A. Grosz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chugh%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tarang Chugh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+A+K" target="_blank" rel="noopener" style="color:#0000EE;">Anil K. Jain</a><br>
<font size="3">
Abstract: The vulnerability of automated fingerprint recognition systems to presentation attacks (PA), i.e., spoof or altered fingers, has been a growing concern, warranting the development of accurate and efficient presentation attack detection (PAD) methods. However, one major limitation of the existing PAD solutions is their poor generalization to new PA materials and fingerprint sensors, not used in training. In this study, we propose a robust PAD solution with improved cross-material and cross-sensor generalization. Specifically, we build on top of any CNN-based architecture trained for fingerprint spoof detection combined with cross-material spoof generalization using a style transfer network wrapper. We also incorporate adversarial representation learning (ARL) in deep neural networks (DNN) to learn sensor and material invariant representations for PAD. Experimental results on LivDet 2015 and 2017 public domain datasets exhibit the effectiveness of the proposed approach. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：指纹自动识别系统演示攻击（PA），即，恶搞或改变手指的漏洞，已经日益受到关注，warranting的准确，高效演示的攻击检测（PAD）方法的发展。然而，现有的PAD解决方案的一个主要限制是其泛化差新的PA材料和指纹传感器，在训练中不使用。在这项研究中，我们提出了具有改进的横材料和跨传感器的概括鲁棒PAD溶液。具体来说，我们建立在任何基于CNN的架构训练指纹欺骗检测与使用式传送网络封装器横材料欺骗一般化组合的顶部。我们也纳入深层神经网络（DNN）对抗性表示学习（ARL），以了解传感器和材料恒定表征的PAD。在2015年LivDet至2017年公共领域的数据集实验结果显示了该方法的有效性。</font>
</div>


<hr>
<div id="paper63"> <b>63. Efficient Scale Estimation Methods using Lightweight Deep Convolutional  Neural Networks for Visual Tracking</b>  <a href="https://arxiv.org/pdf/2004.02933" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title63" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Marvasti-Zadeh%2C+S+M" target="_blank" rel="noopener" style="color:#0000EE;">Seyed Mojtaba Marvasti-Zadeh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ghanei-Yakhdan%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hossein Ghanei-Yakhdan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kasaei%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shohreh Kasaei</a><br>
<font size="3">
Abstract: In recent years, visual tracking methods that are based on discriminative correlation filters (DCF) have been very promising. However, most of these methods suffer from a lack of robust scale estimation skills. Although a wide range of recent DCF-based methods exploit the features that are extracted from deep convolutional neural networks (CNNs) in their translation model, the scale of the visual target is still estimated by hand-crafted features. Whereas the exploitation of CNNs imposes a high computational burden, this paper exploits pre-trained lightweight CNNs models to propose two efficient scale estimation methods, which not only improve the visual tracking performance but also provide acceptable tracking speeds. The proposed methods are formulated based on either holistic or region representation of convolutional feature maps to efficiently integrate into DCF formulations to learn a robust scale model in the frequency domain. Moreover, against the conventional scale estimation methods with iterative feature extraction of different target regions, the proposed methods exploit proposed one-pass feature extraction processes that significantly improve the computational efficiency. Comprehensive experimental results on the OTB-50, OTB-100, TC-128 and VOT-2018 visual tracking datasets demonstrate that the proposed visual tracking methods outperform the state-of-the-art methods, effectively. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近年来，基于辨别相关滤波器（DCF）的视觉跟踪方法已经非常有前景。然而，这些方法大多苦于缺乏强大的规模估算技能。尽管广泛的最近基于DCF-方法利用的是从深卷积神经网络（细胞神经网络）在它们的翻译模型提取的特征，视觉目标的规模仍然由手工制作的特征估计。而细胞神经网络的开采征收较高的计算负担，本文利用预先训练的轻质细胞神经网络模型，提出了两个有效的规模估算方法，这不仅提高了视觉跟踪性能，而且还提供可接受的跟踪速度。所提出的方法是基于卷积特征的任一或整体区域表示配制映射到有效地集成到DCF制剂学在频域中的健壮比例模型。此外，针对具有不同靶区域的迭代特征提取的常规刻度的估计方法，所提出的方法利用提议显著提高计算效率单程特征提取过程。在OTB-50，OTB-100，TC-128和VOT-2018视觉跟踪数据集的综合实验结果表明，所提出的视觉跟踪方法优于国家的最先进的方法，有效。</font>
</div>


<hr>
<div id="paper64"> <b>64. Beyond Background-Aware Correlation Filters: Adaptive Context Modeling  by Hand-Crafted and Deep RGB Features for Visual Tracking</b>  <a href="https://arxiv.org/pdf/2004.02932" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title64" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Marvasti-Zadeh%2C+S+M" target="_blank" rel="noopener" style="color:#0000EE;">Seyed Mojtaba Marvasti-Zadeh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ghanei-Yakhdan%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hossein Ghanei-Yakhdan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kasaei%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shohreh Kasaei</a><br>
<font size="3">
Abstract: In recent years, the background-aware correlation filters have achie-ved a lot of research interest in the visual target tracking. However, these methods cannot suitably model the target appearance due to the exploitation of hand-crafted features. On the other hand, the recent deep learning-based visual tracking methods have provided a competitive performance along with extensive computations. In this paper, an adaptive background-aware correlation filter-based tracker is proposed that effectively models the target appearance by using either the histogram of oriented gradients (HOG) or convolutional neural network (CNN) feature maps. The proposed method exploits the fast 2D non-maximum suppression (NMS) algorithm and the semantic information comparison to detect challenging situations. When the HOG-based response map is not reliable, or the context region has a low semantic similarity with prior regions, the proposed method constructs the CNN context model to improve the target region estimation. Furthermore, the rejection option allows the proposed method to update the CNN context model only on valid regions. Comprehensive experimental results demonstrate that the proposed adaptive method clearly outperforms the accuracy and robustness of visual target tracking compared to the state-of-the-art methods on the OTB-50, OTB-100, TC-128, UAV-123, and VOT-2015 datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近年来，背景感知相关滤波器具有的政绩，粘弹性阻尼器有很多的研究兴趣在视觉目标跟踪。然而，这些方法不能合适地目标外观模型由于手工制作的特征的开发。在另一方面，近期深基于学习的视觉跟踪方法已经提供了广泛的计算沿着竞争力的性能。在本文中，一个自适应背景感知相关性基于过滤器的跟踪器提出了有效的模型通过使用定向梯度（HOG）或卷积神经网络（CNN）特征映射的直方图的目标外观。所提出的方法利用了快速的2D非最大抑制（NMS）算法和语义信息比较，以检测有挑战性的情况。当基于HOG  - 响应图是不可靠的，或上下文区域具有与现有的区域低的语义相似性，所提出的方法构造CNN上下文模型来提高目标区域估计。此外，拒绝选项允许该方法仅在有效区域更新CNN上下文模型。全面实验结果表明，所提出的自适应方法相比对OTB-50，OTB-100，TC-128，UAV-123，和VOT所述状态的最先进的方法明显优于准确性和视觉目标跟踪的鲁棒性-2015数据集。</font>
</div>


<hr>
<div id="paper65"> <b>65. Empirical Upper Bound, Error Diagnosis and Invariance Analysis of Modern  Object Detectors</b>  <a href="https://arxiv.org/pdf/2004.02877" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title65" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Borji%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ali Borji</a><br>
<font size="3">
Abstract: Object detection remains as one of the most notorious open problems in computer vision. Despite large strides in accuracy in recent years, modern object detectors have started to saturate on popular benchmarks raising the question of how far we can reach with deep learning tools and tricks. Here, by employing 2 state-of-the-art object detection benchmarks, and analyzing more than 15 models over 4 large scale datasets, we I) carefully determine the upper bound in AP, which is 91.6% on VOC (test2007), 78.2% on COCO (val2017), and 58.9% on OpenImages V4 (validation), regardless of the IOU threshold. These numbers are much better than the mAP of the best model (47.9% on VOC, and 46.9% on COCO; IOUs=.5:.05:.95), II) characterize the sources of errors in object detectors, in a novel and intuitive way, and find that classification error (confusion with other classes and misses) explains the largest fraction of errors and weighs more than localization and duplicate errors, and III) analyze the invariance properties of models when surrounding context of an object is removed, when an object is placed in an incongruent background, and when images are blurred or flipped vertically. We find that models generate a lot of boxes on empty regions and that context is more important for detecting small objects than larger ones. Our work taps into the tight relationship between object detection and object recognition and offers insights for building better models. Our code is publicly available at this https URL bound.git. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：目的检测遗体作为计算机视觉领域中最臭名昭著的开放问题之一。尽管在精度巨大的进步，近年来，现代对象检测器已经开始饱和流行的基准提高，我们可以在多大程度上与深度学习工具和技巧达到的问题。这里，通过采用2状态的最先进的物体检测的基准，并通过4个分析超过15个模型大规模数据集，我们I）仔细确定在AP，这是91.6％的VOC（test2007），78.2上界％的COCO（val2017），并在OpenImages V4（验证）58.9％，而不管IOU阈值。这些数字是比最好的模型（上VOC 47.9％，和46.9％上COCO;白条= 0.5：0.05：0.95）的地图好得多，II）表征对象检测器的错误的来源，以一种新颖的而直观的方式，发现分类错误（混乱与其他类和未命中）解释错误的最大部分和重量超过本地化和重复的错误，当一个物体的周围的情况下被删除III）分析模型的不变性，当物体被放置在不一致的背景，并且当图像被模糊或垂直翻转。我们发现，模型产生的空白区域有很多的箱子和这方面是比较大的检测小的物体更重要。我们的工作水龙头到建立更好的模型对象的检测和物体识别，并提供见解之间的紧密关系。我们的代码是公开的，在此HTTPS URL bound.git。</font>
</div>


<hr>
<div id="paper66"> <b>66. U-Net Using Stacked Dilated Convolutions for Medical Image Segmentation</b>  <a href="https://arxiv.org/pdf/2004.03466" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title66" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Wang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuhang Wang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Hu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Szu-Yeu Hu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Cheah%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eugene Cheah</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaohong Wang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Wang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingchao Wang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Chen%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei Chen</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Baikpour%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Masoud Baikpour</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Ozturk%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arinc Ozturk</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Li%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qian Li</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Chou%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shinn-Huey Chou</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Lehman%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Connie Lehman</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Kumar%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Viksit Kumar</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Samir%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anthony Samir</a><br>
<font size="3">
Abstract: This paper proposes a novel U-Net variant using stacked dilated convolutions for medical image segmentation (SDU-Net). SDU-Net adopts the architecture of vanilla U-Net with modifications in the encoder and decoder operations (an operation indicates all the processing for feature maps of the same resolution). Unlike vanilla U-Net which incorporates two standard convolutions in each encoder/decoder operation, SDU-Net uses one standard convolution followed by multiple dilated convolutions and concatenates all dilated convolution outputs as input to the next operation. Experiments showed that SDU-Net outperformed vanilla U-Net, attention U-Net (AttU-Net), and recurrent residual U-Net (R2U-Net) in all four tested segmentation tasks while using parameters around 40% of vanilla U-Net's, 17% of AttU-Net's, and 15% of R2U-Net's. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种使用在医学图像分割（SDU-净）堆叠扩张卷积一种新颖的U形网的变体。 SDU-Net的采用香草U形网与在编码器和解码器的操作的修改的结构（操作指示所有相同的分辨率的特征地图的处理）。不像香草U型网，其包括两个标准卷积中的每个编码器/解码器操作，SDU-Net使用一个标准卷积其后是多个扩张卷积并连接所有扩张型卷积输出作为输入到下一操作。实验表明，SDU-Net的表现优于香草掌中宽带，注意掌中宽带（阿图-NET）和复发残留的U网（R2U-网）在所有四个测试的细分任务，同时利用各地的香草掌中宽带的40％参数，阿图的-Net的17％，而R2U-Net的15％。</font>
</div>


<hr>
<div id="paper67"> <b>67. Learning to Accelerate Decomposition for Multi-Directional 3D Printing</b>  <a href="https://arxiv.org/pdf/2004.03450" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title67" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chenming Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yong-Jin Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C+C+L" target="_blank" rel="noopener" style="color:#0000EE;">Charlie C.L. Wang</a><br>
<font size="3">
Abstract: As a strong complementary of additive manufacturing, multi-directional 3D printing has the capability of decreasing or eliminating the need for support structures. Recent work proposed a beam-guided search algorithm to find an optimized sequence of plane-clipping, which gives volume decomposition of a given 3D model. Different printing directions are employed in different regions so that a model can be fabricated with tremendously less supports (or even no support in many cases). To obtain optimized decomposition, a large beam width needs to be used in the search algorithm, which therefore leads to a very time-consuming computation. In this paper, we propose a learning framework that can accelerate the beam-guided search by using only 1/2 of the original beam width to obtain results with similar quality. Specifically, we train a classifier for each pair of candidate clipping planes based on six newly proposed feature metrics from the results of beam-guided search with large beam width. With the help of these feature metrics, both the current and the sequence-dependent information are captured by the classifier to score candidates of clipping. As a result, we can achieve around 2 times acceleration. We test and demonstrate the performance of our accelerated decomposition on a large dataset of models for 3D printing. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：作为添加剂制造的很强的互补性，多方向三维打印具有降低或消除对支撑结构的需要的能力。最近的工作提出了波束制导搜索算法找到平面裁剪，这给给定的3D模型的体积分解的优化序列。不同的印刷方向在不同的区域中使用，使得模型可以与极大较少支撑件（或甚至在许多情况下不支持）来制造。为了获得优化的分解，在搜索算法，可以使用一个大的光束宽度需要其因此导致一个非常耗时的计算。在本文中，我们提出了一个学习框架，可以只使用原来的光束宽度的1/2，以获得类似质量的结果加速了波束制导搜索。具体来说，我们训练分类器对于每一对候选剪切平面的基础上从与大波束宽度光束引导搜索的结果6个新提出的特征指标。有了这些功能指标的帮助下，当前和顺序相关的信息被分类捕捉到得分剪裁的候选人。因此，我们可以实现2倍左右的加速度。我们测试和演示大型数据集的模型，3D打印，我们加快分解的性能。</font>
</div>


<hr>
<div id="paper68"> <b>68. Deep Open Space Segmentation using Automotive Radar</b>  <a href="https://arxiv.org/pdf/2004.03449" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title68" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Nowruzi%2C+F+E" target="_blank" rel="noopener" style="color:#0000EE;">Farzan Erlik Nowruzi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Kolhatkar%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dhanvin Kolhatkar</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Kapoor%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Prince Kapoor</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Hassanat%2C+F+A" target="_blank" rel="noopener" style="color:#0000EE;">Fahed Al Hassanat</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Heravi%2C+E+J" target="_blank" rel="noopener" style="color:#0000EE;">Elnaz Jahani Heravi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Laganiere%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Robert Laganiere</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Rebut%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Julien Rebut</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Malik%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Waqas Malik</a><br>
<font size="3">
Abstract: In this work, we propose the use of radar with advanced deep segmentation models to identify open space in parking scenarios. A publically available dataset of radar observations called SCORP was collected. Deep models are evaluated with various radar input representations. Our proposed approach achieves low memory usage and real-time processing speeds, and is thus very well suited for embedded deployment. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这项工作中，我们提出了先进的深细分车型使用雷达的泊车场景识别开放空间。雷达观测称为SCORP的可公开获得的数据集进行收集。深模型与各种雷达输入交涉评估。我们建议的做法，实现了低内存占用和实时处理速度，因此非常适合用于嵌入式部署。</font>
</div>


<hr>
<div id="paper69"> <b>69. Harmony-Search and Otsu based System for Coronavirus Disease (COVID-19)  Detection using Lung CT Scan Images</b>  <a href="https://arxiv.org/pdf/2004.03431" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title69" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Rajinikanth%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">V. Rajinikanth</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Dey%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nilanjan Dey</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Raj%2C+A+N+J" target="_blank" rel="noopener" style="color:#0000EE;">Alex Noel Joseph Raj</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Hassanien%2C+A+E" target="_blank" rel="noopener" style="color:#0000EE;">Aboul Ella Hassanien</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Santosh%2C+K+C" target="_blank" rel="noopener" style="color:#0000EE;">K.C. Santosh</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Raja%2C+N+S+M" target="_blank" rel="noopener" style="color:#0000EE;">N. Sri Madhava Raja</a><br>
<font size="3">
Abstract: Pneumonia is one of the foremost lung diseases and untreated pneumonia will lead to serious threats for all age groups. The proposed work aims to extract and evaluate the Coronavirus disease (COVID-19) caused pneumonia infection in lung using CT scans. We propose an image-assisted system to extract COVID-19 infected sections from lung CT scans (coronal view). It includes following steps: (i) Threshold filter to extract the lung region by eliminating possible artifacts; (ii) Image enhancement using Harmony-Search-Optimization and Otsu thresholding; (iii) Image segmentation to extract infected region(s); and (iv) Region-of-interest (ROI) extraction (features) from binary image to compute level of severity. The features that are extracted from ROI are then employed to identify the pixel ratio between the lung and infection sections to identify infection level of severity. The primary objective of the tool is to assist the pulmonologist not only to detect but also to help plan treatment process. As a consequence, for mass screening processing, it will help prevent diagnostic burden. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：肺炎是最重要的肺部疾病和未经处理的肺炎之一将导致对所有年龄组的严重威胁。所提出的工作目标，以提取和评估使用CT扫描造成的冠状病毒病（COVID-19）肺炎感染肺。我们提出了一种图像辅助系统来提取肺CT扫描（冠状图）COVID-19感染的部分。它包括以下步骤：（i）门限滤波器通过消除可能伪影来提取肺区域; （ii）使用和声-搜索的优化和大津的阈值图像增强; （ⅲ）的图像分割来提取感染区域（一个或多个）;和（iv）区域的感兴趣（ROI）萃取（特征）从二值图像至严重程度计算水平。那么，从ROI中提取的特征被用来识别肺和感染部分之间的像素比例，以确定严重性的感染水平。该工具的主要目的是协助胸腔不仅检测还能帮助计划处理工艺。因此，大规模筛选处理，这将有助于预防诊断负担。</font>
</div>


<hr>
<div id="paper70"> <b>70. Automated Smartphone based System for Diagnosis of Diabetic Retinopathy</b>  <a href="https://arxiv.org/pdf/2004.03408" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title70" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Hagos%2C+M+T" target="_blank" rel="noopener" style="color:#0000EE;">Misgina Tsighe Hagos</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Kant%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shri Kant</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Bala%2C+S+A" target="_blank" rel="noopener" style="color:#0000EE;">Surayya Ado Bala</a><br>
<font size="3">
Abstract: Early diagnosis of diabetic retinopathy for treatment of the disease has been failing to reach diabetic people living in rural areas. Shortage of trained ophthalmologists, limited availability of healthcare centers, and expensiveness of diagnostic equipment are among the reasons. Although many deep learning-based automatic diagnosis of diabetic retinopathy techniques have been implemented in the literature, these methods still fail to provide a point-of-care diagnosis. This raises the need for an independent diagnostic of diabetic retinopathy that can be used by a non-expert. Recently the usage of smartphones has been increasing across the world. Automated diagnoses of diabetic retinopathy can be deployed on smartphones in order to provide an instant diagnosis to diabetic people residing in remote areas. In this paper, inception based convolutional neural network and binary decision tree-based ensemble of classifiers have been proposed and implemented to detect and classify diabetic retinopathy. The proposed method was further imported into a smartphone application for mobile-based classification, which provides an offline and automatic system for diagnosis of diabetic retinopathy. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：糖尿病性视网膜病变的治疗本病的早期诊断一直未能达到生活在农村的糖尿病人。训练有素的眼科医生，医疗保健中心的有限，以及诊断设备价格昂贵的不足是原因之一。虽然糖尿病视网膜病变技术的许多深学习型自动诊断已在文献中得到落实，这些方法仍不能提供点的护理诊断。这就提出了一个独立的诊断糖尿病性视网膜病变，可以由非专业使用的需要。最近智能手机的使用已经在世界各地越来越多。糖尿病性视网膜病变的诊断自动化可以以提供即时诊断，居住在偏远地区的糖尿病人部署在智能手机上。在本文中，基于以来卷积神经网络，并已提出并实施了用于检测和分类糖尿病视网膜病变分类的二元决策基于树的合奏。所提出的方法，进一步导入用于基于移动设备的分类智能电话应用，其提供了一种用于糖尿病性视网膜病的诊断的脱机及自动系统。</font>
</div>


<hr>
<div id="paper71"> <b>71. Deep Learning on Chest X-ray Images to Detect and Evaluate Pneumonia  Cases at the Era of COVID-19</b>  <a href="https://arxiv.org/pdf/2004.03399" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title71" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Hammoudi%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Karim Hammoudi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Benhabiles%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Halim Benhabiles</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Melkemi%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mahmoud Melkemi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Dornaika%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fadi Dornaika</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Arganda-Carreras%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ignacio Arganda-Carreras</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Collard%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dominique Collard</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Scherpereel%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arnaud Scherpereel</a><br>
<font size="3">
Abstract: Coronavirus disease 2019 (COVID-19) is an infectious disease with first symptoms similar to the flu. COVID-19 appeared first in China and very quickly spreads to the rest of the world, causing then the 2019-20 coronavirus pandemic. In many cases, this disease causes pneumonia. Since pulmonary infections can be observed through radiography images, this paper investigates deep learning methods for automatically analyzing query chest X-ray images with the hope to bring precision tools to health professionals towards screening the COVID-19 and diagnosing confirmed patients. In this context, training datasets, deep learning architectures and analysis strategies have been experimented from publicly open sets of chest X-ray images. Tailored deep learning models are proposed to detect pneumonia infection cases, notably viral cases. It is assumed that viral pneumonia cases detected during an epidemic COVID-19 context have a high probability to presume COVID-19 infections. Moreover, easy-to-apply health indicators are proposed for estimating infection status and predicting patient status from the detected pneumonia cases. Experimental results show possibilities of training deep learning models over publicly open sets of chest X-ray images towards screening viral pneumonia. Chest X-ray test images of COVID-19 infected patients are successfully diagnosed through detection models retained for their performances. The efficiency of proposed health indicators is highlighted through simulated scenarios of patients presenting infections and health problems by combining real and synthetic health data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：冠状病毒病2019（COVID-19）是一种传染病，与类似流感的首发症状。 COVID-19首次在中国并很快出现蔓延到世界其他地区，造成那么2019-20冠状病毒大流行。在许多情况下，这种疾病会导致肺炎。由于肺部感染可通过X线摄影图像可以观察到，本文探讨深学习，希望把精密工具，以卫生专业人员对筛选COVID-19和诊断确诊病人自动分析查询胸部X射线图像的方法。在此背景下，训练数据集，深度学习架构和分析战略已经从公众开放集胸部X射线图像的实验。提出量身定制的深度学习模型来检测肺炎感染病例，特别是病毒病例。据推测，疫情COVID-19方面有很高的概率假定COVID-19感染过程中检测到病毒性肺炎病例。此外，容易申请卫生指标都提出了从检测肺炎病例估计感染状况及预测患者状态。实验结果表明朝筛选病毒性肺炎培养深度学习模式在公开开集胸部X射线图像的可能性。 COVID-19感染患者胸部X线测试图像诊断成功通过检测模型保留了他们的表演。提出卫生指标的效率是通过患者通过结合实际的和合成的健康数据呈现感染和健康问题的模拟场景突出。</font>
</div>


<hr>
<div id="paper72"> <b>72. Complete CVDL Methodology for Investigating Hydrodynamic Instabilities</b>  <a href="https://arxiv.org/pdf/2004.03374" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title72" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/physics?searchtype=author&query=Harel%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Re'em Harel</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Rusanovsky%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matan Rusanovsky</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Fridman%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yehonatan Fridman</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Shimony%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Assaf Shimony</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Oren%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gal Oren</a><br>
<font size="3">
Abstract: In fluid dynamics, one of the most important research fields is hydrodynamic instabilities and their evolution in different flow regimes. The investigation of said instabilities is concerned with the highly non-linear dynamics. Currently, three main methods are used for understanding of such phenomenon namely analytical models, experiments and simulations - and all of them are primarily investigated and correlated using human expertise. In this work we claim and demonstrate that a major portion of this research effort could and should be analysed using recent breakthrough advancements in the field of Computer Vision with Deep Learning (CVDL, or Deep Computer-Vision). Specifically, we target and evaluate specific state-of-the-art techniques such as Image Retrieval, Template Matching, Parameters Regression and Spatiotemporal Prediction - for the quantitative and qualitative benefits they provide. In order to do so we focus in this research on one of the most representative instabilities, the Rayleigh-Taylor one, simulate its behaviour and create an open-sourced state-of-the-art annotated database (RayleAI). Finally, we use adjusted experimental results and novel physical loss methodologies to validate the correspondence of the predicted results to actual physical reality to prove the models efficiency. The techniques which were developed and proved in this work can be served as essential tools for physicists in the field of hydrodynamics for investigating a variety of physical systems, and also could be used via Transfer Learning to other instabilities research. A part of the techniques can be easily applied on already exist simulation results. All models as well as the data-set that was created for this work, are publicly available at: this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在流体动力学中，最重要的研究领域之一是流体力学不稳定性以及它们在不同的流态变化。说的不稳定性的研究关注的是高度非线性动力学。目前，使用三种主要方法对这类现象的理解，即分析模型，实验和模拟 - 和所有的人主要是调查和使用人的专业相关。在这项工作中，我们要求，并表明这项研究工作的主要部分可以而且应该在计算机视觉与深度学习（CVDL，或深计算机视觉）领域使用最近的突破进展进行分析。具体来说，我们的目标和评价国家的最先进的特殊技术，如图像检索，模板匹配，参数回归和时空预测 - 为他们提供的定量和定性的好处。为了做到这一点，我们在这项研究专注于最有代表性的不稳定性之一，瑞利 - 泰勒之一，模拟其行为，并创建一个开放源代码的国家的最先进的注释数据库（RayleAI）。最后，我们使用调整后的实验结果和新颖的物理损失的方法来验证预测结果与实际的物理现实的对应关系，证明了模型的效率。它被开发并证明了在此工作的技术可作为必不可少的工具，物理学家在流体力学领域调查的各种物理系统，并且还可以通过迁移学习到其他不稳定性研究中使用。的技术的部分可以容易地应用于上已存在的模拟结果。所有型号以及这是对这项工作产生的数据集，都公布于：此HTTPS URL。</font>
</div>


<hr>
<div id="paper73"> <b>73. Convolutional Neural Networks based automated segmentation and labelling  of the lumbar spine X-ray</b>  <a href="https://arxiv.org/pdf/2004.03364" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title73" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Konya%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sandor Konya</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=R%2C+S+N+T" target="_blank" rel="noopener" style="color:#0000EE;">Sai Natarajan T R</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Allouch%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hassan Allouch</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Nahleh%2C+K+A" target="_blank" rel="noopener" style="color:#0000EE;">Kais Abu Nahleh</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Dogheim%2C+O+Y" target="_blank" rel="noopener" style="color:#0000EE;">Omneya Yakout Dogheim</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Boehm%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Heinrich Boehm</a><br>
<font size="3">
Abstract: The aim of this study is to investigate the segmentation accuracies of different segmentation networks trained on 730 manually annotated lateral lumbar spine X-rays. Instance segmentation networks were compared to semantic segmentation networks. The study cohort comprised diseased spines and postoperative images with metallic implants. The average mean accuracy and mean intersection over union (IoU) was up to 3 percent better for the best performing instance segmentation model, the average pixel accuracy and weighted IoU were slightly better for the best performing semantic segmentation model. Moreover, the inferences of the instance segmentation models are easier to implement for further processing pipelines in clinical decision support. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本研究的目的是调查上训练730手动注释腰椎侧位X射线不同的分割网络的分割精度。例如分割网络进行了比较，语义分割网络。该研究组由患病刺和术后影像与金属植入物。平均的平均精确度和平均交叉点上接头（IOU）达更好为表现最佳实例分割模型中，平均像素精度3％和加权IOU略好为最佳执行语义分割模型。此外，例如分割模型的推论更容易实现在临床决策支持进一步的处理管道。</font>
</div>


<hr>
<div id="paper74"> <b>74. Binary Neural Networks: A Survey</b>  <a href="https://arxiv.org/pdf/2004.03333" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title74" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haotong Qin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruihao Gong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xianglong Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiao Bai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingkuan Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sebe%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nicu Sebe</a><br>
<font size="3">
Abstract: The binary neural network, largely saving the storage and computation, serves as a promising technique for deploying deep models on resource-limited devices. However, the binarization inevitably causes severe information loss, and even worse, its discontinuity brings difficulty to the optimization of the deep network. To address these issues, a variety of algorithms have been proposed, and achieved satisfying progress in recent years. In this paper, we present a comprehensive survey of these algorithms, mainly categorized into the native solutions directly conducting binarization, and the optimized ones using techniques like minimizing the quantization error, improving the network loss function, and reducing the gradient error. We also investigate other practical aspects of binary neural networks such as the hardware-friendly design and the training tricks. Then, we give the evaluation and discussions on different tasks, including image classification, object detection and semantic segmentation. Finally, the challenges that may be faced in future research are prospected. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：二元神经网络，在很大程度上节约了存储和计算，作为对资源有限的设备部署深车型有前途的技术。然而，二值化不可避免地导致严重的信息丢失，甚至更糟的是，它的不连续性带来的困难深网络的优化。为了解决这些问题，不同的算法已经被提出，并取得了近年来满足进展。在本文中，我们提出的这些算法的全面调查，主要分为天然溶液直接进行二值化，并利用技术，例如最小化量化误差，提高了网络的损失函数，并减小梯度误差的优化的。我们还调查二元神经网络的其他实际问题，如硬件的人性化设计和训练技巧。然后，我们给不同的任务，包括图像分类，目标检测和语义分割评价和讨论。最后，可能会面临在今后的研究面临的挑战进行了展望。</font>
</div>


<hr>
<div id="paper75"> <b>75. Two-Stage Resampling for Convolutional Neural Network Training in the  Imbalanced Colorectal Cancer Image Classification</b>  <a href="https://arxiv.org/pdf/2004.03332" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title75" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Koziarski%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michał Koziarski</a><br>
<font size="3">
Abstract: Data imbalance remains one of the open challenges in the contemporary machine learning. It is especially prevalent in case of medical data, such as histopathological images. Traditional data-level approaches for dealing with data imbalance are ill-suited for image data: oversampling methods such as SMOTE and its derivatives lead to creation of unrealistic synthetic observations, whereas undersampling reduces the amount of available data, critical for successful training of convolutional neural networks. To alleviate the problems associated with over- and undersampling we propose a novel two-stage resampling methodology, in which we initially use the oversampling techniques in the image space to leverage a large amount of data for training of a convolutional neural network, and afterwards apply undersampling in the feature space to fine-tune the last layers of the network. Experiments conducted on a colorectal cancer image dataset indicate the usefulness of the proposed approach. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：数据的不平衡依然是当代机器学习开放的挑战之一。它在医疗数据，例如组织病理学图像的情况下是特别普遍。传统的数据电平与数据失衡处理是不适合用于图像数据接近：过采样方法，诸如SMOTE和其衍生物导致创建不切实际合成观测，而欠减少了可用的数据量，为的卷积神经成功的培训临界网络。为了缓解与过压和欠我们提出了一个新颖的2级重采样方法，在我们最初使用过采样技术，在图像空间利用大量数据的卷积神经网络训练，事后申请相关的问题欠的功能空间，以微调网络的最后一层。在结直肠癌的图像数据集进行的实验表明该方法的有效性。</font>
</div>


<hr>
<div id="paper76"> <b>76. Teacher-Class Network: A Neural Network Compression Mechanism</b>  <a href="https://arxiv.org/pdf/2004.03281" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title76" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Malik%2C+S+M" target="_blank" rel="noopener" style="color:#0000EE;">Shaiq Munir Malik</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tharani%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohbat Tharani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Taj%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Murtaza Taj</a><br>
<font size="3">
Abstract: To solve the problem of the overwhelming size of Deep Neural Networks (DNN) several compression schemes have been proposed, one of them is teacher-student. Teacher-student tries to transfer knowledge from a complex teacher network to a simple student network. In this paper, we propose a novel method called a teacher-class network consisting of a single teacher and multiple student networks (i.e. class of students). Instead of transferring knowledge to one student only, the proposed method transfers a chunk of knowledge about the entire solution to each student. Our students are not trained for problem-specific logits, they are trained to mimic knowledge (dense representation) learned by the teacher network. Thus unlike the logits-based single student approach, the combined knowledge learned by the class of students can be used to solve other problems as well. These students can be designed to satisfy a given budget, e.g. for comparative purposes we kept the collective parameters of all the students less than or equivalent to that of a single student in the teacher-student approach . These small student networks are trained independently, making it possible to train and deploy models on memory deficient devices as well as on parallel processing systems such as data centers. The proposed teacher-class architecture is evaluated on several benchmark datasets including MNIST, FashionMNIST, IMDB Movie Reviews and CAMVid on multiple tasks including classification, sentiment classification and segmentation. Our approach outperforms the state-of-the-art single student approach in terms of accuracy as well as computational cost and in many cases it achieves an accuracy equivalent to the teacher network while having 10-30 times fewer parameters. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：为解决深层神经网络铺天盖地的大小的问题（DNN）几种压缩方案已经被提出，其中之一是师生。师生试图从复杂的教师网络传输知识，以一个简单的学生网络。在本文中，我们提出了一个所谓的老师级的网络，由一个单一的教师和学生多网络（即班的学生）的新方法。相反，知识转移到只有一个学生的，所提出的方法转移对整个解决方案，每个学生知识的一大块。我们的学生没有受过训练的问题，具体logits，他们被训练老师网了解到模仿知识（密表示）。因此，不同于基于logits一个学生的做法，由班级的学生学到的综合知识可以用来解决其他的问题。这些学生可以被设计成以给定的预算，例如出于比较的目的，我们一直都在学生的集体参数小于或等同于师生办法一个学生的。这些小的学生网络被独立地训练，使得可以在存储器缺陷的设备，以及对并行处理系统，诸如数据中心训练和部署模型。建议的老师级架构的几个基准数据集包括MNIST，FashionMNIST，IMDB电影评论和CAMVid多个任务，包括分类，情感分类和分割评估。我们的方法优于在精度方面，以及计算成本的国家的最先进的一个学生的做法，而且在许多情况下，它同时具有10-30倍的参数达到少的精度相当于老师网络。</font>
</div>


<hr>
<div id="paper77"> <b>77. Autoencoders for Unsupervised Anomaly Segmentation in Brain MR Images: A  Comparative Study</b>  <a href="https://arxiv.org/pdf/2004.03271" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title77" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Baur%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christoph Baur</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Denner%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stefan Denner</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Wiestler%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Benedikt Wiestler</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Albarqouni%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shadi Albarqouni</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Navab%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nassir Navab</a><br>
<font size="3">
Abstract: Deep unsupervised representation learning has recently led to new approaches in the field of Unsupervised Anomaly Detection (UAD) in brain MRI. The main principle behind these works is to learn a model of normal anatomy by learning to compress and recover healthy data. This allows to spot abnormal structures from erroneous recoveries of compressed, potentially anomalous samples. The concept is of great interest to the medical image analysis community as it i) relieves from the need of vast amounts of manually segmented training data---a necessity for and pitfall of current supervised Deep Learning---and ii) theoretically allows to detect arbitrary, even rare pathologies which supervised approaches might fail to find. To date, the experimental design of most works hinders a valid comparison, because i) they are evaluated against different datasets and different pathologies, ii) use different image resolutions and iii) different model architectures with varying complexity. The intent of this work is to establish comparability among recent methods by utilizing a single architecture, a single resolution and the same dataset(s). Besides providing a ranking of the methods, we also try to answer questions like i) how many healthy training subjects are needed to model normality and ii) if the reviewed approaches are also sensitive to domain shift. Further, we identify open challenges and provide suggestions for future community efforts and research directions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深无监督表示学习最近导致无监督异常检测（UAD）脑MRI领域的新途径。这些作品背后的主要原理是通过学习来压缩和恢复健康的资料来了解正常解剖模型。这允许从压缩的，潜在异常样品的错误回收率发现异常的结构。这个概念是非常感兴趣的医学图像分析领域，因为它我）可释放从需要大量人工分段训练数据的---一个必要性和当前的陷阱监督深度学习---和ii）在理论上允许检测任意的，即使是监督的方法可能无法发现罕见的病症。迄今为止，大多数作品的实验设计阻碍了有效的比较，因为我）他们针对不同的数据集，不同的病症进行评估，二）使用复杂程度不同不同的图像分辨率以及iii）不同的模型架构。这样做的目的这项工作是利用一个单一的架构，一个单一的分辨率和相同的数据集（县）最近建立方法之间的可比性。除了提供一个排序的方法，我们也尝试回答这样的问题我）许多健康的训练科目是如何需要模型正常和ii）如审查方法也是域转变敏感。此外，我们确定开放的挑战，并为未来社会的努力和研究方向的建议。</font>
</div>


<hr>
<div id="paper78"> <b>78. Inspector Gadget: A Data Programming-based Labeling System for  Industrial Images</b>  <a href="https://arxiv.org/pdf/2004.03264" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title78" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Heo%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Geon Heo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Roh%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuji Roh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hwang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Seonghyeon Hwang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dayun Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Whang%2C+S+E" target="_blank" rel="noopener" style="color:#0000EE;">Steven Euijong Whang</a><br>
<font size="3">
Abstract: As machine learning for images becomes democratized in the Software 2.0 era, one of the serious bottlenecks is securing enough labeled data for training. This problem is especially critical in a manufacturing setting where smart factories rely on machine learning for product quality control by analyzing industrial images. Such images are typically large and may only need to be partially analyzed where only a small portion is problematic (e.g., identifying defects on a surface). Since manual labeling these images is expensive, weak supervision is an attractive alternative where the idea is to generate weak labels that are not perfect, but can be produced at scale. Data programming is a recent paradigm in this category where it uses human knowledge in the form of labeling functions and combines them into a generative model. Data programming has been successful in applications based on text or structured data and can also be applied to images usually if one can find a way to convert them into structured data. In this work, we expand the horizon of data programming by directly applying it to images without this conversion, which is a common scenario for industrial applications. We propose Inspector Gadget, an image labeling system that combines crowdsourcing, data augmentation, and data programming to produce weak labels at scale for image classification. We perform experiments on real industrial image datasets and show that Inspector Gadget obtains better accuracy than state-of-the-art techniques: Snuba, GOGGLES, and self-learning baselines using convolutional neural networks (CNNs) without pre-training. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着机器学习的图像软件2.0时代变得民主化，严重瓶颈之一是培训确保足够的标签数据。这个问题是在制造环境，让智能工厂通过分析工业图像依靠机器学习对产品质量的控制尤为重要。这样的图像典型地是大的，并且可以只需要部分地进行分析，其中只有一小部分是有问题（例如，识别表面上的缺陷）。由于手工贴标这些图像是昂贵的，监管不力是一个有吸引力的选择，其中的想法是产生微弱的标签，是不完美的，但是可以在规模生产。数据编程是这一类，其中它使用的标签功能形式的人类知识和它们组合成一个生成模型在最近的范例。数据编程已成功在基于文本或结构化数据的应用程序，也可以应用到图像通常如果能找到一种方法，将它们转换成结构化数据。在这项工作中，我们通过它直接应用于图像，而这种转换，这是工业应用的常见场景扩展数据编程的视野。我们建议神探，图像标签系统，结合众包，数据增强，以及数据编程大规模生产薄弱标签图像分类。我们进行真实工业图像数据集，并表明神探获得比国家的最先进的技术，更准确的实验：Snuba，护目镜和自我学习的基线使用卷积神经网络（细胞神经网络），而前培训。</font>
</div>


<hr>
<div id="paper79"> <b>79. Iconify: Converting Photographs into Icons</b>  <a href="https://arxiv.org/pdf/2004.03179" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title79" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Karamatsu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Takuro Karamatsu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Benitez-Garcia%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gibran Benitez-Garcia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yanai%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Keiji Yanai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Uchida%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Seiichi Uchida</a><br>
<font size="3">
Abstract: In this paper, we tackle a challenging domain conversion task between photo and icon images. Although icons often originate from real object images (i.e., photographs), severe abstractions and simplifications are applied to generate icon images by professional graphic designers. Moreover, there is no one-to-one correspondence between the two domains, for this reason we cannot use it as the ground-truth for learning a direct conversion function. Since generative adversarial networks (GAN) can undertake the problem of domain conversion without any correspondence, we test CycleGAN and UNIT to generate icons from objects segmented from photo images. Our experiments with several image datasets prove that CycleGAN learns sufficient abstraction and simplification ability to generate icon-like images. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们将处理照片和图标图像之间的挑战域转换任务。尽管图标常常从真实对象图像（即，照片），严重的抽象和简化起源被施加到生成图标由专业图形设计师图像。此外，还有两个域之间没有一个一一对应，因为这个原因，我们不能把它作为地面真学习的直接转换功能。由于生成对抗网络（GAN）可在没有任何对应承接域转换的问题，我们测试CycleGAN和单元，产生图标从物体从照片图像分割。我们与几个图像数据集实验证明，CycleGAN学会生成图标般的画面足够的抽象和简化的能力。</font>
</div>


<hr>
<div id="paper80"> <b>80. Deep Attentive Generative Adversarial Network for Photo-Realistic Image  De-Quantization</b>  <a href="https://arxiv.org/pdf/2004.03150" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title80" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang Zhang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Hu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Changhui Hu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Lu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaobo Lu</a><br>
<font size="3">
Abstract: Most of current display devices are with eight or higher bit-depth. However, the quality of most multimedia tools cannot achieve this bit-depth standard for the generating images. De-quantization can improve the visual quality of low bit-depth image to display on high bit-depth screen. This paper proposes DAGAN algorithm to perform super-resolution on image intensity resolution, which is orthogonal to the spatial resolution, realizing photo-realistic de-quantization via an end-to-end learning pattern. Until now, this is the first attempt to apply Generative Adversarial Network (GAN) framework for image de-quantization. Specifically, we propose the Dense Residual Self-attention (DenseResAtt) module, which is consisted of dense residual blocks armed with self-attention mechanism, to pay more attention on high-frequency information. Moreover, the series connection of sequential DenseResAtt modules forms deep attentive network with superior discriminative learning ability in image de-quantization, modeling representative feature maps to recover as much useful information as possible. In addition, due to the adversarial learning framework can reliably produce high quality natural images, the specified content loss as well as the adversarial loss are back-propagated to optimize the training of model. Above all, DAGAN is able to generate the photo-realistic high bit-depth image without banding artifacts. Experiment results on several public benchmarks prove that the DAGAN algorithm possesses ability to achieve excellent visual effect and satisfied quantitative performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：目前大多数显示设备都与八个或更高位深度。然而，大多数的多媒体工具的质量无法达到发电的图像，此位深度的标准。去量化可以提高低比特深度的图像的视觉质量，以显示高比特深度屏幕上。本文提出的算法DAGAN执行使图像强度分辨率，这是正交的空间分辨率，经由端部至端学习图案实现照片般逼真的反量化超分辨率。到现在为止，这是应用剖成对抗性网络（GAN）的框架图像去量化的首次尝试。具体来说，我们提出了密集的残余自我关注（DenseResAtt）模块，包括具有自注意机制武装密集的残余块，更注重于高频率的信息。此外，顺序DenseResAtt的串联连接模块的形式在图像具有优异的判别学习能力深周到网络去量化，造型代表特征映射到恢复尽可能多的有用信息。另外，由于对抗学习框架能够可靠地产生高质量的自然图像，以及对抗损耗反向传播来优化模型的训练指定的内容丢失。首先，DAGAN能够在没有带状伪影，生成逼真的高位深度的图像。在几个公共基准测试实验结果证明，该DAGAN算法具有实现出色的视觉效果和满意的定量绩效能力。</font>
</div>


<hr>
<div id="paper81"> <b>81. Plug-and-play ISTA converges with kernel denoisers</b>  <a href="https://arxiv.org/pdf/2004.03145" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title81" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Gavaskar%2C+R+G" target="_blank" rel="noopener" style="color:#0000EE;">Ruturaj G. Gavaskar</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Chaudhury%2C+K+N" target="_blank" rel="noopener" style="color:#0000EE;">Kunal N. Chaudhury</a><br>
<font size="3">
Abstract: Plug-and-play (PnP) method is a recent paradigm for image regularization, where the proximal operator (associated with some given regularizer) in an iterative algorithm is replaced with a powerful denoiser. Algorithmically, this involves repeated inversion (of the forward model) and denoising until convergence. Remarkably, PnP regularization produces promising results for several restoration applications. However, a fundamental question in this regard is the theoretical convergence of the PnP iterations, since the algorithm is not strictly derived from an optimization framework. This question has been investigated in recent works, but there are still many unresolved problems. For example, it is not known if convergence can be guaranteed if we use generic kernel denoisers (e.g. nonlocal means) within the ISTA framework (PnP-ISTA). We prove that, under reasonable assumptions, fixed-point convergence of PnP-ISTA is indeed guaranteed for linear inverse problems such as deblurring, inpainting and superresolution (the assumptions are verifiable for inpainting). We compare our theoretical findings with existing results, validate them numerically, and explain their practical relevance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：插件和播放（PNP）的方法是进行图像正规化，其中近端操作（与一些给出正则相关）的迭代算法替换为功能强大的降噪最近的范例。在算法上，这涉及重复反转（正演模型）和去噪直至收敛。值得注意的是，即插即用正规化产生可喜的成果数恢复应用。然而，在这方面的一个基本问题是即插即用的迭代理论收敛，因为算法不严格从优化框架的。这个问题已经在近期的作品被查处，但仍有许多未解决的问题。例如，它不知道是否能收敛，如果我们的ISTA框架用（PnP-ISTA）内使用通用内核denoisers（例如非局部方式）得到保证。我们证明了，在合理的假设，即插即用-ISTA的定点收敛确实保用线性逆问题，如去模糊，图像修复和超分辨率（假设是可验证为补绘）。我们比较我们的理论成果与现有的结果，数值对其进行验证，并解释他们的实际意义。</font>
</div>


<hr>
<div id="paper82"> <b>82. Generalized Label Enhancement with Sample Correlations</b>  <a href="https://arxiv.org/pdf/2004.03104" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title82" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qinghai Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jihua Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haoyu Tang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinyuan Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhongyu Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huimin Lu</a><br>
<font size="3">
Abstract: Recently, label distribution learning (LDL) has drawn much attention in machine learning, where LDL model is learned from labeled instances. Different from single-label and multi-label annotations, label distributions describe the instance by multiple labels with different intensities and accommodates to more general conditions. As most existing machine learning datasets merely provide logical labels, label distributions are unavailable in many real-world applications. To handle this problem, we propose two novel label enhancement methods, i.e., Label Enhancement with Sample Correlations (LESC) and generalized Label Enhancement with Sample Correlations (gLESC). More specifically, LESC employs a low-rank representation of samples in the feature space, and gLESC leverages a tensor multi-rank minimization to further investigate sample correlations in both the feature space and label space. Benefit from the sample correlation, the proposed method can boost the performance of LE. Extensive experiments on 14 benchmark datasets demonstrate that LESC and gLESC can achieve state-of-the-art results as compared to previous label enhancement baselines. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在机器学习，其中LDL模型是从标记实例了解到近日，标签分发学习（LDL）已经引起广泛关注。从单标签和多标签注释不同，标签分布由多个标签具有不同强度和容纳更一般条件描述该实例。由于大多数现有的机器学习数据集仅仅是提供合理的标签，标签分布在许多现实世界的应用程序不可用。为了解决这个问题，我们提出两种新标签增强方法，即，标签与增强样本相关性（LESC）和通用标签与增强样本相关性（gLESC）。更具体地，采用LESC在特征空间中的样本的低秩表示，并且利用gLESC张量多秩最小化，以进一步调查在特征空间和标签空间二者样本相关性。受益于样本相关系数，该方法可以提高LE的性能。 14个基准数据集大量的实验证明，LESC和gLESC相比以前的标签增强基线可以达到国家的先进成果。</font>
</div>


<hr>
<div id="paper83"> <b>83. COVID-Xpert: An AI Powered Population Screening of COVID-19 Cases Using  Chest Radiography Images</b>  <a href="https://arxiv.org/pdf/2004.03042" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title83" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Li</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhu%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dongxiao Zhu</a><br>
<font size="3">
Abstract: With the increasing demand for millions of COVID-19 tests, Computed Tomography (CT) based test has emerged as a promising alternative to the gold standard RT-PCR test. However, it is primarily provided in emergency department and hospital settings due to the need for expensive equipment and trained radiologists. The accurate, rapid yet inexpensive test that is suitable for population screening of COVID-19 cases at mobile, urgent and primary care settings is urgently needed. Here we design a deep convolutional neural network (CNN) that extracts X-ray Chest Radiography (XCR) imaging features from large scale pneumonia and normal training cases and refine them with a small amount of COVID-19 cases to learn the imaging features that are capable of automatically discriminating COVID-19 cases from pneumonia and/or normal XCR imaging cases. We demonstrate the strong potential of our XCR based population screening approach, COVID-Xpert, for detecting COVID-19 cases through an impressive experimental performance. The trained models and information of the compiled data set are available from this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着数以百万计的COVID-19测试的需求不断增加，计算机断层扫描（CT）的测试已经成为一种很有前途的替代金本位RT-PCR试验。但是，它主要是在急诊室和医院的设置，由于需要昂贵的设备和训练有素的放射科医生提供。迫切需要准确，快速而廉价的测试是适合COVID-19案件人群筛查在移动，紧迫和初级保健设置。在这里，我们设计了一个深刻的卷积神经网络（CNN）提取X射线胸片（XCR）成像，从大型肺炎和正常训练的情况下拥有和使用的COVID-19的情况下少量改进他们学习的成像功能，是能够从肺炎和/或正常XCR成像情况下自动判别COVID-19的情况。我们证明了我们基于XCR人群的筛查方法，COVID-的Xpert的巨大潜力，通过令人印象深刻的性能试验检测COVID-19的情况。训练有素的模型和汇编的数据集的信息都可以从这个HTTPS URL。</font>
</div>


<hr>
<div id="paper84"> <b>84. Dense Steerable Filter CNNs for Exploiting Rotational Symmetry in  Histology Images</b>  <a href="https://arxiv.org/pdf/2004.03037" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title84" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Graham%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simon Graham</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Epstein%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Epstein</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Rajpoot%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nasir Rajpoot</a><br>
<font size="3">
Abstract: Histology images are inherently symmetric under rotation, where each orientation is equally as likely to appear. However, this rotational symmetry is not widely utilised as prior knowledge in modern Convolutional Neural Networks (CNNs), resulting in data hungry models that learn independent features at each orientation. Allowing CNNs to be rotation-equivariant removes the necessity to learn this set of transformations from the data and instead frees up model capacity, allowing more discriminative features to be learned. This reduction in the number of required parameters also reduces the risk of overfitting. In this paper, we propose Dense Steerable Filter CNNs (DSF-CNNs) that use group convolutions with multiple rotated copies of each filter in a densely connected framework. Each filter is defined as a linear combination of steerable basis filters, enabling exact rotation and decreasing the number of trainable parameters compared to standard filters. We also provide the first in-depth comparison of different rotation-equivariant CNNs for histology image analysis and demonstrate the advantage of encoding rotational symmetry into modern architectures. We show that DSF-CNNs achieve state-of-the-art performance, with significantly fewer parameters, when applied to three different tasks in the area of computational pathology: breast tumour classification, colon gland segmentation and multi-tissue nuclear segmentation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：组织学图像下旋转，其中每个方向同样为可能出现本质上是对称的。然而，这种旋转对称性没有被广泛用作现代卷积神经网络（细胞神经网络）的先验知识，从而导致数据在学习每个方向无关的特性饿车型。允许细胞神经网络是旋转等变删除要学习这一套从数据转换的必要性，而是腾出模型能力，让更多的判别特征的教训。这减少所需的参数的数量也减少了过度拟合的风险。在本文中，我们提出一种具有在一密集连接的框架各滤波器的多个循环副本密集方向可调滤波器细胞神经网络（DSF-细胞神经网络）的在使用组卷积。每个滤波器被定义为可转向的基础滤波器的线性组合，从而实现精确旋转和降低与标准过滤器可训练参数的数量。我们还提供不同的旋转等变细胞神经网络的组织学图像分析的第一次深入的比较和论证编码旋转对称到现代建筑的优势。我们发现，DSF-细胞神经网络实现国家的最先进的性能，更少的显著参数，当计算病变区域应用到三个不同的任务：乳腺肿瘤分类，大肠腺分割和多组织核分割。</font>
</div>


<hr>
<div id="paper85"> <b>85. Evolving Normalization-Activation Layers</b>  <a href="https://arxiv.org/pdf/2004.02967" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title85" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hanxiao Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Brock%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrew Brock</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Simonyan%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Karen Simonyan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Le%2C+Q+V" target="_blank" rel="noopener" style="color:#0000EE;">Quoc V. Le</a><br>
<font size="3">
Abstract: Normalization layers and activation functions are critical components in deep neural networks that frequently co-locate with each other. Instead of designing them separately, we unify them into a single computation graph, and evolve its structure starting from low-level primitives. Our layer search algorithm leads to the discovery of EvoNorms, a set of new normalization-activation layers that go beyond existing design patterns. Several of these layers enjoy the property of being independent from the batch statistics. Our experiments show that EvoNorms not only excel on a variety of image classification models including ResNets, MobileNets and EfficientNets, but also transfer well to Mask R-CNN for instance segmentation and BigGAN for image synthesis, outperforming BatchNorm and GroupNorm based layers by a significant margin in many cases. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：归一化的层和激活功能是在深神经网络经常彼此共定位的关键组件。相反，单独设计他们的，我们统一成一个单一的计算图，并发展其结构从低级原开始。我们的层搜索算法导致EvoNorms的发现，一组超越现有的设计模式新归激活层。一些这些层的享受是独立于批次的统计特性。我们的实验表明，EvoNorms不仅擅长对各种图像分类模型，包括ResNets，MobileNets和EfficientNets，也转移阱掩盖R-CNN例如分割和BigGAN图像合成，由显著利润率跑赢BatchNorm和GroupNorm基础层在许多情况下。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>PROCJX
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://procjx.github.io/2020/04/08/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-04-08/" title="【arxiv论文】 Computer Vision and Pattern Recognition 2020-04-08">https://procjx.github.io/2020/04/08/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-04-08/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">

        
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2020/04/08/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-04-08/" rel="next" title="【arxiv论文】 Computation and Language 2020-04-08">
                  <i class="fa fa-chevron-left"></i> 【arxiv论文】 Computation and Language 2020-04-08
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/2020/04/08/%E3%80%90git%20%E4%BD%BF%E7%94%A8%E3%80%91%E5%A6%82%E4%BD%95%E5%9C%A8%E6%9C%AC%E5%9C%B0%E7%AE%A1%E7%90%86%E5%92%8C%E5%88%87%E6%8D%A2%E5%A4%9A%E4%B8%AA-github-%E8%B4%A6%E5%8F%B7/" rel="prev" title="【git 使用】如何在本地管理和切换多个 github 账号">
                  【git 使用】如何在本地管理和切换多个 github 账号 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>

        
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang -->
<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9197824246"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="gitalk-container"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#目录"><span class="nav-number">1.</span> <span class="nav-text">目录</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#摘要"><span class="nav-number">2.</span> <span class="nav-text">摘要</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="PROCJX"
    src="/images/procjx.png">
  <p class="site-author-name" itemprop="name">PROCJX</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">348</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">71</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/procjx" title="GitHub &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;procjx" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:procjx@gmail.com" title="E-Mail &amp;rarr; mailto:procjx@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>


<!--
      
        <script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
        <script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
        <div class="widget-wrap">
            <h3 class="widget-title">标签云</h3>
            <div id="myCanvasContainer" class="widget tagcloud">
                <canvas width="250" height="250" id="resCanvas" style="width=100%">
                    <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AAAI/" rel="tag">AAAI</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ACL/" rel="tag">ACL</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Accepted-Papers/" rel="tag">Accepted Papers</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ArXiv/" rel="tag">ArXiv</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/" rel="tag">BERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS20SI/" rel="tag">CS20SI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS224d/" rel="tag">CS224d</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/" rel="tag">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CVPR/" rel="tag">CVPR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Context/" rel="tag">Context</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cross-Lingual/" rel="tag">Cross Lingual</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dialog-System/" rel="tag">Dialog System</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse/" rel="tag">Discourse</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Ranking/" rel="tag">Discourse Ranking</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Structure/" rel="tag">Discourse Structure</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Document-NMT/" rel="tag">Document NMT</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EMNLP/" rel="tag">EMNLP</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Extractive/" rel="tag">Extractive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICLR/" rel="tag">ICLR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICML/" rel="tag">ICML</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IJCAI/" rel="tag">IJCAI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Inter-Sentence/" rel="tag">Inter-Sentence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Keyphrase-Generation/" rel="tag">Keyphrase Generation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NAACL/" rel="tag">NAACL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIPS/" rel="tag">NIPS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NMT/" rel="tag">NMT</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Relation-Extraction/" rel="tag">Neural Relation Extraction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RST/" rel="tag">RST</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Relation-Constraints/" rel="tag">Relation Constraints</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Summarization/" rel="tag">Summarization</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Translation/" rel="tag">Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Word-Translation/" rel="tag">Word Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/alias/" rel="tag">alias</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/git/" rel="tag">git</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pip/" rel="tag">pip</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/screen/" rel="tag">screen</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/" rel="tag">shell</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tgz/" rel="tag">tgz</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tts/" rel="tag">tts</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1/" rel="tag">冒泡</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/" rel="tag">冒泡排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%99%E4%BD%9C%E5%8A%A9%E6%89%8B/" rel="tag">写作助手</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8E%8B%E7%BC%A9/" rel="tag">压缩</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/" rel="tag">发送邮件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%88%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">合并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%8E%E5%8F%B0/" rel="tag">后台</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/" rel="tag">基数排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F/" rel="tag">希尔排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6/" rel="tag">归并</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">归并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/" rel="tag">快速排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F/" rel="tag">批量</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4/" rel="tag">批量删除</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F/" rel="tag">排序</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5/" rel="tag">插入</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/" rel="tag">插入排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%99%E7%A8%8B/" rel="tag">教程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/" rel="tag">斐波那契数列</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9D%80%E6%AD%BB%E8%BF%9B%E7%A8%8B/" rel="tag">杀死进程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B1%89%E8%AF%BA%E5%A1%94/" rel="tag">汉诺塔</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%A7%A3%E5%8E%8B/" rel="tag">解压</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%B0%B7%E6%AD%8C%E7%BF%BB%E8%AF%91/" rel="tag">谷歌翻译</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BF%AD%E4%BB%A3%E5%9B%9E%E7%BF%BB/" rel="tag">迭代回翻</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9/" rel="tag">选择</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/" rel="tag">选择排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%99%84%E4%BB%B6/" rel="tag">附件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3/" rel="tag">非监督</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%A2%86%E5%9F%9F%E9%80%82%E5%BA%94/" rel="tag">领域适应</a><span class="tag-list-count">1</span></li></ul>
                </canvas>
            </div>
        </div>
        
-->
        
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-hengfu -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9879871597"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-chuizhi -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="1662238719"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-zhengfangxing -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="6699421902"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PROCJX</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.4.2
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>












        
      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  <script src="/js/local-search.js"></script>













  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '2286ab64f5194d9d79ce',
      clientSecret: 'f912492bec2391664b40478f50f2f943376768d6',
      repo: 'procjx.github.io',
      owner: 'procjx',
      admin: ['procjx'],
      id: '192eb981c19a9f018a70e535e062a733',
        language: 'zh-CN',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
