<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/procjx.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/procjxfavicon32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/procjxfavicon16x16.ico">
  <link rel="mask-icon" href="/images/procjx.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.4.2',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

<!-- Google Adsense -->
<!--
<script async src="//pagead2.googlesyndication.com/
pagead/js/adsbygoogle.js"></script>
<script>
(adsbygoogle = window.adsbygoogle || []).push({
google_ad_client: "pub-1179774715076800",
enable_page_level_ads: true
});
</script>
-->

<script data-ad-client="ca-pub-1179774715076800" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>


<meta name="google-site-verification" content="cEiGwg0T8Rj5msmuEcGYZTh5nnf05EhCXy0gp2Ml5BI" />
<meta name="baidu-site-verification" content="noSKHe8MJs" />

  <meta name="description" content="目录  1. Low Resource Neural Machine Translation: A Benchmark for Five African  Languages [PDF] 摘要  2. Evaluating Amharic Machine Translation [PDF] 摘要  3. On the Integration of LinguisticFeatures into S">
<meta property="og:type" content="article">
<meta property="og:title" content="【arxiv论文】 Computation and Language 2020-04-01">
<meta property="og:url" content="https:&#x2F;&#x2F;procjx.github.io&#x2F;2020&#x2F;04&#x2F;01&#x2F;%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-04-01&#x2F;index.html">
<meta property="og:site_name" content="PROCJX&#39;s BLOGS">
<meta property="og:description" content="目录  1. Low Resource Neural Machine Translation: A Benchmark for Five African  Languages [PDF] 摘要  2. Evaluating Amharic Machine Translation [PDF] 摘要  3. On the Integration of LinguisticFeatures into S">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2020-04-01T05:00:05.047Z">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://procjx.github.io/2020/04/01/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-04-01/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>【arxiv论文】 Computation and Language 2020-04-01 | PROCJX's BLOGS</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">PROCJX's BLOGS</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">WITH LOVE OF WORLD</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-fw fa-download"></i>资源</a>

  </li>
        <li class="menu-item menu-item-arxiv">

    <a href="/arxiv/" rel="section"><i class="fa fa-fw fa-file-pdf-o"></i>arxiv论文</a>

  </li>
        <li class="menu-item menu-item-deadline">

    <a href="/deadline/" rel="section"><i class="fa fa-fw fa-calendar"></i>会议截稿</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://procjx.github.io/2020/04/01/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-04-01/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/procjx.png">
      <meta itemprop="name" content="PROCJX">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PROCJX's BLOGS">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          【arxiv论文】 Computation and Language 2020-04-01
        </h2>

        <div class="post-meta">
        
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-04-01 11:30:20 / 修改时间：13:00:05" itemprop="dateCreated datePublished" datetime="2020-04-01T11:30:20+08:00">2020-04-01</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/arxiv/" itemprop="url" rel="index">
                    <span itemprop="name">arxiv</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/arxiv/CL/" itemprop="url" rel="index">
                    <span itemprop="name">CL</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
              <span>26k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
              <span>44 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Low Resource Neural Machine Translation: A Benchmark for Five African  Languages <a href="https://arxiv.org/pdf/2003.14402" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Evaluating Amharic Machine Translation <a href="https://arxiv.org/pdf/2003.14386" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> On the Integration of LinguisticFeatures into Statistical and Neural  Machine Translation <a href="https://arxiv.org/pdf/2003.14324" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Graph Enhanced Representation Learning for News Recommendation <a href="https://arxiv.org/pdf/2003.14292" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Inherent Dependency Displacement Bias of Transition-Based Algorithms <a href="https://arxiv.org/pdf/2003.14282" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Appraisal Theories for Emotion Classification in Text <a href="https://arxiv.org/pdf/2003.14155" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Understanding Cross-Lingual Syntactic Transfer in Multilingual Recurrent  Neural Networks <a href="https://arxiv.org/pdf/2003.14056" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> MULTEXT-East <a href="https://arxiv.org/pdf/2003.14026" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> SPARQA: Skeleton-based Semantic Parsing for Complex Questions over  Knowledge Bases <a href="https://arxiv.org/pdf/2003.13956" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Procedural Reading Comprehension with Attribute-Aware Context Flow <a href="https://arxiv.org/pdf/2003.13878" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> The European Language Technology Landscape in 2020: Language-Centric and  Human-Centric AI for Cross-Cultural Communication in Multilingual Europe <a href="https://arxiv.org/pdf/2003.13833" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Span-based discontinuous constituency parsing: a family of exact  chart-based algorithms with time complexities from O(n^6) down to O(n^3) <a href="https://arxiv.org/pdf/2003.13785" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Semantic-based End-to-End Learning for Typhoon Intensity Prediction <a href="https://arxiv.org/pdf/2003.13779" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Characterizing Speech Adversarial Examples Using Self-Attention U-Net  Enhancement <a href="https://arxiv.org/pdf/2003.13917" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> A Hierarchical Transformer for Unsupervised Parsing <a href="https://arxiv.org/pdf/2003.13841" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Sign Language Transformers: Joint End-to-end Sign Language Recognition  and Translation <a href="https://arxiv.org/pdf/2003.13830" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Amharic Abstractive Text Summarization <a href="https://arxiv.org/pdf/2003.13721" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Low Resource Neural Machine Translation: A Benchmark for Five African  Languages</b>  <a href="https://arxiv.org/pdf/2003.14402" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lakew%2C+S+M" target="_blank" rel="noopener" style="color:#0000EE;">Surafel M. Lakew</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matteo Negri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marco Turchi</a><br>
<font size="3">
Abstract: Recent advents in Neural Machine Translation (NMT) have shown improvements in low-resource language (LRL) translation tasks. In this work, we benchmark NMT between English and five African LRL pairs (Swahili, Amharic, Tigrigna, Oromo, Somali [SATOS]). We collected the available resources on the SATOS languages to evaluate the current state of NMT for LRLs. Our evaluation, comparing a baseline single language pair NMT model against semi-supervised learning, transfer learning, and multilingual modeling, shows significant performance improvements both in the En-LRL and LRL-En directions. In terms of averaged BLEU score, the multilingual approach shows the largest gains, up to +5 points, in six out of ten translation directions. To demonstrate the generalization capability of each model, we also report results on multi-domain test sets. We release the standardized experimental data and the test sets for future works addressing the challenges of NMT in under-resourced settings, in particular for the SATOS languages. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在神经机器翻译（NMT）最近的降临已经表明在资源匮乏的语言（LRL）翻译任务的改进。在这项工作中，我们英语和五个非洲LRL对（斯瓦希里语，阿姆哈拉语，提格雷，奥罗莫，索马里[SATOS]）之间的基准NMT。我们收集的SATOS语言的可用资源，以评估NMT的当前状态LRLS。我们的评估，对半监督学习，迁移学习，和多语种的造型，表演显著的性能提升比较基准单一语言对NMT模型无论是在恩LRL和LRL恩方向。在平均得分BLEU而言，多语种的做法显示了最大的收益，最多+5点，在六个十个翻译方向。为了展示每个模型的泛化能力，我们还报告多域测试的结果集。我们发布的标准化实验数据和试验组未来工作解决NMT在资源贫乏的挑战，特别是对于SATOS语言。</font>
</div>


<hr>
<div id="paper2"> <b>2. Evaluating Amharic Machine Translation</b>  <a href="https://arxiv.org/pdf/2003.14386" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hadgu%2C+A+T" target="_blank" rel="noopener" style="color:#0000EE;">Asmelash Teka Hadgu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Beaudoin%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adam Beaudoin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Aregawi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Abel Aregawi</a><br>
<font size="3">
Abstract: Machine translation (MT) systems are now able to provide very accurate results for high resource language pairs. However, for many low resource languages, MT is still under active research. In this paper, we develop and share a dataset to automatically evaluate the quality of MT systems for Amharic. We compare two commercially available MT systems that support translation of Amharic to and from English to assess the current state of MT for Amharic. The BLEU score results show that the results for Amharic translation are promising but still low. We hope that this dataset will be useful to the research community both in academia and industry as a benchmark to evaluate Amharic MT systems. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：机器翻译（MT）系统现在能够提供高资源语言对非常准确的结果。然而，对于许多低资源语言，MT仍然是活跃的研究下。在本文中，我们开发和共享数据集自动评估阿姆哈拉语MT系统的质量。我们比较两种市售MT系统，其提供英语阿姆哈拉语的翻译支持，以评估MT为阿姆哈拉语的当前状态。该BLEU评分结果表明，阿姆哈拉语翻译的结果是令人鼓舞的，但仍处于低位。我们希望此数据集都将在学术界和工业界是研究社会有用的作为基准来评估阿姆哈拉语的机器翻译系统。</font>
</div>


<hr>
<div id="paper3"> <b>3. On the Integration of LinguisticFeatures into Statistical and Neural  Machine Translation</b>  <a href="https://arxiv.org/pdf/2003.14324" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Vanmassenhove%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eva Vanmassenhove</a><br>
<font size="3">
Abstract: New machine translations (MT) technologies are emerging rapidly and with them, bold claims of achieving human parity such as: (i) the results produced approach "accuracy achieved by average bilingual human translators" (Wu et al., 2017b) or (ii) the "translation quality is at human parity when compared to professional human translators" (Hassan et al., 2018) have seen the light of day (Laubli et al., 2018). Aside from the fact that many of these papers craft their own definition of human parity, these sensational claims are often not supported by a complete analysis of all aspects involved in translation. Establishing the discrepancies between the strengths of statistical approaches to MT and the way humans translate has been the starting point of our research. By looking at MT output and linguistic theory, we were able to identify some remaining issues. The problems range from simple number and gender agreement errors to more complex phenomena such as the correct translation of aspectual values and tenses. Our experiments confirm, along with other studies (Bentivogli et al., 2016), that neural MT has surpassed statistical MT in many aspects. However, some problems remain and others have emerged. We cover a series of problems related to the integration of specific linguistic features into statistical and neural MT, aiming to analyse and provide a solution to some of them. Our work focuses on addressing three main research questions that revolve around the complex relationship between linguistics and MT in general. We identify linguistic information that is lacking in order for automatic translation systems to produce more accurate translations and integrate additional features into the existing pipelines. We identify overgeneralization or 'algorithmic bias' as a potential drawback of neural MT and link it to many of the remaining linguistic issues. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：新的机器翻译（MT）技术正在迅速和他们一起出现，实现人的平等，如大胆的要求：（Wu等，2017b）（i）该结果产生办法“准确度平均双语翻译人员实现了”或（ii）所述“相比，专业翻译时翻译质量是在人的奇偶性”（Hassan等人，2018）所看到的一天中的光（Laubli等人，2018）。除了一个事实，即许多这些论文凭自己的手艺人平等的定义，这些耸人听闻的索赔往往不是由参与翻译各方面进行全面的分析支持。建立的统计方法，以MT的长处和人类的方式之间的差异翻译一直是我们研究的出发点。通过观察MT输出和语言学理论，我们能够确定一些遗留问题。这些问题从简单的数量和性别协议错误，更复杂的现象，如体貌值和时态的正确翻译。我们的实验证实，与其他研究沿（Bentivogli等，2016），即神经MT已经超越统计MT在很多方面。然而，一些问题仍然存在，其他人已经出现。我们涵盖了一系列相关的具体语言特征纳入统计和神经MT的整合问题，旨在分析，并提供解决其中的一些。我们的工作重点是解决这一围绕在普通语言学和MT之间的复杂关系三个主要研究问题。我们确定缺乏，以便自动翻译系统，以产生更准确的翻译和整合的附加功能到现有的管道，语言信息。我们确定以偏概全或“算法偏见”的神经MT的一个潜在的缺点并将其链接到许多剩余的语言问题。</font>
</div>


<hr>
<div id="paper4"> <b>4. Graph Enhanced Representation Learning for News Recommendation</b>  <a href="https://arxiv.org/pdf/2003.14292" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Suyu Ge</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chuhan Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fangzhao Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Qi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yongfeng Huang</a><br>
<font size="3">
Abstract: With the explosion of online news, personalized news recommendation becomes increasingly important for online news platforms to help their users find interesting information. Existing news recommendation methods achieve personalization by building accurate news representations from news content and user representations from their direct interactions with news (e.g., click), while ignoring the high-order relatedness between users and news. Here we propose a news recommendation method which can enhance the representation learning of users and news by modeling their relatedness in a graph setting. In our method, users and news are both viewed as nodes in a bipartite graph constructed from historical user click behaviors. For news representations, a transformer architecture is first exploited to build news semantic representations. Then we combine it with the information from neighbor news in the graph via a graph attention network. For user representations, we not only represent users from their historically clicked news, but also attentively incorporate the representations of their neighbor users in the graph. Improved performances on a large-scale real-world dataset validate the effectiveness of our proposed method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着网络新闻的爆炸，个性化的新闻推荐成为在线新闻平台越来越重要，以帮助他们的用户发现有趣的信息。现有的新闻推荐方法，通过建立从新闻的内容和用户表示准确的消息表示从他们的直接相互作用新闻（例如，单击），而忽略了用户和新闻之间的高阶关联实现个性化。在这里，我们提出了一个新闻推荐方法，它可以通过在图形设置的建模关联增强用户和新闻的代表学习。在我们的方法中，用户和新闻都来自历史用户点击行为构建的二分图看作节点。对于新闻表示，变压器架构首先利用来构建消息的语义表示。然后，我们从邻居消息，在图中通过图表注意网络的信息结合起来。对于用户表示，我们不仅代表用户从他们的历史点击消息，也聚精会神地纳入他们的邻居用户表示在图中。在大规模真实世界的性能改善数据集验证了我们提出的方法的有效性。</font>
</div>


<hr>
<div id="paper5"> <b>5. Inherent Dependency Displacement Bias of Transition-Based Algorithms</b>  <a href="https://arxiv.org/pdf/2003.14282" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Anderson%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mark Anderson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=G%C3%B3mez-Rodr%C3%ADguez%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Carlos Gómez-Rodríguez</a><br>
<font size="3">
Abstract: A wide variety of transition-based algorithms are currently used for dependency parsers. Empirical studies have shown that performance varies across different treebanks in such a way that one algorithm outperforms another on one treebank and the reverse is true for a different treebank. There is often no discernible reason for what causes one algorithm to be more suitable for a certain treebank and less so for another. In this paper we shed some light on this by introducing the concept of an algorithm's inherent dependency displacement distribution. This characterises the bias of the algorithm in terms of dependency displacement, which quantify both distance and direction of syntactic relations. We show that the similarity of an algorithm's inherent distribution to a treebank's displacement distribution is clearly correlated to the algorithm's parsing performance on that treebank, specifically with highly significant and substantial correlations for the predominant sentence lengths in Universal Dependency treebanks. We also obtain results which show a more discrete analysis of dependency displacement does not result in any meaningful correlations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：各种各样的基于过渡的算法目前用于依赖分析器。实证研究表明，表现在不同的树库这样一个算法优于另一对一个树库和正好相反的不同的树库变化。常常有没有明显的理由是什么原因导致一个算法更适合有一定的树库，而较少另一个。在本文中，我们提供一些线索这光通过引入算法固有的依赖位移分布的概念。此表征算法在依赖位移而言，其量化距离和句法关系的方向偏置。我们证明了算法的固有分配到树库的位移分布的相似性明显相关，对树库算法的解析性能，特别是与环球依赖树库主要的句子长度高度显著和实质性的关联。我们还获得结果，其显示依赖位移的多个离散的分析没有产生任何有意义的相关性。</font>
</div>


<hr>
<div id="paper6"> <b>6. Appraisal Theories for Emotion Classification in Text</b>  <a href="https://arxiv.org/pdf/2003.14155" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hofmann%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jan Hofmann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Troiano%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Enrica Troiano</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sassenberg%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kai Sassenberg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Klinger%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roman Klinger</a><br>
<font size="3">
Abstract: Automatic emotion categorization has been predominantly formulated as text classification in which textual units are assigned to an emotion from a predefined inventory, for instance following the fundamental emotion classes proposed by Paul Ekman (fear, joy, anger, disgust, sadness, surprise) or Robert Plutchik (adding trust, anticipation). This approach ignores existing psychological theories to some degree, which provide explanations regarding the perception of events (for instance, that somebody experiences fear when they discover a snake because of the appraisal as being an unpleasant and non-controllable situation), even without having access to explicit reports what an experiencer of an emotion is feeling (for instance expressing this with the words "I am afraid."). Automatic classification approaches therefore need to learn properties of events as latent variables (for instance that the uncertainty and effort associated with discovering the snake leads to fear). With this paper, we propose to make such interpretations of events explicit, following theories of cognitive appraisal of events and show their potential for emotion classification when being encoded in classification models. Our results show that high quality appraisal dimension assignments in event descriptions lead to an improvement in the classification of discrete emotion categories. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自动情感分类已经主要配制成继保罗·埃克曼（恐惧，喜悦，愤怒，厌恶，悲伤，惊讶）提出的基本情感类文本分类中的文本单元被分配到情感，从预定义清单，例如或罗伯特普拉奇克（增加信任，期待）。这种方法忽略了现有的心理学理论在一定程度上，它提供了关于事件的看法（比如，有人经历恐惧时，他们发现，因为鉴定为不愉快的和不可控的局面蛇）的解释，即使没有接入以明确的报告有什么情感的体验者感觉（如的话表达这个“我害怕。”）。因此，自动分类方法需要了解事件潜在变量的特性（例如与发现蛇线索相关的不确定性和精力去担心）。有了这个文件，我们建议把事件这样的解释明确，下列事件的认知评价的理论和展示自己的情感类别潜力分类模型进行编码时。我们的研究结果表明，在事件描述高品质的评价维度的分配导致离散的情感类别进行分类的改善。</font>
</div>


<hr>
<div id="paper7"> <b>7. Understanding Cross-Lingual Syntactic Transfer in Multilingual Recurrent  Neural Networks</b>  <a href="https://arxiv.org/pdf/2003.14056" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Dhar%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Prajit Dhar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bisazza%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arianna Bisazza</a><br>
<font size="3">
Abstract: It is now established that modern neural language models can be successfully trained on multiple languages simultaneously without changes to the underlying architecture, providing an easy way to adapt a variety of NLP models to low-resource languages. But what kind of knowledge is really shared among languages within these models? Does multilingual training mostly lead to an alignment of the lexical representation spaces or does it also enable the sharing of purely grammatical knowledge? In this paper we dissect different forms of cross-lingual transfer and look for its most determining factors, using a variety of models and probing tasks. We find that exposing our language models to a related language does not always increase grammatical knowledge in the target language, and that optimal conditions for lexical-semantic transfer may not be optimal for syntactic transfer. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现在建立现代神经语言模型可以成功地培训了多国语言同时不改变底层架构，提供了一种简单的方法，以适应各种NLP模式，以低资源语言。但是，什么样的知识在这些模型中语言之间共享真的？是否多语种培训主要是导致词汇表示空间的排列或者它也使纯粹的语法知识共享？在本文中，我们仔细研究不同形式的跨语言转移，寻找最确定的因素，使用各种模型和探测任务。我们发现，暴露我们的语言模型相关的语言并不总是增加语法知识在目标语言，以及词汇语义转移在最佳条件可能不是语法转移最佳。</font>
</div>


<hr>
<div id="paper8"> <b>8. MULTEXT-East</b>  <a href="https://arxiv.org/pdf/2003.14026" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Erjavec%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tomaž Erjavec</a><br>
<font size="3">
Abstract: MULTEXT-East language resources, a multilingual dataset for language engineering research, focused on the morphosyntactic level of linguistic description. The MULTEXT-East dataset includes the EAGLES-based morphosyntactic specifications, morphosyntactic lexicons, and an annotated multilingual corpora. The parallel corpus, the novel "1984" by George Orwell, is sentence aligned and contains hand-validated morphosyntactic descriptions and lemmas. The resources are uniformly encoded in XML, using the Text Encoding Initiative Guidelines, TEI P5, and cover 16 languages: Bulgarian, Croatian, Czech, English, Estonian, Hungarian, Macedonian, Persian, Polish, Resian, Romanian, Russian, Serbian, Slovak, Slovene, and Ukrainian. This dataset is extensively documented, and freely available for research purposes. This case study gives a history of the development of the MULTEXT-East resources, presents their encoding and components, discusses related work and gives some conclusions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：MULTEXT东的语言资源，语言工程研究一个多语种的数据集，侧重于语言描述的形态句法水平。该MULTEXT东集包括老鹰基于形态句法规格，形态句法词汇和注释的多语语料库。平行语料库，小说“1984”由乔治奥威尔，是句子对准并包含手验证形态句法描述和引理。这些资源在XML统一编码，使用文本编码倡议准则，TEI P5，并覆盖16种语言：保加利亚语，克罗地亚语，捷克语，英语，爱沙尼亚，匈牙利，马其顿语，波斯语，波兰语，Resian，罗马尼亚语，俄语，塞尔维亚语，斯洛伐克语，斯洛文尼亚，和乌克兰语。此数据集详细介绍，并且免费提供用于研究目的。本案例研究给出了MULTEXT东资源的发展历史，介绍了自己的编码和组件，讨论了相关工作，并给出一些结论。</font>
</div>


<hr>
<div id="paper9"> <b>9. SPARQA: Skeleton-based Semantic Parsing for Complex Questions over  Knowledge Bases</b>  <a href="https://arxiv.org/pdf/2003.13956" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yawei Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lingling Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gong Cheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuzhong Qu</a><br>
<font size="3">
Abstract: Semantic parsing transforms a natural language question into a formal query over a knowledge base. Many existing methods rely on syntactic parsing like dependencies. However, the accuracy of producing such expressive formalisms is not satisfying on long complex questions. In this paper, we propose a novel skeleton grammar to represent the high-level structure of a complex question. This dedicated coarse-grained formalism with a BERT-based parsing algorithm helps to improve the accuracy of the downstream fine-grained semantic parsing. Besides, to align the structure of a question with the structure of a knowledge base, our multi-strategy method combines sentence-level and word-level semantics. Our approach shows promising performance on several datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：语义解析转换自然语言问题到了一个知识库的正式查询。许多现有的方法依赖于像依赖句法分析。然而，生产这种表现形式主义的精度不满足于长而复杂的问题。在本文中，我们提出了一个新颖的骨架语法来表示一个复杂的问题的高层次的结构。与基于BERT的解析算法，这种专用的粗粒度形式主义有助于提高下游细粒度语义分析的准确性。此外，对准一个问题的结构，知识库，我们的多策略方法结合句子层次和单词级语义结构。我们的做法显示了几个数据集有前途的性能。</font>
</div>


<hr>
<div id="paper10"> <b>10. Procedural Reading Comprehension with Attribute-Aware Context Flow</b>  <a href="https://arxiv.org/pdf/2003.13878" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Amini%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aida Amini</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bosselut%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Antoine Bosselut</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mishra%2C+B+D" target="_blank" rel="noopener" style="color:#0000EE;">Bhavana Dalvi Mishra</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yejin Choi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hajishirzi%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hannaneh Hajishirzi</a><br>
<font size="3">
Abstract: Procedural texts often describe processes (e.g., photosynthesis and cooking) that happen over entities (e.g., light, food). In this paper, we introduce an algorithm for procedural reading comprehension by translating the text into a general formalism that represents processes as a sequence of transitions over entity attributes (e.g., location, temperature). Leveraging pre-trained language models, our model obtains entity-aware and attribute-aware representations of the text by joint prediction of entity attributes and their transitions. Our model dynamically obtains contextual encodings of the procedural text exploiting information that is encoded about previous and current states to predict the transition of a certain attribute which can be identified as a span of text or from a pre-defined set of classes. Moreover, our model achieves state of the art results on two procedural reading comprehension datasets, namely ProPara and npn-cooking </font>
<br>
<font size="2" style="line-height:30px;">
摘要：程序文本常常描述发生在实体（例如，光，食品）过程（例如，光合作用和烹饪）。在本文中，我们通过翻译文本成表示流程作为以上实体的属性（例如，位置，温度）转换的序列的一般性介绍形式主义对程序阅读理解的算法。利用预先训练的语言模型，通过实体的联合预测的文字我们的模型取得实体感知和属性感知表示属性及其过渡。我们的模型动态获取的被编码有关以前和当前状态来预测特定属性，其可以被识别为文本的跨度或从预先定义的一组类的过渡的程序文本利用信息上下文编码。此外，我们的模型实现了两个程序阅读理解的数据集，即ProPara和NPN烹饪艺术成果状态</font>
</div>


<hr>
<div id="paper11"> <b>11. The European Language Technology Landscape in 2020: Language-Centric and  Human-Centric AI for Cross-Cultural Communication in Multilingual Europe</b>  <a href="https://arxiv.org/pdf/2003.13833" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Rehm%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Georg Rehm</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marheinecke%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Katrin Marheinecke</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hegele%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stefanie Hegele</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Piperidis%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stelios Piperidis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bontcheva%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kalina Bontcheva</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Haji%C4%8D%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jan Hajič</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Choukri%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Khalid Choukri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vasi%C4%BCjevs%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrejs Vasiļjevs</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Backfried%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gerhard Backfried</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Prinz%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christoph Prinz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=P%C3%A9rez%2C+J+M+G" target="_blank" rel="noopener" style="color:#0000EE;">José Manuel Gómez Pérez</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Meertens%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Luc Meertens</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lukowicz%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Paul Lukowicz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=van+Genabith%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Josef van Genabith</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=L%C3%B6sch%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrea Lösch</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Slusallek%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Philipp Slusallek</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Irgens%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Morten Irgens</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gatellier%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Patrick Gatellier</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=K%C3%B6hler%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Joachim Köhler</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bars%2C+L+L" target="_blank" rel="noopener" style="color:#0000EE;">Laure Le Bars</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Anastasiou%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dimitra Anastasiou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Auksori%C5%ABt%C4%97%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Albina Auksoriūtė</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bel%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Núria Bel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Branco%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">António Branco</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Budin%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gerhard Budin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Daelemans%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Walter Daelemans</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=De+Smedt%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Koenraad De Smedt</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Garab%C3%ADk%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Radovan Garabík</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gavriilidou%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maria Gavriilidou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gromann%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dagmar Gromann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Koeva%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Svetla Koeva</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Krek%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simon Krek</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Krstev%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cvetana Krstev</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lind%C3%A9n%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Krister Lindén</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Magnini%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bernardo Magnini</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Odijk%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jan Odijk</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ogrodniczuk%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maciej Ogrodniczuk</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=R%C3%B6gnvaldsson%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eiríkur Rögnvaldsson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rosner%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mike Rosner</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pedersen%2C+B+S" target="_blank" rel="noopener" style="color:#0000EE;">Bolette Sandford Pedersen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Skadi%C5%86a%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Inguna Skadiņa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tadi%C4%87%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marko Tadić</a><br>
<font size="3">
Abstract: Multilingualism is a cultural cornerstone of Europe and firmly anchored in the European treaties including full language equality. However, language barriers impacting business, cross-lingual and cross-cultural communication are still omnipresent. Language Technologies (LTs) are a powerful means to break down these barriers. While the last decade has seen various initiatives that created a multitude of approaches and technologies tailored to Europe's specific needs, there is still an immense level of fragmentation. At the same time, AI has become an increasingly important concept in the European Information and Communication Technology area. For a few years now, AI, including many opportunities, synergies but also misconceptions, has been overshadowing every other topic. We present an overview of the European LT landscape, describing funding programmes, activities, actions and challenges in the different countries with regard to LT, including the current state of play in industry and the LT market. We present a brief overview of the main LT-related activities on the EU level in the last ten years and develop strategic guidance with regard to four key dimensions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：语文是欧洲的文化基石，欧洲条约，包括完整的语言平等牢牢地固定。然而，影响业务语言障碍，跨语言，跨文化的沟通仍然无所不在。语言技术（LTS）是一个强有力的手段来打破这些障碍。虽然过去十年里，创建的针对欧洲的特定需求的方法和技术的多种各项举措，仍然有碎裂的巨大的水平。与此同时，AI已经成为欧洲信息和通信技术领域一个日益重要的概念。有几年，现在，AI，包括很多的机会，协同效应也误解，已经盖过所有其他的话题。我们目前的欧洲LT景观的概况，介绍资助计划，活动，行动和挑战，在不同的国家对于LT，包括对行业发挥当前状态和LT市场。我们在过去十年呈现在欧盟层面的主要LT-相关活动的简要概述，发展战略指导方面的四个关键维度。</font>
</div>


<hr>
<div id="paper12"> <b>12. Span-based discontinuous constituency parsing: a family of exact  chart-based algorithms with time complexities from O(n^6) down to O(n^3)</b>  <a href="https://arxiv.org/pdf/2003.13785" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Corro%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Caio Corro</a><br>
<font size="3">
Abstract: We introduce a novel chart-based algorithm for span-based parsing of discontinuous constituency trees of block degree two, including ill-nested structures. In particular, we show that we can build variants of our parser with smaller search spaces and time complexities ranging from $\mathcal O(n^6)$ down to $\mathcal O(n^3)$. The cubic time variant covers 98\% of constituents observed in linguistic treebanks while having the same complexity as continuous constituency parsers. We evaluate our approach on German and English treebanks (Negra, Tiger and Discontinuous PTB) and report state-of-the-art results in the fully supervised setting. We also experiment with pre-trained word embeddings and \bert{}-based neural networks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们介绍一种新颖的基于图表的算法块度为2的不连续的选区树木，包括不良嵌套结构的基于整体范围的解析。特别是，我们表明，我们可以建立我们的解析器的变种较小的搜索空间和时间的复杂性，从$ \ mathcal为O（n ^ 6）$下降到$ \ mathcal为O（n ^ 3）$。立方时变盖98 \在语言树库观察到同时具有相同的复杂性，因为连续选区解析器成分％。我们评估我们的德语和英语树库（内格拉，老虎和不连续PTB），并在充分监督的设置状态的最先进的报告结果的方法。我们还尝试使用预训练字的嵌入和\ BERT {}  - 基于神经网络。</font>
</div>


<hr>
<div id="paper13"> <b>13. Semantic-based End-to-End Learning for Typhoon Intensity Prediction</b>  <a href="https://arxiv.org/pdf/2003.13779" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zahera%2C+H+M" target="_blank" rel="noopener" style="color:#0000EE;">Hamada M. Zahera</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sherif%2C+M+A" target="_blank" rel="noopener" style="color:#0000EE;">Mohamed Ahmed Sherif</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ngonga%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Axel Ngonga</a><br>
<font size="3">
Abstract: Disaster prediction is one of the most critical tasks towards disaster surveillance and preparedness. Existing technologies employ different machine learning approaches to predict incoming disasters from historical environmental data. However, for short-term disasters (e.g., earthquakes), historical data alone has a limited prediction capability. Therefore, additional sources of warnings are required for accurate prediction. We consider social media as a supplementary source of knowledge in addition to historical environmental data. However, social media posts (e.g., tweets) is very informal and contains only limited content. To alleviate these limitations, we propose the combination of semantically-enriched word embedding models to represent entities in tweets with their semantic representations computed with the traditionalword2vec. Moreover, we study how the correlation between social media posts and typhoons magnitudes (also called intensities)-in terms of volume and sentiments of tweets-. Based on these insights, we propose an end-to-end based framework that learns from disaster-related tweets and environmental data to improve typhoon intensity prediction. This paper is an extension of our work originally published in K-CAP 2019 [32]. We extended this paper by building our framework with state-of-the-art deep neural models, up-dated our dataset with new typhoons and their tweets to-date and benchmark our approach against recent baselines in disaster prediction. Our experimental results show that our approach outperforms the accuracy of the state-of-the-art baselines in terms of F1-score with (CNN by12.1%and BiLSTM by3.1%) improvement compared with last experiments </font>
<br>
<font size="2" style="line-height:30px;">
摘要：灾难预测是对灾害的监测和防范最重要的任务之一。现有的技术使用不同的机器学习方法来预测从历史环境数据传入灾害。然而，对于短期灾害（例如，地震），历史数据单独具有有限的预测能力。因此，需要用于精确预测警告其他来源。我们认为社交媒体知识的除了历史环境数据的补充来源。然而，社交媒体帖子（如微博）是很正规，并且只包含有限的内容。为了减轻这些限制，我们提出了语义丰富的字嵌入模型的组合来表示鸣叫实体与他们同traditionalword2vec计算语义表示。此外，我们研究如何社交媒体帖子和台风量级（也称为强度）-in体积方面和tweets-的情绪之间的关系。根据这些分析，我们提出了一个终端到终端的基础框架，从灾害相关的微博和环境数据获悉，以改善台风强度预报。本文是我们工作的延伸最初发表于K-CAP 2019 [32]。我们通过建立我们的框架与国家的最先进的深层神经模型扩展本文，上月我们新的台风和他们的鸣叫最新和基准我们对在灾害预测近期基线的方法数据集。我们的实验结果显示，与去年相比，实验我们的方法优于国家的最先进的基线的准确性，F1-得分方面与（CNN by12.1和BiLSTM by3.1％％）的改善</font>
</div>


<hr>
<div id="paper14"> <b>14. Characterizing Speech Adversarial Examples Using Self-Attention U-Net  Enhancement</b>  <a href="https://arxiv.org/pdf/2003.13917" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Yang%2C+C+H" target="_blank" rel="noopener" style="color:#0000EE;">Chao-Han Huck Yang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Qi%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Qi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Chen%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pin-Yu Chen</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Ma%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoli Ma</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Lee%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chin-Hui Lee</a><br>
<font size="3">
Abstract: Recent studies have highlighted adversarial examples as ubiquitous threats to the deep neural network (DNN) based speech recognition systems. In this work, we present a U-Net based attention model, U-Net$_{At}$, to enhance adversarial speech signals. Specifically, we evaluate the model performance by interpretable speech recognition metrics and discuss the model performance by the augmented adversarial training. Our experiments show that our proposed U-Net$_{At}$ improves the perceptual evaluation of speech quality (PESQ) from 1.13 to 2.78, speech transmission index (STI) from 0.65 to 0.75, short-term objective intelligibility (STOI) from 0.83 to 0.96 on the task of speech enhancement with adversarial speech examples. We conduct experiments on the automatic speech recognition (ASR) task with adversarial audio attacks. We find that (i) temporal features learned by the attention network are capable of enhancing the robustness of DNN based ASR models; (ii) the generalization power of DNN based ASR model could be enhanced by applying adversarial training with an additive adversarial data augmentation. The ASR metric on word-error-rates (WERs) shows that there is an absolute 2.22 $\%$ decrease under gradient-based perturbation, and an absolute 2.03 $\%$ decrease, under evolutionary-optimized perturbation, which suggests that our enhancement models with adversarial training can further secure a resilient ASR system. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近的研究强调了对抗的例子，以基于语音识别系统的深层神经网络（DNN）无处不在的威胁。在这项工作中，我们提出了一个基于掌中注意模型，U-NET $ _ {}在$，以增强对抗语音信号。具体而言，我们评估的解释语音识别度量模型的性能，并讨论通过增强对抗训练模型的性能。我们的实验表明，我们提出的掌中$ _ {在} $从提高0.65语音质量（PESQ）从1.13到2.78，语音传输指数（STI）的感知评估至0.75，短期目标清晰度（Stoi旅馆） 0.83〜0.96语音增强与对抗性言论例子的任务。我们与敌对音频攻击的自动语音识别（ASR）任务进行实验。我们发现受关注网络了解到，（我）的时间特征能够增强基于DNN ASR模型的鲁棒性; （ii）基于DNN ASR模型的泛化功率可以通过与添加剂对抗性数据扩张施加对抗训练来增强。在字错误率（WERS）表示ASR指标有下基于梯度的扰动绝对2.22 $ \％$减少，绝对2.03 $ \％$减少，在进化优化的扰动，这表明我们的增强型号，对抗性训练可以进一步保证弹性ASR系统。</font>
</div>


<hr>
<div id="paper15"> <b>15. A Hierarchical Transformer for Unsupervised Parsing</b>  <a href="https://arxiv.org/pdf/2003.13841" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Thillaisundaram%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ashok Thillaisundaram</a><br>
<font size="3">
Abstract: The underlying structure of natural language is hierarchical; words combine into phrases, which in turn form clauses. An awareness of this hierarchical structure can aid machine learning models in performing many linguistic tasks. However, most such models just process text sequentially and there is no bias towards learning hierarchical structure encoded into their architecture. In this paper, we extend the recent transformer model (Vaswani et al., 2017) by enabling it to learn hierarchical representations. To achieve this, we adapt the ordering mechanism introduced in Shen et al., 2018, to the self-attention module of the transformer architecture. We train our new model on language modelling and then apply it to the task of unsupervised parsing. We achieve reasonable results on the freely available subset of the WSJ10 dataset with an F1-score of about 50%. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自然语言的底层结构是分层次的;单词组合成短语，这反过来格式条款。这种分层结构的意识，可以在执行多任务的语言帮助机器学习模型。然而，大多数这样的模型只是过程的文字顺序并没有对学习编码到他们的建筑层次结构没有偏差。在本文中，我们将使其能够学习层次交涉扩展最近变压器模型（瓦斯瓦尼等，2017）。为了实现这一目标，我们适应沉等人，2018推出的排序机制，变压器架构的自我关注的模块。我们培训我们的语言建模新模型，然后把它应用到无人监督的解析任务。我们实现上约50％的F1-得分WSJ10数据集的免费的子集合理的结果。</font>
</div>


<hr>
<div id="paper16"> <b>16. Sign Language Transformers: Joint End-to-end Sign Language Recognition  and Translation</b>  <a href="https://arxiv.org/pdf/2003.13830" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Camgoz%2C+N+C" target="_blank" rel="noopener" style="color:#0000EE;">Necati Cihan Camgoz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Koller%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Oscar Koller</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hadfield%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simon Hadfield</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bowden%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Richard Bowden</a><br>
<font size="3">
Abstract: Prior work on Sign Language Translation has shown that having a mid-level sign gloss representation (effectively recognizing the individual signs) improves the translation performance drastically. In fact, the current state-of-the-art in translation requires gloss level tokenization in order to work. We introduce a novel transformer based architecture that jointly learns Continuous Sign Language Recognition and Translation while being trainable in an end-to-end manner. This is achieved by using a Connectionist Temporal Classification (CTC) loss to bind the recognition and translation problems into a single unified architecture. This joint approach does not require any ground-truth timing information, simultaneously solving two co-dependant sequence-to-sequence learning problems and leads to significant performance gains. We evaluate the recognition and translation performances of our approaches on the challenging RWTH-PHOENIX-Weather-2014T (PHOENIX14T) dataset. We report state-of-the-art sign language recognition and translation results achieved by our Sign Language Transformers. Our translation networks outperform both sign video to spoken language and gloss to spoken language translation models, in some cases more than doubling the performance (9.58 vs. 21.80 BLEU-4 Score). We also share new baseline translation results using transformer networks for several other text-to-text sign language translation tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在手语翻译以前的工作表明，具有中等水平的标志光泽表示（有效识别个人标志）大大提高了翻译性能。事实上，当前国家的最先进的翻译要求，以工作的光泽度符号化。我们引入新的基于变压器的架构，共同学习连续手语识别和翻译，同时在终端到终端的方式接受训练的。这是通过使用一个联结颞分类（CTC）损失以结合识别和翻译的问题到一个统一架构实现。这种联合方式不需要任何地面实况时序信息，同时解决两个共同的序列依赖性对序列学习的问题，并导致显著的性能提升。我们评估我们对挑战RWTH-PHOENIX-天气-2014T（PHOENIX14T）数据集的方法的识别和翻译的表演。我们提出我们的手语变压器实现国家的最先进的手语识别和翻译结果。我们的翻译网络超越这两个标牌视频口语和光泽口语翻译模式，在某些情况下，多表现为（9.58与21.80 BLEU-4分数）翻一番。我们还使用共享网络变压器其他几个文本到文本的手语翻译工作的新基准翻译结果。</font>
</div>


<hr>
<div id="paper17"> <b>17. Amharic Abstractive Text Summarization</b>  <a href="https://arxiv.org/pdf/2003.13721" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zaki%2C+A+M" target="_blank" rel="noopener" style="color:#0000EE;">Amr M. Zaki</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Khalil%2C+M+I" target="_blank" rel="noopener" style="color:#0000EE;">Mahmoud I. Khalil</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Abbas%2C+H+M" target="_blank" rel="noopener" style="color:#0000EE;">Hazem M. Abbas</a><br>
<font size="3">
Abstract: Text Summarization is the task of condensing long text into just a handful of sentences. Many approaches have been proposed for this task, some of the very first were building statistical models (Extractive Methods) capable of selecting important words and copying them to the output, however these models lacked the ability to paraphrase sentences, as they simply select important words without actually understanding their contexts nor understanding their meaning, here comes the use of Deep Learning based architectures (Abstractive Methods), which effectively tries to understand the meaning of sentences to build meaningful summaries. In this work we discuss one of these new novel approaches which combines curriculum learning with Deep Learning, this model is called Scheduled Sampling. We apply this work to one of the most widely spoken African languages which is the Amharic Language, as we try to enrich the African NLP community with top-notch Deep Learning architectures. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自动文摘是冷凝长文成只是一个句子的几个任务。许多方法已经被提出了这一任务，一些最先被建立统计模型（提取方法）能够选择重要的词，并将它们复制到输出，但是这些模型的缺乏能力套用的句子，因为他们只需选择重要的词而无需实际了解他们的背景，也不了解他们的意思，这里来使用基于深度学习架构（写意方法），从而有效地试图理解句子的意思来构建有意义的总结。在这项工作中，我们讨论相结合的课程学习与深度学习这些新的新的方法之一，这种模式被称为计划采样。我们这项工作适用于最广泛使用的非洲语言是阿姆哈拉语语言之一，当我们试图以丰富顶尖的深度学习架构非洲NLP社区。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>PROCJX
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://procjx.github.io/2020/04/01/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-04-01/" title="【arxiv论文】 Computation and Language 2020-04-01">https://procjx.github.io/2020/04/01/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-04-01/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">

        
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2020/03/31/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-03-31/" rel="next" title="【arxiv论文】 Computer Vision and Pattern Recognition 2020-03-31">
                  <i class="fa fa-chevron-left"></i> 【arxiv论文】 Computer Vision and Pattern Recognition 2020-03-31
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/2020/04/01/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-04-01/" rel="prev" title="【arxiv论文】 Computer Vision and Pattern Recognition 2020-04-01">
                  【arxiv论文】 Computer Vision and Pattern Recognition 2020-04-01 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>

        
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang -->
<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9197824246"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="gitalk-container"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#目录"><span class="nav-number">1.</span> <span class="nav-text">目录</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#摘要"><span class="nav-number">2.</span> <span class="nav-text">摘要</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="PROCJX"
    src="/images/procjx.png">
  <p class="site-author-name" itemprop="name">PROCJX</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">379</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">71</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/procjx" title="GitHub &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;procjx" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:procjx@gmail.com" title="E-Mail &amp;rarr; mailto:procjx@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>


<!--
      
        <script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
        <script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
        <div class="widget-wrap">
            <h3 class="widget-title">标签云</h3>
            <div id="myCanvasContainer" class="widget tagcloud">
                <canvas width="250" height="250" id="resCanvas" style="width=100%">
                    <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AAAI/" rel="tag">AAAI</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ACL/" rel="tag">ACL</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Accepted-Papers/" rel="tag">Accepted Papers</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ArXiv/" rel="tag">ArXiv</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/" rel="tag">BERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS20SI/" rel="tag">CS20SI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS224d/" rel="tag">CS224d</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/" rel="tag">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CVPR/" rel="tag">CVPR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Context/" rel="tag">Context</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cross-Lingual/" rel="tag">Cross Lingual</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dialog-System/" rel="tag">Dialog System</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse/" rel="tag">Discourse</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Ranking/" rel="tag">Discourse Ranking</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Structure/" rel="tag">Discourse Structure</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Document-NMT/" rel="tag">Document NMT</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EMNLP/" rel="tag">EMNLP</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Extractive/" rel="tag">Extractive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICLR/" rel="tag">ICLR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICML/" rel="tag">ICML</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IJCAI/" rel="tag">IJCAI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Inter-Sentence/" rel="tag">Inter-Sentence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Keyphrase-Generation/" rel="tag">Keyphrase Generation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NAACL/" rel="tag">NAACL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIPS/" rel="tag">NIPS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NMT/" rel="tag">NMT</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Relation-Extraction/" rel="tag">Neural Relation Extraction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RST/" rel="tag">RST</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Relation-Constraints/" rel="tag">Relation Constraints</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Summarization/" rel="tag">Summarization</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Translation/" rel="tag">Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Word-Translation/" rel="tag">Word Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/alias/" rel="tag">alias</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/git/" rel="tag">git</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pip/" rel="tag">pip</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/screen/" rel="tag">screen</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/" rel="tag">shell</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tgz/" rel="tag">tgz</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tts/" rel="tag">tts</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1/" rel="tag">冒泡</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/" rel="tag">冒泡排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%99%E4%BD%9C%E5%8A%A9%E6%89%8B/" rel="tag">写作助手</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8E%8B%E7%BC%A9/" rel="tag">压缩</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/" rel="tag">发送邮件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%88%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">合并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%8E%E5%8F%B0/" rel="tag">后台</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/" rel="tag">基数排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F/" rel="tag">希尔排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6/" rel="tag">归并</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">归并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/" rel="tag">快速排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F/" rel="tag">批量</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4/" rel="tag">批量删除</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F/" rel="tag">排序</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5/" rel="tag">插入</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/" rel="tag">插入排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%99%E7%A8%8B/" rel="tag">教程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/" rel="tag">斐波那契数列</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9D%80%E6%AD%BB%E8%BF%9B%E7%A8%8B/" rel="tag">杀死进程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B1%89%E8%AF%BA%E5%A1%94/" rel="tag">汉诺塔</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%A7%A3%E5%8E%8B/" rel="tag">解压</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%B0%B7%E6%AD%8C%E7%BF%BB%E8%AF%91/" rel="tag">谷歌翻译</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BF%AD%E4%BB%A3%E5%9B%9E%E7%BF%BB/" rel="tag">迭代回翻</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9/" rel="tag">选择</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/" rel="tag">选择排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%99%84%E4%BB%B6/" rel="tag">附件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3/" rel="tag">非监督</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%A2%86%E5%9F%9F%E9%80%82%E5%BA%94/" rel="tag">领域适应</a><span class="tag-list-count">1</span></li></ul>
                </canvas>
            </div>
        </div>
        
-->
        
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-hengfu -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9879871597"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-chuizhi -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="1662238719"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-zhengfangxing -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="6699421902"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PROCJX</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.4.2
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>












        
      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  <script src="/js/local-search.js"></script>













  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '2286ab64f5194d9d79ce',
      clientSecret: 'f912492bec2391664b40478f50f2f943376768d6',
      repo: 'procjx.github.io',
      owner: 'procjx',
      admin: ['procjx'],
      id: 'f65374f73998c72c2fe8a8460b008b9c',
        language: 'zh-CN',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
