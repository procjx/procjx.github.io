<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/procjx.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/procjxfavicon32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/procjxfavicon16x16.ico">
  <link rel="mask-icon" href="/images/procjx.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.4.2',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

<!-- Google Adsense -->
<!--
<script async src="//pagead2.googlesyndication.com/
pagead/js/adsbygoogle.js"></script>
<script>
(adsbygoogle = window.adsbygoogle || []).push({
google_ad_client: "pub-1179774715076800",
enable_page_level_ads: true
});
</script>
-->

<script data-ad-client="ca-pub-1179774715076800" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>


<meta name="google-site-verification" content="cEiGwg0T8Rj5msmuEcGYZTh5nnf05EhCXy0gp2Ml5BI" />
<meta name="baidu-site-verification" content="noSKHe8MJs" />

  <meta name="description" content="目录  1. Upper, Middle and Lower Region Learning for Facial Action Unit Detection [PDF] 摘要  2. Deep Convolutional Neural Networks with Spatial Regularization, Volume  and Star-shape Priori for Image Seg">
<meta property="og:type" content="article">
<meta property="og:title" content="【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-11">
<meta property="og:url" content="https:&#x2F;&#x2F;procjx.github.io&#x2F;2020&#x2F;02&#x2F;11&#x2F;%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-11&#x2F;index.html">
<meta property="og:site_name" content="PROCJX&#39;s BLOGS">
<meta property="og:description" content="目录  1. Upper, Middle and Lower Region Learning for Facial Action Unit Detection [PDF] 摘要  2. Deep Convolutional Neural Networks with Spatial Regularization, Volume  and Star-shape Priori for Image Seg">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2020-03-04T17:06:04.893Z">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://procjx.github.io/2020/02/11/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-11/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-11 | PROCJX's BLOGS</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">PROCJX's BLOGS</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">WITH LOVE OF WORLD</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-fw fa-download"></i>资源</a>

  </li>
        <li class="menu-item menu-item-arxiv">

    <a href="/arxiv/" rel="section"><i class="fa fa-fw fa-file-pdf-o"></i>arxiv论文</a>

  </li>
        <li class="menu-item menu-item-deadline">

    <a href="/deadline/" rel="section"><i class="fa fa-fw fa-calendar"></i>会议截稿</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://procjx.github.io/2020/02/11/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/procjx.png">
      <meta itemprop="name" content="PROCJX">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PROCJX's BLOGS">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-11
        </h2>

        <div class="post-meta">
        
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-02-11 18:08:26" itemprop="dateCreated datePublished" datetime="2020-02-11T18:08:26+08:00">2020-02-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-05 01:06:04" itemprop="dateModified" datetime="2020-03-05T01:06:04+08:00">2020-03-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/arxiv/" itemprop="url" rel="index">
                    <span itemprop="name">arxiv</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/arxiv/CV/" itemprop="url" rel="index">
                    <span itemprop="name">CV</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
              <span>20k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
              <span>34 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Upper, Middle and Lower Region Learning for Facial Action Unit Detection <a href="https://arxiv.org/pdf/2002.04023" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Deep Convolutional Neural Networks with Spatial Regularization, Volume  and Star-shape Priori for Image Segmentation <a href="https://arxiv.org/pdf/2002.03989" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Unconstrained Periocular Recognition: Using Generative Deep Learning  Frameworks for Attribute Normalization <a href="https://arxiv.org/pdf/2002.03985" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> StickyPillars: Robust feature matching on point clouds using Graph  Neural Networks <a href="https://arxiv.org/pdf/2002.03983" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Joint Encoding of Appearance and Motion Features with Self-supervision  for First Person Action Recognition <a href="https://arxiv.org/pdf/2002.03982" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> RePose: Learning Deep Kinematic Priors for Fast Human Pose Estimation <a href="https://arxiv.org/pdf/2002.03933" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> 6DoF Object Pose Estimation via Differentiable Proxy Voting Loss <a href="https://arxiv.org/pdf/2002.03923" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Hierarchical Multi-Process Fusion for Visual Place Recognition <a href="https://arxiv.org/pdf/2002.03895" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> CIFAR-10 Image Classification Using Feature Ensembles <a href="https://arxiv.org/pdf/2002.03846" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Exploiting Temporal Coherence for Multi-modal Video Categorization <a href="https://arxiv.org/pdf/2002.03844" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Attentive Group Equivariant Convolutional Networks <a href="https://arxiv.org/pdf/2002.03830" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Level Three Synthetic Fingerprint Generation <a href="https://arxiv.org/pdf/2002.03809" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Automatic image-based identification and biomass estimation of  invertebrates <a href="https://arxiv.org/pdf/2002.03807" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> CONVINCE: Collaborative Cross-Camera Video Analytics at the Edge <a href="https://arxiv.org/pdf/2002.03797" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Deep Learning for Classifying Food Waste <a href="https://arxiv.org/pdf/2002.03786" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Multi-stream Faster RCNN for Mitosis Counting in Breast Cancer Images <a href="https://arxiv.org/pdf/2002.03781" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Towards Deep Machine Reasoning: a Prototype-based Deep Neural Network  with Decision Tree Inference <a href="https://arxiv.org/pdf/2002.03776" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Deriving Emotions and Sentiments from Visual Content: A Disaster  Analysis Use Case <a href="https://arxiv.org/pdf/2002.03773" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Improving the Evaluation of Generative Models with Fuzzy Logic <a href="https://arxiv.org/pdf/2002.03772" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Learning Numerical Observers using Unsupervised Domain Adaptation <a href="https://arxiv.org/pdf/2002.03763" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Music2Dance: Music-driven Dance Generation using WaveNet <a href="https://arxiv.org/pdf/2002.03761" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> An Empirical Study of Person Re-Identification with Attributes <a href="https://arxiv.org/pdf/2002.03752" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Weighted Average Precision: Adversarial Example Detection in the Visual  Perception of Autonomous Vehicles <a href="https://arxiv.org/pdf/2002.03751" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> An Overview of Two Age Synthesis and Estimation Techniques <a href="https://arxiv.org/pdf/2002.03750" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> DFKI Cabin Simulator: A Test Platform for Visual In-Cabin Monitoring  Functions <a href="https://arxiv.org/pdf/2002.03749" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> Black Box Explanation by Learning Image Exemplars in the Latent Feature  Space <a href="https://arxiv.org/pdf/2002.03746" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Dynamic Error-bounded Lossy Compression (EBLC) to Reduce the Bandwidth  Requirement for Real-time Vision-based Pedestrian Safety Applications <a href="https://arxiv.org/pdf/2002.03742" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> Efficient Scene Text Detection with Textual Attention Tower <a href="https://arxiv.org/pdf/2002.03741" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Convolutional Hierarchical Attention Network for Query-Focused Video  Summarization <a href="https://arxiv.org/pdf/2002.03740" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> Localizing Multi-scale Semantic Patches for Image Classification <a href="https://arxiv.org/pdf/2002.03737" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> Universal Semantic Segmentation for Fisheye Urban Driving Images <a href="https://arxiv.org/pdf/2002.03736" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> Real-Time Object Detection and Recognition on Low-Compute Humanoid  Robots using Deep Learning <a href="https://arxiv.org/pdf/2002.03735" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> Iterative energy-based projection on a normal data manifold for anomaly  localization <a href="https://arxiv.org/pdf/2002.03734" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
<div id="title34">
<b>34.</b> Robust Multimodal Image Registration Using Deep Recurrent Reinforcement  Learning <a href="https://arxiv.org/pdf/2002.03733" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper34" style="color:#0000EE;">摘要</a><br></div>
<div id="title35">
<b>35.</b> Impact of Data Quality on Deep Neural Network Training <a href="https://arxiv.org/pdf/2002.03732" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper35" style="color:#0000EE;">摘要</a><br></div>
<div id="title36">
<b>36.</b> RSnet: An improvement for Darknet <a href="https://arxiv.org/pdf/2002.03729" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper36" style="color:#0000EE;">摘要</a><br></div>
<div id="title37">
<b>37.</b> Driver Drowsiness Detection Model Using Convolutional Neural Networks  Techniques for Android Application <a href="https://arxiv.org/pdf/2002.03728" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper37" style="color:#0000EE;">摘要</a><br></div>
<div id="title38">
<b>38.</b> Durocmien: A deep framework for duroc skeleton extraction in constraint  environment <a href="https://arxiv.org/pdf/2002.03727" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper38" style="color:#0000EE;">摘要</a><br></div>
<div id="title39">
<b>39.</b> Deep Frequent Spatial Temporal Learning for Face Anti-Spoofing <a href="https://arxiv.org/pdf/2002.03723" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper39" style="color:#0000EE;">摘要</a><br></div>
<div id="title40">
<b>40.</b> Unsupervised deep clustering for predictive texture pattern discovery in  medical images <a href="https://arxiv.org/pdf/2002.03721" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper40" style="color:#0000EE;">摘要</a><br></div>
<div id="title41">
<b>41.</b> Fabricated Pictures Detection with Graph Matching <a href="https://arxiv.org/pdf/2002.03720" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper41" style="color:#0000EE;">摘要</a><br></div>
<div id="title42">
<b>42.</b> Knowledge Distillation for Brain Tumor Segmentation <a href="https://arxiv.org/pdf/2002.03688" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper42" style="color:#0000EE;">摘要</a><br></div>
<div id="title43">
<b>43.</b> Deep Multi-task Multi-label CNN for Effective Facial Attribute  Classification <a href="https://arxiv.org/pdf/2002.03683" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper43" style="color:#0000EE;">摘要</a><br></div>
<div id="title44">
<b>44.</b> Uncertainty Estimation for End-To-End Learned Dense Stereo Matching via  Probabilistic Deep Learning <a href="https://arxiv.org/pdf/2002.03663" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper44" style="color:#0000EE;">摘要</a><br></div>
<div id="title45">
<b>45.</b> Distribution Distillation Loss: Generic Approach for Improving Face  Recognition from Hard Samples <a href="https://arxiv.org/pdf/2002.03662" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper45" style="color:#0000EE;">摘要</a><br></div>
<div id="title46">
<b>46.</b> CRVOS: Clue Refining Network for Video Object Segmentation <a href="https://arxiv.org/pdf/2002.03651" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper46" style="color:#0000EE;">摘要</a><br></div>
<div id="title47">
<b>47.</b> Collaborative Training of Balanced Random Forests for Open Set Domain  Adaptation <a href="https://arxiv.org/pdf/2002.03642" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper47" style="color:#0000EE;">摘要</a><br></div>
<div id="title48">
<b>48.</b> End-to-End Facial Deep Learning Feature Compression with Teacher-Student  Enhancement <a href="https://arxiv.org/pdf/2002.03627" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper48" style="color:#0000EE;">摘要</a><br></div>
<div id="title49">
<b>49.</b> Post-Comparison Mitigation of Demographic Bias in Face Recognition Using  Fair Score Normalization <a href="https://arxiv.org/pdf/2002.03592" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper49" style="color:#0000EE;">摘要</a><br></div>
<div id="title50">
<b>50.</b> Prototype Refinement Network for Few-Shot Segmentation <a href="https://arxiv.org/pdf/2002.03579" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper50" style="color:#0000EE;">摘要</a><br></div>
<div id="title51">
<b>51.</b> Automatic detection and counting of retina cell nuclei using deep  learning <a href="https://arxiv.org/pdf/2002.03563" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper51" style="color:#0000EE;">摘要</a><br></div>
<div id="title52">
<b>52.</b> FAU, Facial Expressions, Valence and Arousal: A Multi-task Solution <a href="https://arxiv.org/pdf/2002.03557" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper52" style="color:#0000EE;">摘要</a><br></div>
<div id="title53">
<b>53.</b> Vehicle Driving Assistant <a href="https://arxiv.org/pdf/2002.03556" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper53" style="color:#0000EE;">摘要</a><br></div>
<div id="title54">
<b>54.</b> From Anchor Generation to Distribution Alignment: Learning a  Discriminative Embedding Space for Zero-Shot Recognition <a href="https://arxiv.org/pdf/2002.03554" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper54" style="color:#0000EE;">摘要</a><br></div>
<div id="title55">
<b>55.</b> UGRWO-Sampling: A modified random walk under-sampling approach based on  graphs to imbalanced data classification <a href="https://arxiv.org/pdf/2002.03521" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper55" style="color:#0000EE;">摘要</a><br></div>
<div id="title56">
<b>56.</b> A New Perspective for Flexible Feature Gathering in Scene Text  Recognition Via Character Anchor Pooling <a href="https://arxiv.org/pdf/2002.03509" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper56" style="color:#0000EE;">摘要</a><br></div>
<div id="title57">
<b>57.</b> Segmenting unseen industrial components in a heavy clutter using rgb-d  fusion and synthetic data <a href="https://arxiv.org/pdf/2002.03501" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper57" style="color:#0000EE;">摘要</a><br></div>
<div id="title58">
<b>58.</b> ABBA: Saliency-Regularized Motion-Based Adversarial Blur Attack <a href="https://arxiv.org/pdf/2002.03500" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper58" style="color:#0000EE;">摘要</a><br></div>
<div id="title59">
<b>59.</b> Medical Image Registration Using Deep Neural Networks: A Comprehensive  Review <a href="https://arxiv.org/pdf/2002.03401" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper59" style="color:#0000EE;">摘要</a><br></div>
<div id="title60">
<b>60.</b> Two-Stream Aural-Visual Affect Analysis in the Wild <a href="https://arxiv.org/pdf/2002.03399" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper60" style="color:#0000EE;">摘要</a><br></div>
<div id="title61">
<b>61.</b> MS-Net: Multi-Site Network for Improving Prostate Segmentation with  Heterogeneous MRI Data <a href="https://arxiv.org/pdf/2002.03366" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper61" style="color:#0000EE;">摘要</a><br></div>
<div id="title62">
<b>62.</b> Weakly Supervised Attention Pyramid Convolutional Neural Network for  Fine-Grained Visual Classification <a href="https://arxiv.org/pdf/2002.03353" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper62" style="color:#0000EE;">摘要</a><br></div>
<div id="title63">
<b>63.</b> Dynamic Inference: A New Approach Toward Efficient Video Action  Recognition <a href="https://arxiv.org/pdf/2002.03342" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper63" style="color:#0000EE;">摘要</a><br></div>
<div id="title64">
<b>64.</b> VIFB: A Visible and Infrared Image Fusion Benchmark <a href="https://arxiv.org/pdf/2002.03322" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper64" style="color:#0000EE;">摘要</a><br></div>
<div id="title65">
<b>65.</b> Unlabeled Data Deployment for Classification of Diabetic Retinopathy  Images Using Knowledge Transfer <a href="https://arxiv.org/pdf/2002.03321" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper65" style="color:#0000EE;">摘要</a><br></div>
<div id="title66">
<b>66.</b> FSD-10: A Dataset for Competitive Sports Content Analysis <a href="https://arxiv.org/pdf/2002.03312" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper66" style="color:#0000EE;">摘要</a><br></div>
<div id="title67">
<b>67.</b> Face Hallucination with Finishing Touches <a href="https://arxiv.org/pdf/2002.03308" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper67" style="color:#0000EE;">摘要</a><br></div>
<div id="title68">
<b>68.</b> Splitting Convolutional Neural Network Structures for Efficient  Inference <a href="https://arxiv.org/pdf/2002.03302" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper68" style="color:#0000EE;">摘要</a><br></div>
<div id="title69">
<b>69.</b> Convolutional Neural Network Pruning Using Filter Attenuation <a href="https://arxiv.org/pdf/2002.03299" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper69" style="color:#0000EE;">摘要</a><br></div>
<div id="title70">
<b>70.</b> PointHop++: A Lightweight Learning Model on Point Sets for 3D  Classification <a href="https://arxiv.org/pdf/2002.03281" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper70" style="color:#0000EE;">摘要</a><br></div>
<div id="title71">
<b>71.</b> Asymmetric Rejection Loss for Fairer Face Recognition <a href="https://arxiv.org/pdf/2002.03276" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper71" style="color:#0000EE;">摘要</a><br></div>
<div id="title72">
<b>72.</b> Learning efficient structured dictionary for image classification <a href="https://arxiv.org/pdf/2002.03271" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper72" style="color:#0000EE;">摘要</a><br></div>
<div id="title73">
<b>73.</b> Weakly-Supervised Multi-Person Action Recognition in 360$^{\circ}$  Videos <a href="https://arxiv.org/pdf/2002.03266" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper73" style="color:#0000EE;">摘要</a><br></div>
<div id="title74">
<b>74.</b> GradMix: Multi-source Transfer across Domains and Tasks <a href="https://arxiv.org/pdf/2002.03264" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper74" style="color:#0000EE;">摘要</a><br></div>
<div id="title75">
<b>75.</b> Ensemble of Deep Convolutional Neural Networks for Automatic Pavement  Crack Detection and Measurement <a href="https://arxiv.org/pdf/2002.03241" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper75" style="color:#0000EE;">摘要</a><br></div>
<div id="title76">
<b>76.</b> Multi-Label Class Balancing Algorithm for Action Unit Detection <a href="https://arxiv.org/pdf/2002.03238" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper76" style="color:#0000EE;">摘要</a><br></div>
<div id="title77">
<b>77.</b> Intrinsic Dimension Estimation via Nearest Constrained Subspace  Classifier <a href="https://arxiv.org/pdf/2002.03228" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper77" style="color:#0000EE;">摘要</a><br></div>
<div id="title78">
<b>78.</b> Exocentric to Egocentric Image Generation via Parallel Generative  Adversarial Network <a href="https://arxiv.org/pdf/2002.03219" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper78" style="color:#0000EE;">摘要</a><br></div>
<div id="title79">
<b>79.</b> Spatial-Temporal Multi-Cue Network for Continuous Sign Language  Recognition <a href="https://arxiv.org/pdf/2002.03187" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper79" style="color:#0000EE;">摘要</a><br></div>
<div id="title80">
<b>80.</b> Sparsity-Aware Deep Learning for Automatic 4D Facial Expression  Recognition <a href="https://arxiv.org/pdf/2002.03157" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper80" style="color:#0000EE;">摘要</a><br></div>
<div id="title81">
<b>81.</b> CTM: Collaborative Temporal Modeling for Action Recognition <a href="https://arxiv.org/pdf/2002.03152" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper81" style="color:#0000EE;">摘要</a><br></div>
<div id="title82">
<b>82.</b> Multi-Modality Cascaded Fusion Technology for Autonomous Driving <a href="https://arxiv.org/pdf/2002.03138" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper82" style="color:#0000EE;">摘要</a><br></div>
<div id="title83">
<b>83.</b> Symbiotic Attention with Privileged Information for Egocentric Action  Recognition <a href="https://arxiv.org/pdf/2002.03137" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper83" style="color:#0000EE;">摘要</a><br></div>
<div id="title84">
<b>84.</b> Variable-Viewpoint Representations for 3D Object Recognition <a href="https://arxiv.org/pdf/2002.03131" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper84" style="color:#0000EE;">摘要</a><br></div>
<div id="title85">
<b>85.</b> Attacking Optical Character Recognition (OCR) Systems with Adversarial  Watermarks <a href="https://arxiv.org/pdf/2002.03095" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper85" style="color:#0000EE;">摘要</a><br></div>
<div id="title86">
<b>86.</b> Bone Suppression on Chest Radiographs With Adversarial Learning <a href="https://arxiv.org/pdf/2002.03073" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper86" style="color:#0000EE;">摘要</a><br></div>
<div id="title87">
<b>87.</b> Local Facial Attribute Transfer through Inpainting <a href="https://arxiv.org/pdf/2002.03040" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper87" style="color:#0000EE;">摘要</a><br></div>
<div id="title88">
<b>88.</b> Unsupervised Discovery of Interpretable Directions in the GAN Latent  Space <a href="https://arxiv.org/pdf/2002.03754" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper88" style="color:#0000EE;">摘要</a><br></div>
<div id="title89">
<b>89.</b> Learning End-to-End Lossy Image Compression: A Benchmark <a href="https://arxiv.org/pdf/2002.03711" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper89" style="color:#0000EE;">摘要</a><br></div>
<div id="title90">
<b>90.</b> Distributed Bayesian Matrix Decomposition for Big Data Mining and  Clustering <a href="https://arxiv.org/pdf/2002.03703" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper90" style="color:#0000EE;">摘要</a><br></div>
<div id="title91">
<b>91.</b> Adversarial TCAV -- Robust and Effective Interpretation of Intermediate  Layers in Neural Networks <a href="https://arxiv.org/pdf/2002.03549" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper91" style="color:#0000EE;">摘要</a><br></div>
<div id="title92">
<b>92.</b> Multi-object Monocular SLAM for Dynamic Environments <a href="https://arxiv.org/pdf/2002.03528" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper92" style="color:#0000EE;">摘要</a><br></div>
<div id="title93">
<b>93.</b> Ultra High Fidelity Image Compression with $\ell_\infty$-constrained  Encoding and Deep Decoding <a href="https://arxiv.org/pdf/2002.03482" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper93" style="color:#0000EE;">摘要</a><br></div>
<div id="title94">
<b>94.</b> Semi-Supervised Class Discovery <a href="https://arxiv.org/pdf/2002.03480" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper94" style="color:#0000EE;">摘要</a><br></div>
<div id="title95">
<b>95.</b> A Deep Learning Approach to Automate High-Resolution Blood Vessel  Reconstruction on Computerized Tomography Images With or Without the Use of  Contrast Agent <a href="https://arxiv.org/pdf/2002.03463" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper95" style="color:#0000EE;">摘要</a><br></div>
<div id="title96">
<b>96.</b> A Unified End-to-End Framework for Efficient Deep Image Compression <a href="https://arxiv.org/pdf/2002.03370" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper96" style="color:#0000EE;">摘要</a><br></div>
<div id="title97">
<b>97.</b> Multi-Task Learning by a Top-Down Control Network <a href="https://arxiv.org/pdf/2002.03335" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper97" style="color:#0000EE;">摘要</a><br></div>
<div id="title98">
<b>98.</b> Out-of-Distribution Detection with Distance Guarantee in Deep Generative  Models <a href="https://arxiv.org/pdf/2002.03328" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper98" style="color:#0000EE;">摘要</a><br></div>
<div id="title99">
<b>99.</b> Holographic Image Sensing <a href="https://arxiv.org/pdf/2002.03314" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper99" style="color:#0000EE;">摘要</a><br></div>
<div id="title100">
<b>100.</b> Soft Threshold Weight Reparameterization for Learnable Sparsity <a href="https://arxiv.org/pdf/2002.03231" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper100" style="color:#0000EE;">摘要</a><br></div>
<div id="title101">
<b>101.</b> Correction of Chromatic Aberration from a Single Image Using Keypoints <a href="https://arxiv.org/pdf/2002.03196" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper101" style="color:#0000EE;">摘要</a><br></div>
<div id="title102">
<b>102.</b> Deep No-reference Tone Mapped Image Quality Assessment <a href="https://arxiv.org/pdf/2002.03165" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper102" style="color:#0000EE;">摘要</a><br></div>
<div id="title103">
<b>103.</b> Ramifications and Diminution of Image Noise in Iris Recognition System <a href="https://arxiv.org/pdf/2002.03125" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper103" style="color:#0000EE;">摘要</a><br></div>
<div id="title104">
<b>104.</b> An Empirical Evaluation of Perturbation-based Defenses <a href="https://arxiv.org/pdf/2002.03080" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper104" style="color:#0000EE;">摘要</a><br></div>
<div id="title105">
<b>105.</b> Predictive online optimisation with applications to optical flow <a href="https://arxiv.org/pdf/2002.03053" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper105" style="color:#0000EE;">摘要</a><br></div>
<div id="title106">
<b>106.</b> Cognitive Anthropomorphism of AI: How Humans and Computers Classify  Images <a href="https://arxiv.org/pdf/2002.03024" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper106" style="color:#0000EE;">摘要</a><br></div>
<div id="title107">
<b>107.</b> Improving the Adversarial Robustness of Transfer Learning via Noisy  Feature Distillation <a href="https://arxiv.org/pdf/2002.02998" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper107" style="color:#0000EE;">摘要</a><br></div>
<div id="title108">
<b>108.</b> DropCluster: A structured dropout for convolutional networks <a href="https://arxiv.org/pdf/2002.02997" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper108" style="color:#0000EE;">摘要</a><br></div>
<div id="title109">
<b>109.</b> SS-Auto: A Single-Shot, Automatic Structured Weight Pruning Framework of  DNNs with Ultra-High Efficiency <a href="https://arxiv.org/pdf/2001.08839" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper109" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Upper, Middle and Lower Region Learning for Facial Action Unit Detection</b>  <a href="https://arxiv.org/pdf/2002.04023" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yao Xia</a><br>
<font size="3">
Abstract: Facial action units (AUs) detection is fundamental to facial expression analysis. As AU occur only in a small area of face, region based learning has been widely recognized useful for AU detection. Most region based studies focus on a small region where the AU occurs. Focusing on a specific region is helpful in eliminating the influence of identity, but to be risk for losing information. It is difficult to find balance. In this study, I propose a simple strategy. I divide the face into three large regions, upper, middle and lower region, and group AUs based on where it occurs. I propose a new end-to-end deep learning framework named three regions based attention network (TRA-Net). After extracting the global feature, TRA-Net uses a hard attention module to extract three feature maps, each of which contains only a specific region. Each region-specific feature map is fed to an independent branch. For each branch, three continuous soft attention modules are used to extract higher-level features for final AU detection. In the DISFA dataset, this model achieves the highest F1 scores for the detection of AU1, AU2 and AU4, and produces the highest accuracy in comparison with the state-of-the-art methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：面部动作单元（AU）检测是面部表情分析的基础。由于AU只发生在脸上的小区域，基于区域的学习已得到广泛认可的AU检测有用。大多数基于区域的研究重点放在非盟发生小区域。专注于一个特定的区域是在消除身份的影响力有帮助，但对信息丢失的风险。这是很难找到平衡点。在这项研究中，我提出了一个简单的策略。我划分面为三个大区域，上部，中部和下部区域，并且组的AU基于其中它发生。我建议命名为三个区域以关注网络（TRA-网）一个新的终端到终端的深度学习的框架。提取全局特征后，TRA-Net使用硬关注模块中提取三个特征的地图，每一个都包含只针对特定区域。每个区域特异性特征地图被馈送到一个独立的分支。对于每个分支，三个连续软注意模块用于提取最终AU检测较高级别的功能。在DISFA数据集，该模型获得了最高的分数F1用于检测AU1，AU2和AU4的，并产生最高的精度在与国家的最先进的方法相比。</font>
</div>


<hr>
<div id="paper2"> <b>2. Deep Convolutional Neural Networks with Spatial Regularization, Volume  and Star-shape Priori for Image Segmentation</b>  <a href="https://arxiv.org/pdf/2002.03989" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiangyue Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tai%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xue-cheng Tai</a><br>
<font size="3">
Abstract: We use Deep Convolutional Neural Networks (DCNNs) for image segmentation problems. DCNNs can well extract the features from natural images. However, the classification functions in the existing network architecture of CNNs are simple and lack capabilities to handle important spatial information in a way that have been done for many well-known traditional variational models. Prior such as spatial regularity, volume prior and object shapes cannot be well handled by existing DCNNs. We propose a novel Soft Threshold Dynamics (STD) framework which can easily integrate many spatial priors of the classical variational models into the DCNNs for image segmentation. The novelty of our method is to interpret the softmax activation function as a dual variable in a variational problem, and thus many spatial priors can be imposed in the dual space. From this viewpoint, we can build a STD based framework which can enable the outputs of DCNNs to have many special priors such as spatial regularity, volume constraints and star-shape priori. The proposed method is a general mathematical framework and it can be applied to any semantic segmentation DCNNs. To show the efficiency and accuracy of our method, we applied it to the popular DeepLabV3+ image segmentation network, and the experiments results show that our method can work efficiently on data-driven image segmentation DCNNs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们使用深卷积神经网络（DCNNs）图像分割问题。 DCNNs能很好地提取自然图像的功能。然而，在细胞神经网络的现有网络架构的分类功能简单，缺乏能力来处理已为许多著名的传统模式变做一种方式重要的空间信息。现有如空间规律性，体积之前和对象的形状不能被很好地现有DCNNs处理。我们提出了一个新颖的软阈值的动力学（STD）的框架，可以很容易的经典车型变了许多空间先验融入DCNNs的图像分割。我们的方法的新颖性在于解释SOFTMAX激活函数如在变分问题双重可变，因此许多空间先验可以在对偶空间的罚款。从该观点出发，我们可以建立一个基于STD框架，可以使DCNNs的输出以有许多特殊的先验诸如空间规律性，体积限制和星形先验。所提出的方法是一般的数学框架，它可以被应用到任何语义分割DCNNs。为了显示我们的方法的效率和准确性，我们将其运用到流行DeepLabV3 +图像分割网络，实验结果表明，该方法可以在数据驱动的图像分割DCNNs提高工作效率。</font>
</div>


<hr>
<div id="paper3"> <b>3. Unconstrained Periocular Recognition: Using Generative Deep Learning  Frameworks for Attribute Normalization</b>  <a href="https://arxiv.org/pdf/2002.03985" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zanlorensi%2C+L+A" target="_blank" rel="noopener" style="color:#0000EE;">Luiz A. Zanlorensi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Proen%C3%A7a%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hugo Proença</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Menotti%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Menotti</a><br>
<font size="3">
Abstract: Ocular biometric systems working in unconstrained environments usually face the problem of small within-class compactness caused by the multiple factors that jointly degrade the quality of the obtained data. In this work, we propose an attribute normalization strategy based on deep learning generative frameworks, that reduces the variability of the samples used in pairwise comparisons, without reducing their discriminability. The proposed method can be seen as a preprocessing step that contributes for data regularization and improves the recognition accuracy, being fully agnostic to the recognition strategy used. As proof of concept, we consider the "eyeglasses" and "gaze" factors, comparing the levels of performance of five different recognition methods with/without using the proposed normalization strategy. Also, we introduce a new dataset for unconstrained periocular recognition, composed of images acquired by mobile devices, particularly suited to perceive the impact of "wearing eyeglasses" in recognition effectiveness. Our experiments were performed in two different datasets, and support the usefulness of our attribute normalization scheme to improve the recognition performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：眼在不受约束的环境中工作的生物识别系统通常面临所造成的多种因素共同降解所获得的数据的质量小的类内紧凑的问题。在这项工作中，我们提出了一种基于深度学习生成框架属性正常化的策略，即减少了两两比较用的样品的可变性，而不会降低他们的辨别力。所提出的方法可以被看作是一个预处理步骤，对于数据的正则化有助于，提高了识别精度，被完全不可知的使用的识别策略。作为概念验证，我们认为“眼镜”和“凝视”的因素，在不使用所提出的标准化战略比较与/五种不同的识别方法的性能水平。此外，我们介绍的无约束眼周识别一个新的数据集，由移动设备，特别适合于感知“戴眼镜”的在识别有效性的影响获取的图像所组成。我们的实验是在两个不同的数据集进行，并支持我们的属性正常化方案，以提高识别性能的实用性。</font>
</div>


<hr>
<div id="paper4"> <b>4. StickyPillars: Robust feature matching on point clouds using Graph  Neural Networks</b>  <a href="https://arxiv.org/pdf/2002.03983" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Simon%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Martin Simon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fischer%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kai Fischer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Milz%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stefan Milz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Witt%2C+C+T" target="_blank" rel="noopener" style="color:#0000EE;">Christian Tobias Witt</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gross%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Horst-Michael Gross</a><br>
<font size="3">
Abstract: StickyPillars introduces a sparse feature matching method on point clouds. It is the first approach applying Graph Neural Networks on point clouds to stick points of interest. The feature estimation and assignment relies on the optimal transport problem, where the cost is based on the neural network itself. We utilize a Graph Neural Network for context aggregation with the aid of multihead self and cross attention. In contrast to image based feature matching methods, the architecture learns feature extraction in an end-to-end manner. Hence, the approach does not rely on handcrafted features. Our method outperforms state-of-the art matching algorithms, while providing real-time capability. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：StickyPillars介绍了点云稀疏特征匹配方法。它是将点云图的神经网络坚持的兴趣点的第一种方法。该功能估计和分配依赖于最佳的交通问题，其中成本是基于神经网络本身。我们利用图的神经网络模型多头自我和交叉关注的援助范围内聚集。与基于图像特征匹配方法，该架构获悉设有在端至端的方式提取。因此，该方法不依赖于手工制作的特点。我们的方法优于国家的本领域匹配算法，同时提供实时能力。</font>
</div>


<hr>
<div id="paper5"> <b>5. Joint Encoding of Appearance and Motion Features with Self-supervision  for First Person Action Recognition</b>  <a href="https://arxiv.org/pdf/2002.03982" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Planamente%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mirco Planamente</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bottino%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrea Bottino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Caputo%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Barbara Caputo</a><br>
<font size="3">
Abstract: Wearable cameras are becoming more and more popular in several applications, increasing the interest of the research community in developing approaches for recognizing actions from a first-person point of view. An open challenge is how to cope with the limited amount of motion information available about the action itself, as opposed to the more investigated third-person action recognition scenario. When focusing on manipulation tasks, videos tend to record only parts of the movement, making crucial the understanding of the objects being manipulated and of their context. Previous works addressed this issue with two-stream architectures, one dedicated to modeling the appearance of objects involved in the action, another dedicated to extracting motion features from optical flow. In this paper, we argue that features from these two information channels should be learned jointly to capture the spatio-temporal correlations between the two in a better way. To this end, we propose a single stream architecture able to do so, thanks to the addition of a self-supervised block that uses a pretext motion segmentation task to intertwine motion and appearance knowledge. Experiments on several publicly available databases show the power of our approach. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：可穿戴式摄像机正变得越来越流行在几个应用程序，增加了研究界在发展从一个第一人称的角度认识行动方案的兴趣。一个开放的挑战是如何应对提供了有关行动本身数量有限的运动信息，而不是更多的研究第三人称动作识别场景。当着眼于操作任务，视频往往只记录运动的部件，使得关键的被操纵的对象的理解和他们的背景。以前的作品中解决了这个问题有两个流架构下，一个专门用于模拟参与行动对象的外观，另一个专门用于提取运动从光流的特征。在本文中，我们认为，这两个信息渠道功能应共同学会了捕捉两者之间的时空相关性以更好的方式。为此，我们提出了一个单一的数据流架构能够这样做，由于增加使用的借口运动分割任务纠结运动和外观知识自我监督的块。几个公共数据库实验证明我们的方法的力量。</font>
</div>


<hr>
<div id="paper6"> <b>6. RePose: Learning Deep Kinematic Priors for Fast Human Pose Estimation</b>  <a href="https://arxiv.org/pdf/2002.03933" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Isack%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hossam Isack</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Haene%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christian Haene</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Keskin%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cem Keskin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bouaziz%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sofien Bouaziz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Boykov%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuri Boykov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Izadi%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shahram Izadi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Khamis%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sameh Khamis</a><br>
<font size="3">
Abstract: We propose a novel efficient and lightweight model for human pose estimation from a single image. Our model is designed to achieve competitive results at a fraction of the number of parameters and computational cost of various state-of-the-art methods. To this end, we explicitly incorporate part-based structural and geometric priors in a hierarchical prediction framework. At the coarsest resolution, and in a manner similar to classical part-based approaches, we leverage the kinematic structure of the human body to propagate convolutional feature updates between the keypoints or body parts. Unlike classical approaches, we adopt end-to-end training to learn this geometric prior through feature updates from data. We then propagate the feature representation at the coarsest resolution up the hierarchy to refine the predicted pose in a coarse-to-fine fashion. The final network effectively models the geometric prior and intuition within a lightweight deep neural network, yielding state-of-the-art results for a model of this size on two standard datasets, Leeds Sports Pose and MPII Human Pose. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们从一个单一的形象提出了人体姿势估计一种新型高效和轻质的模型。我们的模型设计在参数和各种先进设备，最先进的方法计算成本的一小部分，以实现竞争的结果。为此，我们明确地纳入一个分层的预测基于框架部分结构和几何先验。在粗糙的分辨率，并以类似经典的基于部分的方法的方式，我们利用人体的运动结构传播的关键点或身体部位之间的卷积功能更新。不同于传统的方法，我们采用终端到终端的培训，学习这种几何之前通过功能从数据更新。然后，我们传播的特征表示，在最粗分辨率高达层次细化预测姿态在粗到精的方式。最终的网络有效地模型轻质深层神经网络内的几何之前和直觉，产生国家的最先进的结果对于该尺寸的两种标准数据集的模型，利兹体育姿和MPII人体姿势。</font>
</div>


<hr>
<div id="paper7"> <b>7. 6DoF Object Pose Estimation via Differentiable Proxy Voting Loss</b>  <a href="https://arxiv.org/pdf/2002.03923" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhuang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zheyu Zhuang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Koniusz%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Piotr Koniusz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongdong Li</a><br>
<font size="3">
Abstract: Estimating a 6DOF object pose from a single image is very challenging due to occlusions or textureless appearances. Vector-field based keypoint voting has demonstrated its effectiveness and superiority on tackling those issues. However, direct regression of vector-fields neglects that the distances between pixels and keypoints also affect the deviations of hypotheses dramatically. In other words, small errors in direction vectors may generate severely deviated hypotheses when pixels are far away from a keypoint. In this paper, we aim to reduce such errors by incorporating the distances between pixels and keypoints into our objective. To this end, we develop a simple yet effective differentiable proxy voting loss (DPVL) which mimics the hypothesis selection in the voting procedure. By exploiting our voting loss, we are able to train our network in an end-to-end manner. Experiments on widely used datasets, i.e. LINEMOD and Occlusion LINEMOD, manifest that our DPVL improves pose estimation performance significantly and speeds up the training convergence. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：从估计单个图像6自由度对象姿势非常由于遮挡或无纹理出场挑战。矢量场根据关键点投票已经证明对解决这些问题，它的有效性和优越性。然而，矢量场忽略的直接回归的像素和关键点之间的距离也影响假设的偏差显着。换言之，在方向矢量小误差可能产生严重偏离假设当像素远离关键点。在本文中，我们的目标是通过将像素和关键点之间的距离为我们的目标，以减少此类错误。为此，我们开发了一个简单而有效的微代理投票损失（DPVL），它模仿了投票过程中的假设选择。通过利用我们的投票损失，我们能够训练我们的网络中的终端到终端的方式。广泛使用的数据集的实验，即LINEMOD和闭塞LINEMOD，体现我们的DPVL显著改善姿势估计性能并加快训练收敛。</font>
</div>


<hr>
<div id="paper8"> <b>8. Hierarchical Multi-Process Fusion for Visual Place Recognition</b>  <a href="https://arxiv.org/pdf/2002.03895" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hausler%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stephen Hausler</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Milford%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Milford</a><br>
<font size="3">
Abstract: Combining multiple complementary techniques together has long been regarded as a way to improve performance. In visual localization, multi-sensor fusion, multi-process fusion of a single sensing modality, and even combinations of different localization techniques have been shown to result in improved performance. However, merely fusing together different localization techniques does not account for the varying performance characteristics of different localization techniques. In this paper we present a novel, hierarchical localization system that explicitly benefits from three varying characteristics of localization techniques: the distribution of their localization hypotheses, their appearance- and viewpoint-invariant properties, and the resulting differences in where in an environment each system works well and fails. We show how two techniques deployed hierarchically work better than in parallel fusion, how combining two different techniques works better than two levels of a single technique, even when the single technique has superior individual performance, and develop two and three-tier hierarchical structures that progressively improve localization performance. Finally, we develop a stacked hierarchical framework where localization hypotheses from techniques with complementary characteristics are concatenated at each layer, significantly improving retention of the correct hypothesis through to the final localization stage. Using two challenging datasets, we show the proposed system outperforming state-of-the-art techniques. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：结合使用多种互补技术的配合一直被认为是提高性能的一种方式。在视觉定位，多传感器融合，单个感测模态的多进程融合，和不同的定位技术，即使组合已显示导致改善的性能。然而，仅仅融合在一起不同的定位技术不考虑不同的定位技术不同的性能特点。在本文中，我们提出了一种新的分层定位系统，从定位技术3个变特征明确的好处：其本地化的假说，他们appearance-和观点不变性质的分配，并在一个环境中，其中产生的差异各系统的工作原理以及与失败。我们发现分级部署两种技术如何更好地工作比并行融合，如何结合两种不同的技术更好地工作比单一技术两个层面，即使在单一技术具有优异的个人表现和发展二，三梯队层次结构是渐进提高定位性能。最后，我们开发了一个层叠的分级框架，其中从具有互补特性的技术定位的假设，在每个层级联，通过对最终定位阶段显著提高正确假设的保持。使用两个挑战数据集，我们证明了该系统超越国家的最先进的技术。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>PROCJX
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://procjx.github.io/2020/02/11/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-11/" title="【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-11">https://procjx.github.io/2020/02/11/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-11/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">

        
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2020/02/11/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-11/" rel="next" title="【arxiv论文】 Computation and Language 2020-02-11">
                  <i class="fa fa-chevron-left"></i> 【arxiv论文】 Computation and Language 2020-02-11
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/2020/02/12/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-12/" rel="prev" title="【arxiv论文】 Computation and Language 2020-02-12">
                  【arxiv论文】 Computation and Language 2020-02-12 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>

        
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang -->
<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9197824246"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="gitalk-container"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#目录"><span class="nav-number">1.</span> <span class="nav-text">目录</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#摘要"><span class="nav-number">2.</span> <span class="nav-text">摘要</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="PROCJX"
    src="/images/procjx.png">
  <p class="site-author-name" itemprop="name">PROCJX</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">370</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">71</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/procjx" title="GitHub &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;procjx" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:procjx@gmail.com" title="E-Mail &amp;rarr; mailto:procjx@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>


<!--
      
        <script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
        <script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
        <div class="widget-wrap">
            <h3 class="widget-title">标签云</h3>
            <div id="myCanvasContainer" class="widget tagcloud">
                <canvas width="250" height="250" id="resCanvas" style="width=100%">
                    <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AAAI/" rel="tag">AAAI</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ACL/" rel="tag">ACL</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Accepted-Papers/" rel="tag">Accepted Papers</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ArXiv/" rel="tag">ArXiv</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/" rel="tag">BERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS20SI/" rel="tag">CS20SI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS224d/" rel="tag">CS224d</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/" rel="tag">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CVPR/" rel="tag">CVPR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Context/" rel="tag">Context</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cross-Lingual/" rel="tag">Cross Lingual</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dialog-System/" rel="tag">Dialog System</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse/" rel="tag">Discourse</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Ranking/" rel="tag">Discourse Ranking</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Structure/" rel="tag">Discourse Structure</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Document-NMT/" rel="tag">Document NMT</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EMNLP/" rel="tag">EMNLP</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Extractive/" rel="tag">Extractive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICLR/" rel="tag">ICLR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICML/" rel="tag">ICML</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IJCAI/" rel="tag">IJCAI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Inter-Sentence/" rel="tag">Inter-Sentence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Keyphrase-Generation/" rel="tag">Keyphrase Generation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NAACL/" rel="tag">NAACL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIPS/" rel="tag">NIPS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NMT/" rel="tag">NMT</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Relation-Extraction/" rel="tag">Neural Relation Extraction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RST/" rel="tag">RST</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Relation-Constraints/" rel="tag">Relation Constraints</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Summarization/" rel="tag">Summarization</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Translation/" rel="tag">Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Word-Translation/" rel="tag">Word Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/alias/" rel="tag">alias</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/git/" rel="tag">git</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pip/" rel="tag">pip</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/screen/" rel="tag">screen</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/" rel="tag">shell</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tgz/" rel="tag">tgz</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tts/" rel="tag">tts</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1/" rel="tag">冒泡</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/" rel="tag">冒泡排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%99%E4%BD%9C%E5%8A%A9%E6%89%8B/" rel="tag">写作助手</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8E%8B%E7%BC%A9/" rel="tag">压缩</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/" rel="tag">发送邮件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%88%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">合并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%8E%E5%8F%B0/" rel="tag">后台</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/" rel="tag">基数排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F/" rel="tag">希尔排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6/" rel="tag">归并</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">归并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/" rel="tag">快速排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F/" rel="tag">批量</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4/" rel="tag">批量删除</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F/" rel="tag">排序</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5/" rel="tag">插入</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/" rel="tag">插入排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%99%E7%A8%8B/" rel="tag">教程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/" rel="tag">斐波那契数列</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9D%80%E6%AD%BB%E8%BF%9B%E7%A8%8B/" rel="tag">杀死进程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B1%89%E8%AF%BA%E5%A1%94/" rel="tag">汉诺塔</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%A7%A3%E5%8E%8B/" rel="tag">解压</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%B0%B7%E6%AD%8C%E7%BF%BB%E8%AF%91/" rel="tag">谷歌翻译</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BF%AD%E4%BB%A3%E5%9B%9E%E7%BF%BB/" rel="tag">迭代回翻</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9/" rel="tag">选择</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/" rel="tag">选择排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%99%84%E4%BB%B6/" rel="tag">附件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3/" rel="tag">非监督</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%A2%86%E5%9F%9F%E9%80%82%E5%BA%94/" rel="tag">领域适应</a><span class="tag-list-count">1</span></li></ul>
                </canvas>
            </div>
        </div>
        
-->
        
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-hengfu -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9879871597"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-chuizhi -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="1662238719"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-zhengfangxing -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="6699421902"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PROCJX</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.4.2
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>












        
      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  <script src="/js/local-search.js"></script>













  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '2286ab64f5194d9d79ce',
      clientSecret: 'f912492bec2391664b40478f50f2f943376768d6',
      repo: 'procjx.github.io',
      owner: 'procjx',
      admin: ['procjx'],
      id: 'e87abf0fb3e14a1c16326f364125bdec',
        language: 'zh-CN',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
