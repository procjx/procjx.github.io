<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/procjx.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/procjxfavicon32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/procjxfavicon16x16.ico">
  <link rel="mask-icon" href="/images/procjx.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.4.2',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

<!-- Google Adsense -->
<!--
<script async src="//pagead2.googlesyndication.com/
pagead/js/adsbygoogle.js"></script>
<script>
(adsbygoogle = window.adsbygoogle || []).push({
google_ad_client: "pub-1179774715076800",
enable_page_level_ads: true
});
</script>
-->

<script data-ad-client="ca-pub-1179774715076800" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>


<meta name="google-site-verification" content="cEiGwg0T8Rj5msmuEcGYZTh5nnf05EhCXy0gp2Ml5BI" />
<meta name="baidu-site-verification" content="noSKHe8MJs" />

  <meta name="description" content="目录  1. Cross-Lingual Pre-Training Based Transfer for Zero-Shot Neural Machine Translation, AAAI 2020 [PDF] 摘要  2. Harnessing GANs for Zero-Shot Learning of New Classes in Visual Speech Recognition, AA">
<meta property="og:type" content="article">
<meta property="og:title" content="【NLP】 2014-2020 Zero-Shot 相关论文整理">
<meta property="og:url" content="https:&#x2F;&#x2F;procjx.github.io&#x2F;2020&#x2F;12&#x2F;09&#x2F;%E3%80%90NLP%E3%80%912014-2020%20Zero-Shot%20%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E6%95%B4%E7%90%86&#x2F;index.html">
<meta property="og:site_name" content="PROCJX&#39;s BLOGS">
<meta property="og:description" content="目录  1. Cross-Lingual Pre-Training Based Transfer for Zero-Shot Neural Machine Translation, AAAI 2020 [PDF] 摘要  2. Harnessing GANs for Zero-Shot Learning of New Classes in Visual Speech Recognition, AA">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2020-12-09T13:21:51.409Z">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://procjx.github.io/2020/12/09/%E3%80%90NLP%E3%80%912014-2020%20Zero-Shot%20%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E6%95%B4%E7%90%86/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>【NLP】 2014-2020 Zero-Shot 相关论文整理 | PROCJX's BLOGS</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">PROCJX's BLOGS</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">WITH LOVE OF WORLD</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-fw fa-download"></i>资源</a>

  </li>
        <li class="menu-item menu-item-arxiv">

    <a href="/arxiv/" rel="section"><i class="fa fa-fw fa-file-pdf-o"></i>arxiv论文</a>

  </li>
        <li class="menu-item menu-item-deadline">

    <a href="/deadline/" rel="section"><i class="fa fa-fw fa-calendar"></i>会议截稿</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://procjx.github.io/2020/12/09/%E3%80%90NLP%E3%80%912014-2020%20Zero-Shot%20%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E6%95%B4%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/procjx.png">
      <meta itemprop="name" content="PROCJX">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PROCJX's BLOGS">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          【NLP】 2014-2020 Zero-Shot 相关论文整理
        </h2>

        <div class="post-meta">
        
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-12-09 21:17:52 / 修改时间：21:21:51" itemprop="dateCreated datePublished" datetime="2020-12-09T21:17:52+08:00">2020-12-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AC%E8%AE%BA%E6%96%87/" itemprop="url" rel="index">
                    <span itemprop="name">AC论文</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AC%E8%AE%BA%E6%96%87/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
              <span>359k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
              <span>9:58</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Cross-Lingual Pre-Training Based Transfer for Zero-Shot Neural Machine Translation, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/5341/5197" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Harnessing GANs for Zero-Shot Learning of New Classes in Visual Speech Recognition, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/5649/5505" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Query-Driven Multi-Instance Learning, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/5836/5692" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Attribute Propagation Network for Graph Zero-Shot Learning, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/5923/5779" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Meta-Learning for Generalized Zero-Shot Learning, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6069/5925" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Zero-Shot Text-to-SQL Learning with Auxiliary Task, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6246/6102" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Cross-Lingual Natural Language Generation via Pre-Training, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6256/6112" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Two-Level Transformer and Auxiliary Coherence Modeling for Improved Text Segmentation, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6284/6140" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Unsupervised Interlingual Semantic Representations from Sentence Embeddings for Zero-Shot Cross-Lingual Transfer, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6302/6158" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> MA-DST: Multi-Attention-Based Scalable Dialog State Tracking, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6322/6178" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Towards Zero-Shot Learning for Automatic Phonemic Transcription, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6341/6197" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Attention-Informed Mixed-Language Training for Zero-Shot Cross-Lingual Task-Oriented Dialogue Systems, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6362/6218" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Generative Adversarial Zero-Shot Relational Learning for Knowledge Graphs, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6392/6248" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Towards Scalable Multi-Domain Conversational Agents: The Schema-Guided Dialogue Dataset, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6394/6250" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Evaluating the Cross-Lingual Effectiveness of Massively Multilingual Neural Machine Translation, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6414/6270" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Low Resource Sequence Tagging with Weak Labels, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6415/6271" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> A Comparison of Architectures and Pretraining Methods for Contextualized Multilingual Word Embeddings, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6443/6299" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Deep Bayesian Nonparametric Learning of Rules and Plans from Demonstrations with a Learned Automaton Prior, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6559/6415" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Detecting Human-Object Interactions via Functional Generalization, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6616/6470" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Feature Deformation Meta-Networks in Image Captioning of Novel Objects, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6620/6474" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Zero-Shot Ingredient Recognition by Multi-Relational Graph Convolutional Network, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6626/6480" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Zero Shot Learning with the Isoperimetric Loss, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6698/6552" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Learning Cross-Aligned Latent Embeddings for Zero-Shot Cross-Modal Retrieval, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6817/6671" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Zero-Shot Learning from Adversarial Feature Residual to Compact Visual Feature, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6821/6675" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Context-Aware Zero-Shot Recognition, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6841/6695" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> A Variational Autoencoder with Deep Embedding Model for Generalized Zero-Shot Learning, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6844/6698" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Learning Meta Model for Zero- and Few-Shot Face Anti-Spoofing, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6866/6720" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> Improved Visual-Semantic Alignment for Zero-Shot Object Detection, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6868/6722" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Consistent Video Style Transfer via Compound Regularization, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6905/6759" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> Mining on Heterogeneous Manifolds for Zero-Shot Cross-Modal Image Retrieval, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6949/6803" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> Zero-Shot Sketch-Based Image Retrieval via Graph Convolution Network, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6993/6847" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> GTNet: Generative Transfer Network for Zero-Shot Object Detection, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6996/6850" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> Motion-Attentive Transition for Zero-Shot Video Object Segmentation, AAAI 2020 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/7008/6862" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
<div id="title34">
<b>34.</b> Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain Dialogue State Tracking, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.12.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper34" style="color:#0000EE;">摘要</a><br></div>
<div id="title35">
<b>35.</b> Unknown Intent Detection Using Gaussian Mixture Model with an Application to Zero-shot Intent Classification, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.99.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper35" style="color:#0000EE;">摘要</a><br></div>
<div id="title36">
<b>36.</b> Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.148.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper36" style="color:#0000EE;">摘要</a><br></div>
<div id="title37">
<b>37.</b> On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.151.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper37" style="color:#0000EE;">摘要</a><br></div>
<div id="title38">
<b>38.</b> Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.252.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper38" style="color:#0000EE;">摘要</a><br></div>
<div id="title39">
<b>39.</b> Zero-shot Text Classification via Reinforced Self-training, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.272.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper39" style="color:#0000EE;">摘要</a><br></div>
<div id="title40">
<b>40.</b> Structure-Level Knowledge Distillation For Multilingual Sequence Labeling, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.304.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper40" style="color:#0000EE;">摘要</a><br></div>
<div id="title41">
<b>41.</b> Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.324.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper41" style="color:#0000EE;">摘要</a><br></div>
<div id="title42">
<b>42.</b> On the Cross-lingual Transferability of Monolingual Representations, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.421.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper42" style="color:#0000EE;">摘要</a><br></div>
<div id="title43">
<b>43.</b> Finding Universal Grammatical Relations in Multilingual BERT, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.493.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper43" style="color:#0000EE;">摘要</a><br></div>
<div id="title44">
<b>44.</b> Multi-Cell Compositional LSTM for NER Domain Adaptation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.524.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper44" style="color:#0000EE;">摘要</a><br></div>
<div id="title45">
<b>45.</b> Language to Network: Conditional Parameter Adaptation with Natural Language Descriptions, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.625.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper45" style="color:#0000EE;">摘要</a><br></div>
<div id="title46">
<b>46.</b> CompGuessWhat?!: A Multi-task Evaluation Framework for Grounded Language Learning, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.682.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper46" style="color:#0000EE;">摘要</a><br></div>
<div id="title47">
<b>47.</b> Translationese as a Language in “Multilingual” NMT, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.691.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper47" style="color:#0000EE;">摘要</a><br></div>
<div id="title48">
<b>48.</b> ZeroShotCeres: Zero-Shot Relation Extraction from Semi-Structured Webpages, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.721.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper48" style="color:#0000EE;">摘要</a><br></div>
<div id="title49">
<b>49.</b> Should All Cross-Lingual Embeddings Speak English?, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.766.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper49" style="color:#0000EE;">摘要</a><br></div>
<div id="title50">
<b>50.</b> Zero-shot North Korean to English Neural Machine Translation by Character Tokenization and Phoneme Decomposition, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-srw.11.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper50" style="color:#0000EE;">摘要</a><br></div>
<div id="title51">
<b>51.</b> Cross-Lingual Disaster-related Multi-label Tweet Classification with Manifold Mixup, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-srw.39.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper51" style="color:#0000EE;">摘要</a><br></div>
<div id="title52">
<b>52.</b> Language Models as Fact Checkers?, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.fever-1.5.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper52" style="color:#0000EE;">摘要</a><br></div>
<div id="title53">
<b>53.</b> A Deep Reinforced Model for Zero-Shot Cross-Lingual Summarization with Bilingual Semantic Similarity Rewards, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.ngt-1.7.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper53" style="color:#0000EE;">摘要</a><br></div>
<div id="title54">
<b>54.</b> From Machine Reading Comprehension to Dialogue State Tracking: Bridging the Gap, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.nlp4convai-1.10.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper54" style="color:#0000EE;">摘要</a><br></div>
<div id="title55">
<b>55.</b> On-The-Fly Information Retrieval Augmentation for Language Models, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.nuse-1.14.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper55" style="color:#0000EE;">摘要</a><br></div>
<div id="title56">
<b>56.</b> Unsupervised Commonsense Question Answering with Self-Talk, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.373.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper56" style="color:#0000EE;">摘要</a><br></div>
<div id="title57">
<b>57.</b> Zero-Shot Cross-Lingual Transfer with Meta Learning, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.368.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper57" style="color:#0000EE;">摘要</a><br></div>
<div id="title58">
<b>58.</b> Event Extraction by Answering (Almost) Natural Questions, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.49.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper58" style="color:#0000EE;">摘要</a><br></div>
<div id="title59">
<b>59.</b> XL-WiC: A Multilingual Benchmark for Evaluating Semantic Contextualization, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.584.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper59" style="color:#0000EE;">摘要</a><br></div>
<div id="title60">
<b>60.</b> Cross-lingual Spoken Language Understanding with Regularized Representation Alignment, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.587.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper60" style="color:#0000EE;">摘要</a><br></div>
<div id="title61">
<b>61.</b> XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.185.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper61" style="color:#0000EE;">摘要</a><br></div>
<div id="title62">
<b>62.</b> UDapter: Language Adaptation for Truly Universal Dependency Parsing, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.180.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper62" style="color:#0000EE;">摘要</a><br></div>
<div id="title63">
<b>63.</b> Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.8.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper63" style="color:#0000EE;">摘要</a><br></div>
<div id="title64">
<b>64.</b> Context-Aware Answer Extraction in Question Answering, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.189.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper64" style="color:#0000EE;">摘要</a><br></div>
<div id="title65">
<b>65.</b> MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.194.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper65" style="color:#0000EE;">摘要</a><br></div>
<div id="title66">
<b>66.</b> Grounded Adaptation for Zero-shot Executable Semantic Parsing, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.558.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper66" style="color:#0000EE;">摘要</a><br></div>
<div id="title67">
<b>67.</b> Zero-Shot Crosslingual Sentence Simplification, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.415.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper67" style="color:#0000EE;">摘要</a><br></div>
<div id="title68">
<b>68.</b> Identifying Elements Essential for BERT’s Multilinguality, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.358.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper68" style="color:#0000EE;">摘要</a><br></div>
<div id="title69">
<b>69.</b> KGLM: Pretrained Knowledge-Grounded Language Model for Data-to-Text Generation, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.697.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper69" style="color:#0000EE;">摘要</a><br></div>
<div id="title70">
<b>70.</b> Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.554.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper70" style="color:#0000EE;">摘要</a><br></div>
<div id="title71">
<b>71.</b> Multi-task Learning for Multilingual Neural Machine Translation, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.75.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper71" style="color:#0000EE;">摘要</a><br></div>
<div id="title72">
<b>72.</b> MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.617.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper72" style="color:#0000EE;">摘要</a><br></div>
<div id="title73">
<b>73.</b> Event Extraction as Machine Reading Comprehension, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.128.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper73" style="color:#0000EE;">摘要</a><br></div>
<div id="title74">
<b>74.</b> Revisiting Modularized Multilingual NMT to Meet Industrial Demands, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.476.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper74" style="color:#0000EE;">摘要</a><br></div>
<div id="title75">
<b>75.</b> Cold-start and Interpretability: Turning Regular Expressions into Trainable Recurrent Neural Networks, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.258.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper75" style="color:#0000EE;">摘要</a><br></div>
<div id="title76">
<b>76.</b> PRover: Proof Generation for Interpretable Reasoning over Rules, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.9.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper76" style="color:#0000EE;">摘要</a><br></div>
<div id="title77">
<b>77.</b> Translation Artifacts in Cross-lingual Transfer Learning, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.618.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper77" style="color:#0000EE;">摘要</a><br></div>
<div id="title78">
<b>78.</b> An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.607.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper78" style="color:#0000EE;">摘要</a><br></div>
<div id="title79">
<b>79.</b> LAReQA: Language-agnostic Answer Retrieval from a Multilingual Pool, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.477.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper79" style="color:#0000EE;">摘要</a><br></div>
<div id="title80">
<b>80.</b> Generationary or: “How We Went beyond Word Sense Inventories and Learned to Gloss”, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.585.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper80" style="color:#0000EE;">摘要</a><br></div>
<div id="title81">
<b>81.</b> From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.363.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper81" style="color:#0000EE;">摘要</a><br></div>
<div id="title82">
<b>82.</b> Self-Supervised Knowledge Triplet Learning for Zero-shot Question Answering, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.11.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper82" style="color:#0000EE;">摘要</a><br></div>
<div id="title83">
<b>83.</b> Zero-Shot Stance Detection: A Dataset and Model Using Generalized Topic Representations, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.717.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper83" style="color:#0000EE;">摘要</a><br></div>
<div id="title84">
<b>84.</b> Scalable Zero-shot Entity Linking with Dense Entity Retrieval, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.519.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper84" style="color:#0000EE;">摘要</a><br></div>
<div id="title85">
<b>85.</b> CHARM: Inferring Personal Attributes from Conversations, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.434.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper85" style="color:#0000EE;">摘要</a><br></div>
<div id="title86">
<b>86.</b> Design Challenges in Low-resource Cross-lingual Entity Linking, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.521.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper86" style="color:#0000EE;">摘要</a><br></div>
<div id="title87">
<b>87.</b> AutoQA: From Databases to Q&A Semantic Parsers with Only Synthetic Training Data, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.31.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper87" style="color:#0000EE;">摘要</a><br></div>
<div id="title88">
<b>88.</b> On the Evaluation of Contextual Embeddings for Zero-Shot Cross-Lingual Transfer Learning, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.40.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper88" style="color:#0000EE;">摘要</a><br></div>
<div id="title89">
<b>89.</b> The Multilingual Amazon Reviews Corpus, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.369.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper89" style="color:#0000EE;">摘要</a><br></div>
<div id="title90">
<b>90.</b> Multi-label Few/Zero-shot Learning with Knowledge Aggregated from Multiple Label Graphs, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.235.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper90" style="color:#0000EE;">摘要</a><br></div>
<div id="title91">
<b>91.</b> An Empirical Study of Pre-trained Transformers for Arabic Information Extraction, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.382.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper91" style="color:#0000EE;">摘要</a><br></div>
<div id="title92">
<b>92.</b> Language Adapters for Zero Shot Neural Machine Translation, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.361.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper92" style="color:#0000EE;">摘要</a><br></div>
<div id="title93">
<b>93.</b> SLEDGE: A Simple Yet Effective Zero-Shot Baseline for Coronavirus Scientific Knowledge Search, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.emnlp-main.341.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper93" style="color:#0000EE;">摘要</a><br></div>
<div id="title94">
<b>94.</b> ZEST: Zero-shot Learning from Text Descriptions using Textual Similarity and Visual Summarization, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.50.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper94" style="color:#0000EE;">摘要</a><br></div>
<div id="title95">
<b>95.</b> Document Ranking with a Pretrained Sequence-to-Sequence Model, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.63.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper95" style="color:#0000EE;">摘要</a><br></div>
<div id="title96">
<b>96.</b> Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.83.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper96" style="color:#0000EE;">摘要</a><br></div>
<div id="title97">
<b>97.</b> Hybrid Emoji-Based Masked Language Models for Zero-Shot Abusive Language Detection, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.84.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper97" style="color:#0000EE;">摘要</a><br></div>
<div id="title98">
<b>98.</b> CodeBERT: A Pre-Trained Model for Programming and Natural Languages, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.139.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper98" style="color:#0000EE;">摘要</a><br></div>
<div id="title99">
<b>99.</b> On the Language Neutrality of Pre-trained Multilingual Representations, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.150.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper99" style="color:#0000EE;">摘要</a><br></div>
<div id="title100">
<b>100.</b> Zero-Shot Rationalization by Multi-Task Transfer Learning from Question Answering, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.198.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper100" style="color:#0000EE;">摘要</a><br></div>
<div id="title101">
<b>101.</b> Sparse and Decorrelated Representations for Stable Zero-shot NMT, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.205.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper101" style="color:#0000EE;">摘要</a><br></div>
<div id="title102">
<b>102.</b> Zero-shot Entity Linking with Efficient Long Range Sequence Modeling, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.228.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper102" style="color:#0000EE;">摘要</a><br></div>
<div id="title103">
<b>103.</b> Extending Multilingual BERT to Low-Resource Languages, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.240.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper103" style="color:#0000EE;">摘要</a><br></div>
<div id="title104">
<b>104.</b> Towards Zero Shot Conditional Summarization with Adaptive Multi-task Fine-Tuning, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.289.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper104" style="color:#0000EE;">摘要</a><br></div>
<div id="title105">
<b>105.</b> Internal and External Pressures on Language Emergence: Least Effort, Object Constancy and Frequency, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.397.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper105" style="color:#0000EE;">摘要</a><br></div>
<div id="title106">
<b>106.</b> Learning to Classify Human Needs of Events from Category Definitions with Prototypical Instantiation, EMNLP 2020 <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.421.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper106" style="color:#0000EE;">摘要</a><br></div>
<div id="title107">
<b>107.</b> Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model, ICLR 2020 <a href="https://openreview.net/pdf?id=BJlzm64tDH" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper107" style="color:#0000EE;">摘要</a><br></div>
<div id="title108">
<b>108.</b> Locality and Compositionality in Zero-Shot Learning, ICLR 2020 <a href="https://openreview.net/pdf?id=Hye_V0NKwr" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper108" style="color:#0000EE;">摘要</a><br></div>
<div id="title109">
<b>109.</b> Multilingual Alignment of Contextual Word Representations, ICLR 2020 <a href="https://openreview.net/pdf?id=r1xCMyBtPS" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper109" style="color:#0000EE;">摘要</a><br></div>
<div id="title110">
<b>110.</b> Empirical Bayes Transductive Meta-Learning with Synthetic Gradients, ICLR 2020 <a href="https://openreview.net/pdf?id=Hkg-xgrYvH" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper110" style="color:#0000EE;">摘要</a><br></div>
<div id="title111">
<b>111.</b> Program Guided Agent, ICLR 2020 <a href="https://openreview.net/pdf?id=BkxUvnEYDH" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper111" style="color:#0000EE;">摘要</a><br></div>
<div id="title112">
<b>112.</b> Meta-Learning without Memorization, ICLR 2020 <a href="https://openreview.net/pdf?id=BklEFpEYwS" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper112" style="color:#0000EE;">摘要</a><br></div>
<div id="title113">
<b>113.</b> Dynamics-Aware Unsupervised Discovery of Skills, ICLR 2020 <a href="https://openreview.net/pdf?id=HJgLZR4KvH" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper113" style="color:#0000EE;">摘要</a><br></div>
<div id="title114">
<b>114.</b> Convolutional Conditional Neural Processes, ICLR 2020 <a href="https://openreview.net/pdf?id=Skey4eBYPS" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper114" style="color:#0000EE;">摘要</a><br></div>
<div id="title115">
<b>115.</b> Lifelong Zero-Shot Learning, IJCAI 2020 <a href="https://www.ijcai.org/proceedings/2020/0077.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper115" style="color:#0000EE;">摘要</a><br></div>
<div id="title116">
<b>116.</b> Zero-Shot Object Detection via Learning an Embedding from Semantic Space to Visual Space, IJCAI 2020 <a href="https://www.ijcai.org/proceedings/2020/0126.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper116" style="color:#0000EE;">摘要</a><br></div>
<div id="title117">
<b>117.</b> Progressive Domain-Independent Feature Decomposition Network for Zero-Shot Sketch-Based Image Retrieval, IJCAI 2020 <a href="https://www.ijcai.org/proceedings/2020/0137.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper117" style="color:#0000EE;">摘要</a><br></div>
<div id="title118">
<b>118.</b> CoSDA-ML: Multi-Lingual Code-Switching Data Augmentation for Zero-Shot Cross-Lingual NLP, IJCAI 2020 <a href="https://www.ijcai.org/proceedings/2020/0533.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper118" style="color:#0000EE;">摘要</a><br></div>
<div id="title119">
<b>119.</b> Generalized Zero-Shot Text Classification for ICD Coding, IJCAI 2020 <a href="https://www.ijcai.org/proceedings/2020/0556.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper119" style="color:#0000EE;">摘要</a><br></div>
<div id="title120">
<b>120.</b> Zero Shot Learning for Code Education: Rubric Sampling with Deep Learning Inference, AAAI 2019 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/3857/3735" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper120" style="color:#0000EE;">摘要</a><br></div>
<div id="title121">
<b>121.</b> From Zero-Shot Learning to Cold-Start Recommendation, AAAI 2019 <a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4324/4202" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper121" style="color:#0000EE;">摘要</a><br></div>
<div id="title122">
<b>122.</b> Zero-Shot Adaptive Transfer for Conversational Language Understanding, AAAI 2019 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/4634/4512" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper122" style="color:#0000EE;">摘要</a><br></div>
<div id="title123">
<b>123.</b> Zero-Shot Neural Transfer for Cross-Lingual Entity Linking, AAAI 2019 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/4670/4548" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper123" style="color:#0000EE;">摘要</a><br></div>
<div id="title124">
<b>124.</b> Analysis of Joint Multilingual Sentence Representations and Semantic K-Nearest Neighbor Graphs, AAAI 2019 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/4677/4555" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper124" style="color:#0000EE;">摘要</a><br></div>
<div id="title125">
<b>125.</b> QUAREL: A Dataset and Models for Answering Questions about Qualitative Relationships, AAAI 2019 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/4687/4565" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper125" style="color:#0000EE;">摘要</a><br></div>
<div id="title126">
<b>126.</b> Energy Confused Adversarial Metric Learning for Zero-Shot Image Retrieval and Clustering, AAAI 2019 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/4822/4695" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper126" style="color:#0000EE;">摘要</a><br></div>
<div id="title127">
<b>127.</b> I Know the Relationships: Zero-Shot Action Recognition via Two-Stream Graph Convolutional Networks and Knowledge Graphs, AAAI 2019 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/4843/4716" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper127" style="color:#0000EE;">摘要</a><br></div>
<div id="title128">
<b>128.</b> Dual-View Ranking with Hardness Assessment for Zero-Shot Learning, AAAI 2019 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/4850/4723" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper128" style="color:#0000EE;">摘要</a><br></div>
<div id="title129">
<b>129.</b> Zero-Shot Object Detection with Textual Descriptions, AAAI 2019 <a href="https://144.208.67.177/ojs/index.php/AAAI/article/view/4891/4764" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper129" style="color:#0000EE;">摘要</a><br></div>
<div id="title130">
<b>130.</b> Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video Captioning, AAAI 2019 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/4926/4799" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper130" style="color:#0000EE;">摘要</a><br></div>
<div id="title131">
<b>131.</b> Towards Fluid Machine Intelligence: Can We Make a Gifted AI?, AAAI 2019 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/5046/4919" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper131" style="color:#0000EE;">摘要</a><br></div>
<div id="title132">
<b>132.</b> Transductive Zero-Shot Learning via Visual Center Adaptation, AAAI 2019 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/5166/5039" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper132" style="color:#0000EE;">摘要</a><br></div>
<div id="title133">
<b>133.</b> Massively Multilingual Transfer for NER, ACL 2019 <a href="https://www.aclweb.org/anthology/P19-1015.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper133" style="color:#0000EE;">摘要</a><br></div>
<div id="title134">
<b>134.</b> Cross-Domain Generalization of Neural Constituency Parsers, ACL 2019 <a href="https://www.aclweb.org/anthology/P19-1031.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper134" style="color:#0000EE;">摘要</a><br></div>
<div id="title135">
<b>135.</b> Know What You Don’t Know: Modeling a Pragmatic Speaker that Refers to Objects of Unknown Categories, ACL 2019 <a href="https://www.aclweb.org/anthology/P19-1063.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper135" style="color:#0000EE;">摘要</a><br></div>
<div id="title136">
<b>136.</b> Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems, ACL 2019 <a href="https://www.aclweb.org/anthology/P19-1078.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper136" style="color:#0000EE;">摘要</a><br></div>
<div id="title137">
<b>137.</b> A Compact and Language-Sensitive Multilingual Translation Method, ACL 2019 <a href="https://www.aclweb.org/anthology/P19-1117.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper137" style="color:#0000EE;">摘要</a><br></div>
<div id="title138">
<b>138.</b> Improved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations, ACL 2019 <a href="https://www.aclweb.org/anthology/P19-1121.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper138" style="color:#0000EE;">摘要</a><br></div>
<div id="title139">
<b>139.</b> XQA: A Cross-lingual Open-domain Question Answering Dataset, ACL 2019 <a href="https://www.aclweb.org/anthology/P19-1227.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper139" style="color:#0000EE;">摘要</a><br></div>
<div id="title140">
<b>140.</b> Zero-Shot Cross-Lingual Abstractive Sentence Summarization through Teaching Generation and Attention, ACL 2019 <a href="https://www.aclweb.org/anthology/P19-1305.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper140" style="color:#0000EE;">摘要</a><br></div>
<div id="title141">
<b>141.</b> Zero-Shot Entity Linking by Reading Entity Descriptions, ACL 2019 <a href="https://www.aclweb.org/anthology/P19-1335.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper141" style="color:#0000EE;">摘要</a><br></div>
<div id="title142">
<b>142.</b> A Multilingual BPE Embedding Space for Universal Sentiment Lexicon Induction, ACL 2019 <a href="https://www.aclweb.org/anthology/P19-1341.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper142" style="color:#0000EE;">摘要</a><br></div>
<div id="title143">
<b>143.</b> Towards Language Agnostic Universal Representations, ACL 2019 <a href="https://www.aclweb.org/anthology/P19-1395.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper143" style="color:#0000EE;">摘要</a><br></div>
<div id="title144">
<b>144.</b> Zero-Shot Semantic Parsing for Instructions, ACL 2019 <a href="https://www.aclweb.org/anthology/P19-1438.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper144" style="color:#0000EE;">摘要</a><br></div>
<div id="title145">
<b>145.</b> How Multilingual is Multilingual BERT?, ACL 2019 <a href="https://www.aclweb.org/anthology/P19-1493.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper145" style="color:#0000EE;">摘要</a><br></div>
<div id="title146">
<b>146.</b> Label-Agnostic Sequence Labeling by Copying Nearest Neighbors, ACL 2019 <a href="https://www.aclweb.org/anthology/P19-1533.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper146" style="color:#0000EE;">摘要</a><br></div>
<div id="title147">
<b>147.</b> Robust Zero-Shot Cross-Domain Slot Filling with Example Values, ACL 2019 <a href="https://www.aclweb.org/anthology/P19-1547.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper147" style="color:#0000EE;">摘要</a><br></div>
<div id="title148">
<b>148.</b> Zero-shot Word Sense Disambiguation using Sense Definition Embeddings, ACL 2019 <a href="https://www.aclweb.org/anthology/P19-1568.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper148" style="color:#0000EE;">摘要</a><br></div>
<div id="title149">
<b>149.</b> Large-Scale Multi-Label Text Classification on EU Legislation, ACL 2019 <a href="https://www.aclweb.org/anthology/P19-1636.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper149" style="color:#0000EE;">摘要</a><br></div>
<div id="title150">
<b>150.</b> From Bilingual to Multilingual Neural Machine Translation by Incremental Training, ACL 2019 <a href="https://www.aclweb.org/anthology/P19-2033.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper150" style="color:#0000EE;">摘要</a><br></div>
<div id="title151">
<b>151.</b> Multilingual NMT with a Language-Independent Attention Bridge, ACL 2019 <a href="https://www.aclweb.org/anthology/W19-4305.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper151" style="color:#0000EE;">摘要</a><br></div>
<div id="title152">
<b>152.</b> Specializing Distributional Vectors of All Words for Lexical Entailment, ACL 2019 <a href="https://www.aclweb.org/anthology/W19-4310.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper152" style="color:#0000EE;">摘要</a><br></div>
<div id="title153">
<b>153.</b> Best Practices for Learning Domain-Specific Cross-Lingual Embeddings, ACL 2019 <a href="https://www.aclweb.org/anthology/W19-4327.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper153" style="color:#0000EE;">摘要</a><br></div>
<div id="title154">
<b>154.</b> Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model, ACL 2019 <a href="https://www.aclweb.org/anthology/W19-4330.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper154" style="color:#0000EE;">摘要</a><br></div>
<div id="title155">
<b>155.</b> Improving Zero-shot Translation with Language-Independent Constraints, ACL 2019 <a href="https://www.aclweb.org/anthology/W19-5202.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper155" style="color:#0000EE;">摘要</a><br></div>
<div id="title156">
<b>156.</b> Evaluating the Supervised and Zero-shot Performance of Multi-lingual Translation Models, ACL 2019 <a href="https://www.aclweb.org/anthology/W19-5319.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper156" style="color:#0000EE;">摘要</a><br></div>
<div id="title157">
<b>157.</b> Latent-Variable Generative Models for Data-Efficient Text Classification, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-1048.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper157" style="color:#0000EE;">摘要</a><br></div>
<div id="title158">
<b>158.</b> Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-1077.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper158" style="color:#0000EE;">摘要</a><br></div>
<div id="title159">
<b>159.</b> Pivot-based Transfer Learning for Neural Machine Translation between Non-English Languages, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-1080.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper159" style="color:#0000EE;">摘要</a><br></div>
<div id="title160">
<b>160.</b> Multilingual Neural Machine Translation with Language Clustering, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-1089.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper160" style="color:#0000EE;">摘要</a><br></div>
<div id="title161">
<b>161.</b> Zero-shot Cross-lingual Dialogue Systems with Transferable Latent Variables, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-1129.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper161" style="color:#0000EE;">摘要</a><br></div>
<div id="title162">
<b>162.</b> DyKgChat: Benchmarking Dialogue Generation Grounding on Dynamic Knowledge Graphs, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-1194.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper162" style="color:#0000EE;">摘要</a><br></div>
<div id="title163">
<b>163.</b> 75 Languages, 1 Model: Parsing Universal Dependencies Universally, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-1279.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper163" style="color:#0000EE;">摘要</a><br></div>
<div id="title164">
<b>164.</b> Towards Zero-shot Language Modeling, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-1288.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper164" style="color:#0000EE;">摘要</a><br></div>
<div id="title165">
<b>165.</b> Out-of-Domain Detection for Low-Resource Text Classification Tasks, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-1364.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper165" style="color:#0000EE;">摘要</a><br></div>
<div id="title166">
<b>166.</b> Global Reasoning over Database Structures for Text-to-SQL Parsing, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-1378.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper166" style="color:#0000EE;">摘要</a><br></div>
<div id="title167">
<b>167.</b> Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-1404.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper167" style="color:#0000EE;">摘要</a><br></div>
<div id="title168">
<b>168.</b> Reconstructing Capsule Networks for Zero-shot Intent Classification, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-1486.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper168" style="color:#0000EE;">摘要</a><br></div>
<div id="title169">
<b>169.</b> Open Domain Web Keyphrase Extraction Beyond Language Modeling, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-1521.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper169" style="color:#0000EE;">摘要</a><br></div>
<div id="title170">
<b>170.</b> MultiFiT: Efficient Multi-lingual Language Model Fine-tuning, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-1572.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper170" style="color:#0000EE;">摘要</a><br></div>
<div id="title171">
<b>171.</b> Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-1575.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper171" style="color:#0000EE;">摘要</a><br></div>
<div id="title172">
<b>172.</b> Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-1607.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper172" style="color:#0000EE;">摘要</a><br></div>
<div id="title173">
<b>173.</b> Zero-Resource Neural Machine Translation with Monolingual Pivot Data, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-5610.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper173" style="color:#0000EE;">摘要</a><br></div>
<div id="title174">
<b>174.</b> Domain Adaptation with BERT-based Domain Classification and Data Selection, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-6109.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper174" style="color:#0000EE;">摘要</a><br></div>
<div id="title175">
<b>175.</b> Few-Shot and Zero-Shot Learning for Historical Text Normalization, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-6112.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper175" style="color:#0000EE;">摘要</a><br></div>
<div id="title176">
<b>176.</b> From Monolingual to Multilingual FAQ Assistant using Multilingual Co-training, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-6113.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper176" style="color:#0000EE;">摘要</a><br></div>
<div id="title177">
<b>177.</b> X-WikiRE: A Large, Multilingual Resource for Relation Extraction as Machine Comprehension, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-6130.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper177" style="color:#0000EE;">摘要</a><br></div>
<div id="title178">
<b>178.</b> Zero-Shot Cross-lingual Name Retrieval for Low-Resource Languages, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-6131.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper178" style="color:#0000EE;">摘要</a><br></div>
<div id="title179">
<b>179.</b> Zero-shot Dependency Parsing with Pre-trained Multilingual Sentence Representations, EMNLP 2019 <a href="https://www.aclweb.org/anthology/D19-6132.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper179" style="color:#0000EE;">摘要</a><br></div>
<div id="title180">
<b>180.</b> Dimensionality Reduction for Representing the Knowledge of Probabilistic Models, ICLR 2019 <a href="https://openreview.net/pdf?id=SygD-hCcF7" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper180" style="color:#0000EE;">摘要</a><br></div>
<div id="title181">
<b>181.</b> Efficient Amortised Bayesian Inference for Hierarchical and Nonlinear Dynamical Systems, ICML 2019 <a href="http://proceedings.mlr.press/v97/meeds19a/meeds19a.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper181" style="color:#0000EE;">摘要</a><br></div>
<div id="title182">
<b>182.</b> Zero-Shot Knowledge Distillation in Deep Networks, ICML 2019 <a href="http://proceedings.mlr.press/v97/nayak19a/nayak19a.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper182" style="color:#0000EE;">摘要</a><br></div>
<div id="title183">
<b>183.</b> AutoVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss, ICML 2019 <a href="http://proceedings.mlr.press/v97/qian19c/qian19c.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper183" style="color:#0000EE;">摘要</a><br></div>
<div id="title184">
<b>184.</b> Context-Aware Zero-Shot Learning for Object Recognition, ICML 2019 <a href="http://proceedings.mlr.press/v97/zablocki19a/zablocki19a.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper184" style="color:#0000EE;">摘要</a><br></div>
<div id="title185">
<b>185.</b> Co-Representation Network for Generalized Zero-Shot Learning, ICML 2019 <a href="http://proceedings.mlr.press/v97/zhang19l/zhang19l.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper185" style="color:#0000EE;">摘要</a><br></div>
<div id="title186">
<b>186.</b> Learning Classifiers for Target Domain with Limited or No Labels, ICML 2019 <a href="http://proceedings.mlr.press/v97/zhu19d/zhu19d.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper186" style="color:#0000EE;">摘要</a><br></div>
<div id="title187">
<b>187.</b> Generalized Zero-Shot Vehicle Detection in Remote Sensing Imagery via Coarse-to-Fine Framework, IJCAI 2019 <a href="https://www.ijcai.org/proceedings/2019/0097.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper187" style="color:#0000EE;">摘要</a><br></div>
<div id="title188">
<b>188.</b> Zero-shot Learning with Many Classes by High-rank Deep Embedding Networks, IJCAI 2019 <a href="https://www.ijcai.org/proceedings/2019/0337.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper188" style="color:#0000EE;">摘要</a><br></div>
<div id="title189">
<b>189.</b> Landmark Selection for Zero-shot Learning, IJCAI 2019 <a href="https://www.ijcai.org/proceedings/2019/0338.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper189" style="color:#0000EE;">摘要</a><br></div>
<div id="title190">
<b>190.</b> Graph and Autoencoder Based Feature Extraction for Zero-shot Learning, IJCAI 2019 <a href="https://www.ijcai.org/proceedings/2019/0421.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper190" style="color:#0000EE;">摘要</a><br></div>
<div id="title191">
<b>191.</b> Learning Image-Specific Attributes by Hyperbolic Neighborhood Graph Propagation, IJCAI 2019 <a href="https://www.ijcai.org/proceedings/2019/0554.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper191" style="color:#0000EE;">摘要</a><br></div>
<div id="title192">
<b>192.</b> Zero-shot Metric Learning, IJCAI 2019 <a href="https://www.ijcai.org/proceedings/2019/0555.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper192" style="color:#0000EE;">摘要</a><br></div>
<div id="title193">
<b>193.</b> Phylogenic Multi-Lingual Dependency Parsing, NAACL 2019 <a href="https://www.aclweb.org/anthology/N19-1017.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper193" style="color:#0000EE;">摘要</a><br></div>
<div id="title194">
<b>194.</b> Description-Based Zero-shot Fine-Grained Entity Typing, NAACL 2019 <a href="https://www.aclweb.org/anthology/N19-1087.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper194" style="color:#0000EE;">摘要</a><br></div>
<div id="title195">
<b>195.</b> Generating Token-Level Explanations for Natural Language Inference, NAACL 2019 <a href="https://www.aclweb.org/anthology/N19-1101.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper195" style="color:#0000EE;">摘要</a><br></div>
<div id="title196">
<b>196.</b> Integrating Semantic Knowledge to Tackle Zero-shot Text Classification, NAACL 2019 <a href="https://www.aclweb.org/anthology/N19-1108.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper196" style="color:#0000EE;">摘要</a><br></div>
<div id="title197">
<b>197.</b> Consistency by Agreement in Zero-Shot Neural Machine Translation, NAACL 2019 <a href="https://www.aclweb.org/anthology/N19-1121.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper197" style="color:#0000EE;">摘要</a><br></div>
<div id="title198">
<b>198.</b> Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing, NAACL 2019 <a href="https://www.aclweb.org/anthology/N19-1162.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper198" style="color:#0000EE;">摘要</a><br></div>
<div id="title199">
<b>199.</b> Measuring Immediate Adaptation Performance for Neural Machine Translation, NAACL 2019 <a href="https://www.aclweb.org/anthology/N19-1206.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper199" style="color:#0000EE;">摘要</a><br></div>
<div id="title200">
<b>200.</b> Zero-Shot Cross-Lingual Opinion Target Extraction, NAACL 2019 <a href="https://www.aclweb.org/anthology/N19-1257.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper200" style="color:#0000EE;">摘要</a><br></div>
<div id="title201">
<b>201.</b> Cross-lingual Annotation Projection Is Effective for Neural Part-of-Speech Tagging, NAACL 2019 <a href="https://www.aclweb.org/anthology/W19-1425.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper201" style="color:#0000EE;">摘要</a><br></div>
<div id="title202">
<b>202.</b> Extreme Multi-Label Legal Text Classification: A Case Study in EU Legislation, NAACL 2019 <a href="https://www.aclweb.org/anthology/W19-2209.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper202" style="color:#0000EE;">摘要</a><br></div>
<div id="title203">
<b>203.</b> Dataset Mention Extraction and Classification, NAACL 2019 <a href="https://www.aclweb.org/anthology/W19-2604.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper203" style="color:#0000EE;">摘要</a><br></div>
<div id="title204">
<b>204.</b> Zero-shot Learning via Simultaneous Generating and Learning, NeurIPS 2019 <a href="https://papers.nips.cc/paper/8300-zero-shot-learning-via-simultaneous-generating-and-learning.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper204" style="color:#0000EE;">摘要</a><br></div>
<div id="title205">
<b>205.</b> Zero-Shot Semantic Segmentation, NeurIPS 2019 <a href="https://papers.nips.cc/paper/8338-zero-shot-semantic-segmentation.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper205" style="color:#0000EE;">摘要</a><br></div>
<div id="title206">
<b>206.</b> Dual Adversarial Semantics-Consistent Network for Generalized Zero-Shot Learning, NeurIPS 2019 <a href="https://papers.nips.cc/paper/8846-dual-adversarial-semantics-consistent-network-for-generalized-zero-shot-learning.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper206" style="color:#0000EE;">摘要</a><br></div>
<div id="title207">
<b>207.</b> Non-Stationary Markov Decision Processes, a Worst-Case Approach using Model-Based Reinforcement Learning, NeurIPS 2019 <a href="https://papers.nips.cc/paper/8942-non-stationary-markov-decision-processes-a-worst-case-approach-using-model-based-reinforcement-learning.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper207" style="color:#0000EE;">摘要</a><br></div>
<div id="title208">
<b>208.</b> A Structured Prediction Approach for Generalization in Cooperative Multi-Agent Reinforcement Learning, NeurIPS 2019 <a href="https://papers.nips.cc/paper/9024-a-structured-prediction-approach-for-generalization-in-cooperative-multi-agent-reinforcement-learning.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper208" style="color:#0000EE;">摘要</a><br></div>
<div id="title209">
<b>209.</b> Zero-shot Knowledge Transfer via Adversarial Belief Matching, NeurIPS 2019 <a href="https://papers.nips.cc/paper/9151-zero-shot-knowledge-transfer-via-adversarial-belief-matching.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper209" style="color:#0000EE;">摘要</a><br></div>
<div id="title210">
<b>210.</b> Transductive Zero-Shot Learning with Visual Structure Constraint, NeurIPS 2019 <a href="https://papers.nips.cc/paper/9188-transductive-zero-shot-learning-with-visual-structure-constraint.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper210" style="color:#0000EE;">摘要</a><br></div>
<div id="title211">
<b>211.</b> Semantic-Guided Multi-Attention Localization for Zero-Shot Learning, NeurIPS 2019 <a href="https://papers.nips.cc/paper/9632-semantic-guided-multi-attention-localization-for-zero-shot-learning.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper211" style="color:#0000EE;">摘要</a><br></div>
<div id="title212">
<b>212.</b> Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond, TACL 2019 <a href="https://www.aclweb.org/anthology/Q19-1038.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper212" style="color:#0000EE;">摘要</a><br></div>
<div id="title213">
<b>213.</b> Adversarial Zero-shot Learning With Semantic Augmentation, AAAI 2018 <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16805/15965" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper213" style="color:#0000EE;">摘要</a><br></div>
<div id="title214">
<b>214.</b> Joint Dictionaries for Zero-Shot Learning, AAAI 2018 <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16404/16723" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper214" style="color:#0000EE;">摘要</a><br></div>
<div id="title215">
<b>215.</b> FiLM: Visual Reasoning with a General Conditioning Layer, AAAI 2018 <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16528/16646" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper215" style="color:#0000EE;">摘要</a><br></div>
<div id="title216">
<b>216.</b> Zero-Shot Learning via Class-Conditioned Deep Generative Models, AAAI 2018 <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16087/16709" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper216" style="color:#0000EE;">摘要</a><br></div>
<div id="title217">
<b>217.</b> Generalizing and Improving Bilingual Word Embedding Mappings with a Multi-Step Framework of Linear Transformations, AAAI 2018 <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16935/16781" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper217" style="color:#0000EE;">摘要</a><br></div>
<div id="title218">
<b>218.</b> Neural Cross-Lingual Entity Linking, AAAI 2018 <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16501/16101" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper218" style="color:#0000EE;">摘要</a><br></div>
<div id="title219">
<b>219.</b> Zero-Shot Learning With Attribute Selection, AAAI 2018 <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16350/16272" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper219" style="color:#0000EE;">摘要</a><br></div>
<div id="title220">
<b>220.</b> Deep Semantic Structural Constraints for Zero-Shot Learning, AAAI 2018 <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16309/16294" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper220" style="color:#0000EE;">摘要</a><br></div>
<div id="title221">
<b>221.</b> Visual Relationship Detection With Deep Structural Ranking, AAAI 2018 <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16491/16300" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper221" style="color:#0000EE;">摘要</a><br></div>
<div id="title222">
<b>222.</b> Towards Affordable Semantic Searching: Zero-Shot Retrieval via Dominant Attributes, AAAI 2018 <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16626/16314" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper222" style="color:#0000EE;">摘要</a><br></div>
<div id="title223">
<b>223.</b> HCVRD: A Benchmark for Large-Scale Human-Centered Visual Relationship Detection, AAAI 2018 <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16444/16362" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper223" style="color:#0000EE;">摘要</a><br></div>
<div id="title224">
<b>224.</b> Zero-shot Learning of Classifiers from Natural Language Quantification, ACL 2018 <a href="https://www.aclweb.org/anthology/P18-1029.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper224" style="color:#0000EE;">摘要</a><br></div>
<div id="title225">
<b>225.</b> Zero-Shot Transfer Learning for Event Extraction, ACL 2018 <a href="https://www.aclweb.org/anthology/P18-1201.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper225" style="color:#0000EE;">摘要</a><br></div>
<div id="title226">
<b>226.</b> A Deep Relevance Model for Zero-Shot Document Filtering, ACL 2018 <a href="https://www.aclweb.org/anthology/P18-1214.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper226" style="color:#0000EE;">摘要</a><br></div>
<div id="title227">
<b>227.</b> Learning Simplifications for Specific Target Audiences, ACL 2018 <a href="https://www.aclweb.org/anthology/P18-2113.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper227" style="color:#0000EE;">摘要</a><br></div>
<div id="title228">
<b>228.</b> A Comparison of Transformer and Recurrent Neural Networks on Multilingual Neural Machine Translation, COLING 2018 <a href="https://www.aclweb.org/anthology/C18-1054.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper228" style="color:#0000EE;">摘要</a><br></div>
<div id="title229">
<b>229.</b> Multilingual Neural Machine Translation with Task-Specific Attention, COLING 2018 <a href="https://www.aclweb.org/anthology/C18-1263.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper229" style="color:#0000EE;">摘要</a><br></div>
<div id="title230">
<b>230.</b> Adversarial Propagation and Zero-Shot Cross-Lingual Transfer of Word Vector Specialization, EMNLP 2018 <a href="https://www.aclweb.org/anthology/D18-1026.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper230" style="color:#0000EE;">摘要</a><br></div>
<div id="title231">
<b>231.</b> Contextual Parameter Generation for Universal Neural Machine Translation, EMNLP 2018 <a href="https://www.aclweb.org/anthology/D18-1039.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper231" style="color:#0000EE;">摘要</a><br></div>
<div id="title232">
<b>232.</b> Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing, EMNLP 2018 <a href="https://www.aclweb.org/anthology/D18-1190.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper232" style="color:#0000EE;">摘要</a><br></div>
<div id="title233">
<b>233.</b> Zero-Shot Open Entity Typing as Type-Compatible Grounding, EMNLP 2018 <a href="https://www.aclweb.org/anthology/D18-1231.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper233" style="color:#0000EE;">摘要</a><br></div>
<div id="title234">
<b>234.</b> Joint Multilingual Supervision for Cross-lingual Entity Linking, EMNLP 2018 <a href="https://www.aclweb.org/anthology/D18-1270.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper234" style="color:#0000EE;">摘要</a><br></div>
<div id="title235">
<b>235.</b> Zero-shot User Intent Detection via Capsule Neural Networks, EMNLP 2018 <a href="https://www.aclweb.org/anthology/D18-1348.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper235" style="color:#0000EE;">摘要</a><br></div>
<div id="title236">
<b>236.</b> Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces, EMNLP 2018 <a href="https://www.aclweb.org/anthology/D18-1352.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper236" style="color:#0000EE;">摘要</a><br></div>
<div id="title237">
<b>237.</b> Zero-shot Relation Classification as Textual Entailment, EMNLP 2018 <a href="https://www.aclweb.org/anthology/W18-5511.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper237" style="color:#0000EE;">摘要</a><br></div>
<div id="title238">
<b>238.</b> A neural interlingua for multilingual machine translation, EMNLP 2018 <a href="https://www.aclweb.org/anthology/W18-6309.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper238" style="color:#0000EE;">摘要</a><br></div>
<div id="title239">
<b>239.</b> Zero-Shot Visual Imitation, ICLR 2018 <a href="https://openreview.net/pdf?id=BkisuzWRW" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper239" style="color:#0000EE;">摘要</a><br></div>
<div id="title240">
<b>240.</b> Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models, ICLR 2018 <a href="https://openreview.net/pdf?id=Sy8XvGb0-" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper240" style="color:#0000EE;">摘要</a><br></div>
<div id="title241">
<b>241.</b> NerveNet: Learning Structured Policy with Graph Neural Networks, ICLR 2018 <a href="https://openreview.net/pdf?id=S1sqHMZCb" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper241" style="color:#0000EE;">摘要</a><br></div>
<div id="title242">
<b>242.</b> Interactive Grounded Language Acquisition and Generalization in a 2D World, ICLR 2018 <a href="https://openreview.net/pdf?id=H1UOm4gA-" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper242" style="color:#0000EE;">摘要</a><br></div>
<div id="title243">
<b>243.</b> Compositional Obverter Communication Learning from Raw Visual Input, ICLR 2018 <a href="https://openreview.net/pdf?id=rknt2Be0-" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper243" style="color:#0000EE;">摘要</a><br></div>
<div id="title244">
<b>244.</b> Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks, ICML 2018 <a href="http://proceedings.mlr.press/v80/lake18a/lake18a.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper244" style="color:#0000EE;">摘要</a><br></div>
<div id="title245">
<b>245.</b> MSplit LBI: Realizing Feature Selection and Dense Estimation Simultaneously in Few-shot and Zero-shot Learning, ICML 2018 <a href="http://proceedings.mlr.press/v80/zhao18c/zhao18c.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper245" style="color:#0000EE;">摘要</a><br></div>
<div id="title246">
<b>246.</b> Dual Adversarial Networks for Zero-shot Cross-media Retrieval, IJCAI 2018 <a href="https://www.ijcai.org/proceedings/2018/0092.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper246" style="color:#0000EE;">摘要</a><br></div>
<div id="title247">
<b>247.</b> Visual Data Synthesis via GAN for Zero-Shot Video Classification, IJCAI 2018 <a href="https://www.ijcai.org/proceedings/2018/0157.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper247" style="color:#0000EE;">摘要</a><br></div>
<div id="title248">
<b>248.</b> Zero Shot Learning via Low-rank Embedded Semantic AutoEncoder, IJCAI 2018 <a href="https://www.ijcai.org/proceedings/2018/0345.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper248" style="color:#0000EE;">摘要</a><br></div>
<div id="title249">
<b>249.</b> Adaptive Graph Guided Embedding for Multi-label Annotation, IJCAI 2018 <a href="https://www.ijcai.org/proceedings/2018/0388.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper249" style="color:#0000EE;">摘要</a><br></div>
<div id="title250">
<b>250.</b> Co-training Embeddings of Knowledge Graphs and Entity Descriptions for Cross-lingual Entity Alignment, IJCAI 2018 <a href="https://www.ijcai.org/proceedings/2018/0556.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper250" style="color:#0000EE;">摘要</a><br></div>
<div id="title251">
<b>251.</b> Implicit Non-linear Similarity Scoring for Recognizing Unseen Classes, IJCAI 2018 <a href="https://www.ijcai.org/proceedings/2018/0680.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper251" style="color:#0000EE;">摘要</a><br></div>
<div id="title252">
<b>252.</b> Robust Multi-view Representation: A Unified Perspective from Multi-view Learning to Domain Adaption, IJCAI 2018 <a href="https://www.ijcai.org/proceedings/2018/0767.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper252" style="color:#0000EE;">摘要</a><br></div>
<div id="title253">
<b>253.</b> Zero-Shot Question Generation from Knowledge Graphs for Unseen Predicates and Entity Types, NAACL 2018 <a href="https://www.aclweb.org/anthology/N18-1020.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper253" style="color:#0000EE;">摘要</a><br></div>
<div id="title254">
<b>254.</b> Zero-Shot Sequence Labeling: Transferring Knowledge from Sentences to Tokens, NAACL 2018 <a href="https://www.aclweb.org/anthology/N18-1027.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper254" style="color:#0000EE;">摘要</a><br></div>
<div id="title255">
<b>255.</b> Universal Neural Machine Translation for Extremely Low Resource Languages, NAACL 2018 <a href="https://www.aclweb.org/anthology/N18-1032.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper255" style="color:#0000EE;">摘要</a><br></div>
<div id="title256">
<b>256.</b> Domain-Invariant Projection Learning for Zero-Shot Recognition, NeurIPS 2018 <a href="https://papers.nips.cc/paper/7380-domain-invariant-projection-learning-for-zero-shot-recognition.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper256" style="color:#0000EE;">摘要</a><br></div>
<div id="title257">
<b>257.</b> Generalized Zero-Shot Learning with Deep Calibration Network, NeurIPS 2018 <a href="https://papers.nips.cc/paper/7471-generalized-zero-shot-learning-with-deep-calibration-network.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper257" style="color:#0000EE;">摘要</a><br></div>
<div id="title258">
<b>258.</b> Zero-Shot Transfer with Deictic Object-Oriented Representation in Reinforcement Learning, NeurIPS 2018 <a href="https://papers.nips.cc/paper/7497-zero-shot-transfer-with-deictic-object-oriented-representation-in-reinforcement-learning.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper258" style="color:#0000EE;">摘要</a><br></div>
<div id="title259">
<b>259.</b> Supervising Unsupervised Learning, NeurIPS 2018 <a href="https://papers.nips.cc/paper/7747-supervising-unsupervised-learning.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper259" style="color:#0000EE;">摘要</a><br></div>
<div id="title260">
<b>260.</b> Stacked Semantics-Guided Attention Model for Fine-Grained Zero-Shot Learning, NeurIPS 2018 <a href="https://papers.nips.cc/paper/7839-stacked-semantics-guided-attention-model-for-fine-grained-zero-shot-learning.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper260" style="color:#0000EE;">摘要</a><br></div>
<div id="title261">
<b>261.</b> Hierarchical Reinforcement Learning for Zero-shot Generalization with Subtask Dependencies, NeurIPS 2018 <a href="https://papers.nips.cc/paper/7946-hierarchical-reinforcement-learning-for-zero-shot-generalization-with-subtask-dependencies.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper261" style="color:#0000EE;">摘要</a><br></div>
<div id="title262">
<b>262.</b> Hardware Conditioned Policies for Multi-Robot Transfer Learning, NeurIPS 2018 <a href="https://papers.nips.cc/paper/8145-hardware-conditioned-policies-for-multi-robot-transfer-learning.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper262" style="color:#0000EE;">摘要</a><br></div>
<div id="title263">
<b>263.</b> Transfer of Deep Reactive Policies for MDP Planning, NeurIPS 2018 <a href="https://papers.nips.cc/paper/8293-transfer-of-deep-reactive-policies-for-mdp-planning.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper263" style="color:#0000EE;">摘要</a><br></div>
<div id="title264">
<b>264.</b> Learning Attributes from the Crowdsourced Relative Labels, AAAI 2017 <a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14260/13951" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper264" style="color:#0000EE;">摘要</a><br></div>
<div id="title265">
<b>265.</b> DECK: Discovering Event Composition Knowledge from Web Images for Zero-Shot Event Detection and Recounting in Videos, AAAI 2017 <a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14624/14277" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper265" style="color:#0000EE;">摘要</a><br></div>
<div id="title266">
<b>266.</b> Zero-Shot Recognition via Direct Classifier Learning with Transferred Samples and Pseudo Labels, AAAI 2017 <a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14160/14281" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper266" style="color:#0000EE;">摘要</a><br></div>
<div id="title267">
<b>267.</b> Problems in Large-Scale Image Classification, AAAI 2017 <a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14167/14007" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper267" style="color:#0000EE;">摘要</a><br></div>
<div id="title268">
<b>268.</b> Transfer of Knowledge through Collective Learning, AAAI 2017 <a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14698/14013" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper268" style="color:#0000EE;">摘要</a><br></div>
<div id="title269">
<b>269.</b> Obtaining referential word meanings from visual and distributional information: Experiments on object naming, ACL 2017 <a href="https://www.aclweb.org/anthology/P17-1023.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper269" style="color:#0000EE;">摘要</a><br></div>
<div id="title270">
<b>270.</b> One-Shot Neural Cross-Lingual Transfer for Paradigm Completion, ACL 2017 <a href="https://www.aclweb.org/anthology/P17-1182.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper270" style="color:#0000EE;">摘要</a><br></div>
<div id="title271">
<b>271.</b> Zero-Shot Activity Recognition with Verb Attribute Induction, EMNLP 2017 <a href="https://www.aclweb.org/anthology/D17-1099.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper271" style="color:#0000EE;">摘要</a><br></div>
<div id="title272">
<b>272.</b> Knowledge Distillation for Bilingual Dictionary Induction, EMNLP 2017 <a href="https://www.aclweb.org/anthology/D17-1264.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper272" style="color:#0000EE;">摘要</a><br></div>
<div id="title273">
<b>273.</b> DARLA: Improving Zero-Shot Transfer in Reinforcement Learning, ICML 2017 <a href="http://proceedings.mlr.press/v70/higgins17a/higgins17a.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper273" style="color:#0000EE;">摘要</a><br></div>
<div id="title274">
<b>274.</b> Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics, ICML 2017 <a href="http://proceedings.mlr.press/v70/kansky17a/kansky17a.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper274" style="color:#0000EE;">摘要</a><br></div>
<div id="title275">
<b>275.</b> Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning, ICML 2017 <a href="http://proceedings.mlr.press/v70/oh17a/oh17a.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper275" style="color:#0000EE;">摘要</a><br></div>
<div id="title276">
<b>276.</b> SitNet: Discrete Similarity Transfer Network for Zero-shot Hashing, IJCAI 2017 <a href="https://www.ijcai.org/proceedings/2017/0245.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper276" style="color:#0000EE;">摘要</a><br></div>
<div id="title277">
<b>277.</b> Synthesizing Samples for Zero-shot Learning, IJCAI 2017 <a href="https://www.ijcai.org/proceedings/2017/0246.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper277" style="color:#0000EE;">摘要</a><br></div>
<div id="title278">
<b>278.</b> Ordinal Zero-Shot Learning, IJCAI 2017 <a href="https://www.ijcai.org/proceedings/2017/0266.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper278" style="color:#0000EE;">摘要</a><br></div>
<div id="title279">
<b>279.</b> Boosted Zero-Shot Learning with Semantic Correlation Regularization, IJCAI 2017 <a href="https://www.ijcai.org/proceedings/2017/0362.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper279" style="color:#0000EE;">摘要</a><br></div>
<div id="title280">
<b>280.</b> Prototypical Networks for Few-shot Learning, NeurIPS 2017 <a href="https://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper280" style="color:#0000EE;">摘要</a><br></div>
<div id="title281">
<b>281.</b> Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation, TACL 2017 <a href="https://www.aclweb.org/anthology/Q17-1024.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper281" style="color:#0000EE;">摘要</a><br></div>
<div id="title282">
<b>282.</b> All-in Text: Learning Document, Label, and Word Representations Jointly, AAAI 2016 <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12058/11827" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper282" style="color:#0000EE;">摘要</a><br></div>
<div id="title283">
<b>283.</b> Relational Knowledge Transfer for Zero-Shot Learning, AAAI 2016 <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11802/11854" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper283" style="color:#0000EE;">摘要</a><br></div>
<div id="title284">
<b>284.</b> Representation Learning of Knowledge Graphs with Entity Descriptions, AAAI 2016 <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12216/12004" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper284" style="color:#0000EE;">摘要</a><br></div>
<div id="title285">
<b>285.</b> Dynamic Concept Composition for Zero-Example Event Detection, AAAI 2016 <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12499/12116" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper285" style="color:#0000EE;">摘要</a><br></div>
<div id="title286">
<b>286.</b> Zero-Shot Event Detection by Multimodal Distributional Semantic Embedding of Videos, AAAI 2016 <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12165/12118" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper286" style="color:#0000EE;">摘要</a><br></div>
<div id="title287">
<b>287.</b> Concepts Not Alone: Exploring Pairwise Relationships for Zero-Shot Video Activity Recognition, AAAI 2016 <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12259/12119" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper287" style="color:#0000EE;">摘要</a><br></div>
<div id="title288">
<b>288.</b> Transductive Zero-Shot Recognition via Shared Model Space Learning, AAAI 2016 <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11975/12120" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper288" style="color:#0000EE;">摘要</a><br></div>
<div id="title289">
<b>289.</b> Exploiting View-Specific Appearance Similarities Across Classes for Zero-Shot Pose Prediction: A Metric Learning Approach, AAAI 2016 <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12438/12125" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper289" style="color:#0000EE;">摘要</a><br></div>
<div id="title290">
<b>290.</b> Label Embedding for Zero-shot Fine-grained Named Entity Typing, COLING 2016 <a href="https://www.aclweb.org/anthology/C16-1017.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper290" style="color:#0000EE;">摘要</a><br></div>
<div id="title291">
<b>291.</b> Exploring Distributional Representations and Machine Translation for Aspect-based Cross-lingual Sentiment Classification, COLING 2016 <a href="https://www.aclweb.org/anthology/C16-1152.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper291" style="color:#0000EE;">摘要</a><br></div>
<div id="title292">
<b>292.</b> Gaussian Visual-Linguistic Embedding for Zero-Shot Recognition, EMNLP 2016 <a href="https://www.aclweb.org/anthology/D16-1089.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper292" style="color:#0000EE;">摘要</a><br></div>
<div id="title293">
<b>293.</b> Using Task Features for Zero-Shot Knowledge Transfer in Lifelong Learning, IJCAI 2016 <a href="https://www.ijcai.org/Proceedings/16/Papers/232.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper293" style="color:#0000EE;">摘要</a><br></div>
<div id="title294">
<b>294.</b> Unsupervised Learning on Neural Network Outputs: With Application in Zero-Shot Learning, IJCAI 2016 <a href="https://www.ijcai.org/Proceedings/16/Papers/485.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper294" style="color:#0000EE;">摘要</a><br></div>
<div id="title295">
<b>295.</b> Local Similarity-Aware Deep Feature Embedding, NeurIPS 2016 <a href="https://papers.nips.cc/paper/6368-local-similarity-aware-deep-feature-embedding.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper295" style="color:#0000EE;">摘要</a><br></div>
<div id="title296">
<b>296.</b> Learning Deep Parsimonious Representations, NeurIPS 2016 <a href="https://papers.nips.cc/paper/6263-learning-deep-parsimonious-representations.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper296" style="color:#0000EE;">摘要</a><br></div>
<div id="title297">
<b>297.</b> Exploring Semantic Inter-Class Relationships (SIR) for Zero-Shot Action Recognition, AAAI 2015 <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9929/9810" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper297" style="color:#0000EE;">摘要</a><br></div>
<div id="title298">
<b>298.</b> Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning, ACL 2015 <a href="https://www.aclweb.org/anthology/P15-1027.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper298" style="color:#0000EE;">摘要</a><br></div>
<div id="title299">
<b>299.</b> Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning, ACL 2015 <a href="https://www.aclweb.org/anthology/P15-1027.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper299" style="color:#0000EE;">摘要</a><br></div>
<div id="title300">
<b>300.</b> A Model of Zero-Shot Learning of Spoken Language Understanding, EMNLP 2015 <a href="https://www.aclweb.org/anthology/D15-1027.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper300" style="color:#0000EE;">摘要</a><br></div>
<div id="title301">
<b>301.</b> Image-Mediated Learning for Zero-Shot Cross-Lingual Document Retrieval, EMNLP 2015 <a href="https://www.aclweb.org/anthology/D15-1070.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper301" style="color:#0000EE;">摘要</a><br></div>
<div id="title302">
<b>302.</b> A Unified Perspective on Multi-Domain and Multi-Task Learning, ICLR 2015 <a href="https://arxiv.org/pdf/1412.7489.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper302" style="color:#0000EE;">摘要</a><br></div>
<div id="title303">
<b>303.</b> An embarrassingly simple approach to zero-shot learning, ICML 2015 <a href="http://proceedings.mlr.press/v37/romera-paredes15.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper303" style="color:#0000EE;">摘要</a><br></div>
<div id="title304">
<b>304.</b> Semantic Concept Discovery for Large-Scale Zero-Shot Event Detection, IJCAI 2015 <a href="https://www.ijcai.org/Proceedings/15/Papers/316.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper304" style="color:#0000EE;">摘要</a><br></div>
<div id="title305">
<b>305.</b> From Visual Attributes to Adjectives through Decompositional Distributional Semantics, TACL 2015 <a href="https://www.aclweb.org/anthology/Q15-1014.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper305" style="color:#0000EE;">摘要</a><br></div>
<div id="title306">
<b>306.</b> Zero-shot Entity Extraction from Web Pages, ACL 2014 <a href="https://www.aclweb.org/anthology/P14-1037.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper306" style="color:#0000EE;">摘要</a><br></div>
<div id="title307">
<b>307.</b> Zero-Shot Learning by Convex Combination of Semantic Embeddings, ICLR 2014 <a href="https://arxiv.org/pdf/1312.5650.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper307" style="color:#0000EE;">摘要</a><br></div>
<div id="title308">
<b>308.</b> Zero-Shot Learning and Clustering for Semantic Utterance Classification, ICLR 2014 <a href="https://arxiv.org/pdf/1401.0509.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper308" style="color:#0000EE;">摘要</a><br></div>
<div id="title309">
<b>309.</b> Zero-shot recognition with unreliable attributes, NeurIPS 2014 <a href="https://papers.nips.cc/paper/5290-zero-shot-recognition-with-unreliable-attributes.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper309" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>                    <!-- procjx-wenzhang2 -->                     <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins>                     <script>                         (adsbygoogle = window.adsbygoogle || []).push({});                     </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Cross-Lingual Pre-Training Based Transfer for Zero-Shot Neural Machine Translation</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/5341/5197" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: AI and the Web<br>&nbsp;&nbsp;<i>Baijun Ji, Zhirui Zhang, Xiangyu Duan, Min Zhang, Boxing Chen, Weihua Luo</i><br>
<font size="3">
Transfer learning between different language pairs has shown its effectiveness for Neural Machine Translation (NMT) in low-resource scenario. However, existing transfer methods involving a common target language are far from success in the extreme scenario of zero-shot translation, due to the language space mismatch problem between transferor (the parent model) and transferee (the child model) on the source side. To address this challenge, we propose an effective transfer learning approach based on cross-lingual pre-training. Our key idea is to make all source languages share the same feature space and thus enable a smooth transition for zero-shot translation. To this end, we introduce one monolingual pre-training method and two bilingual pre-training methods to obtain a universal encoder for different languages. Once the universal encoder is constructed, the parent model built on such encoder is trained with large-scale annotated data and then directly applied in zero-shot translation scenario. Experiments on two public datasets show that our approach significantly outperforms strong pivot-based baseline and various multilingual NMT approaches.</font>
<br>
</div>


<hr>
<div id="paper2"> <b>2. Harnessing GANs for Zero-Shot Learning of New Classes in Visual Speech Recognition</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/5649/5505" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Humans and AI<br>&nbsp;&nbsp;<i>Yaman Kumar, Dhruva Sahrawat, Shubham Maheshwari, Debanjan Mahata, Amanda Stent, Yifang Yin, Rajiv Ratn Shah, Roger Zimmermann</i><br>
<font size="3">
Visual Speech Recognition (VSR) is the process of recognizing or interpreting speech by watching the lip movements of the speaker. Recent machine learning based approaches model VSR as a classification problem; however, the scarcity of training data leads to error-prone systems with very low accuracies in predicting unseen classes. To solve this problem, we present a novel approach to zero-shot learning by generating new classes using Generative Adversarial Networks (GANs), and show how the addition of unseen class samples increases the accuracy of a VSR system by a significant margin of 27% and allows it to handle speaker-independent out-of-vocabulary phrases. We also show that our models are language agnostic and therefore capable of seamlessly generating, using English training data, videos for a new language (Hindi). To the best of our knowledge, this is the first work to show empirical evidence of the use of GANs for generating training samples of unseen classes in the domain of VSR, hence facilitating zero-shot learning. We make the added videos for new classes publicly available along with our code1.</font>
<br>
</div>


<hr>
<div id="paper3"> <b>3. Query-Driven Multi-Instance Learning</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/5836/5692" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Machine Learning<br>&nbsp;&nbsp;<i>Yen-Chi Hsu, Cheng-Yao Hong, Ming-Sui Lee, Tyng-Luh Liu</i><br>
<font size="3">
We introduce a query-driven approach (qMIL) to multi-instance learning where the queries aim to uncover the class labels embodied in a given bag of instances. Specifically, it solves a multi-instance multi-label learning (MIML) problem with a more challenging setting than the conventional one. Each MIML bag in our formulation is annotated only with a binary label indicating whether the bag contains the instance of a certain class and the query is specified by the word2vec of a class label/name. To learn a deep-net model for qMIL, we construct a network component that achieves a generalized compatibility measure for query-visual co-embedding and yields proper instance attentions to the given query. The bag representation is then formed as the attention-weighted sum of the instances' weights, and passed to the classification layer at the end of the network. In addition, the qMIL formulation is flexible for extending the network to classify unseen class labels, leading to a new technique to solve the zero-shot MIML task through an iterative querying process. Experimental results on action classification over video clips and three MIML datasets from MNIST, CIFAR10 and Scene are provided to demonstrate the effectiveness of our method.</font>
<br>
</div>


<hr>
<div id="paper4"> <b>4. Attribute Propagation Network for Graph Zero-Shot Learning</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/5923/5779" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Machine Learning<br>&nbsp;&nbsp;<i>Lu Liu, Tianyi Zhou, Guodong Long, Jing Jiang, Chengqi Zhang</i><br>
<font size="3">
The goal of zero-shot learning (ZSL) is to train a model to classify samples of classes that were not seen during training. To address this challenging task, most ZSL methods relate unseen test classes to seen(training) classes via a pre-defined set of attributes that can describe all classes in the same semantic space, so the knowledge learned on the training classes can be adapted to unseen classes. In this paper, we aim to optimize the attribute space for ZSL by training a propagation mechanism to refine the semantic attributes of each class based on its neighbors and related classes on a graph of classes. We show that the propagated attributes can produce classifiers for zero-shot classes with significantly improved performance in different ZSL settings. The graph of classes is usually free or very cheap to acquire such as WordNet or ImageNet classes. When the graph is not provided, given pre-defined semantic embeddings of the classes, we can learn a mechanism to generate the graph in an end-to-end manner along with the propagation mechanism. However, this graph-aided technique has not been well-explored in the literature. In this paper, we introduce the “attribute propagation network (APNet)”, which is composed of 1) a graph propagation model generating attribute vector for each class and 2) a parameterized nearest neighbor (NN) classifier categorizing an image to the class with the nearest attribute vector to the image's embedding. For better generalization over unseen classes, different from previous methods, we adopt a meta-learning strategy to train the propagation mechanism and the similarity metric for the NN classifier on multiple sub-graphs, each associated with a classification task over a subset of training classes. In experiments with two zero-shot learning settings and five benchmark datasets, APNet achieves either compelling performance or new state-of-the-art results.</font>
<br>
</div>


<hr>
<div id="paper5"> <b>5. Meta-Learning for Generalized Zero-Shot Learning</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6069/5925" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Machine Learning<br>&nbsp;&nbsp;<i>Vinay Kumar Verma, Dhanajit Brahma, Piyush Rai</i><br>
<font size="3">
Learning to classify unseen class samples at test time is popularly referred to as zero-shot learning (ZSL). If test samples can be from training (seen) as well as unseen classes, it is a more challenging problem due to the existence of strong bias towards seen classes. This problem is generally known as generalized zero-shot learning (GZSL). Thanks to the recent advances in generative models such as VAEs and GANs, sample synthesis based approaches have gained considerable attention for solving this problem. These approaches are able to handle the problem of class bias by synthesizing unseen class samples. However, these ZSL/GZSL models suffer due to the following key limitations: (i) Their training stage learns a class-conditioned generator using only seen class data and the training stage does not explicitly learn to generate the unseen class samples; (ii) They do not learn a generic optimal parameter which can easily generalize for both seen and unseen class generation; and (iii) If we only have access to a very few samples per seen class, these models tend to perform poorly. In this paper, we propose a meta-learning based generative model that naturally handles these limitations. The proposed model is based on integrating model-agnostic meta learning with a Wasserstein GAN (WGAN) to handle (i) and (iii), and uses a novel task distribution to handle (ii). Our proposed model yields significant improvements on standard ZSL as well as more challenging GZSL setting. In ZSL setting, our model yields 4.5%, 6.0%, 9.8%, and 27.9% relative improvements over the current state-of-the-art on CUB, AWA1, AWA2, and aPY datasets, respectively.</font>
<br>
</div>


<hr>
<div id="paper6"> <b>6. Zero-Shot Text-to-SQL Learning with Auxiliary Task</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6246/6102" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Natural Language Processing<br>&nbsp;&nbsp;<i>Shuaichen Chang, Pengfei Liu, Yun Tang, Jing Huang, Xiaodong He, Bowen Zhou</i><br>
<font size="3">
Recent years have seen great success in the use of neural seq2seq models on the text-to-SQL task. However, little work has paid attention to how these models generalize to realistic unseen data, which naturally raises a question: does this impressive performance signify a perfect generalization model, or are there still some limitations?</font>
<br>
</div>


<hr>
<div id="paper7"> <b>7. Cross-Lingual Natural Language Generation via Pre-Training</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6256/6112" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Natural Language Processing<br>&nbsp;&nbsp;<i>Zewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-Ling Mao, Heyan Huang</i><br>
<font size="3">
In this work we focus on transferring supervision signals of natural language generation (NLG) tasks between multiple languages. We propose to pretrain the encoder and the decoder of a sequence-to-sequence model under both monolingual and cross-lingual settings. The pre-training objective encourages the model to represent different languages in the shared space, so that we can conduct zero-shot cross-lingual transfer. After the pre-training procedure, we use monolingual data to fine-tune the pre-trained model on downstream NLG tasks. Then the sequence-to-sequence model trained in a single language can be directly evaluated beyond that language (i.e., accepting multi-lingual input and producing multi-lingual output). Experimental results on question generation and abstractive summarization show that our model outperforms the machine-translation-based pipeline methods for zero-shot cross-lingual generation. Moreover, cross-lingual transfer improves NLG performance of low-resource languages by leveraging rich-resource language data. Our implementation and data are available at https://github.com/CZWin32768/xnlg.</font>
<br>
</div>


<hr>
<div id="paper8"> <b>8. Two-Level Transformer and Auxiliary Coherence Modeling for Improved Text Segmentation</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6284/6140" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Natural Language Processing<br>&nbsp;&nbsp;<i>Goran Glavas, Swapna Somasundaran</i><br>
<font size="3">
Breaking down the structure of long texts into semantically coherent segments makes the texts more readable and supports downstream applications like summarization and retrieval. Starting from an apparent link between text coherence and segmentation, we introduce a novel supervised model for text segmentation with simple but explicit coherence modeling. Our model – a neural architecture consisting of two hierarchically connected Transformer networks – is a multi-task learning model that couples the sentence-level segmentation objective with the coherence objective that differentiates correct sequences of sentences from corrupt ones. The proposed model, dubbed Coherence-Aware Text Segmentation (CATS), yields state-of-the-art segmentation performance on a collection of benchmark datasets. Furthermore, by coupling CATS with cross-lingual word embeddings, we demonstrate its effectiveness in zero-shot language transfer: it can successfully segment texts in languages unseen in training.</font>
<br>
</div>


<hr>
<div id="paper9"> <b>9. Unsupervised Interlingual Semantic Representations from Sentence Embeddings for Zero-Shot Cross-Lingual Transfer</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6302/6158" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Natural Language Processing<br>&nbsp;&nbsp;<i>Channy Hong, Jaeyeon Lee, Jungkwon Lee</i><br>
<font size="3">
As numerous modern NLP models demonstrate high-performance in various tasks when trained with resource-rich language data sets such as those of English, there has been a shift in attention to the idea of applying such learning to low-resource languages via zero-shot or few-shot cross-lingual transfer. While the most prominent efforts made previously on achieving this feat entails the use of parallel corpora for sentence alignment training, we seek to generalize further by assuming plausible scenarios in which such parallel data sets are unavailable. In this work, we present a novel architecture for training interlingual semantic representations on top of sentence embeddings in a completely unsupervised manner, and demonstrate its effectiveness in zero-shot cross-lingual transfer in natural language inference task. Furthermore, we showcase a method of leveraging this framework in a few-shot scenario, and finally analyze the distributional and permutational alignment across languages of these interlingual semantic representations.</font>
<br>
</div>


<hr>
<div id="paper10"> <b>10. MA-DST: Multi-Attention-Based Scalable Dialog State Tracking</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6322/6178" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Natural Language Processing<br>&nbsp;&nbsp;<i>Adarsh Kumar, Peter Ku, Anuj Kumar Goyal, Angeliki Metallinou, Dilek Hakkani-Tür</i><br>
<font size="3">
Task oriented dialog agents provide a natural language interface for users to complete their goal. Dialog State Tracking (DST), which is often a core component of these systems, tracks the system's understanding of the user's goal throughout the conversation. To enable accurate multi-domain DST, the model needs to encode dependencies between past utterances and slot semantics and understand the dialog context, including long-range cross-domain references. We introduce a novel architecture for this task to encode the conversation history and slot semantics more robustly by using attention mechanisms at multiple granularities. In particular, we use cross-attention to model relationships between the context and slots at different semantic levels and self-attention to resolve cross-domain coreferences. In addition, our proposed architecture does not rely on knowing the domain ontologies beforehand and can also be used in a zero-shot setting for new domains or unseen slot values. Our model improves the joint goal accuracy by 5% (absolute) in the full-data setting and by up to 2% (absolute) in the zero-shot setting over the present state-of-the-art on the MultiWoZ 2.1 dataset.</font>
<br>
</div>


<hr>
<div id="paper11"> <b>11. Towards Zero-Shot Learning for Automatic Phonemic Transcription</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6341/6197" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Natural Language Processing<br>&nbsp;&nbsp;<i>Xinjian Li, Siddharth Dalmia, David R. Mortensen, Juncheng Li, Alan W. Black, Florian Metze</i><br>
<font size="3">
Automatic phonemic transcription tools are useful for low-resource language documentation. However, due to the lack of training sets, only a tiny fraction of languages have phonemic transcription tools. Fortunately, multilingual acoustic modeling provides a solution given limited audio training data. A more challenging problem is to build phonemic transcribers for languages with zero training data. The difficulty of this task is that phoneme inventories often differ between the training languages and the target language, making it infeasible to recognize unseen phonemes. In this work, we address this problem by adopting the idea of zero-shot learning. Our model is able to recognize unseen phonemes in the target language without any training data. In our model, we decompose phonemes into corresponding articulatory attributes such as vowel and consonant. Instead of predicting phonemes directly, we first predict distributions over articulatory attributes, and then compute phoneme distributions with a customized acoustic model. We evaluate our model by training it using 13 languages and testing it using 7 unseen languages. We find that it achieves 7.7% better phoneme error rate on average over a standard multilingual model.</font>
<br>
</div>


<hr>
<div id="paper12"> <b>12. Attention-Informed Mixed-Language Training for Zero-Shot Cross-Lingual Task-Oriented Dialogue Systems</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6362/6218" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Natural Language Processing<br>&nbsp;&nbsp;<i>Zihan Liu, Genta Indra Winata, Zhaojiang Lin, Peng Xu, Pascale Fung</i><br>
<font size="3">
Recently, data-driven task-oriented dialogue systems have achieved promising performance in English. However, developing dialogue systems that support low-resource languages remains a long-standing challenge due to the absence of high-quality data. In order to circumvent the expensive and time-consuming data collection, we introduce Attention-Informed Mixed-Language Training (MLT), a novel zero-shot adaptation method for cross-lingual task-oriented dialogue systems. It leverages very few task-related parallel word pairs to generate code-switching sentences for learning the inter-lingual semantics across languages. Instead of manually selecting the word pairs, we propose to extract source words based on the scores computed by the attention layer of a trained English task-related model and then generate word pairs using existing bilingual dictionaries. Furthermore, intensive experiments with different cross-lingual embeddings demonstrate the effectiveness of our approach. Finally, with very few word pairs, our model achieves significant zero-shot adaptation performance improvements in both cross-lingual dialogue state tracking and natural language understanding (i.e., intent detection and slot filling) tasks compared to the current state-of-the-art approaches, which utilize a much larger amount of bilingual data.</font>
<br>
</div>


<hr>
<div id="paper13"> <b>13. Generative Adversarial Zero-Shot Relational Learning for Knowledge Graphs</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6392/6248" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Natural Language Processing<br>&nbsp;&nbsp;<i>Pengda Qin, Xin Wang, Wenhu Chen, Chunyun Zhang, Weiran Xu, William Yang Wang</i><br>
<font size="3">
Large-scale knowledge graphs (KGs) are shown to become more important in current information systems. To expand the coverage of KGs, previous studies on knowledge graph completion need to collect adequate training instances for newly-added relations. In this paper, we consider a novel formulation, zero-shot learning, to free this cumbersome curation. For newly-added relations, we attempt to learn their semantic features from their text descriptions and hence recognize the facts of unseen relations with no examples being seen. For this purpose, we leverage Generative Adversarial Networks (GANs) to establish the connection between text and knowledge graph domain: The generator learns to generate the reasonable relation embeddings merely with noisy text descriptions. Under this setting, zero-shot learning is naturally converted to a traditional supervised classification task. Empirically, our method is model-agnostic that could be potentially applied to any version of KG embeddings, and consistently yields performance improvements on NELL and Wiki dataset.</font>
<br>
</div>


<hr>
<div id="paper14"> <b>14. Towards Scalable Multi-Domain Conversational Agents: The Schema-Guided Dialogue Dataset</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6394/6250" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Natural Language Processing<br>&nbsp;&nbsp;<i>Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, Pranav Khaitan</i><br>
<font size="3">
Virtual assistants such as Google Assistant, Alexa and Siri provide a conversational interface to a large number of services and APIs spanning multiple domains. Such systems need to support an ever-increasing number of services with possibly overlapping functionality. Furthermore, some of these services have little to no training data available. Existing public datasets for task-oriented dialogue do not sufficiently capture these challenges since they cover few domains and assume a single static ontology per domain. In this work, we introduce the the Schema-Guided Dialogue (SGD) dataset, containing over 16k multi-domain conversations spanning 16 domains. Our dataset exceeds the existing task-oriented dialogue corpora in scale, while also highlighting the challenges associated with building large-scale virtual assistants. It provides a challenging testbed for a number of tasks including language understanding, slot filling, dialogue state tracking and response generation. Along the same lines, we present a schema-guided paradigm for task-oriented dialogue, in which predictions are made over a dynamic set of intents and slots, provided as input, using their natural language descriptions. This allows a single dialogue system to easily support a large number of services and facilitates simple integration of new services without requiring additional training data. Building upon the proposed paradigm, we release a model for dialogue state tracking capable of zero-shot generalization to new APIs, while remaining competitive in the regular setting.</font>
<br>
</div>


<hr>
<div id="paper15"> <b>15. Evaluating the Cross-Lingual Effectiveness of Massively Multilingual Neural Machine Translation</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6414/6270" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Natural Language Processing<br>&nbsp;&nbsp;<i>Aditya Siddhant, Melvin Johnson, Henry Tsai, Naveen Ari, Jason Riesa, Ankur Bapna, Orhan Firat, Karthik Raman</i><br>
<font size="3">
The recently proposed massively multilingual neural machine translation (NMT) system has been shown to be capable of translating over 100 languages to and from English within a single model (Aharoni, Johnson, and Firat 2019). Its improved translation performance on low resource languages hints at potential cross-lingual transfer capability for downstream tasks. In this paper, we evaluate the cross-lingual effectiveness of representations from the encoder of a massively multilingual NMT model on 5 downstream classification and sequence labeling tasks covering a diverse set of over 50 languages. We compare against a strong baseline, multilingual BERT (mBERT) (Devlin et al. 2018), in different cross-lingual transfer learning scenarios and show gains in zero-shot transfer in 4 out of these 5 tasks.</font>
<br>
</div>


<hr>
<div id="paper16"> <b>16. Low Resource Sequence Tagging with Weak Labels</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6415/6271" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Natural Language Processing<br>&nbsp;&nbsp;<i>Edwin Simpson, Jonas Pfeiffer, Iryna Gurevych</i><br>
<font size="3">
Current methods for sequence tagging depend on large quantities of domain-specific training data, limiting their use in new, user-defined tasks with few or no annotations. While crowdsourcing can be a cheap source of labels, it often introduces errors that degrade the performance of models trained on such crowdsourced data. Another solution is to use transfer learning to tackle low resource sequence labelling, but current approaches rely heavily on similar high resource datasets in different languages. In this paper, we propose a domain adaptation method using Bayesian sequence combination to exploit pre-trained models and unreliable crowdsourced data that does not require high resource data in a different language. Our method boosts performance by learning the relationship between each labeller and the target task and trains a sequence labeller on the target domain with little or no gold-standard data. We apply our approach to labelling diagnostic classes in medical and educational case studies, showing that the model achieves strong performance though zero-shot transfer learning and is more effective than alternative ensemble methods. Using NER and information extraction tasks, we show how our approach can train a model directly from crowdsourced labels, outperforming pipeline approaches that first aggregate the crowdsourced data, then train on the aggregated labels.</font>
<br>
</div>


<hr>
<div id="paper17"> <b>17. A Comparison of Architectures and Pretraining Methods for Contextualized Multilingual Word Embeddings</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6443/6299" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Natural Language Processing<br>&nbsp;&nbsp;<i>Niels van der Heijden, Samira Abnar, Ekaterina Shutova</i><br>
<font size="3">
The lack of annotated data in many languages is a well-known challenge within the field of multilingual natural language processing (NLP). Therefore, many recent studies focus on zero-shot transfer learning and joint training across languages to overcome data scarcity for low-resource languages. In this work we (i) perform a comprehensive comparison of state-of-the-art multilingual word and sentence encoders on the tasks of named entity recognition (NER) and part of speech (POS) tagging; and (ii) propose a new method for creating multilingual contextualized word embeddings, compare it to multiple baselines and show that it performs at or above state-of-the-art level in zero-shot transfer settings. Finally, we show that our method allows for better knowledge sharing across languages in a joint training setting.</font>
<br>
</div>


<hr>
<div id="paper18"> <b>18. Deep Bayesian Nonparametric Learning of Rules and Plans from Demonstrations with a Learned Automaton Prior</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6559/6415" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Reasoning under Uncertainty<br>&nbsp;&nbsp;<i>Brandon Araki, Kiran Vodrahalli, Thomas Leech, Cristian Ioan Vasile, Mark Donahue, Daniela Rus</i><br>
<font size="3">
We introduce a method to learn imitative policies from expert demonstrations that are interpretable and manipulable. We achieve interpretability by modeling the interactions between high-level actions as an automaton with connections to formal logic. We achieve manipulability by integrating this automaton into planning, so that changes to the automaton have predictable effects on the learned behavior. These qualities allow a human user to first understand what the model has learned, and then either correct the learned behavior or zero-shot generalize to new, similar tasks. We build upon previous work by no longer requiring additional supervised information which is hard to collect in practice. We achieve this by using a deep Bayesian nonparametric hierarchical model. We test our model on several domains and also show results for a real-world implementation on a mobile robotic arm platform.</font>
<br>
</div>


<hr>
<div id="paper19"> <b>19. Detecting Human-Object Interactions via Functional Generalization</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6616/6470" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Vision<br>&nbsp;&nbsp;<i>Ankan Bansal, Sai Saketh Rambhatla, Abhinav Shrivastava, Rama Chellappa</i><br>
<font size="3">
We present an approach for detecting human-object interactions (HOIs) in images, based on the idea that humans interact with functionally similar objects in a similar manner. The proposed model is simple and efficiently uses the data, visual features of the human, relative spatial orientation of the human and the object, and the knowledge that functionally similar objects take part in similar interactions with humans. We provide extensive experimental validation for our approach and demonstrate state-of-the-art results for HOI detection. On the HICO-Det dataset our method achieves a gain of over 2.5% absolute points in mean average precision (mAP) over state-of-the-art. We also show that our approach leads to significant performance gains for zero-shot HOI detection in the seen object setting. We further demonstrate that using a generic object detector, our model can generalize to interactions involving previously unseen objects.</font>
<br>
</div>


<hr>
<div id="paper20"> <b>20. Feature Deformation Meta-Networks in Image Captioning of Novel Objects</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6620/6474" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Vision<br>&nbsp;&nbsp;<i>Tingjia Cao, Ke Han, Xiaomei Wang, Lin Ma, Yanwei Fu, Yu-Gang Jiang, Xiangyang Xue</i><br>
<font size="3">
This paper studies the task of image captioning with novel objects, which only exist in testing images. Intrinsically, this task can reflect the generalization ability of models in understanding and captioning the semantic meanings of visual concepts and objects unseen in training set, sharing the similarity to one/zero-shot learning. The critical difficulty thus comes from that no paired images and sentences of the novel objects can be used to help train the captioning model. Inspired by recent work (Chen et al. 2019b) that boosts one-shot learning by learning to generate various image deformations, we propose learning meta-networks for deforming features for novel object captioning. To this end, we introduce the feature deformation meta-networks (FDM-net), which is trained on source data, and learn to adapt to the novel object features detected by the auxiliary detection model. FDM-net includes two sub-nets: feature deformation, and scene graph sentence reconstruction, which produce the augmented image features and corresponding sentences, respectively. Thus, rather than directly deforming images, FDM-net can efficiently and dynamically enlarge the paired images and texts by learning to deform image features. Extensive experiments are conducted on the widely used novel object captioning dataset, and the results show the effectiveness of our FDM-net. Ablation study and qualitative visualization further give insights of our model.</font>
<br>
</div>


<hr>
<div id="paper21"> <b>21. Zero-Shot Ingredient Recognition by Multi-Relational Graph Convolutional Network</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6626/6480" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Vision<br>&nbsp;&nbsp;<i>Jingjing Chen, Liangming Pan, Zhipeng Wei, Xiang Wang, Chong-Wah Ngo, Tat-Seng Chua</i><br>
<font size="3">
Recognizing ingredients for a given dish image is at the core of automatic dietary assessment, attracting increasing attention from both industry and academia. Nevertheless, the task is challenging due to the difficulty of collecting and labeling sufficient training data. On one hand, there are hundred thousands of food ingredients in the world, ranging from the common to rare. Collecting training samples for all of the ingredient categories is difficult. On the other hand, as the ingredient appearances exhibit huge visual variance during the food preparation, it requires to collect the training samples under different cooking and cutting methods for robust recognition. Since obtaining sufficient fully annotated training data is not easy, a more practical way of scaling up the recognition is to develop models that are capable of recognizing unseen ingredients. Therefore, in this paper, we target the problem of ingredient recognition with zero training samples. More specifically, we introduce multi-relational GCN (graph convolutional network) that integrates ingredient hierarchy, attribute as well as co-occurrence for zero-shot ingredient recognition. Extensive experiments on both Chinese and Japanese food datasets are performed to demonstrate the superior performance of multi-relational GCN and shed light on zero-shot ingredients recognition.</font>
<br>
</div>


<hr>
<div id="paper22"> <b>22. Zero Shot Learning with the Isoperimetric Loss</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6698/6552" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Vision<br>&nbsp;&nbsp;<i>Shay Deutsch, Andrea L. Bertozzi, Stefano Soatto</i><br>
<font size="3">
We introduce the isoperimetric loss as a regularization criterion for learning the map from a visual representation to a semantic embedding, to be used to transfer knowledge to unknown classes in a zero-shot learning setting. We use a pre-trained deep neural network model as a visual representation of image data, a Word2Vec embedding of class labels, and linear maps between the visual and semantic embedding spaces. However, the spaces themselves are not linear, and we postulate the sample embedding to be populated by noisy samples near otherwise smooth manifolds. We exploit the graph structure defined by the sample points to regularize the estimates of the manifolds by inferring the graph connectivity using a generalization of the isoperimetric inequalities from Riemannian geometry to graphs. Surprisingly, this regularization alone, paired with the simplest baseline model, outperforms the state-of-the-art among fully automated methods in zero-shot learning benchmarks such as AwA and CUB. This improvement is achieved solely by learning the structure of the underlying spaces by imposing regularity.</font>
<br>
</div>


<hr>
<div id="paper23"> <b>23. Learning Cross-Aligned Latent Embeddings for Zero-Shot Cross-Modal Retrieval</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6817/6671" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Vision<br>&nbsp;&nbsp;<i>Kaiyi Lin, Xing Xu, Lianli Gao, Zheng Wang, Heng Tao Shen</i><br>
<font size="3">
Zero-Shot Cross-Modal Retrieval (ZS-CMR) is an emerging research hotspot that aims to retrieve data of new classes across different modality data. It is challenging for not only the heterogeneous distributions across different modalities, but also the inconsistent semantics across seen and unseen classes. A handful of recently proposed methods typically borrow the idea from zero-shot learning, i.e., exploiting word embeddings of class labels (i.e., class-embeddings) as common semantic space, and using generative adversarial network (GAN) to capture the underlying multimodal data structures, as well as strengthen relations between input data and semantic space to generalize across seen and unseen classes. In this paper, we propose a novel method termed Learning Cross-Aligned Latent Embeddings (LCALE) as an alternative to these GAN based methods for ZS-CMR. Unlike using the class-embeddings as the semantic space, our method seeks for a shared low-dimensional latent space of input multimodal features and class-embeddings by modality-specific variational autoencoders. Notably, we align the distributions learned from multimodal input features and from class-embeddings to construct latent embeddings that contain the essential cross-modal correlation associated with unseen classes. Effective cross-reconstruction and cross-alignment criterions are further developed to preserve class-discriminative information in latent space, which benefits the efficiency for retrieval and enable the knowledge transfer to unseen classes. We evaluate our model using four benchmark datasets on image-text retrieval tasks and one large-scale dataset on image-sketch retrieval tasks. The experimental results show that our method establishes the new state-of-the-art performance for both tasks on all datasets.</font>
<br>
</div>


<hr>
<div id="paper24"> <b>24. Zero-Shot Learning from Adversarial Feature Residual to Compact Visual Feature</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6821/6675" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Vision<br>&nbsp;&nbsp;<i>Bo Liu, Qiulei Dong, Zhanyi Hu</i><br>
<font size="3">
Recently, many zero-shot learning (ZSL) methods focused on learning discriminative object features in an embedding feature space, however, the distributions of the unseen-class features learned by these methods are prone to be partly overlapped, resulting in inaccurate object recognition. Addressing this problem, we propose a novel adversarial network to synthesize compact semantic visual features for ZSL, consisting of a residual generator, a prototype predictor, and a discriminator. The residual generator is to generate the visual feature residual, which is integrated with a visual prototype predicted via the prototype predictor for synthesizing the visual feature. The discriminator is to distinguish the synthetic visual features from the real ones extracted from an existing categorization CNN. Since the generated residuals are generally numerically much smaller than the distances among all the prototypes, the distributions of the unseen-class features synthesized by the proposed network are less overlapped. In addition, considering that the visual features from categorization CNNs are generally inconsistent with their semantic features, a simple feature selection strategy is introduced for extracting more compact semantic visual features. Extensive experimental results on six benchmark datasets demonstrate that our method could achieve a significantly better performance than existing state-of-the-art methods by ∼1.2-13.2% in most cases.</font>
<br>
</div>


<hr>
<div id="paper25"> <b>25. Context-Aware Zero-Shot Recognition</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6841/6695" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Vision<br>&nbsp;&nbsp;<i>Ruotian Luo, Ning Zhang, Bohyung Han, Linjie Yang</i><br>
<font size="3">
We present a novel problem setting in zero-shot learning, zero-shot object recognition and detection in the context. Contrary to the traditional zero-shot learning methods, which simply infers unseen categories by transferring knowledge from the objects belonging to semantically similar seen categories, we aim to understand the identity of the novel objects in an image surrounded by the known objects using the inter-object relation prior. Specifically, we leverage the visual context and the geometric relationships between all pairs of objects in a single image, and capture the information useful to infer unseen categories. We integrate our context-aware zero-shot learning framework into the traditional zero-shot learning techniques seamlessly using a Conditional Random Field (CRF). The proposed algorithm is evaluated on both zero-shot region classification and zero-shot detection tasks. The results on Visual Genome (VG) dataset show that our model significantly boosts performance with the additional visual context compared to traditional methods.</font>
<br>
</div>


<hr>
<div id="paper26"> <b>26. A Variational Autoencoder with Deep Embedding Model for Generalized Zero-Shot Learning</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6844/6698" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Vision<br>&nbsp;&nbsp;<i>Peirong Ma, Xiao Hu</i><br>
<font size="3">
Generalized zero-shot learning (GZSL) is a challenging task that aims to recognize not only unseen classes unavailable during training, but also seen classes used at training stage. It is achieved by transferring knowledge from seen classes to unseen classes via a shared semantic space (e.g. attribute space). Most existing GZSL methods usually learn a cross-modal mapping between the visual feature space and the semantic space. However, the mapping model learned only from the seen classes will produce an inherent bias when used in the unseen classes. In order to tackle such a problem, this paper integrates a deep embedding network (DE) and a modified variational autoencoder (VAE) into a novel model (DE-VAE) to learn a latent space shared by both image features and class embeddings. Specifically, the proposed model firstly employs DE to learn the mapping from the semantic space to the visual feature space, and then utilizes VAE to transform both original visual features and the features obtained by the mapping into latent features. Finally, the latent features are used to train a softmax classifier. Extensive experiments on four GZSL benchmark datasets show that the proposed model significantly outperforms the state of the arts.</font>
<br>
</div>


<hr>
<div id="paper27"> <b>27. Learning Meta Model for Zero- and Few-Shot Face Anti-Spoofing</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6866/6720" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Vision<br>&nbsp;&nbsp;<i>Yunxiao Qin, Chenxu Zhao, Xiangyu Zhu, Zezheng Wang, Zitong Yu, Tianyu Fu, Feng Zhou, Jingping Shi, Zhen Lei</i><br>
<font size="3">
Face anti-spoofing is crucial to the security of face recognition systems. Most previous methods formulate face anti-spoofing as a supervised learning problem to detect various predefined presentation attacks, which need large scale training data to cover as many attacks as possible. However, the trained model is easy to overfit several common attacks and is still vulnerable to unseen attacks. To overcome this challenge, the detector should: 1) learn discriminative features that can generalize to unseen spoofing types from predefined presentation attacks; 2) quickly adapt to new spoofing types by learning from both the predefined attacks and a few examples of the new spoofing types. Therefore, we define face anti-spoofing as a zero- and few-shot learning problem. In this paper, we propose a novel Adaptive Inner-update Meta Face Anti-Spoofing (AIM-FAS) method to tackle this problem through meta-learning. Specifically, AIM-FAS trains a meta-learner focusing on the task of detecting unseen spoofing types by learning from predefined living and spoofing faces and a few examples of new attacks. To assess the proposed approach, we propose several benchmarks for zero- and few-shot FAS. Experiments show its superior performances on the presented benchmarks to existing methods in existing zero-shot FAS protocols.</font>
<br>
</div>


<hr>
<div id="paper28"> <b>28. Improved Visual-Semantic Alignment for Zero-Shot Object Detection</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6868/6722" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Vision<br>&nbsp;&nbsp;<i>Shafin Rahman, Salman H. Khan, Nick Barnes</i><br>
<font size="3">
Zero-shot object detection is an emerging research topic that aims to recognize and localize previously ‘unseen’ objects. This setting gives rise to several unique challenges, e.g., highly imbalanced positive vs. negative instance ratio, proper alignment between visual and semantic concepts and the ambiguity between background and unseen classes. Here, we propose an end-to-end deep learning framework underpinned by a novel loss function that handles class-imbalance and seeks to properly align the visual and semantic cues for improved zero-shot learning. We call our objective the ‘Polarity loss’ because it explicitly maximizes the gap between positive and negative predictions. Such a margin maximizing formulation is not only important for visual-semantic alignment but it also resolves the ambiguity between background and unseen objects. Further, the semantic representations of objects are noisy, thus complicating the alignment between visual and semantic domains. To this end, we perform metric learning using a ‘Semantic vocabulary’ of related concepts that refines the noisy semantic embeddings and establishes a better synergy between visual and semantic domains. Our approach is inspired by the embodiment theories in cognitive science, that claim human semantic understanding to be grounded in past experiences (seen objects), related linguistic concepts (word vocabulary) and the visual perception (seen/unseen object images). Our extensive results on MS-COCO and Pascal VOC datasets show significant improvements over state of the art.1</font>
<br>
</div>


<hr>
<div id="paper29"> <b>29. Consistent Video Style Transfer via Compound Regularization</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6905/6759" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Vision<br>&nbsp;&nbsp;<i>Wenjing Wang, Jizheng Xu, Li Zhang, Yue Wang, Jiaying Liu</i><br>
<font size="3">
Recently, neural style transfer has drawn many attentions and significant progresses have been made, especially for image style transfer. However, flexible and consistent style transfer for videos remains a challenging problem. Existing training strategies, either using a significant amount of video data with optical flows or introducing single-frame regularizers, have limited performance on real videos. In this paper, we propose a novel interpretation of temporal consistency, based on which we analyze the drawbacks of existing training strategies; and then derive a new compound regularization. Experimental results show that the proposed regularization can better balance the spatial and temporal performance, which supports our modeling. Combining with the new cost formula, we design a zero-shot video style transfer framework. Moreover, for better feature migration, we introduce a new module to dynamically adjust inter-channel distributions. Quantitative and qualitative results demonstrate the superiority of our method over other state-of-the-art style transfer methods. Our project is publicly available at: https://daooshee.github.io/CompoundVST/.</font>
<br>
</div>


<hr>
<div id="paper30"> <b>30. Mining on Heterogeneous Manifolds for Zero-Shot Cross-Modal Image Retrieval</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6949/6803" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Vision<br>&nbsp;&nbsp;<i>Fan Yang, Zheng Wang, Jing Xiao, Shin'ichi Satoh</i><br>
<font size="3">
Most recent approaches for the zero-shot cross-modal image retrieval map images from different modalities into a uniform feature space to exploit their relevance by using a pre-trained model. Based on the observation that manifolds of zero-shot images are usually deformed and incomplete, we argue that the manifolds of unseen classes are inevitably distorted during the training of a two-stream model that simply maps images from different modalities into a uniform space. This issue directly leads to poor cross-modal retrieval performance. We propose a bi-directional random walk scheme to mining more reliable relationships between images by traversing heterogeneous manifolds in the feature space of each modality. Our proposed method benefits from intra-modal distributions to alleviate the interference caused by noisy similarities in the cross-modal feature space. As a result, we achieved great improvement in the performance of the thermal v.s. visible image retrieval task. The code of this paper: https://github.com/fyang93/cross-modal-retrieval</font>
<br>
</div>


<hr>
<div id="paper31"> <b>31. Zero-Shot Sketch-Based Image Retrieval via Graph Convolution Network</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6993/6847" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Vision<br>&nbsp;&nbsp;<i>Zhaolong Zhang, Yuejie Zhang, Rui Feng, Tao Zhang, Weiguo Fan</i><br>
<font size="3">
Zero-Shot Sketch-based Image Retrieval (ZS-SBIR) has been proposed recently, putting the traditional Sketch-based Image Retrieval (SBIR) under the setting of zero-shot learning. Dealing with both the challenges in SBIR and zero-shot learning makes it become a more difficult task. Previous works mainly focus on utilizing one kind of information, i.e., the visual information or the semantic information. In this paper, we propose a SketchGCN model utilizing the graph convolution network, which simultaneously considers both the visual information and the semantic information. Thus, our model can effectively narrow the domain gap and transfer the knowledge. Furthermore, we generate the semantic information from the visual information using a Conditional Variational Autoencoder rather than only map them back from the visual space to the semantic space, which enhances the generalization ability of our model. Besides, feature loss, classification loss, and semantic loss are introduced to optimize our proposed SketchGCN model. Our model gets a good performance on the challenging Sketchy and TU-Berlin datasets.</font>
<br>
</div>


<hr>
<div id="paper32"> <b>32. GTNet: Generative Transfer Network for Zero-Shot Object Detection</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6996/6850" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Vision<br>&nbsp;&nbsp;<i>Shizhen Zhao, Changxin Gao, Yuanjie Shao, Lerenhan Li, Changqian Yu, Zhong Ji, Nong Sang</i><br>
<font size="3">
We propose a Generative Transfer Network (GTNet) for zero-shot object detection (ZSD). GTNet consists of an Object Detection Module and a Knowledge Transfer Module. The Object Detection Module can learn large-scale seen domain knowledge. The Knowledge Transfer Module leverages a feature synthesizer to generate unseen class features, which are applied to train a new classification layer for the Object Detection Module. In order to synthesize features for each unseen class with both the intra-class variance and the IoU variance, we design an IoU-Aware Generative Adversarial Network (IoUGAN) as the feature synthesizer, which can be easily integrated into GTNet. Specifically, IoUGAN consists of three unit models: Class Feature Generating Unit (CFU), Foreground Feature Generating Unit (FFU), and Background Feature Generating Unit (BFU). CFU generates unseen features with the intra-class variance conditioned on the class semantic embeddings. FFU and BFU add the IoU variance to the results of CFU, yielding class-specific foreground and background features, respectively. We evaluate our method on three public datasets and the results demonstrate that our method performs favorably against the state-of-the-art ZSD approaches.</font>
<br>
</div>


<hr>
<div id="paper33"> <b>33. Motion-Attentive Transition for Zero-Shot Video Object Segmentation</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/7008/6862" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title33" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2020. AAAI Technical Track: Vision<br>&nbsp;&nbsp;<i>Tianfei Zhou, Shunzhou Wang, Yi Zhou, Yazhou Yao, Jianwu Li, Ling Shao</i><br>
<font size="3">
In this paper, we present a novel Motion-Attentive Transition Network (MATNet) for zero-shot video object segmentation, which provides a new way of leveraging motion information to reinforce spatio-temporal object representation. An asymmetric attention block, called Motion-Attentive Transition (MAT), is designed within a two-stream encoder, which transforms appearance features into motion-attentive representations at each convolutional stage. In this way, the encoder becomes deeply interleaved, allowing for closely hierarchical interactions between object motion and appearance. This is superior to the typical two-stream architecture, which treats motion and appearance separately in each stream and often suffers from overfitting to appearance information. Additionally, a bridge network is proposed to obtain a compact, discriminative and scale-sensitive representation for multi-level encoder features, which is further fed into a decoder to achieve segmentation results. Extensive experiments on three challenging public benchmarks (i.e., DAVIS-16, FBMS and Youtube-Objects) show that our model achieves compelling performance against the state-of-the-arts. Code is available at: https://github.com/tfzhou/MATNet.</font>
<br>
</div>


<hr>
<div id="paper34"> <b>34. Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain Dialogue State Tracking</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.12.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title34" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Giovanni Campagna, Agata Foryciarz, Mehrad Moradshahi, Monica Lam</i><br>
<font size="3">
Zero-shot transfer learning for multi-domain dialogue state tracking can allow us to handle new domains without incurring the high cost of data acquisition. This paper proposes new zero-short transfer learning technique for dialogue state tracking where the in-domain training data are all synthesized from an abstract dialogue model and the ontology of the domain. We show that data augmentation through synthesized data can improve the accuracy of zero-shot learning for both the TRADE model and the BERT-based SUMBT model on the MultiWOZ 2.1 dataset. We show training with only synthesized in-domain data on the SUMBT model can reach about 2/3 of the accuracy obtained with the full training dataset. We improve the zero-shot learning state of the art on average across domains by 21%.</font>
<br>
</div>


<hr>
<div id="paper35"> <b>35. Unknown Intent Detection Using Gaussian Mixture Model with an Application to Zero-shot Intent Classification</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.99.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title35" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Guangfeng Yan, Lu Fan, Qimai Li, Han Liu, Xiaotong Zhang, Xiao-Ming Wu, Albert Y.S. Lam</i><br>
<font size="3">
User intent classification plays a vital role in dialogue systems. Since user intent may frequently change over time in many realistic scenarios, unknown (new) intent detection has become an essential problem, where the study has just begun. This paper proposes a semantic-enhanced Gaussian mixture model (SEG) for unknown intent detection. In particular, we model utterance embeddings with a Gaussian mixture distribution and inject dynamic class semantic information into Gaussian means, which enables learning more class-concentrated embeddings that help to facilitate downstream outlier detection. Coupled with a density-based outlier detection algorithm, SEG achieves competitive results on three real task-oriented dialogue datasets in two languages for unknown intent detection. On top of that, we propose to integrate SEG as an unknown intent identifier into existing generalized zero-shot intent classification models to improve their performance. A case study on a state-of-the-art method, ReCapsNet, shows that SEG can push the classification performance to a significantly higher level.</font>
<br>
</div>


<hr>
<div id="paper36"> <b>36. Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.148.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title36" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Biao Zhang, Philip Williams, Ivan Titov, Rico Sennrich</i><br>
<font size="3">
Massively multilingual models for neural machine translation (NMT) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations. In this paper, we explore ways to improve them. We argue that multilingual NMT requires stronger modeling capacity to support language pairs with varying typological characteristics, and overcome this bottleneck via language-specific components and deepening NMT architectures. We identify the off-target translation issue (i.e. translating into a wrong target language) as the major source of the inferior zero-shot performance, and propose random online backtranslation to enforce the translation of unseen training language pairs. Experiments on OPUS-100 (a novel multilingual dataset with 100 languages) show that our approach substantially narrows the performance gap with bilingual models in both one-to-many and many-to-many settings, and improves zero-shot performance by ~10 BLEU, approaching conventional pivot-based methods.</font>
<br>
</div>


<hr>
<div id="paper37"> <b>37. On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.151.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title37" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Wei Zhao, Goran Glavaš, Maxime Peyrard, Yang Gao, Robert West, Steffen Eger</i><br>
<font size="3">
Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity. In this paper, we concern ourselves with reference-free machine translation (MT) evaluation where we directly compare source texts to (sometimes low-quality) system translations, which represents a natural adversarial setup for multilingual encoders. Reference-free evaluation holds the promise of web-scale comparison of MT systems. We systematically investigate a range of metrics based on state-of-the-art cross-lingual semantic representations obtained with pretrained M-BERT and LASER. We find that they perform poorly as semantic encoders for reference-free MT evaluation and identify their two key limitations, namely, (a) a semantic mismatch between representations of mutual translations and, more prominently, (b) the inability to punish “translationese”, i.e., low-quality literal translations. We propose two partial remedies: (1) post-hoc re-alignment of the vector spaces and (2) coupling of semantic-similarity based metrics with target-side language modeling. In segment-level MT evaluation, our best metric surpasses reference-based BLEU by 5.7 correlation points.</font>
<br>
</div>


<hr>
<div id="paper38"> <b>38. Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.252.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title38" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Aditya Siddhant, Ankur Bapna, Yuan Cao, Orhan Firat, Mia Chen, Sneha Kudugunta, Naveen Arivazhagan, Yonghui Wu</i><br>
<font size="3">
Over the last few years two promising research directions in low-resource neural machine translation (NMT) have emerged. The first focuses on utilizing high-resource languages to improve the quality of low-resource languages via multilingual NMT. The second direction employs monolingual data with self-supervision to pre-train translation models, followed by fine-tuning on small amounts of supervised data. In this work, we join these two lines of research and demonstrate the efficacy of monolingual data with self-supervision in multilingual NMT. We offer three major results: (i) Using monolingual data significantly boosts the translation quality of low-resource languages in multilingual models. (ii) Self-supervision improves zero-shot translation quality in multilingual models. (iii) Leveraging monolingual data with self-supervision provides a viable path towards adding new languages to multilingual models, getting up to 33 BLEU on ro-en translation without any parallel data or back-translation.</font>
<br>
</div>


<hr>
<div id="paper39"> <b>39. Zero-shot Text Classification via Reinforced Self-training</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.272.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title39" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Zhiquan Ye, Yuxia Geng, Jiaoyan Chen, Jingmin Chen, Xiaoxiao Xu, SuHang Zheng, Feng Wang, Jun Zhang, Huajun Chen</i><br>
<font size="3">
Zero-shot learning has been a tough problem since no labeled data is available for unseen classes during training, especially for classes with low similarity. In this situation, transferring from seen classes to unseen classes is extremely hard. To tackle this problem, in this paper we propose a self-training based method to efficiently leverage unlabeled data. Traditional self-training methods use fixed heuristics to select instances from unlabeled data, whose performance varies among different datasets. We propose a reinforcement learning framework to learn data selection strategy automatically and provide more reliable selection. Experimental results on both benchmarks and a real-world e-commerce dataset show that our approach significantly outperforms previous methods in zero-shot text classification</font>
<br>
</div>


<hr>
<div id="paper40"> <b>40. Structure-Level Knowledge Distillation For Multilingual Sequence Labeling</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.304.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title40" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Fei Huang, Kewei Tu</i><br>
<font size="3">
Multilingual sequence labeling is a task of predicting label sequences using a single unified model for multiple languages. Compared with relying on multiple monolingual models, using a multilingual model has the benefit of a smaller model size, easier in online serving, and generalizability to low-resource languages. However, current multilingual models still underperform individual monolingual models significantly due to model capacity limitations. In this paper, we propose to reduce the gap between monolingual models and the unified multilingual model by distilling the structural knowledge of several monolingual models (teachers) to the unified multilingual model (student). We propose two novel KD methods based on structure-level information: (1) approximately minimizes the distance between the student’s and the teachers’ structure-level probability distributions, (2) aggregates the structure-level knowledge to local distributions and minimizes the distance between two local probability distributions. Our experiments on 4 multilingual tasks with 25 datasets show that our approaches outperform several strong baselines and have stronger zero-shot generalizability than both the baseline model and teacher models.</font>
<br>
</div>


<hr>
<div id="paper41"> <b>41. Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.324.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title41" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Haipeng Sun, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita, Tiejun Zhao</i><br>
<font size="3">
Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.</font>
<br>
</div>


<hr>
<div id="paper42"> <b>42. On the Cross-lingual Transferability of Monolingual Representations</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.421.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title42" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Mikel Artetxe, Sebastian Ruder, Dani Yogatama</i><br>
<font size="3">
State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.</font>
<br>
</div>


<hr>
<div id="paper43"> <b>43. Finding Universal Grammatical Relations in Multilingual BERT</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.493.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title43" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Ethan A. Chi, John Hewitt, Christopher D. Manning</i><br>
<font size="3">
Recent work has found evidence that Multilingual BERT (mBERT), a transformer-based multilingual masked language model, is capable of zero-shot cross-lingual transfer, suggesting that some aspects of its representations are shared cross-lingually. To better understand this overlap, we extend recent work on finding syntactic trees in neural networks’ internal representations to the multilingual setting. We show that subspaces of mBERT representations recover syntactic tree distances in languages other than English, and that these subspaces are approximately shared across languages. Motivated by these results, we present an unsupervised analysis method that provides evidence mBERT learns representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies taxonomy. This evidence suggests that even without explicit supervision, multilingual masked language models learn certain linguistic universals.</font>
<br>
</div>


<hr>
<div id="paper44"> <b>44. Multi-Cell Compositional LSTM for NER Domain Adaptation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.524.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title44" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Chen Jia, Yue Zhang</i><br>
<font size="3">
Cross-domain NER is a challenging yet practical problem. Entity mentions can be highly different across domains. However, the correlations between entity types can be relatively more stable across domains. We investigate a multi-cell compositional LSTM structure for multi-task learning, modeling each entity type using a separate cell state. With the help of entity typed units, cross-domain knowledge transfer can be made in an entity type level. Theoretically, the resulting distinct feature distributions for each entity type make it more powerful for cross-domain transfer. Empirically, experiments on four few-shot and zero-shot datasets show our method significantly outperforms a series of multi-task learning methods and achieves the best results.</font>
<br>
</div>


<hr>
<div id="paper45"> <b>45. Language to Network: Conditional Parameter Adaptation with Natural Language Descriptions</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.625.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title45" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Tian Jin, Zhun Liu, Shengjia Yan, Alexandre Eichenberger, Louis-Philippe Morency</i><br>
<font size="3">
Transfer learning using ImageNet pre-trained models has been the de facto approach in a wide range of computer vision tasks. However, fine-tuning still requires task-specific training data. In this paper, we propose N3 (Neural Networks from Natural Language) - a new paradigm of synthesizing task-specific neural networks from language descriptions and a generic pre-trained model. N3 leverages language descriptions to generate parameter adaptations as well as a new task-specific classification layer for a pre-trained neural network, effectively “fine-tuning” the network for a new task using only language descriptions as input. To the best of our knowledge, N3 is the first method to synthesize entire neural networks from natural language. Experimental results show that N3 can out-perform previous natural-language based zero-shot learning methods across 4 different zero-shot image classification benchmarks. We also demonstrate a simple method to help identify keywords in language descriptions leveraged by N3 when synthesizing model parameters.</font>
<br>
</div>


<hr>
<div id="paper46"> <b>46. CompGuessWhat?!: A Multi-task Evaluation Framework for Grounded Language Learning</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.682.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title46" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Alessandro Suglia, Ioannis Konstas, Andrea Vanzo, Emanuele Bastianelli, Desmond Elliott, Stella Frank, Oliver Lemon</i><br>
<font size="3">
Approaches to Grounded Language Learning are commonly focused on a single task-based final performance measure which may not depend on desirable properties of the learned hidden representations, such as their ability to predict object attributes or generalize to unseen situations. To remedy this, we present GroLLA, an evaluation framework for Grounded Language Learning with Attributes based on three sub-tasks: 1) Goal-oriented evaluation; 2) Object attribute prediction evaluation; and 3) Zero-shot evaluation. We also propose a new dataset CompGuessWhat?! as an instance of this framework for evaluating the quality of learned neural representations, in particular with respect to attribute grounding. To this end, we extend the original GuessWhat?! dataset by including a semantic layer on top of the perceptual one. Specifically, we enrich the VisualGenome scene graphs associated with the GuessWhat?! images with several attributes from resources such as VISA and ImSitu. We then compare several hidden state representations from current state-of-the-art approaches to Grounded Language Learning. By using diagnostic classifiers, we show that current models’ learned representations are not expressive enough to encode object attributes (average F1 of 44.27). In addition, they do not learn strategies nor representations that are robust enough to perform well when novel scenes or objects are involved in gameplay (zero-shot best accuracy 50.06%).</font>
<br>
</div>


<hr>
<div id="paper47"> <b>47. Translationese as a Language in “Multilingual” NMT</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.691.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title47" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Parker Riley, Isaac Caswell, Markus Freitag, David Grangier</i><br>
<font size="3">
Machine translation has an undesirable propensity to produce “translationese” artifacts, which can lead to higher BLEU scores while being liked less by human raters. Motivated by this, we model translationese and original (i.e. natural) text as separate languages in a multilingual model, and pose the question: can we perform zero-shot translation between original source text and original target text? There is no data with original source and original target, so we train a sentence-level classifier to distinguish translationese from original target text, and use this classifier to tag the training data for an NMT model. Using this technique we bias the model to produce more natural outputs at test time, yielding gains in human evaluation scores on both accuracy and fluency. Additionally, we demonstrate that it is possible to bias the model to produce translationese and game the BLEU score, increasing it while decreasing human-rated quality. We analyze these outputs using metrics measuring the degree of translationese, and present an analysis of the volatility of heuristic-based train-data tagging.</font>
<br>
</div>


<hr>
<div id="paper48"> <b>48. ZeroShotCeres: Zero-Shot Relation Extraction from Semi-Structured Webpages</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.721.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title48" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Colin Lockard, Prashant Shiralkar, Xin Luna Dong, Hannaneh Hajishirzi</i><br>
<font size="3">
In many documents, such as semi-structured webpages, textual semantics are augmented with additional information conveyed using visual elements including layout, font size, and color. Prior work on information extraction from semi-structured websites has required learning an extraction model specific to a given template via either manually labeled or distantly supervised data from that template. In this work, we propose a solution for “zero-shot” open-domain relation extraction from webpages with a previously unseen template, including from websites with little overlap with existing sources of knowledge for distant supervision and websites in entirely new subject verticals. Our model uses a graph neural network-based approach to build a rich representation of text fields on a webpage and the relationships between them, enabling generalization to new templates. Experiments show this approach provides a 31% F1 gain over a baseline for zero-shot extraction in a new subject vertical.</font>
<br>
</div>


<hr>
<div id="paper49"> <b>49. Should All Cross-Lingual Embeddings Speak English?</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.766.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title49" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Antonios Anastasopoulos, Graham Neubig</i><br>
<font size="3">
Most of recent work in cross-lingual word embeddings is severely Anglocentric. The vast majority of lexicon induction evaluation dictionaries are between English and another language, and the English embedding space is selected by default as the hub when learning in a multilingual setting. With this work, however, we challenge these practices. First, we show that the choice of hub language can significantly impact downstream lexicon induction zero-shot POS tagging performance. Second, we both expand a standard English-centered evaluation dictionary collection to include all language pairs using triangulation, and create new dictionaries for under-represented languages. Evaluating established methods over all these language pairs sheds light into their suitability for aligning embeddings from distant languages and presents new challenges for the field. Finally, in our analysis we identify general guidelines for strong cross-lingual embedding baselines, that extend to language pairs that do not include English.</font>
<br>
</div>


<hr>
<div id="paper50"> <b>50. Zero-shot North Korean to English Neural Machine Translation by Character Tokenization and Phoneme Decomposition</b>  <a href="https://www.aclweb.org/anthology/2020.acl-srw.11.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title50" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. Student Research Workshop<br>&nbsp;&nbsp;<i>Hwichan Kim, Tosho Hirasawa, Mamoru Komachi</i><br>
<font size="3">
The primary limitation of North Korean to English translation is the lack of a parallel corpus; therefore, high translation accuracy cannot be achieved. To address this problem, we propose a zero-shot approach using South Korean data, which are remarkably similar to North Korean data. We train a neural machine translation model after tokenizing a South Korean text at the character level and decomposing characters into phonemes.We demonstrate that our method can effectively learn North Korean to English translation and improve the BLEU scores by +1.01 points in comparison with the baseline.</font>
<br>
</div>


<hr>
<div id="paper51"> <b>51. Cross-Lingual Disaster-related Multi-label Tweet Classification with Manifold Mixup</b>  <a href="https://www.aclweb.org/anthology/2020.acl-srw.39.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title51" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. Student Research Workshop<br>&nbsp;&nbsp;<i>Jishnu Ray Chowdhury, Cornelia Caragea, Doina Caragea</i><br>
<font size="3">
Distinguishing informative and actionable messages from a social media platform like Twitter is critical for facilitating disaster management. For this purpose, we compile a multilingual dataset of over 130K samples for multi-label classification of disaster-related tweets. We present a masking-based loss function for partially labelled samples and demonstrate the effectiveness of Manifold Mixup in the text domain. Our main model is based on Multilingual BERT, which we further improve with Manifold Mixup. We show that our model generalizes to unseen disasters in the test set. Furthermore, we analyze the capability of our model for zero-shot generalization to new languages. Our code, dataset, and other resources are available on Github.</font>
<br>
</div>


<hr>
<div id="paper52"> <b>52. Language Models as Fact Checkers?</b>  <a href="https://www.aclweb.org/anthology/2020.fever-1.5.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title52" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the Third Workshop on Fact Extraction and VERification (FEVER)<br>&nbsp;&nbsp;<i>Nayeon Lee, Belinda Li, Sinong Wang, Wen-tau Yih, Hao Ma, Madian Khabsa</i><br>
<font size="3">
Recent work has suggested that language models (LMs) store both common-sense and factual knowledge learned from pre-training data. In this paper, we leverage this implicit knowledge to create an effective end-to-end fact checker using a solely a language model, without any external knowledge or explicit retrieval components. While previous work on extracting knowledge from LMs have focused on the task of open-domain question answering, to the best of our knowledge, this is the first work to examine the use of language models as fact checkers. In a closed-book setting, we show that our zero-shot LM approach outperforms a random baseline on the standard FEVER task, and that our finetuned LM compares favorably with standard baselines. Though we do not ultimately outperform methods which use explicit knowledge bases, we believe our exploration shows that this method is viable and has much room for exploration.</font>
<br>
</div>


<hr>
<div id="paper53"> <b>53. A Deep Reinforced Model for Zero-Shot Cross-Lingual Summarization with Bilingual Semantic Similarity Rewards</b>  <a href="https://www.aclweb.org/anthology/2020.ngt-1.7.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title53" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the Fourth Workshop on Neural Generation and Translation<br>&nbsp;&nbsp;<i>Zi-Yi Dou, Sachin Kumar, Yulia Tsvetkov</i><br>
<font size="3">
Cross-lingual text summarization aims at generating a document summary in one language given input in another language. It is a practically important but under-explored task, primarily due to the dearth of available data. Existing methods resort to machine translation to synthesize training data, but such pipeline approaches suffer from error propagation. In this work, we propose an end-to-end cross-lingual text summarization model. The model uses reinforcement learning to directly optimize a bilingual semantic similarity metric between the summaries generated in a target language and gold summaries in a source language. We also introduce techniques to pre-train the model leveraging monolingual summarization and machine translation objectives. Experimental results in both English–Chinese and English–German cross-lingual summarization settings demonstrate the effectiveness of our methods. In addition, we find that reinforcement learning models with bilingual semantic similarity as rewards generate more fluent sentences than strong baselines.</font>
<br>
</div>


<hr>
<div id="paper54"> <b>54. From Machine Reading Comprehension to Dialogue State Tracking: Bridging the Gap</b>  <a href="https://www.aclweb.org/anthology/2020.nlp4convai-1.10.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title54" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 2nd Workshop on Natural Language Processing for Conversational AI<br>&nbsp;&nbsp;<i>Shuyang Gao, Sanchit Agarwal, Di Jin, Tagyoung Chung, Dilek Hakkani-Tur</i><br>
<font size="3">
Dialogue state tracking (DST) is at the heart of task-oriented dialogue systems. However, the scarcity of labeled data is an obstacle to building accurate and robust state tracking systems that work across a variety of domains. Existing approaches generally require some dialogue data with state information and their ability to generalize to unknown domains is limited. In this paper, we propose using machine reading comprehension (RC) in state tracking from two perspectives: model architectures and datasets. We divide the slot types in dialogue state into categorical or extractive to borrow the advantages from both multiple-choice and span-based reading comprehension models. Our method achieves near the current state-of-the-art in joint goal accuracy on MultiWOZ 2.1 given full training data. More importantly, by leveraging machine reading comprehension datasets, our method outperforms the existing approaches by many a large margin in few-shot scenarios when the availability of in-domain data is limited. Lastly, even without any state tracking data, i.e., zero-shot scenario, our proposed approach achieves greater than 90% average slot accuracy in 12 out of 30 slots in MultiWOZ 2.1.</font>
<br>
</div>


<hr>
<div id="paper55"> <b>55. On-The-Fly Information Retrieval Augmentation for Language Models</b>  <a href="https://www.aclweb.org/anthology/2020.nuse-1.14.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title55" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the First Joint Workshop on Narrative Understanding, Storylines, and Events<br>&nbsp;&nbsp;<i>Hai Wang, David McAllester</i><br>
<font size="3">
Here we experiment with the use of information retrieval as an augmentation for pre-trained language models. The text corpus used in information retrieval can be viewed as form of episodic memory which grows over time. By augmenting GPT 2.0 with information retrieval we achieve a zero shot 15% relative reduction in perplexity on Gigaword corpus without any re-training. We also validate our IR augmentation on an event co-reference task.</font>
<br>
</div>


<hr>
<div id="paper56"> <b>56. Unsupervised Commonsense Question Answering with Self-Talk</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.373.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title56" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi</i><br>
<font size="3">
Natural language understanding involves reading between the lines with implicit background knowledge. Current systems either rely on pre-trained language models as the sole implicit source of world knowledge, or resort to external knowledge bases (KBs) to incorporate additional relevant knowledge. We propose an unsupervised framework based on self-talk as a novel alternative to multiple-choice commonsense tasks. Inspired by inquiry-based discovery learning (Bruner, 1961), our approach inquires language models with a number of information seeking questions such as "what is the definition of..." to discover additional background knowledge. Empirical results demonstrate that the self-talk procedure substantially improves the performance of zero-shot language model baselines on four out of six commonsense benchmarks, and competes with models that obtain knowledge from external KBs. While our approach improves performance on several benchmarks, the self-talk induced knowledge even when leading to correct answers is not always seen as helpful by human judges, raising interesting questions about the inner-workings of pre-trained language models for commonsense reasoning.</font>
<br>
</div>


<hr>
<div id="paper57"> <b>57. Zero-Shot Cross-Lingual Transfer with Meta Learning</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.368.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title57" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Farhad Nooralahzadeh, Giannis Bekoulis, Johannes Bjerva, Isabelle Augenstein</i><br>
<font size="3">
Learning what to share between tasks has become a topic of great importance, as strategic sharing of knowledge has been shown to improve downstream task performance. This is particularly important for multilingual applications, as most languages in the world are under-resourced. Here, we consider the setting of training models on multiple different languages at the same time, when little or no data is available for languages other than English. We show that this challenging setup can be approached using meta-learning: in addition to training a source language model, another model learns to select which training instances are the most beneficial to the first. We experiment using standard supervised, zero-shot cross-lingual, as well as few-shot cross-lingual settings for different natural language understanding tasks (natural language inference, question answering). Our extensive experimental setup demonstrates the consistent effectiveness of meta-learning for a total of 15 languages. We improve upon the state-of-the-art for zero-shot and few-shot NLI (on MultiNLI and XNLI) and QA (on the MLQA dataset). A comprehensive error analysis indicates that the correlation of typological features between languages can partly explain when parameter sharing learned via meta-learning is beneficial.</font>
<br>
</div>


<hr>
<div id="paper58"> <b>58. Event Extraction by Answering (Almost) Natural Questions</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.49.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title58" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Xinya Du, Claire Cardie</i><br>
<font size="3">
The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task that extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (i.e., in a zero-shot learning setting).</font>
<br>
</div>


<hr>
<div id="paper59"> <b>59. XL-WiC: A Multilingual Benchmark for Evaluating Semantic Contextualization</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.584.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title59" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Alessandro Raganato, Tommaso Pasini, Jose Camacho-Collados, Mohammad Taher Pilehvar</i><br>
<font size="3">
The ability to correctly model distinct meanings of a word is crucial for the effectiveness of semantic representation techniques.However, most existing evaluation benchmarks for assessing this criterion are tied to sense inventories (usually WordNet), restricting their usage to a small subset of knowledge-based representation techniques.The Word-in-Context dataset (WiC) addresses the dependence on sense inventories by reformulating the standard disambiguation task as a binary classification problem; but, it is limited to the English language.We put forward a large multilingual benchmark, XL-WiC, featuring gold standards in 12 new languages from varied language families and with different degrees of resource availability, opening room for evaluation scenarios such as zero-shot cross-lingual transfer.We perform a series of experiments to determine the reliability of the datasets and to set performance baselines for several recent contextualized multilingual models.Experimental results show that even when no tagged instances are available for a target language, models trained solely on the English data can attain competitive performance in the task of distinguishing different meanings of a word, even for distant languages. XL-WiC is available at https://pilehvar.github.io/xlwic/.</font>
<br>
</div>


<hr>
<div id="paper60"> <b>60. Cross-lingual Spoken Language Understanding with Regularized Representation Alignment</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.587.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title60" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Zihan Liu, Genta Indra Winata, Peng Xu, Zhaojiang Lin, Pascale Fung</i><br>
<font size="3">
Despite the promising results of current cross-lingual models for spoken language understanding systems, they still suffer from imperfect cross-lingual representation alignments between the source and target languages, which makes the performance sub-optimal. To cope with this issue, we propose a regularization approach to further align word-level and sentence-level representations across languages without any external resource. First, we regularize the representation of user utterances based on their corresponding labels. Second, we regularize the latent variable model (Liu et al., 2019) by leveraging adversarial training to disentangle the latent variables. Experiments on the cross-lingual spoken language understanding task show that our model outperforms current state-of-the-art methods in both few-shot and zero-shot scenarios, and our model, trained on a few-shot setting with only 3\% of the target language training data, achieves comparable performance to the supervised training with all the training data.</font>
<br>
</div>


<hr>
<div id="paper61"> <b>61. XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.185.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title61" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Edoardo Maria Ponti, Goran Glavaš, Olga Majewska, Qianchu Liu, Ivan Vulić, Anna Korhonen</i><br>
<font size="3">
In order to simulate human language capacity, natural language processing systems must be able to reason about the dynamics of everyday situations, including their possible causes and effects. Moreover, they should be able to generalise the acquired world knowledge to new languages, modulo cultural differences. Advances in machine reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages, which includes resource-poor languages like Eastern Apurímac Quechua and Haitian Creole. We evaluate a range of state-of-the-art models on this novel dataset, revealing that the performance of current methods based on multilingual pretraining and zero-shot fine-tuning falls short compared to translation-based transfer. Finally, we propose strategies to adapt multilingual models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. The XCOPA dataset is freely available at github.com/cambridgeltl/xcopa.</font>
<br>
</div>


<hr>
<div id="paper62"> <b>62. UDapter: Language Adaptation for Truly Universal Dependency Parsing</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.180.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title62" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Ahmet Üstün, Arianna Bisazza, Gosse Bouma, Gertjan van Noord</i><br>
<font size="3">
Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality. However, cross-language interference and restrained model capacity remain major obstacles. To address this, we propose a novel multilingual task adaptation approach based on contextual parameter generation and adapter modules. This approach enables to learn adapters via language embeddings while sharing model parameters across languages. It also allows for an easy but effective integration of existing linguistic typology features into the parsing network.The resulting parser, UDapter, outperforms strong monolingual and multilingual baselines on the majority of both high-resource and low-resource (zero-shot) languages, showing the success of the proposed adaptation approach. Our in-depth analyses show that soft parameter sharing via typological features is key to this success.</font>
<br>
</div>


<hr>
<div id="paper63"> <b>63. Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.8.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title63" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Brian Thompson, Matt Post</i><br>
<font size="3">
We frame the task of machine translation evaluation as one of scoring machine translation output with a sequence-to-sequence paraphraser, conditioned on a human reference. We propose training the paraphraser as a multilingual NMT system, treating paraphrasing as a zero-shot translation task (e.g., Czech to Czech). This results in the paraphraser’s output mode being centered around a copy of the input sequence, which represents the best case scenario where the MT system output matches a human reference. Our method is simple and intuitive, and does not require human judgements for training. Our single model (trained in 39 languages) outperforms or statistically ties with all prior metrics on the WMT 2019 segment-level shared metrics task in all languages (excluding Gujarati where the model had no training data). We also explore using our model for the task of quality estimation as a metric—conditioning on the source instead of the reference—and find that it significantly outperforms every submission to the WMT 2019 shared task on quality estimation in every language pair.</font>
<br>
</div>


<hr>
<div id="paper64"> <b>64. Context-Aware Answer Extraction in Question Answering</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.189.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title64" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Yeon Seonwoo, Ji-Hoon Kim, Jung-Woo Ha, Alice Oh</i><br>
<font size="3">
Extractive QA models have shown very promising performance in predicting the correct answer to a question for a given passage.However, they sometimes result in predicting the correct answer text but in a context irrelevant to the given question.This discrepancy becomes especially important as the number of occurrences of the answer text in a passage increases.To resolve this issue, we propose BLANC (BLock AttentioN for Context prediction) based on two main ideas: context prediction as an auxiliary task in multi-task learning manner, and a block attention method that learns the context prediction task.With experiments on reading comprehension, we show that BLANC outperforms the state-of-the-art QA models, and the performance gap increases as the number of answer text occurrences increases.We also conduct an experiment of training the models using SQuAD and predicting the supporting facts on HotpotQA and show that BLANC outperforms all baseline models in this zero-shot setting.</font>
<br>
</div>


<hr>
<div id="paper65"> <b>65. MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.194.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title65" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Andreas Rücklé, Jonas Pfeiffer, Iryna Gurevych</i><br>
<font size="3">
We study the zero-shot transfer capabilities of text matching models on a massive scale, by self-supervised training on 140 source domains from community question answering forums in English. We investigate the model performances on nine benchmarks of answer selection and question similarity tasks, and show that all 140 models transfer surprisingly well, where the large majority of models substantially outperforms common IR baselines. We also demonstrate that considering a broad selection of source domains is crucial for obtaining the best zero-shot transfer performances, which contrasts the standard procedure that merely relies on the largest and most similar domains. In addition, we extensively study how to best combine multiple source domains. We propose to incorporate self-supervised with supervised multi-task learning on all available source domains. Our best zero-shot transfer model considerably outperforms in-domain BERT and the previous state of the art on six benchmarks. Fine-tuning of our model with in-domain data results in additional large gains and achieves the new state of the art on all nine benchmarks.</font>
<br>
</div>


<hr>
<div id="paper66"> <b>66. Grounded Adaptation for Zero-shot Executable Semantic Parsing</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.558.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title66" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Victor Zhong, Mike Lewis, Sida I. Wang, Luke Zettlemoyer</i><br>
<font size="3">
We propose Grounded Adaptation for Zeroshot Executable Semantic Parsing (GAZP) to adapt an existing semantic parser to new environments (e.g. new database schemas). GAZP combines a forward semantic parser with a backward utterance generator to synthesize data (e.g. utterances and SQL queries) in the new environment, then selects cycle-consistent examples to adapt the parser. Unlike data-augmentation, which typically synthesizes unveriﬁed examples in the training environment, GAZP synthesizes examples in the new environment whose input-output consistency are veriﬁed through execution. On the Spider, Sparc, and CoSQL zero-shot semantic parsing tasks, GAZP improves logical form and execution accuracy of the baseline parser. Our analyses show that GAZP outperforms data-augmentation in the training environment, performance increases with the amount of GAZP-synthesized data, and cycle-consistency is central to successful adaptation.</font>
<br>
</div>


<hr>
<div id="paper67"> <b>67. Zero-Shot Crosslingual Sentence Simplification</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.415.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title67" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Jonathan Mallinson, Rico Sennrich, Mirella Lapata</i><br>
<font size="3">
Sentence simplification aims to make sentences easier to read and understand. Recent approaches have shown promising results with encoder-decoder models trained on large amounts of parallel data which often only exists in English. We propose a zero-shot modeling framework which transfers simplification knowledge from English to another language (for which no parallel simplification corpus exists) while generalizing across languages and tasks. A shared transformer encoder constructs language-agnostic representations, with a combination of task-specific encoder layers added on top (e.g., for translation and simplification). Empirical results using both human and automatic metrics show that our approach produces better simplifications than unsupervised and pivot-based methods.</font>
<br>
</div>


<hr>
<div id="paper68"> <b>68. Identifying Elements Essential for BERT’s Multilinguality</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.358.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title68" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Philipp Dufter, Hinrich Schütze</i><br>
<font size="3">
It has been shown that multilingual BERT (mBERT) yields high quality multilingual representations and enables effective zero-shot transfer. This is surprising given that mBERT does not use any crosslingual signal during training. While recent literature has studied this phenomenon, the reasons for the multilinguality are still somewhat obscure. We aim to identify architectural properties of BERT and linguistic properties of languages that are necessary for BERT to become multilingual. To allow for fast experimentation we propose an efficient setup with small BERT models trained on a mix of synthetic and natural data. Overall, we identify four architectural and two linguistic elements that influence multilinguality. Based on our insights, we experiment with a multilingual pretraining setup that modifies the masking strategy using VecMap, i.e., unsupervised embedding alignment. Experiments on XNLI with three languages indicate that our findings transfer from our small setup to larger scale settings.</font>
<br>
</div>


<hr>
<div id="paper69"> <b>69. KGLM: Pretrained Knowledge-Grounded Language Model for Data-to-Text Generation</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.697.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title69" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Wenhu Chen, Yu Su, Xifeng Yan, William Yang Wang</i><br>
<font size="3">
Data-to-text generation has recently attracted substantial interests due to its wide applications. Existing methods have shown impressive performance on an array of tasks. However, they rely on a significant amount of labeled data for each task, which is costly to acquire and thus limits their application to new tasks and domains. In this paper, we propose to leverage pre-training and transfer learning to address this issue. We propose a knowledge-grounded pre-training (KGPT), which consists of two parts, 1) a general knowledge-grounded generation model to generate knowledge-enriched text. 2) a pre-training paradigm on a massive knowledge-grounded text corpus crawled from the web. The pre-trained model can be fine-tuned on various data-to-text generation tasks to generate task-specific text. We adopt three settings, namely fully-supervised, zero-shot, few-shot to evaluate its effectiveness. Under the fully-supervised setting, our model can achieve remarkable gains over the known baselines. Under zero-shot setting, our model without seeing any examples achieves over 30 ROUGE-L on WebNLG while all other baselines fail. Under the few-shot setting, our model only needs about one-fifteenth as many labeled examples to achieve the same level of performance as baseline models. These experiments consistently prove the strong generalization ability of our proposed framework\footnote\urlhttps://github.com/wenhuchen/KGPT.</font>
<br>
</div>


<hr>
<div id="paper70"> <b>70. Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.554.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title70" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Isabel Papadimitriou, Dan Jurafsky</i><br>
<font size="3">
We propose transfer learning as a method for analyzing the encoding of grammatical structure in neural language models. We train LSTMs on non-linguistic data and evaluate their performance on natural language to assess which kinds of data induce generalizable structural features that LSTMs can use for natural language. We find that training on non-linguistic data with latent structure (MIDI music or Java code) improves test performance on natural language, despite no overlap in surface form or vocabulary. To pinpoint the kinds of abstract structure that models may be encoding to lead to this improvement, we run similar experiments with two artificial parentheses languages: one which has a hierarchical recursive structure, and a control which has paired tokens but no recursion. Surprisingly, training a model on either of these artificial languages leads the same substantial gains when testing on natural language. Further experiments on transfer between natural languages controlling for vocabulary overlap show that zero-shot performance on a test language is highly correlated with typological syntactic similarity to the training language, suggesting that representations induced by pre-training correspond to the cross-linguistic syntactic properties. Our results provide insights into the ways that neural models represent abstract syntactic structure, and also about the kind of structural inductive biases which allow for natural language acquisition.</font>
<br>
</div>


<hr>
<div id="paper71"> <b>71. Multi-task Learning for Multilingual Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.75.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title71" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Yiren Wang, ChengXiang Zhai, Hany Hassan</i><br>
<font size="3">
While monolingual data has been shown to be useful in improving bilingual neural machine translation (NMT), effectively and efficiently leveraging monolingual data for Multilingual NMT (MNMT) systems is a less explored area. In this work, we propose a multi-task learning (MTL) framework that jointly trains the model with the translation task on bitext data and two denoising tasks on the monolingual data. We conduct extensive empirical studies on MNMT systems with $10$ language pairs from WMT datasets. We show that the proposed approach can effectively improve the translation quality for both high-resource and low-resource languages with large margin, achieving significantly better results than the individual bilingual models. We also demonstrate the efficacy of the proposed approach in the zero-shot setup for language pairs without bitext training data. Furthermore, we show the effectiveness of MTL over pre-training approaches for both NMT and cross-lingual transfer learning NLU tasks; the proposed approach outperforms massive scale models trained on single task.</font>
<br>
</div>


<hr>
<div id="paper72"> <b>72. MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.617.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title72" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, Sebastian Ruder</i><br>
<font size="3">
The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language. MAD-X outperforms the state of the art in cross lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering. Our code and adapters are available at AdapterHub.ml.</font>
<br>
</div>


<hr>
<div id="paper73"> <b>73. Event Extraction as Machine Reading Comprehension</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.128.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title73" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Jian Liu, Yubo Chen, Kang Liu, Wei Bi, Xiaojiang Liu</i><br>
<font size="3">
Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts.Previous methods for EE typically model it as a classification task, which are usually prone to the data scarcity problem.In this paper, we propose a new learning paradigm of EE, by explicitly casting it as a machine reading comprehension problem (MRC).Our approach includes an unsupervised question generation process, which can transfer event schema into a set of natural questions, followed by a BERT-based question-answering process to retrieve answers as EE results. This learning paradigm enables us to strengthen the reasoning process of EE, by introducing sophisticated models in MRC, and relieve the data scarcity problem, by introducing the large-scale datasets in MRC.The empirical results show that: i) our approach attains state-of-the-art performance by considerable margins over previous methods. ii) Our model is excelled in the data-scarce scenario, for example, obtaining 49.8\% in F1 for event argument extraction with only 1\% data, compared with 2.2\% of the previous method. iii) Our model also fits with zero-shot scenarios, achieving $37.0\%$ and $16\%$ in F1 on two datasets without using any EE training data.</font>
<br>
</div>


<hr>
<div id="paper74"> <b>74. Revisiting Modularized Multilingual NMT to Meet Industrial Demands</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.476.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title74" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Sungwon Lyu, Bokyung Son, Kichang Yang, Jaekyoung Bae</i><br>
<font size="3">
The complete sharing of parameters for multilingual translation (1-1) has been the mainstream approach in current research. However, degraded performance due to the capacity bottleneck and low maintainability hinders its extensive adoption in industries. In this study, we revisit the multilingual neural machine translation model that only share modules among the same languages (M2) as a practical alternative to 1-1 to satisfy industrial requirements. Through comprehensive experiments, we identify the benefits of multi-way training and demonstrate that the M2 can enjoy these benefits without suffering from the capacity bottleneck. Furthermore, the interlingual space of the M2 allows convenient modification of the model.By leveraging trained modules, we find that incrementally added modules exhibit better performance than singly trained models. The zero-shot performance of the added modules is even comparable to supervised models. Our findings suggest that the M2 can be a competent candidate for multilingual translation in industries.</font>
<br>
</div>


<hr>
<div id="paper75"> <b>75. Cold-start and Interpretability: Turning Regular Expressions into Trainable Recurrent Neural Networks</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.258.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title75" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Chengyue Jiang, Yinggong Zhao, Shanbo Chu, Libin Shen, Kewei Tu</i><br>
<font size="3">
Neural networks can achieve impressive performance on many natural language processing applications, but they typically need large labeled data for training and are not easily interpretable. On the other hand, symbolic rules such as regular expressions are interpretable, require no training, and often achieve decent accuracy; but rules cannot benefit from labeled data when available and hence underperform neural networks in rich-resource scenarios.In this paper, we propose a type of recurrent neural networks called FA-RNNs that combine the advantages of neural networks and regular expression rules. An FA-RNN can be converted from regular expressions and deployed in zero-shot and cold-start scenarios. It can also utilize labeled data for training to achieve improved prediction accuracy. After training, an FA-RNN often remains interpretable and can be converted back into regular expressions.We apply FA-RNNs to text classification and observe that FA-RNNs significantly outperform previous neural approaches in both zero-shot and low-resource settings and remain very competitive in rich-resource settings.</font>
<br>
</div>


<hr>
<div id="paper76"> <b>76. PRover: Proof Generation for Interpretable Reasoning over Rules</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.9.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title76" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Swarnadeep Saha, Sayan Ghosh, Shashank Srivastava, Mohit Bansal</i><br>
<font size="3">
Recent work by Clark et al. (2020) shows that transformers can act as "soft theorem provers'' by answering questions over explicitly provided knowledge in natural language. In our work, we take a step closer to emulating formal theorem provers, by proposing PRover, an interpretable transformer-based model that jointly answers binary questions over rule-bases and generates the corresponding proofs. Our model learns to predict nodes and edges corresponding to proof graphs in an efficient constrained training paradigm. During inference, a valid proof, satisfying a set of global constraints is generated. We conduct experiments on synthetic, hand-authored, and human-paraphrased rule-bases to show promising results for QA and proof generation, with strong generalization performance. First, PRover generates proofs with an accuracy of 87%, while retaining or improving performance on the QA task, compared to RuleTakers (up to 6% improvement on zero-shot evaluation). Second, when trained on questions requiring lower depths of reasoning, it generalizes significantly better to higher depths (up to 15% improvement). Third, PRover obtains near perfect QA accuracy of 98% using only 40% of the training data. However, generating proofs for questions requiring higher depths of reasoning becomes challenging, and the accuracy drops to 65% for "depth 5", indicating significant scope for future work.</font>
<br>
</div>


<hr>
<div id="paper77"> <b>77. Translation Artifacts in Cross-lingual Transfer Learning</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.618.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title77" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Mikel Artetxe, Gorka Labaka, Eneko Agirre</i><br>
<font size="3">
Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.</font>
<br>
</div>


<hr>
<div id="paper78"> <b>78. An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.607.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title78" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Ilias Chalkidis, Manos Fergadiotis, Sotiris Kotitsas, Prodromos Malakasiotis, Nikolaos Aletras, Ion Androutsopoulos</i><br>
<font size="3">
Large-scale Multi-label Text Classification (LMTC) has a wide range of Natural Language Processing (NLP) applications and presents interesting challenges. First, not all labels are well represented in the training set, due to the very large label set and the skewed label distributions of \lmtc datasets. Also, label hierarchies and differences in human labelling guidelines may affect graph-aware annotation proximity. Finally, the label hierarchies are periodically updated, requiring LMTC models capable of zero-shot generalization. Current state-of-the-art LMTC models employ Label-Wise Attention Networks (LWANs), which (1) typically treat LMTC as flat multi-label classification; (2) may use the label hierarchy to improve zero-shot learning, although this practice is vastly understudied; and (3) have not been combined with pre-trained Transformers (e.g. BERT), which have led to state-of-the-art results in several NLP benchmarks. Here, for the first time, we empirically evaluate a battery of LMTC methods from vanilla LWANs to hierarchical classification approaches and transfer learning, on frequent, few, and zero-shot learning on three datasets from different domains. We show that hierarchical methods based on Probabilistic Label Trees (PLTs) outperform LWANs. Furthermore, we show that Transformer-based approaches outperform the state-of-the-art in two of the datasets, and we propose a new state-of-the-art method which combines BERT with LWAN. Finally, we propose new models that leverage the label hierarchy to improve few and zero-shot learning, considering on each dataset a graph-aware annotation proximity measure that we introduce.</font>
<br>
</div>


<hr>
<div id="paper79"> <b>79. LAReQA: Language-agnostic Answer Retrieval from a Multilingual Pool</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.477.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title79" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Uma Roy, Noah Constant, Rami Al-Rfou, Aditya Barua, Aaron Phillips, Yinfei Yang</i><br>
<font size="3">
We present LAReQA, a challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. Unlike previous cross-lingual tasks, LAReQA tests for ``strong'' cross-lingual alignment, requiring semantically related \textitcross-language pairs to be closer in representation space than unrelated \textitsame-language pairs. This level of alignment is important for the practical task of cross-lingual information retrieval. Building on multilingual BERT (mBERT), we study different strategies for achieving strong alignment. We find that augmenting training data via machine translation is effective, and improves significantly over using mBERT out-of-the-box. Interestingly, model performance on zero-shot variants of our task that only target ``weak" alignment is not predictive of performance on LAReQA\@. This finding underscores our claim that language-agnostic retrieval is a substantively new kind of cross-lingual evaluation, and suggests that measuring both weak and strong alignment will be important for improving cross-lingual systems going forward. We release our dataset and evaluation code at \urlhttps://github.com/google-research-datasets/lareqa.</font>
<br>
</div>


<hr>
<div id="paper80"> <b>80. Generationary or: “How We Went beyond Word Sense Inventories and Learned to Gloss”</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.585.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title80" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Michele Bevilacqua, Marco Maru, Roberto Navigli</i><br>
<font size="3">
Mainstream computational lexical semantics embraces the assumption that word senses can be represented as discrete items of a predefined inventory. In this paper we show this needs not be the case, and propose a unified model that is able to produce contextually appropriate definitions. In our model, Generationary, we employ a novel span-based encoding scheme which we use to fine-tune an English pre-trained Encoder-Decoder system to generate glosses. We show that, even though we drop the need of choosing from a predefined sense inventory, our model can be employed effectively: not only does Generationary outperform previous approaches in the generative task of Definition Modeling in many settings, but it also matches or surpasses the state of the art in discriminative tasks such as Word Sense Disambiguation and Word-in-Context. Finally, we show that Generationary benefits from training on data from multiple inventories, with strong gains on various zero-shot benchmarks, including a novel dataset of definitions for free adjective-noun phrases. The software and reproduction materials are available at http://generationary.org.</font>
<br>
</div>


<hr>
<div id="paper81"> <b>81. From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.363.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title81" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Anne Lauscher, Vinit Ravishankar, Ivan Vulić, Goran Glavaš</i><br>
<font size="3">
Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions.</font>
<br>
</div>


<hr>
<div id="paper82"> <b>82. Self-Supervised Knowledge Triplet Learning for Zero-shot Question Answering</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.11.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title82" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Pratyay Banerjee, Chitta Baral</i><br>
<font size="3">
The aim of all Question Answering (QA) systems is to generalize to unseen questions. Current supervised methods are reliant on expensive data annotation. Moreover, such annotations can introduce unintended annotator bias, making systems focus more on the bias than the actual task. This work proposes Knowledge Triplet Learning (KTL), a self-supervised task over knowledge graphs. We propose heuristics to create synthetic graphs for commonsense and scientific knowledge. We propose using KTL to perform zero-shot question answering, and our experiments show considerable improvements over large pre-trained transformer language models.</font>
<br>
</div>


<hr>
<div id="paper83"> <b>83. Zero-Shot Stance Detection: A Dataset and Model Using Generalized Topic Representations</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.717.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title83" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Emily Allaway, Kathleen McKeown</i><br>
<font size="3">
Stance detection is an important component of understanding hidden influences in everyday life. Since there are thousands of potential topics to take a stance on, most with little to no training data, we focus on zero-shot stance detection: classifying stance from no training examples. In this paper, we present a new dataset for zero-shot stance detection that captures a wider range of topics and lexical variation than in previous datasets. Additionally, we propose a new model for stance detection that implicitly captures relationships between topics using generalized topic representations and show that this model improves performance on a number of challenging linguistic phenomena.</font>
<br>
</div>


<hr>
<div id="paper84"> <b>84. Scalable Zero-shot Entity Linking with Dense Entity Retrieval</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.519.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title84" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Ledell Wu, Fabio Petroni, Martin Josifoski, Sebastian Riedel, Luke Zettlemoyer</i><br>
<font size="3">
This paper introduces a conceptually simple, scalable, and highly effective BERT-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm, where each entity is defined only by a short textual description. The first stage does retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions. Each candidate is then re-ranked with a cross-encoder, that concatenates the mention and entity text. Experiments demonstrate that this approach is state of the art on recent zero-shot benchmarks (6 point absolute gains) and also on more established non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative simplicity (e.g. no explicit entity embeddings or manually engineered mention tables). We also show that bi-encoder linking is very fast with nearest neighbor search (e.g. linking with 5.9 million candidates in 2 milliseconds), and that much of the accuracy gain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation. Our code and models are available at https://github.com/facebookresearch/BLINK.</font>
<br>
</div>


<hr>
<div id="paper85"> <b>85. CHARM: Inferring Personal Attributes from Conversations</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.434.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title85" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Anna Tigunova, Andrew Yates, Paramita Mirza, Gerhard Weikum</i><br>
<font size="3">
Personal knowledge about users’ professions, hobbies, favorite food, and travel preferences, among others, is a valuable asset for individualized AI, such as recommenders or chatbots. Conversations in social media, such as Reddit, are a rich source of data for inferring personal facts. Prior work developed supervised methods to extract this knowledge, but these approaches can not generalize beyond attribute values with ample labeled training samples. This paper overcomes this limitation by devising CHARM: a zero-shot learning method that creatively leverages keyword extraction and document retrieval in order to predict attribute values that were never seen during training. Experiments with large datasets from Reddit show the viability of CHARM for open-ended attributes, such as professions and hobbies.</font>
<br>
</div>


<hr>
<div id="paper86"> <b>86. Design Challenges in Low-resource Cross-lingual Entity Linking</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.521.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title86" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Xingyu Fu, Weijia Shi, Xiaodong Yu, Zian Zhao, Dan Roth</i><br>
<font size="3">
Cross-lingual Entity Linking (XEL), the problem of grounding mentions of entities in a foreign language text into an English knowledge base such as Wikipedia, has seen a lot of research in recent years, with a range of promising techniques. However, current techniques do not rise to the challenges introduced by text in low-resource languages (LRL) and, surprisingly, fail to generalize to text not taken from Wikipedia, on which they are usually trained. This paper provides a thorough analysis of low-resource XEL techniques, focusing on the key step of identifying candidate English Wikipedia titles that correspond to a given foreign language mention. Our analysis indicates that current methods are limited by their reliance on Wikipedia’s interlanguage linksand thus suffer when the foreign language’s Wikipedia is small. We conclude that the LRL setting requires the use of outside-Wikipedia cross-lingual resources and present a simple yet effective zero-shot XEL system, QuEL, that utilizes search engines query logs. With experiments on 25 languages, QuEL shows an average increase of 25% in gold candidate recall and of 13% in end-to-end linking accuracy over state-of-the-art baselines.</font>
<br>
</div>


<hr>
<div id="paper87"> <b>87. AutoQA: From Databases to Q&A Semantic Parsers with Only Synthetic Training Data</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.31.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title87" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Long Paper<br>&nbsp;&nbsp;<i>Silei Xu, Sina Semnani, Giovanni Campagna, Monica Lam</i><br>
<font size="3">
We propose AutoQA, a methodology and toolkit to generate semantic parsers that answer questions on databases, with no manual effort. Given a database schema and its data, AutoQA automatically generates a large set of high-quality questions for training that covers different database operations. It uses automatic paraphrasing combined with template-based parsing to find alternative expressions of an attribute in different parts of speech. It also uses a novel filtered auto-paraphraser to generate correct paraphrases of entire sentences.We apply AutoQA to the Schema2QA dataset and obtain an average logical form accuracy of 62.9% when tested on natural questions, which is only 6.4% lower than a model trained with expert natural language annotations and paraphrase data collected from crowdworkers. To demonstrate the generality of AutoQA, we also apply it to the Overnight dataset. AutoQA achieves 69.8% answer accuracy, 16.4% higher than the state-of-the-art zero-shot models and only 5.2% lower than the same model trained with human data.</font>
<br>
</div>


<hr>
<div id="paper88"> <b>88. On the Evaluation of Contextual Embeddings for Zero-Shot Cross-Lingual Transfer Learning</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.40.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title88" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Short Paper<br>&nbsp;&nbsp;<i>Phillip Keung, Yichao Lu, Julian Salazar, Vikas Bhardwaj</i><br>
<font size="3">
Multilingual contextual embeddings have demonstrated state-of-the-art performance in zero-shot cross-lingual transfer learning, where multilingual BERT is fine-tuned on one source language and evaluated on a different target language. However, published results for mBERT zero-shot accuracy vary as much as 17 points on the MLDoc classification task across four papers. We show that the standard practice of using English dev accuracy for model selection in the zero-shot setting makes it difficult to obtain reproducible results on the MLDoc and XNLI tasks. English dev accuracy is often uncorrelated (or even anti-correlated) with target language accuracy, and zero-shot performance varies greatly at different points in the same fine-tuning run and between different fine-tuning runs. These reproducibility issues are also present for other tasks with different pre-trained embeddings (e.g., MLQA with XLM-R). We recommend providing oracle scores alongside zero-shot results: still fine-tune using English data, but choose a checkpoint with the target dev set. Reporting this upper bound makes results more consistent by avoiding arbitrarily bad checkpoints.</font>
<br>
</div>


<hr>
<div id="paper89"> <b>89. The Multilingual Amazon Reviews Corpus</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.369.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title89" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Short Paper<br>&nbsp;&nbsp;<i>Phillip Keung, Yichao Lu, György Szarvas, Noah A. Smith</i><br>
<font size="3">
We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale collection of Amazon reviews for multilingual text classification. The corpus contains reviews in English, Japanese, German, French, Spanish, and Chinese, which were collected between 2015 and 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID, and the coarse-grained product category (e.g., 'books', 'appliances', etc.) The corpus is balanced across the 5 possible star ratings, so each rating constitutes 20% of the reviews in each language. For each language, there are 200,000, 5,000, and 5,000 reviews in the training, development, and test sets, respectively. We report baseline results for supervised text classification and zero-shot cross-lingual transfer learning by fine-tuning a multilingual BERT model on reviews data. We propose the use of mean absolute error (MAE) instead of classification accuracy for this task, since MAE accounts for the ordinal nature of the ratings.</font>
<br>
</div>


<hr>
<div id="paper90"> <b>90. Multi-label Few/Zero-shot Learning with Knowledge Aggregated from Multiple Label Graphs</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.235.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title90" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Short Paper<br>&nbsp;&nbsp;<i>Jueqing Lu, Lan Du, Ming Liu, Joanna Dipnall</i><br>
<font size="3">
Few/Zero-shot learning is a big challenge of many classifications tasks, where a classifier is required to recognise instances of classes that have very few or even no training samples. It becomes more difficult in multi-label classification, where each instance is labelled with more than one class. In this paper, we present a simple multi-graph aggregation model that fuses knowledge from multiple label graphs encoding different semantic label relationships in order to study how the aggregated knowledge can benefit multi-label zero/few-shot document classification. The model utilises three kinds of semantic information, i.e., the pre-trained word embeddings, label description, and pre-defined label relations. Experimental results derived on two large clinical datasets (i.e., MIMIC-II and MIMIC-III ) and the EU legislation dataset show that methods equipped with the multi-graph knowledge aggregationachieve significant performance improvement across almost all the measures on few/zero-shot labels.</font>
<br>
</div>


<hr>
<div id="paper91"> <b>91. An Empirical Study of Pre-trained Transformers for Arabic Information Extraction</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.382.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title91" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Short Paper<br>&nbsp;&nbsp;<i>Wuwei Lan, Yang Chen, Wei Xu, Alan Ritter</i><br>
<font size="3">
Multilingual pre-trained Transformers, such as mBERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020a), have been shown to enable effective cross-lingual zero-shot transfer. However, their performance on Arabic information extraction (IE) tasks is not very well studied. In this paper, we pre-train a customized bilingual BERT, dubbed GigaBERT, that is designed specifically for Arabic NLP and English-to-Arabic zero-shot transfer learning. We study GigaBERT's effectiveness on zero-short transfer across four IE tasks: named entity recognition, part-of-speech tagging, argument role labeling, and relation extraction. Our best model significantly outperforms mBERT, XLM-RoBERTa, and AraBERT (Antoun et al., 2020) in both the supervised and zero-shot transfer settings. We have made our pre-trained models publicly available at: https://github.com/lanwuwei/GigaBERT.</font>
<br>
</div>


<hr>
<div id="paper92"> <b>92. Language Adapters for Zero Shot Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.361.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title92" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Short Paper<br>&nbsp;&nbsp;<i>Jerin Philip, Alexandre Berard, Matthias Gallé, Laurent Besacier</i><br>
<font size="3">
We propose a novel adapter layer formalism for adapting multilingual models. They are more parameter-efficient than existing adapter layers while obtaining as good or better performance. The layers are specific to one language (as opposed to bilingual adapters) allowing to compose them and generalize to unseen language-pairs. In this zero-shot setting, they obtain a median improvement of +2.77 BLEU points over a strong 20-language multilingual Transformer baseline trained on TED talks.</font>
<br>
</div>


<hr>
<div id="paper93"> <b>93. SLEDGE: A Simple Yet Effective Zero-Shot Baseline for Coronavirus Scientific Knowledge Search</b>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.341.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title93" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Short Paper<br>&nbsp;&nbsp;<i>Sean MacAvaney, Arman Cohan, Nazli Goharian</i><br>
<font size="3">
With worldwide concerns surrounding the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), there is a rapidly growing body of scientific literature on the virus. Clinicians, researchers, and policy-makers need to be able to search these articles effectively. In this work, we present a zero-shot ranking algorithm that adapts to COVID-related scientific literature. Our approach filters training data from another collection down to medical-related queries, uses a neural re-ranking model pre-trained on scientific text (SciBERT), and filters the target document collection. This approach ranks top among zero-shot methods on the TREC COVID Round 1 leaderboard, and exhibits a P@5 of 0.80 and an nDCG@10 of 0.68 when evaluated on both Round 1 and 2 judgments. Despite not relying on TREC-COVID data, our method outperforms models that do. As one of the first search methods to thoroughly evaluate COVID-19 search, we hope that this serves as a strong baseline and helps in the global crisis.</font>
<br>
</div>


<hr>
<div id="paper94"> <b>94. ZEST: Zero-shot Learning from Text Descriptions using Textual Similarity and Visual Summarization</b>  <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.50.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title94" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Findings Short Paper<br>&nbsp;&nbsp;<i>Tzuf Paz-Argaman, Reut Tsarfaty, Gal Chechik, Yuval Atzmon</i><br>
<font size="3">
We study the problem of recognizing visual entities from the textual descriptions of their classes. Specifically, given birds’ images with free-text descriptions of their species, we learn to classify images of previously-unseen species based on specie descriptions. This setup has been studied in the vision community under the name zero-shot learning from text, focusing on learning to transfer knowledge about visual aspects of birds from seen classes to previously-unseen ones. Here, we suggest focusing on the textual description and distilling from the description the most relevant information to effectively match visual features to the parts of the text that discuss them. Specifically, (1) we propose to leverage the similarity between species, reflected in the similarity between text descriptions of the species. (2) we derive visual summaries of the texts, i.e., extractive summaries that focus on the visual features that tend to be reflected in images. We propose a simple attention-based model augmented with the similarity and visual summaries components. Our empirical results consistently and significantly outperform the state-of-the-art on the largest benchmarks for text-based zero-shot learning, illustrating the critical importance of texts for zero-shot image-recognition.</font>
<br>
</div>


<hr>
<div id="paper95"> <b>95. Document Ranking with a Pretrained Sequence-to-Sequence Model</b>  <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.63.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title95" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Findings Short Paper<br>&nbsp;&nbsp;<i>Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, Jimmy Lin</i><br>
<font size="3">
This work proposes the use of a pretrained sequence-to-sequence model for document ranking. Our approach is fundamentally different from a commonly adopted classification-based formulation based on encoder-only pretrained transformer architectures such as BERT. We show how a sequence-to-sequence model can be trained to generate relevance labels as “target tokens”, and how the underlying logits of these target tokens can be interpreted as relevance probabilities for ranking. Experimental results on the MS MARCO passage ranking task show that our ranking approach is superior to strong encoder-only models. On three other document retrieval test collections, we demonstrate a zero-shot transfer-based approach that outperforms previous state-of-the-art models requiring in-domain cross-validation. Furthermore, we find that our approach significantly outperforms an encoder-only architecture in a data-poor setting. We investigate this observation in more detail by varying target tokens to probe the model’s use of latent knowledge. Surprisingly, we find that the choice of target tokens impacts effectiveness, even for words that are closely related semantically. This finding sheds some light on why our sequence-to-sequence formulation for document ranking is effective. Code and models are available at pygaggle.ai.</font>
<br>
</div>


<hr>
<div id="paper96"> <b>96. Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study</b>  <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.83.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title96" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Findings Short Paper<br>&nbsp;&nbsp;<i>Saurabh Kulshreshtha, Jose Luis Redondo Garcia, Ching-Yun Chang</i><br>
<font size="3">
Multilingual BERT (mBERT) has shown reasonable capability for zero-shot cross-lingual transfer when fine-tuned on downstream tasks. Since mBERT is not pre-trained with explicit cross-lingual supervision, transfer performance can further be improved by aligning mBERT with cross-lingual signal. Prior work propose several approaches to align contextualised embeddings. In this paper we analyse how different forms of cross-lingual supervision and various alignment methods influence the transfer capability of mBERT in zero-shot setting. Specifically, we compare parallel corpora vs dictionary-based supervision and rotational vs fine-tuning based alignment methods. We evaluate the performance of different alignment methodologies across eight languages on two tasks: Name Entity Recognition and Semantic Slot Filling. In addition, we propose a novel normalisation method which consistently improves the performance of rotation-based alignment including a notable 3% F1 improvement for distant and typologically dissimilar languages. Importantly we identify the biases of the alignment methods to the type of task and proximity to the transfer language. We also find that supervision from parallel corpus is generally superior to dictionary alignments.</font>
<br>
</div>


<hr>
<div id="paper97"> <b>97. Hybrid Emoji-Based Masked Language Models for Zero-Shot Abusive Language Detection</b>  <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.84.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title97" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Findings Short Paper<br>&nbsp;&nbsp;<i>Michele Corazza, Stefano Menini, Elena Cabrio, Sara Tonelli, Serena Villata</i><br>
<font size="3">
Recent studies have demonstrated the effectiveness of cross-lingual language model pre-training on different NLP tasks, such as natural language inference and machine translation. In our work, we test this approach on social media data, which are particularly challenging to process within this framework, since the limited length of the textual messages and the irregularity of the language make it harder to learn meaningful encodings. More specifically, we propose a hybrid emoji-based Masked Language Model (MLM) to leverage the common information conveyed by emojis across different languages and improve the learned cross-lingual representation of short text messages, with the goal to perform zero- shot abusive language detection. We compare the results obtained with the original MLM to the ones obtained by our method, showing improved performance on German, Italian and Spanish.</font>
<br>
</div>


<hr>
<div id="paper98"> <b>98. CodeBERT: A Pre-Trained Model for Programming and Natural Languages</b>  <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.139.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title98" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Findings Short Paper<br>&nbsp;&nbsp;<i>Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, Ming Zhou</i><br>
<font size="3">
We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both “bimodal” data of NL-PL pairs and “unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.</font>
<br>
</div>


<hr>
<div id="paper99"> <b>99. On the Language Neutrality of Pre-trained Multilingual Representations</b>  <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.150.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title99" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Findings Short Paper<br>&nbsp;&nbsp;<i>Jindřich Libovický, Rudolf Rosa, Alexander Fraser</i><br>
<font size="3">
Multilingual contextual embeddings, such as multilingual BERT and XLM-RoBERTa, have proved useful for many multi-lingual tasks. Previous work probed the cross-linguality of the representations indirectly using zero-shot transfer learning on morphological and syntactic tasks. We instead investigate the language-neutrality of multilingual contextual embeddings directly and with respect to lexical semantics. Our results show that contextual embeddings are more language-neutral and, in general, more informative than aligned static word-type embeddings, which are explicitly trained for language neutrality. Contextual embeddings are still only moderately language-neutral by default, so we propose two simple methods for achieving stronger language neutrality: first, by unsupervised centering of the representation for each language and second, by fitting an explicit projection on small parallel data. Besides, we show how to reach state-of-the-art accuracy on language identification and match the performance of statistical methods for word alignment of parallel sentences without using parallel data.</font>
<br>
</div>


<hr>
<div id="paper100"> <b>100. Zero-Shot Rationalization by Multi-Task Transfer Learning from Question Answering</b>  <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.198.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title100" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Findings Short Paper<br>&nbsp;&nbsp;<i>Po-Nien Kung, Tse-Hsuan Yang, Yi-Cheng Chen, Sheng-Siang Yin, Yun-Nung Chen</i><br>
<font size="3">
Extracting rationales can help human understand which information the model utilizes and how it makes the prediction towards better interpretability. However, annotating rationales requires much effort and only few datasets contain such labeled rationales, making supervised learning for rationalization difficult. In this paper, we propose a novel approach that leverages the benefits of both multi-task learning and transfer learning for generating rationales through question answering in a zero-shot fashion. For two benchmark rationalization datasets, the proposed method achieves comparable or even better performance of rationalization without any supervised signal, demonstrating the great potential of zero-shot rationalization for better interpretability.</font>
<br>
</div>


<hr>
<div id="paper101"> <b>101. Sparse and Decorrelated Representations for Stable Zero-shot NMT</b>  <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.205.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title101" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Findings Short Paper<br>&nbsp;&nbsp;<i>Bokyung Son, Sungwon Lyu</i><br>
<font size="3">
Using a single encoder and decoder for all directions and training with English-centric data is a popular scheme for multilingual NMT. However, zero-shot translation under this scheme is vulnerable to changes in training conditions, as the model degenerates by decoding non-English texts into English regardless of the target specifier token. We present that enforcing both sparsity and decorrelation on encoder intermediate representations with the SLNI regularizer (Aljundi et al., 2019) efficiently mitigates this problem, without performance loss in supervised directions. Notably, effects of SLNI turns out to be irrelevant to promoting language-invariance in encoder representations.</font>
<br>
</div>


<hr>
<div id="paper102"> <b>102. Zero-shot Entity Linking with Efficient Long Range Sequence Modeling</b>  <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.228.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title102" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Findings Short Paper<br>&nbsp;&nbsp;<i>Zonghai Yao, Liangliang Cao, Huapu Pan</i><br>
<font size="3">
This paper considers the problem of zero-shot entity linking, in which a link in the test time may not present in training. Following the prevailing BERT-based research efforts, we find a simple yet effective way is to expand the long-range sequence modeling. Unlike many previous methods, our method does not require expensive pre-training of BERT with long position embeddings. Instead, we propose an efficient position embeddings initialization method called Embedding-repeat, which initializes larger position embeddings based on BERT-Base. On the zero-shot entity linking dataset, our method improves the STOA from 76.06% to 79.08%, and for its long data, the corresponding improvement is from 74.57% to 82.14%. Our experiments suggest the effectiveness of long-range sequence modeling without retraining the BERT model.</font>
<br>
</div>


<hr>
<div id="paper103"> <b>103. Extending Multilingual BERT to Low-Resource Languages</b>  <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.240.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title103" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Findings Short Paper<br>&nbsp;&nbsp;<i>Zihan Wang, Karthikeyan K, Stephen Mayhew, Dan Roth</i><br>
<font size="3">
Multilingual BERT (M-BERT) has been a huge success in both supervised and zero-shot cross-lingual transfer learning. However, this success is focused only on the top 104 languages in Wikipedia it was trained on. In this paper, we propose a simple but effective approach to extend M-BERT E-MBERT so it can benefit any new language, and show that our approach aids languages that are already in M-BERT as well. We perform an extensive set of experiments with Named Entity Recognition (NER) on 27 languages, only 16 of which are in M-BERT, and show an average increase of about 6% F1 on M-BERT languages and 23% F1 increase on new languages. We release models and code at http://cogcomp.org/page/publication_view/912.</font>
<br>
</div>


<hr>
<div id="paper104"> <b>104. Towards Zero Shot Conditional Summarization with Adaptive Multi-task Fine-Tuning</b>  <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.289.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title104" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Findings Short Paper<br>&nbsp;&nbsp;<i>Travis Goodwin, Max Savery, Dina Demner-Fushman</i><br>
<font size="3">
Automatic summarization research has traditionally focused on providing high quality general-purpose summaries of documents. However, there are many applications which require more specific summaries, such as supporting question answering or topic-based literature discovery. In this paper we study the problem of conditional summarization in which content selection and surface realization are explicitly conditioned on an ad-hoc natural language question or topic description. Because of the difficulty in obtaining sufficient reference summaries to support arbitrary conditional summarization, we explore the use of multi-task fine-tuning (MTFT) on twenty-one natural language tasks to enable zero-shot conditional summarization on five tasks. We present four new summarization datasets, two novel “online” or adaptive task-mixing strategies, and report zero-shot performance using T5 and BART, demonstrating that MTFT can improve zero-shot summarization quality.</font>
<br>
</div>


<hr>
<div id="paper105"> <b>105. Internal and External Pressures on Language Emergence: Least Effort, Object Constancy and Frequency</b>  <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.397.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title105" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Findings Short Paper<br>&nbsp;&nbsp;<i>Diana Rodríguez Luna, Edoardo Maria Ponti, Dieuwke Hupkes, Elia Bruni</i><br>
<font size="3">
In previous work, artificial agents were shown to achieve almost perfect accuracy in referential games where they have to communicate to identify images. Nevertheless, the resulting communication protocols rarely display salient features of natural languages, such as compositionality. In this paper, we propose some realistic sources of pressure on communication that avert this outcome. More specifically, we formalise the principle of least effort through an auxiliary objective. Moreover, we explore several game variants, inspired by the principle of object constancy, in which we alter the frequency, position, and luminosity of the objects in the images. We perform an extensive analysis on their effect through compositionality metrics, diagnostic classifiers, and zero-shot evaluation. Our findings reveal that the proposed sources of pressure result in emerging languages with less redundancy, more focus on high-level conceptual information, and better abilities of generalisation. Overall, our contributions reduce the gap between emergent and natural languages.</font>
<br>
</div>


<hr>
<div id="paper106"> <b>106. Learning to Classify Human Needs of Events from Category Definitions with Prototypical Instantiation</b>  <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.421.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title106" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2020. Findings Short Paper<br>&nbsp;&nbsp;<i>Haibo Ding, Zhe Feng</i><br>
<font size="3">
We study the problem of learning an event classifier from human needs category descriptions, which is challenging due to: (1) the use of highly abstract concepts in natural language descriptions, (2) the difficulty of choosing key concepts. To tackle these two challenges, we propose LeaPI, a zero-shot learning method that first automatically generate weak labels by instantiating high-level concepts with prototypical instances and then trains a human needs classifier with the weakly labeled data. To filter noisy concepts, we design a reinforced selection algorithm to choose high-quality concepts for instantiation. Experimental results on the human needs categorization task show that our method outperforms baseline methods, producing substantially better precision.</font>
<br>
</div>


<hr>
<div id="paper107"> <b>107. Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model</b>  <a href="https://openreview.net/pdf?id=BJlzm64tDH" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title107" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICLR 2020. <br>&nbsp;&nbsp;<i>Wenhan Xiong, Jingfei Du, William Yang Wang, Veselin Stoyanov</i><br>
<font size="3">
Recent breakthroughs of pretrained language models have shown the effectiveness of self-supervised learning for a wide range of natural language processing (NLP) tasks. In addition to standard syntactic and semantic NLP tasks, pretrained models achieve strong improvements on tasks that involve real-world knowledge, suggesting that large-scale language modeling could be an implicit method to capture knowledge. In this work, we further investigate the extent to which pretrained models such as BERT capture knowledge using a zero-shot fact completion task. Moreover, we propose a simple yet effective weakly supervised pretraining objective, which explicitly forces the model to incorporate knowledge about real-world entities. Models trained with our new objective yield significant improvements on the fact completion task. When applied to downstream tasks, our model consistently outperforms BERT on four entity-related question answering datasets (i.e., WebQuestions, TriviaQA, SearchQA and Quasar-T) with an average 2.7 F1 improvements and a standard fine-grained entity typing dataset (i.e., FIGER) with 5.7 accuracy gains.</font>
<br>
</div>


<hr>
<div id="paper108"> <b>108. Locality and Compositionality in Zero-Shot Learning</b>  <a href="https://openreview.net/pdf?id=Hye_V0NKwr" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title108" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICLR 2020. <br>&nbsp;&nbsp;<i>Tristan Sylvain, Linda Petrini, R. Devon Hjelm</i><br>
<font size="3">
In this work we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). In order to well-isolate the importance of these properties in learned representations, we impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed.The results of our experiment show how locality, in terms of small parts of the input, and compositionality, i.e. how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization and motivate the focus on more local-aware models in future research directions for representation learning.</font>
<br>
</div>


<hr>
<div id="paper109"> <b>109. Multilingual Alignment of Contextual Word Representations</b>  <a href="https://openreview.net/pdf?id=r1xCMyBtPS" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title109" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICLR 2020. <br>&nbsp;&nbsp;<i>Steven Cao, Nikita Kitaev, Dan Klein</i><br>
<font size="3">
We propose procedures for evaluating and strengthening contextual embedding alignment and show that they are useful in analyzing and improving multilingual BERT. In particular, after our proposed alignment procedure, BERT exhibits significantly improved zero-shot performance on XNLI compared to the base model, remarkably matching pseudo-fully-supervised translate-train models for Bulgarian and Greek. Further, to measure the degree of alignment, we introduce a contextual version of word retrieval and show that it correlates well with downstream zero-shot transfer. Using this word retrieval task, we also analyze BERT and find that it exhibits systematic deficiencies, e.g. worse alignment for open-class parts-of-speech and word pairs written in different scripts, that are corrected by the alignment procedure. These results support contextual alignment as a useful concept for understanding large multilingual pre-trained models.</font>
<br>
</div>


<hr>
<div id="paper110"> <b>110. Empirical Bayes Transductive Meta-Learning with Synthetic Gradients</b>  <a href="https://openreview.net/pdf?id=Hkg-xgrYvH" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title110" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICLR 2020. <br>&nbsp;&nbsp;<i>Shell Xu Hu, Pablo Garcia Moreno, Yang Xiao, Xi Shen, Guillaume Obozinski, Neil D. Lawrence, Andreas C. Damianou</i><br>
<font size="3">
We propose a meta-learning approach that learns from multiple tasks in a transductive setting, by leveraging the unlabeled query set in addition to the  support set to generate a more powerful model for each task. To develop our framework, we revisit the empirical Bayes formulation for multi-task learning.   The evidence lower bound of the marginal log-likelihood of empirical Bayes decomposes as a sum of local KL divergences between the variational posterior and the true posterior on the query set of each task.We derive a novel amortized variational inference that couples all the variational posteriors via a meta-model, which consists of a synthetic gradient   network and an initialization network. Each variational posterior is derived from synthetic gradient descent to approximate the true posterior on the query  set, although where we do not have access to the true gradient.Our results on the Mini-ImageNet and CIFAR-FS benchmarks for episodic few-shot classification outperform previous state-of-the-art methods. Besides, we conduct two zero-shot learning experiments to further explore the potential of the synthetic gradient.</font>
<br>
</div>


<hr>
<div id="paper111"> <b>111. Program Guided Agent</b>  <a href="https://openreview.net/pdf?id=BkxUvnEYDH" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title111" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICLR 2020. <br>&nbsp;&nbsp;<i>Shao-Hua Sun, Te-Lin Wu, Joseph J. Lim</i><br>
<font size="3">
Developing agents that can learn to follow natural language instructions has been an emerging research direction. While being accessible and flexible, natural language instructions can sometimes be ambiguous even to humans. To address this, we propose to utilize programs, structured in a formal language, as a precise and expressive way to specify tasks. We then devise a modular framework that learns to perform a task specified by a program – as different circumstances give rise to diverse ways to accomplish the task, our framework can perceive which circumstance it is currently under, and instruct a multitask policy accordingly to fulfill each subtask of the overall task. Experimental results on a 2D Minecraft environment not only demonstrate that the proposed framework learns to reliably accomplish program instructions and achieves zero-shot generalization to more complex instructions but also verify the efficiency of the proposed modulation mechanism for learning the multitask policy. We also conduct an analysis comparing various models which learn from programs and natural language instructions in an end-to-end fashion.</font>
<br>
</div>


<hr>
<div id="paper112"> <b>112. Meta-Learning without Memorization</b>  <a href="https://openreview.net/pdf?id=BklEFpEYwS" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title112" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICLR 2020. <br>&nbsp;&nbsp;<i>Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, Chelsea Finn</i><br>
<font size="3">
The ability to learn new concepts with small amounts of data is a critical aspect of intelligence that has proven challenging for deep learning methods. Meta-learning has emerged as a promising technique for leveraging data from previous tasks to enable efficient learning of new tasks. However, most meta-learning algorithms implicitly require that the meta-training tasks be mutually-exclusive, such that no single model can solve all of the tasks at once. For example, when creating tasks for few-shot image classification, prior work uses a per-task random assignment of image classes to N-way classification labels. If this is not done, the meta-learner can ignore the task training data and learn a single model that performs all of the meta-training tasks zero-shot, but does not adapt effectively to new image classes.  This requirement means that the user must take great care in designing the tasks, for example by shuffling labels or removing task identifying information from the inputs. In some domains, this makes meta-learning entirely inapplicable. In this paper, we address this challenge by designing a meta-regularization objective using information theory that places precedence on data-driven adaptation. This causes the meta-learner to decide what must be learned from the task training data and what should be inferred from the task testing input. By doing so, our algorithm can successfully use data from non-mutually-exclusive tasks to efficiently adapt to novel tasks. We demonstrate its applicability to both contextual and gradient-based meta-learning algorithms, and apply it in practical settings where applying standard meta-learning has been difficult. Our approach substantially outperforms standard meta-learning algorithms in these settings.</font>
<br>
</div>


<hr>
<div id="paper113"> <b>113. Dynamics-Aware Unsupervised Discovery of Skills</b>  <a href="https://openreview.net/pdf?id=HJgLZR4KvH" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title113" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICLR 2020. <br>&nbsp;&nbsp;<i>Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, Karol Hausman</i><br>
<font size="3">
Conventionally, model-based reinforcement learning (MBRL) aims to learn a global model for the dynamics of the environment. A good model can potentially enable planning algorithms to generate a large variety of behaviors and solve diverse tasks. However, learning an accurate model for complex dynamical systems is difficult, and even then, the model might not generalize well outside the distribution of states on which it was trained. In this work, we combine model-based learning with model-free learning of primitives that make model-based planning easy. To that end, we aim to answer the question: how can we discover skills whose outcomes are easy to predict? We propose an unsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS), which simultaneously discovers predictable behaviors and learns their dynamics. Our method can leverage continuous skill spaces, theoretically, allowing us to learn infinitely many behaviors even for high-dimensional state-spaces. We demonstrate that zero-shot planning in the learned latent space significantly outperforms standard MBRL and model-free goal-conditioned RL, can handle sparse-reward tasks, and substantially improves over prior hierarchical RL methods for unsupervised skill discovery.</font>
<br>
</div>


<hr>
<div id="paper114"> <b>114. Convolutional Conditional Neural Processes</b>  <a href="https://openreview.net/pdf?id=Skey4eBYPS" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title114" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICLR 2020. <br>&nbsp;&nbsp;<i>Jonathan Gordon, Wessel P. Bruinsma, Andrew Y. K. Foong, James Requeima, Yann Dubois, Richard E. Turner</i><br>
<font size="3">
We introduce the Convolutional Conditional Neural Process (ConvCNP), a new member of the Neural Process family that models translation equivariance in the data. Translation equivariance is an important inductive bias for many learning problems including time series modelling, spatial data, and images. The model embeds data sets into an infinite-dimensional function space, as opposed to finite-dimensional vector spaces. To formalize this notion, we extend the theory of neural representations of sets to include functional representations, and demonstrate that any translation-equivariant embedding can be represented using a convolutional deep-set. We evaluate ConvCNPs in several settings, demonstrating that they achieve state-of-the-art performance compared to existing NPs. We demonstrate that building in translation equivariance enables zero-shot generalization to challenging, out-of-domain tasks.</font>
<br>
</div>


<hr>
<div id="paper115"> <b>115. Lifelong Zero-Shot Learning</b>  <a href="https://www.ijcai.org/proceedings/2020/0077.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title115" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2020. <br>&nbsp;&nbsp;<i>Kun Wei, Cheng Deng, Xu Yang</i><br>
<font size="3">
Zero-Shot Learning (ZSL) handles the problem that some testing classes never appear in training set. Existing ZSL methods are designed for learning from a fixed training set, which do not have the ability to capture and accumulate the knowledge of multiple training sets, causing them infeasible to many real-world applications. In this paper, we propose a new ZSL setting, named as Lifelong Zero-Shot Learning (LZSL), which aims to accumulate the knowledge during the learning from multiple datasets and recognize unseen classes of all trained datasets. Besides, a novel method is conducted to realize LZSL, which effectively alleviates the Catastrophic Forgetting in the continuous training process. Specifically, considering those datasets containing different semantic embeddings, we utilize Variational Auto-Encoder to obtain unified semantic representations. Then, we leverage selective retraining strategy to preserve thetrained weights of previous tasks and avoid negative transfer when fine-tuning the entire model. Finally, knowledge distillation is employed to transfer knowledge from previous training stages to current stage. We also design the LZSL evaluation protocol and the challenging benchmarks. Extensive experiments on these benchmarks indicate that our method tackles LZSL problem effectively, while existing ZSL methods fail.</font>
<br>
</div>


<hr>
<div id="paper116"> <b>116. Zero-Shot Object Detection via Learning an Embedding from Semantic Space to Visual Space</b>  <a href="https://www.ijcai.org/proceedings/2020/0126.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title116" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2020. <br>&nbsp;&nbsp;<i>Licheng Zhang, Xianzhi Wang, Lina Yao, Lin Wu, Feng Zheng</i><br>
<font size="3">
Zero-shot object detection (ZSD) has received considerable attention from the community of computer vision in recent years. It aims to simultaneously locate and categorize previously unseen objects during inference. One crucial problem of ZSD is how to accurately predict the label of each object proposal, i.e. categorizing object proposals, when conducting ZSD for unseen categories.Previous ZSD models generally relied on learning an embedding from visual space to semantic space or learning a joint embedding between semantic description and visual representation. As the features in the learned semantic space or the joint projected space tend to suffer from the hubness problem, namely the feature vectors are likely embedded to an area of incorrect labels, and thus it will lead to lower detection precision. In this paper, instead, we propose to learn a deep embedding from the semantic space to the visual space, which enables to well alleviate the hubness problem, because, compared with semantic space or joint embedding space, the distribution in visual space has smaller variance. After learning a deep embedding model, we perform $k$ nearest neighbor search in the visual space of unseen categories to determine the category of each semantic description. Extensive experiments on two public datasets show that our approach significantly outperforms the existing methods.</font>
<br>
</div>


<hr>
<div id="paper117"> <b>117. Progressive Domain-Independent Feature Decomposition Network for Zero-Shot Sketch-Based Image Retrieval</b>  <a href="https://www.ijcai.org/proceedings/2020/0137.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title117" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2020. <br>&nbsp;&nbsp;<i>Xinxun Xu, Muli Yang, Yanhua Yang, Hao Wang</i><br>
<font size="3">
Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) is a specific cross-modal retrieval task for searching natural images given free-hand sketches under the zero-shot scenario. Most existing methods solve this problem by simultaneously projecting visual features and semantic supervision into a low-dimensional common space for efficient retrieval. However, such low-dimensional projection destroys the completeness of semantic knowledge in original semantic space, so that it is unable to transfer useful knowledge well when learning semantic features from different modalities. Moreover, the domain information and semantic information are entangled in visual features, which is not conducive for cross-modal matching since it will hinder the reduction of domain gap between sketch and image. In this paper, we propose a Progressive Domain-independent Feature Decomposition (PDFD) network for ZS-SBIR. Specifically, with the supervision of original semantic knowledge, PDFD decomposes visual features into domain features and semantic ones, and then the semantic features are projected into common space as retrieval features for ZS-SBIR. The progressive projection strategy maintains strong semantic supervision. Besides, to guarantee the retrieval features to capture clean and complete semantic information, the cross-reconstruction loss is introduced to encourage that any combinations of retrieval features and domain features can reconstruct the visual features. Extensive experiments demonstrate the superiority of our PDFD over state-of-the-art competitors.</font>
<br>
</div>


<hr>
<div id="paper118"> <b>118. CoSDA-ML: Multi-Lingual Code-Switching Data Augmentation for Zero-Shot Cross-Lingual NLP</b>  <a href="https://www.ijcai.org/proceedings/2020/0533.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title118" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2020. <br>&nbsp;&nbsp;<i>Libo Qin, Minheng Ni, Yue Zhang, Wanxiang Che</i><br>
<font size="3">
Multi-lingual contextualized embeddings, such as multilingual-BERT (mBERT), haveshown success in a variety of zero-shot cross-lingual tasks.However, these models are limited by having inconsistent contextualized representations of subwords across different languages.Existing work addresses this issue by bilingual projection and fine-tuning technique.We propose a data augmentation framework to generate multi-lingual code-switching data to fine-tune mBERT, which encourages model to align representations from source and multiple target languages once by mixing their context information.Compared with the existing work, our method does not rely on bilingual sentences for training, and requires only one training process for multiple target languages. Experimental results on five tasks with 19 languages show that our method leads to significantly improved performances for all the tasks compared with mBERT.</font>
<br>
</div>


<hr>
<div id="paper119"> <b>119. Generalized Zero-Shot Text Classification for ICD Coding</b>  <a href="https://www.ijcai.org/proceedings/2020/0556.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title119" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2020. <br>&nbsp;&nbsp;<i>Congzheng Song, Shanghang Zhang, Najmeh Sadoughi, Pengtao Xie, Eric P. Xing</i><br>
<font size="3">
The International Classification of Diseases (ICD) is a list of classification codes for the diagnoses. Automatic ICD coding is a multi-label text classification problem with noisy clinical document inputs and long-tailed label distribution, making it difficult for fine-grained classification on both frequent and zero-shot codes at the same time, i.e. generalized zero-shot ICD coding. In this paper, we propose a latent feature generation framework to improve the prediction on unseen codes without compromising the performance on seen codes. Our framework generates semantically meaningful features for zero-shot codes by exploiting ICD code hierarchical structure and reconstructing the code-relevant keywords with a novel cycle architecture. To the best of our knowledge, this is the first adversarial generative model for generalized zero-shot learning on multi-label text classification. Extensive experiments demonstrate the effectiveness of our approach. On the public MIMIC-III dataset, our methods improve the F1 score from nearly 0 to 20.91% for the zero-shot codes, and increase the AUC score by 3% (absolute improvement) from previous state of the art. Code is available at https://github.com/csong27/gzsl_text.</font>
<br>
</div>


<hr>
<div id="paper120"> <b>120. Zero Shot Learning for Code Education: Rubric Sampling with Deep Learning Inference</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/3857/3735" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title120" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2019. AAAI Special Technical Track: AI for Social Impact<br>&nbsp;&nbsp;<i>Mike Wu, Milan Mosse, Noah D. Goodman, Chris Piech</i><br>
<font size="3">
In modern computer science education, massive open online courses (MOOCs) log thousands of hours of data about how students solve coding challenges. Being so rich in data, these platforms have garnered the interest of the machine learning community, with many new algorithms attempting to autonomously provide feedback to help future students learn. But what about those first hundred thousand students? In most educational contexts (i.e. classrooms), assignments do not have enough historical data for supervised learning. In this paper, we introduce a human-in-the-loop “rubric sampling” approach to tackle the “zero shot” feedback challenge. We are able to provide autonomous feedback for the first students working on an introductory programming assignment with accuracy that substantially outperforms data-hungry algorithms and approaches human level fidelity. Rubric sampling requires minimal teacher effort, can associate feedback with specific parts of a student’s solution and can articulate a student’s misconceptions in the language of the instructor. Deep learning inference enables rubric sampling to further improve as more assignment specific student data is acquired. We demonstrate our results on a novel dataset from Code.org, the world’s largest programming education platform.</font>
<br>
</div>


<hr>
<div id="paper121"> <b>121. From Zero-Shot Learning to Cold-Start Recommendation</b>  <a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4324/4202" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title121" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2019. AAAI Technical Track: Machine Learning<br>&nbsp;&nbsp;<i>Jingjing Li, Mengmeng Jing, Ke Lu, Lei Zhu, Yang Yang, Zi Huang</i><br>
<font size="3">
Zero-shot learning (ZSL) and cold-start recommendation (CSR) are two challenging problems in computer vision and recommender system, respectively. In general, they are independently investigated in different communities. This paper, however, reveals that ZSL and CSR are two extensions of the same intension. Both of them, for instance, attempt to predict unseen classes and involve two spaces, one for direct feature representation and the other for supplementary description. Yet there is no existing approach which addresses CSR from the ZSL perspective. This work, for the first time, formulates CSR as a ZSL problem, and a tailor-made ZSL method is proposed to handle CSR. Specifically, we propose a Lowrank Linear Auto-Encoder (LLAE), which challenges three cruxes, i.e., domain shift, spurious correlations and computing efficiency, in this paper. LLAE consists of two parts, a low-rank encoder maps user behavior into user attributes and a symmetric decoder reconstructs user behavior from user attributes. Extensive experiments on both ZSL and CSR tasks verify that the proposed method is a win-win formulation, i.e., not only can CSR be handled by ZSL models with a significant performance improvement compared with several conventional state-of-the-art methods, but the consideration of CSR can benefit ZSL as well.</font>
<br>
</div>


<hr>
<div id="paper122"> <b>122. Zero-Shot Adaptive Transfer for Conversational Language Understanding</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/4634/4512" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title122" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2019. AAAI Technical Track: Natural Language Processing<br>&nbsp;&nbsp;<i>Sungjin Lee, Rahul Jha</i><br>
<font size="3">
Conversational agents such as Alexa and Google Assistant constantly need to increase their language understanding capabilities by adding new domains. A massive amount of labeled data is required for training each new domain. While domain adaptation approaches alleviate the annotation cost, prior approaches suffer from increased training time and suboptimal concept alignments. To tackle this, we introduce a novel Zero-Shot Adaptive Transfer method for slot tagging that utilizes the slot description for transferring reusable concepts across domains, and enjoys efficient training without any explicit concept alignments. Extensive experimentation over a dataset of 10 domains relevant to our commercial personal digital assistant shows that our model outperforms previous state-of-the-art systems by a large margin, and achieves an even higher improvement in the low data regime.</font>
<br>
</div>


<hr>
<div id="paper123"> <b>123. Zero-Shot Neural Transfer for Cross-Lingual Entity Linking</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/4670/4548" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title123" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2019. AAAI Technical Track: Natural Language Processing<br>&nbsp;&nbsp;<i>Shruti Rijhwani, Jiateng Xie, Graham Neubig, Jaime G. Carbonell</i><br>
<font size="3">
Cross-lingual entity linking maps an entity mention in a source language to its corresponding entry in a structured knowledge base that is in a different (target) language. While previous work relies heavily on bilingual lexical resources to bridge the gap between the source and the target languages, these resources are scarce or unavailable for many low-resource languages. To address this problem, we investigate zero-shot cross-lingual entity linking, in which we assume no bilingual lexical resources are available in the source low-resource language. Specifically, we propose pivot-based</font>
<br>
</div>


<hr>
<div id="paper124"> <b>124. Analysis of Joint Multilingual Sentence Representations and Semantic K-Nearest Neighbor Graphs</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/4677/4555" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title124" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2019. AAAI Technical Track: Natural Language Processing<br>&nbsp;&nbsp;<i>Holger Schwenk, Douwe Kiela, Matthijs Douze</i><br>
<font size="3">
Multilingual sentence and document representations are becoming increasingly important. We build on recent advances in multilingual sentence encoders, with a focus on efficiency and large-scale applicability. Specifically, we construct and investigate the k-nn graph over the joint space of 566 million news sentences in seven different languages. We show excellent multilingual retrieval quality on the UN corpus of 11.3M sentences, which extends to the zero-shot case where we have never seen a language. We provide a detailed analysis of both the multilingual sentence encoder for twenty-one European languages and the learned graph. Our sentence encoder is language agnostic and supports code switching.</font>
<br>
</div>


<hr>
<div id="paper125"> <b>125. QUAREL: A Dataset and Models for Answering Questions about Qualitative Relationships</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/4687/4565" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title125" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2019. AAAI Technical Track: Natural Language Processing<br>&nbsp;&nbsp;<i>Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau Yih, Ashish Sabharwal</i><br>
<font size="3">
Many natural la guage questions require recognizing and reasoning with qualitative relationships (e.g., in science, economics, and medicine), but are challenging to answer with corpus-based methods. Qualitative modeling provides tools that support such reasoning, but the semantic parsing task of mapping questions into those models has formidable challenges. We present QUAREL, a dataset of diverse story questions involving qualitative relationships that characterize these challenges, and techniques that begin to address them. The dataset has 2771 questions relating 19 different types of quantities. For example, “Jenny observes that the robot vacuum cleaner moves slower on the living room carpet than on the bedroom carpet. Which carpet has more friction?” We contribute (1) a simple and flexible conceptual framework for representing these kinds of questions; (2) the QUAREL dataset, including logical forms, exemplifying the parsing challenges; and (3) two novel models for this task, built as extensions of type-constrained semantic parsing. The first of these models (called QUASP+) significantly outperforms off-the-shelf tools on QUAREL. The second (QUASP+ZERO) demonstrates zero-shot capability, i.e., the ability to handle new qualitative relationships without requiring additional training data, something not possible with previous models. This work thus makes inroads into answering complex, qualitative questions that require reasoning, and scaling to new relationships at low cost. The dataset and models are available at http://data.allenai.org/quarel.</font>
<br>
</div>


<hr>
<div id="paper126"> <b>126. Energy Confused Adversarial Metric Learning for Zero-Shot Image Retrieval and Clustering</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/4822/4695" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title126" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2019. AAAI Technical Track: Vision<br>&nbsp;&nbsp;<i>Binghui Chen, Weihong Deng</i><br>
<font size="3">
Deep metric learning has been widely applied in many computer vision tasks, and recently, it is more attractive in zeroshot image retrieval and clustering (ZSRC) where a good embedding is requested such that the unseen classes can be distinguished well. Most existing works deem this ’good’ embedding just to be the discriminative one and thus race to devise powerful metric objectives or hard-sample mining strategies for leaning discriminative embedding. However, in this paper, we first emphasize that the generalization ability is a core ingredient of this ’good’ embedding as well and largely affects the metric performance in zero-shot settings as a matter of fact. Then, we propose the Energy Confused Adversarial Metric Learning (ECAML) framework to explicitly optimize a robust metric. It is mainly achieved by introducing an interesting Energy Confusion regularization term, which daringly breaks away from the traditional metric learning idea of discriminative objective devising, and seeks to ’confuse’ the learned model so as to encourage its generalization ability by reducing overfitting on the seen classes. We train this confusion term together with the conventional metric objective in an adversarial manner. Although it seems weird to ’confuse’ the network, we show that our ECAML indeed serves as an efficient regularization technique for metric learning and is applicable to various conventional metric methods. This paper empirically and experimentally demonstrates the importance of learning embedding with good generalization, achieving state-of-theart performances on the popular CUB, CARS, Stanford Online Products and In-Shop datasets for ZSRC tasks. Code available at http://www.bhchen.cn/.</font>
<br>
</div>


<hr>
<div id="paper127"> <b>127. I Know the Relationships: Zero-Shot Action Recognition via Two-Stream Graph Convolutional Networks and Knowledge Graphs</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/4843/4716" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title127" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2019. AAAI Technical Track: Vision<br>&nbsp;&nbsp;<i>Junyu Gao, Tianzhu Zhang, Changsheng Xu</i><br>
<font size="3">
Recently, with the ever-growing action categories, zero-shot action recognition (ZSAR) has been achieved by automatically mining the underlying concepts (e.g., actions, attributes) in videos. However, most existing methods only exploit the visual cues of these concepts but ignore external knowledge information for modeling explicit relationships between them. In fact, humans have remarkable ability to transfer knowledge learned from familiar classes to recognize unfamiliar classes. To narrow the knowledge gap between existing methods and humans, we propose an end-to-end ZSAR framework based on a structured knowledge graph, which can jointly model the relationships between action-attribute, action-action, and attribute-attribute. To effectively leverage the knowledge graph, we design a novel Two-Stream Graph Convolutional Network (TS-GCN) consisting of a classifier branch and an instance branch. Specifically, the classifier branch takes the semantic-embedding vectors of all the concepts as input, then generates the classifiers for action categories. The instance branch maps the attribute embeddings and scores of each video instance into an attribute-feature space. Finally, the generated classifiers are evaluated on the attribute features of each video, and a classification loss is adopted for optimizing the whole network. In addition, a self-attention module is utilized to model the temporal information of videos. Extensive experimental results on three realistic action benchmarks Olympic Sports, HMDB51 and UCF101 demonstrate the favorable performance of our proposed framework.</font>
<br>
</div>


<hr>
<div id="paper128"> <b>128. Dual-View Ranking with Hardness Assessment for Zero-Shot Learning</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/4850/4723" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title128" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2019. AAAI Technical Track: Vision<br>&nbsp;&nbsp;<i>Yuchen Guo, Guiguang Ding, Jungong Han, Xiaohan Ding, Sicheng Zhao, Zheng Wang, Chenggang Yan, Qionghai Dai</i><br>
<font size="3">
Zero-shot learning (ZSL) is to build recognition models for previously unseen target classes which have no labeled data for training by transferring knowledge from some other related auxiliary source classes with abundant labeled samples to the target ones with class attributes as the bridge. The key is to learn a similarity based ranking function between samples and class labels using the labeled source classes so that the proper (unseen) class label for a test sample can be identified by the function. In order to learn the function, single-view ranking based loss is widely used which aims to rank the true label prior to the other labels for a training sample. However, we argue that the ranking can be performed from the other view, which aims to place the images belonging to a label before the images from the other classes. Motivated by it, we propose a novel DuAl-view RanKing (DARK) loss for zeroshot learning simultaneously ranking labels for an image by point-to-point metric and ranking images for a label by pointto-set metric, which is capable of better modeling the relationship between images and classes. In addition, we also notice that previous ZSL approaches mostly fail to well exploit the hardness of training samples, either using only very hard ones or using all samples indiscriminately. In this work, we also introduce a sample hardness assessment method to ZSL which assigns different weights to training samples based on their hardness, which leads to a more accurate and robust ZSL model. Experiments on benchmarks demonstrate that DARK outperforms the state-of-the-arts for (generalized) ZSL.</font>
<br>
</div>


<hr>
<div id="paper129"> <b>129. Zero-Shot Object Detection with Textual Descriptions</b>  <a href="https://144.208.67.177/ojs/index.php/AAAI/article/view/4891/4764" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title129" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2019. AAAI Technical Track: Vision<br>&nbsp;&nbsp;<i>Zhihui Li, Lina Yao, Xiaoqin Zhang, Xianzhi Wang, Salil S. Kanhere, Huaxiang Zhang</i><br>
<font size="3">
Object detection is important in real-world applications. Existing methods mainly focus on object detection with sufficient labelled training data or zero-shot object detection with only concept names. In this paper, we address the challenging problem of zero-shot object detection with natural language description, which aims to simultaneously detect and recognize novel concept instances with textual descriptions. We propose a novel deep learning framework to jointly learn visual units, visual-unit attention and word-level attention, which are combined to achieve word-proposal affinity by an element-wise multiplication. To the best of our knowledge, this is the first work on zero-shot object detection with textual descriptions. Since there is no directly related work in the literature, we investigate plausible solutions based on existing zero-shot object detection for a fair comparison. We conduct extensive experiments on three challenging benchmark datasets. The extensive experimental results confirm the superiority of the proposed model.</font>
<br>
</div>


<hr>
<div id="paper130"> <b>130. Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video Captioning</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/4926/4799" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title130" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2019. AAAI Technical Track: Vision<br>&nbsp;&nbsp;<i>Xin Wang, Jiawei Wu, Da Zhang, Yu Su, William Yang Wang</i><br>
<font size="3">
Although promising results have been achieved in video captioning, existing models are limited to the fixed inventory of activities in the training corpus, and do not generalize to open vocabulary scenarios. Here we introduce a novel task, zeroshot video captioning, that aims at describing out-of-domain videos of unseen activities. Videos of different activities usually require different captioning strategies in many aspects, i.e. word selection, semantic construction, and style expression etc, which poses a great challenge to depict novel activities without paired training data. But meanwhile, similar activities share some of those aspects in common. Therefore, we propose a principled Topic-Aware Mixture of Experts (TAMoE) model for zero-shot video captioning, which learns to compose different experts based on different topic embeddings, implicitly transferring the knowledge learned from seen activities to unseen ones. Besides, we leverage external topic-related text corpus to construct the topic embedding for each activity, which embodies the most relevant semantic vectors within the topic. Empirical results not only validate the effectiveness of our method in utilizing semantic knowledge for video captioning, but also show its strong generalization ability when describing novel activities.</font>
<br>
</div>


<hr>
<div id="paper131"> <b>131. Towards Fluid Machine Intelligence: Can We Make a Gifted AI?</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/5046/4919" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title131" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2019. Senior Member Presentation Track: Blue Sky Papers<br>&nbsp;&nbsp;<i>Ian Davidson, Peter B. Walker</i><br>
<font size="3">
Most applications of machine intelligence have focused on demonstrating crystallized intelligence. Crystallized intelligence relies on accessing problem-specific knowledge, skills and experience stored in long term memory. In this paper, we challenge the AI community to design AIs to completely take tests of fluid intelligence which assess the ability to solve novel problems using problem-independent solving skills. Tests of fluid intelligence such as the NNAT are used extensively by schools to determine entry into gifted education programs. We explain the differences between crystallized and fluid intelligence, the importance and capabilities of machines demonstrating fluid intelligence and pose several challenges to the AI community, including that a machine taking such a test would be considered gifted by school districts in the state of California. Importantly, we show existing work on seemingly related fields such as transfer, zero-shot, life-long and meta learning (in their current form) are not directly capable of demonstrating fluid intelligence but instead are task-transductive mechanisms.</font>
<br>
</div>


<hr>
<div id="paper132"> <b>132. Transductive Zero-Shot Learning via Visual Center Adaptation</b>  <a href="https://aaai.org/ojs/index.php/AAAI/article/view/5166/5039" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title132" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2019. Student Abstract Track<br>&nbsp;&nbsp;<i>Ziyu Wan, Yan Li, Min Yang, Junge Zhang</i><br>
<font size="3">
In this paper, we propose a Visual Center Adaptation Method (VCAM) to address the domain shift problem in zero-shot learning. For the seen classes in the training data, VCAM builds an embedding space by learning the mapping from semantic space to some visual centers. While for unseen classes in the test data, the construction of embedding space is constrained by a symmetric Chamfer-distance term, aiming to adapt the distribution of the synthetic visual centers to that of the real cluster centers. Therefore the learned embedding space can generalize the unseen classes well. Experiments on two widely used datasets demonstrate that our model significantly outperforms state-of-the-art methods.</font>
<br>
</div>


<hr>
<div id="paper133"> <b>133. Massively Multilingual Transfer for NER</b>  <a href="https://www.aclweb.org/anthology/P19-1015.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title133" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. <br>&nbsp;&nbsp;<i>Afshin Rahimi, Yuan Li, Trevor Cohn</i><br>
<font size="3">
In cross-lingual transfer, NLP models over one or more source languages are applied to a low-resource target language. While most prior work has used a single source model or a few carefully selected models, here we consider a “massive” setting with many such models. This setting raises the problem of poor transfer, particularly from distant languages. We propose two techniques for modulating the transfer, suitable for zero-shot or few-shot learning, respectively. Evaluating on named entity recognition, we show that our techniques are much more effective than strong baselines, including standard ensembling, and our unsupervised method rivals oracle selection of the single best individual model.</font>
<br>
</div>


<hr>
<div id="paper134"> <b>134. Cross-Domain Generalization of Neural Constituency Parsers</b>  <a href="https://www.aclweb.org/anthology/P19-1031.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title134" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. <br>&nbsp;&nbsp;<i>Daniel Fried, Nikita Kitaev, Dan Klein</i><br>
<font size="3">
Neural parsers obtain state-of-the-art results on benchmark treebanks for constituency parsing—but to what degree do they generalize to other domains? We present three results about the generalization of neural parsers in a zero-shot setting: training on trees from one corpus and evaluating on out-of-domain corpora. First, neural and non-neural parsers generalize comparably to new domains. Second, incorporating pre-trained encoder representations into neural parsers substantially improves their performance across all domains, but does not give a larger relative improvement for out-of-domain treebanks. Finally, despite the rich input representations they learn, neural parsers still benefit from structured output prediction of output trees, yielding higher exact match accuracy and stronger generalization both to larger text spans and to out-of-domain corpora. We analyze generalization on English and Chinese corpora, and in the process obtain state-of-the-art parsing results for the Brown, Genia, and English Web treebanks.</font>
<br>
</div>


<hr>
<div id="paper135"> <b>135. Know What You Don’t Know: Modeling a Pragmatic Speaker that Refers to Objects of Unknown Categories</b>  <a href="https://www.aclweb.org/anthology/P19-1063.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title135" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. <br>&nbsp;&nbsp;<i>Sina Zarrieß, David Schlangen</i><br>
<font size="3">
Zero-shot learning in Language & Vision is the task of correctly labelling (or naming) objects of novel categories. Another strand of work in L&V aims at pragmatically informative rather than “correct” object descriptions, e.g. in reference games. We combine these lines of research and model zero-shot reference games, where a speaker needs to successfully refer to a novel object in an image. Inspired by models of “rational speech acts”, we extend a neural generator to become a pragmatic speaker reasoning about uncertain object categories. As a result of this reasoning, the generator produces fewer nouns and names of distractor categories as compared to a literal speaker. We show that this conversational strategy for dealing with novel objects often improves communicative success, in terms of resolution accuracy of an automatic listener.</font>
<br>
</div>


<hr>
<div id="paper136"> <b>136. Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems</b>  <a href="https://www.aclweb.org/anthology/P19-1078.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title136" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. <br>&nbsp;&nbsp;<i>Chien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl, Caiming Xiong, Richard Socher, Pascale Fung</i><br>
<font size="3">
Over-dependence on domain ontology and lack of sharing knowledge across domains are two practical and yet less studied problems of dialogue state tracking. Existing approaches generally fall short when tracking unknown slot values during inference and often have difficulties in adapting to new domains. In this paper, we propose a Transferable Dialogue State Generator (TRADE) that generates dialogue states from utterances using copy mechanism, facilitating transfer when predicting (domain, slot, value) triplets not encountered during training. Our model is composed of an utterance encoder, a slot gate, and a state generator, which are shared across domains. Empirical results demonstrate that TRADE achieves state-of-the-art 48.62% joint goal accuracy for the five domains of MultiWOZ, a human-human dialogue dataset. In addition, we show the transferring ability by simulating zero-shot and few-shot dialogue state tracking for unseen domains. TRADE achieves 60.58% joint goal accuracy in one of the zero-shot domains, and is able to adapt to few-shot cases without forgetting already trained domains.</font>
<br>
</div>


<hr>
<div id="paper137"> <b>137. A Compact and Language-Sensitive Multilingual Translation Method</b>  <a href="https://www.aclweb.org/anthology/P19-1117.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title137" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. <br>&nbsp;&nbsp;<i>Yining Wang, Long Zhou, Jiajun Zhang, Feifei Zhai, Jingfang Xu, Chengqing Zong</i><br>
<font size="3">
Multilingual neural machine translation (Multi-NMT) with one encoder-decoder model has made remarkable progress due to its simple deployment. However, this multilingual translation paradigm does not make full use of language commonality and parameter sharing between encoder and decoder. Furthermore, this kind of paradigm cannot outperform the individual models trained on bilingual corpus in most cases. In this paper, we propose a compact and language-sensitive method for multilingual translation. To maximize parameter sharing, we first present a universal representor to replace both encoder and decoder models. To make the representor sensitive for specific languages, we further introduce language-sensitive embedding, attention, and discriminator with the ability to enhance model performance. We verify our methods on various translation scenarios, including one-to-many, many-to-many and zero-shot. Extensive experiments demonstrate that our proposed methods remarkably outperform strong standard multilingual translation systems on WMT and IWSLT datasets. Moreover, we find that our model is especially helpful in low-resource and zero-shot translation scenarios.</font>
<br>
</div>


<hr>
<div id="paper138"> <b>138. Improved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations</b>  <a href="https://www.aclweb.org/anthology/P19-1121.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title138" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. <br>&nbsp;&nbsp;<i>Jiatao Gu, Yong Wang, Kyunghyun Cho, Victor O.K. Li</i><br>
<font size="3">
Zero-shot translation, translating between language pairs on which a Neural Machine Translation (NMT) system has never been trained, is an emergent property when training the system in multilingual settings. However, naive training for zero-shot NMT easily fails, and is sensitive to hyper-parameter setting. The performance typically lags far behind the more conventional pivot-based approach which translates twice using a third language as a pivot. In this work, we address the degeneracy problem due to capturing spurious correlations by quantitatively analyzing the mutual information between language IDs of the source and decoded sentences. Inspired by this analysis, we propose to use two simple but effective approaches: (1) decoder pre-training; (2) back-translation. These methods show significant improvement (4 22 BLEU points) over the vanilla zero-shot translation on three challenging multilingual datasets, and achieve similar or better results than the pivot-based approach.</font>
<br>
</div>


<hr>
<div id="paper139"> <b>139. XQA: A Cross-lingual Open-domain Question Answering Dataset</b>  <a href="https://www.aclweb.org/anthology/P19-1227.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title139" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. <br>&nbsp;&nbsp;<i>Jiahua Liu, Yankai Lin, Zhiyuan Liu, Maosong Sun</i><br>
<font size="3">
Open-domain question answering (OpenQA) aims to answer questions through text retrieval and reading comprehension. Recently, lots of neural network-based models have been proposed and achieved promising results in OpenQA. However, the success of these models relies on a massive volume of training data (usually in English), which is not available in many other languages, especially for those low-resource languages. Therefore, it is essential to investigate cross-lingual OpenQA. In this paper, we construct a novel dataset XQA for cross-lingual OpenQA research. It consists of a training set in English as well as development and test sets in eight other languages. Besides, we provide several baseline systems for cross-lingual OpenQA, including two machine translation-based methods and one zero-shot cross-lingual method (multilingual BERT). Experimental results show that the multilingual BERT model achieves the best results in almost all target languages, while the performance of cross-lingual OpenQA is still much lower than that of English. Our analysis indicates that the performance of cross-lingual OpenQA is related to not only how similar the target language and English are, but also how difficult the question set of the target language is. The XQA dataset is publicly available at http://github.com/thunlp/XQA.</font>
<br>
</div>


<hr>
<div id="paper140"> <b>140. Zero-Shot Cross-Lingual Abstractive Sentence Summarization through Teaching Generation and Attention</b>  <a href="https://www.aclweb.org/anthology/P19-1305.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title140" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. <br>&nbsp;&nbsp;<i>Xiangyu Duan, Mingming Yin, Min Zhang, Boxing Chen, Weihua Luo</i><br>
<font size="3">
Abstractive Sentence Summarization (ASSUM) targets at grasping the core idea of the source sentence and presenting it as the summary. It is extensively studied using statistical models or neural models based on the large-scale monolingual source-summary parallel corpus. But there is no cross-lingual parallel corpus, whose source sentence language is different to the summary language, to directly train a cross-lingual ASSUM system. We propose to solve this zero-shot problem by using resource-rich monolingual ASSUM system to teach zero-shot cross-lingual ASSUM system on both summary word generation and attention. This teaching process is along with a back-translation process which simulates source-summary pairs. Experiments on cross-lingual ASSUM task show that our proposed method is significantly better than pipeline baselines and previous works, and greatly enhances the cross-lingual performances closer to the monolingual performances.</font>
<br>
</div>


<hr>
<div id="paper141"> <b>141. Zero-Shot Entity Linking by Reading Entity Descriptions</b>  <a href="https://www.aclweb.org/anthology/P19-1335.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title141" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. <br>&nbsp;&nbsp;<i>Lajanugen Logeswaran, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin, Honglak Lee</i><br>
<font size="3">
We present the zero-shot entity linking task, where mentions must be linked to unseen entities without in-domain labeled data. The goal is to enable robust transfer to highly specialized domains, and so no metadata or alias tables are assumed. In this setting, entities are only identified by text descriptions, and models must rely strictly on language understanding to resolve the new entities. First, we show that strong reading comprehension models pre-trained on large unlabeled data can be used to generalize to unseen entities. Second, we propose a simple and effective adaptive pre-training strategy, which we term domain-adaptive pre-training (DAP), to address the domain shift problem associated with linking unseen entities in a new domain. We present experiments on a new dataset that we construct for this task and show that DAP improves over strong pre-training baselines, including BERT. The data and code are available at https://github.com/lajanugen/zeshel.</font>
<br>
</div>


<hr>
<div id="paper142"> <b>142. A Multilingual BPE Embedding Space for Universal Sentiment Lexicon Induction</b>  <a href="https://www.aclweb.org/anthology/P19-1341.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title142" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. <br>&nbsp;&nbsp;<i>Mengjie Zhao, Hinrich Schütze</i><br>
<font size="3">
We present a new method for sentiment lexicon induction that is designed to be applicable to the entire range of typological diversity of the world’s languages. We evaluate our method on Parallel Bible Corpus+ (PBC+), a parallel corpus of 1593 languages. The key idea is to use Byte Pair Encodings (BPEs) as basic units for multilingual embeddings. Through zero-shot transfer from English sentiment, we learn a seed lexicon for each language in the domain of PBC+. Through domain adaptation, we then generalize the domain-specific lexicon to a general one. We show – across typologically diverse languages in PBC+ – good quality of seed and general-domain sentiment lexicons by intrinsic and extrinsic and by automatic and human evaluation. We make freely available our code, seed sentiment lexicons for all 1593 languages and induced general-domain sentiment lexicons for 200 languages.</font>
<br>
</div>


<hr>
<div id="paper143"> <b>143. Towards Language Agnostic Universal Representations</b>  <a href="https://www.aclweb.org/anthology/P19-1395.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title143" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. <br>&nbsp;&nbsp;<i>Armen Aghajanyan, Xia Song, Saurabh Tiwary</i><br>
<font size="3">
When a bilingual student learns to solve word problems in math, we expect the student to be able to solve these problem in both languages the student is fluent in, even if the math lessons were only taught in one language. However, current representations in machine learning are language dependent. In this work, we present a method to decouple the language from the problem by learning language agnostic representations and therefore allowing training a model in one language and applying to a different one in a zero shot fashion. We learn these representations by taking inspiration from linguistics, specifically the Universal Grammar hypothesis and learn universal latent representations that are language agnostic. We demonstrate the capabilities of these representations by showing that models trained on a single language using language agnostic representations achieve very similar accuracies in other languages.</font>
<br>
</div>


<hr>
<div id="paper144"> <b>144. Zero-Shot Semantic Parsing for Instructions</b>  <a href="https://www.aclweb.org/anthology/P19-1438.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title144" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. <br>&nbsp;&nbsp;<i>Ofer Givoli, Roi Reichart</i><br>
<font size="3">
We consider a zero-shot semantic parsing task: parsing instructions into compositional logical forms, in domains that were not seen during training. We present a new dataset with 1,390 examples from 7 application domains (e.g. a calendar or a file manager), each example consisting of a triplet: (a) the application’s initial state, (b) an instruction, to be carried out in the context of that state, and (c) the state of the application after carrying out the instruction. We introduce a new training algorithm that aims to train a semantic parser on examples from a set of source domains, so that it can effectively parse instructions from an unknown target domain. We integrate our algorithm into the floating parser of Pasupat and Liang (2015), and further augment the parser with features and a logical form candidate filtering logic, to support zero-shot adaptation. Our experiments with various zero-shot adaptation setups demonstrate substantial performance gains over a non-adapted parser.</font>
<br>
</div>


<hr>
<div id="paper145"> <b>145. How Multilingual is Multilingual BERT?</b>  <a href="https://www.aclweb.org/anthology/P19-1493.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title145" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. <br>&nbsp;&nbsp;<i>Telmo Pires, Eva Schlinger, Dan Garrette</i><br>
<font size="3">
In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.</font>
<br>
</div>


<hr>
<div id="paper146"> <b>146. Label-Agnostic Sequence Labeling by Copying Nearest Neighbors</b>  <a href="https://www.aclweb.org/anthology/P19-1533.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title146" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. <br>&nbsp;&nbsp;<i>Sam Wiseman, Karl Stratos</i><br>
<font size="3">
Retrieve-and-edit based approaches to structured prediction, where structures associated with retrieved neighbors are edited to form new structures, have recently attracted increased interest. However, much recent work merely conditions on retrieved structures (e.g., in a sequence-to-sequence framework), rather than explicitly manipulating them. We show we can perform accurate sequence labeling by explicitly (and only) copying labels from retrieved neighbors. Moreover, because this copying is label-agnostic, we can achieve impressive performance in zero-shot sequence-labeling tasks. We additionally consider a dynamic programming approach to sequence labeling in the presence of retrieved neighbors, which allows for controlling the number of distinct (copied) segments used to form a prediction, and leads to both more interpretable and accurate predictions.</font>
<br>
</div>


<hr>
<div id="paper147"> <b>147. Robust Zero-Shot Cross-Domain Slot Filling with Example Values</b>  <a href="https://www.aclweb.org/anthology/P19-1547.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title147" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. <br>&nbsp;&nbsp;<i>Darsh Shah, Raghav Gupta, Amir Fayazi, Dilek Hakkani-Tur</i><br>
<font size="3">
Task-oriented dialog systems increasingly rely on deep learning-based slot filling models, usually needing extensive labeled training data for target domains. Often, however, little to no target domain training data may be available, or the training and target domain schemas may be misaligned, as is common for web forms on similar websites. Prior zero-shot slot filling models use slot descriptions to learn concepts, but are not robust to misaligned schemas. We propose utilizing both the slot description and a small number of examples of slot values, which may be easily available, to learn semantic representations of slots which are transferable across domains and robust to misaligned schemas. Our approach outperforms state-of-the-art models on two multi-domain datasets, especially in the low-data setting.</font>
<br>
</div>


<hr>
<div id="paper148"> <b>148. Zero-shot Word Sense Disambiguation using Sense Definition Embeddings</b>  <a href="https://www.aclweb.org/anthology/P19-1568.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title148" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. <br>&nbsp;&nbsp;<i>Sawan Kumar, Sharmistha Jat, Karan Saxena, Partha Talukdar</i><br>
<font size="3">
Word Sense Disambiguation (WSD) is a long-standing but open problem in Natural Language Processing (NLP). WSD corpora are typically small in size, owing to an expensive annotation process. Current supervised WSD methods treat senses as discrete labels and also resort to predicting the Most-Frequent-Sense (MFS) for words unseen during training. This leads to poor performance on rare and unseen senses. To overcome this challenge, we propose Extended WSD Incorporating Sense Embeddings (EWISE), a supervised model to perform WSD by predicting over a continuous sense embedding space as opposed to a discrete label space. This allows EWISE to generalize over both seen and unseen senses, thus achieving generalized zero-shot learning. To obtain target sense embeddings, EWISE utilizes sense definitions. EWISE learns a novel sentence encoder for sense definitions by using WordNet relations and also ConvE, a recently proposed knowledge graph embedding method. We also compare EWISE against other sentence encoders pretrained on large corpora to generate definition embeddings. EWISE achieves new state-of-the-art WSD performance.</font>
<br>
</div>


<hr>
<div id="paper149"> <b>149. Large-Scale Multi-Label Text Classification on EU Legislation</b>  <a href="https://www.aclweb.org/anthology/P19-1636.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title149" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. <br>&nbsp;&nbsp;<i>Ilias Chalkidis, Emmanouil Fergadiotis, Prodromos Malakasiotis, Ion Androutsopoulos</i><br>
<font size="3">
We consider Large-Scale Multi-Label Text Classification (LMTC) in the legal domain. We release a new dataset of 57k legislative documents from EUR-LEX, annotated with ∼4.3k EUROVOC labels, which is suitable for LMTC, few- and zero-shot learning. Experimenting with several neural classifiers, we show that BIGRUs with label-wise attention perform better than other current state of the art methods. Domain-specific WORD2VEC and context-sensitive ELMO embeddings further improve performance. We also find that considering only particular zones of the documents is sufficient. This allows us to bypass BERT’s maximum text length limit and fine-tune BERT, obtaining the best results in all but zero-shot learning cases.</font>
<br>
</div>


<hr>
<div id="paper150"> <b>150. From Bilingual to Multilingual Neural Machine Translation by Incremental Training</b>  <a href="https://www.aclweb.org/anthology/P19-2033.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title150" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. Student Research Workshop<br>&nbsp;&nbsp;<i>Carlos Escolano, Marta R. Costa-jussà, José A. R. Fonollosa</i><br>
<font size="3">
Multilingual Neural Machine Translation approaches are based on the use of task specific models and the addition of one more language can only be done by retraining the whole system. In this work, we propose a new training schedule that allows the system to scale to more languages without modification of the previous components based on joint training and language-independent encoder/decoder modules allowing for zero-shot translation. This work in progress shows close results to state-of-the-art in the WMT task.</font>
<br>
</div>


<hr>
<div id="paper151"> <b>151. Multilingual NMT with a Language-Independent Attention Bridge</b>  <a href="https://www.aclweb.org/anthology/W19-4305.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title151" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)<br>&nbsp;&nbsp;<i>Raúl Vázquez, Alessandro Raganato, Jörg Tiedemann, Mathias Creutz</i><br>
<font size="3">
In this paper, we propose an architecture for machine translation (MT) capable of obtaining multilingual sentence representations by incorporating an intermediate attention bridge that is shared across all languages. We train the model with language-specific encoders and decoders that are connected through an inner-attention layer on the encoder side. The attention bridge exploits the semantics from each language for translation and develops into a language-agnostic meaning representation that can efficiently be used for transfer learning. We present a new framework for the efficient development of multilingual neural machine translation (NMT) using this model and scheduled training. We have tested the approach in a systematic way with a multi-parallel data set. The model achieves substantial improvements over strong bilingual models and performs well for zero-shot translation, which demonstrates its ability of abstraction and transfer learning.</font>
<br>
</div>


<hr>
<div id="paper152"> <b>152. Specializing Distributional Vectors of All Words for Lexical Entailment</b>  <a href="https://www.aclweb.org/anthology/W19-4310.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title152" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)<br>&nbsp;&nbsp;<i>Aishwarya Kamath, Jonas Pfeiffer, Edoardo Maria Ponti, Goran Glavaš, Ivan Vulić</i><br>
<font size="3">
Semantic specialization methods fine-tune distributional word vectors using lexical knowledge from external resources (e.g. WordNet) to accentuate a particular relation between words. However, such post-processing methods suffer from limited coverage as they affect only vectors of words seen in the external resources. We present the first post-processing method that specializes vectors of all vocabulary words – including those unseen in the resources – for the asymmetric relation of lexical entailment (LE) (i.e., hyponymy-hypernymy relation). Leveraging a partially LE-specialized distributional space, our POSTLE (i.e., post-specialization for LE) model learns an explicit global specialization function, allowing for specialization of vectors of unseen words, as well as word vectors from other languages via cross-lingual transfer. We capture the function as a deep feed-forward neural network: its objective re-scales vector norms to reflect the concept hierarchy while simultaneously attracting hyponymy-hypernymy pairs to better reflect semantic similarity. An extended model variant augments the basic architecture with an adversarial discriminator. We demonstrate the usefulness and versatility of POSTLE models with different input distributional spaces in different scenarios (monolingual LE and zero-shot cross-lingual LE transfer) and tasks (binary and graded LE). We report consistent gains over state-of-the-art LE-specialization methods, and successfully LE-specialize word vectors for languages without any external lexical knowledge.</font>
<br>
</div>


<hr>
<div id="paper153"> <b>153. Best Practices for Learning Domain-Specific Cross-Lingual Embeddings</b>  <a href="https://www.aclweb.org/anthology/W19-4327.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title153" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)<br>&nbsp;&nbsp;<i>Lena Shakurova, Beata Nyari, Chao Li, Mihai Rotaru</i><br>
<font size="3">
Cross-lingual embeddings aim to represent words in multiple languages in a shared vector space by capturing semantic similarities across languages. They are a crucial component for scaling tasks to multiple languages by transferring knowledge from languages with rich resources to low-resource languages. A common approach to learning cross-lingual embeddings is to train monolingual embeddings separately for each language and learn a linear projection from the monolingual spaces into a shared space, where the mapping relies on a small seed dictionary. While there are high-quality generic seed dictionaries and pre-trained cross-lingual embeddings available for many language pairs, there is little research on how they perform on specialised tasks. In this paper, we investigate the best practices for constructing the seed dictionary for a specific domain. We evaluate the embeddings on the sequence labelling task of Curriculum Vitae parsing and show that the size of a bilingual dictionary, the frequency of the dictionary words in the domain corpora and the source of data (task-specific vs generic) influence performance. We also show that the less training data is available in the low-resource language, the more the construction of the bilingual dictionary matters, and demonstrate that some of the choices are crucial in the zero-shot transfer learning case.</font>
<br>
</div>


<hr>
<div id="paper154"> <b>154. Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model</b>  <a href="https://www.aclweb.org/anthology/W19-4330.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title154" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)<br>&nbsp;&nbsp;<i>Muthu Chidambaram, Yinfei Yang, Daniel Cer, Steve Yuan, Yunhsuan Sung, Brian Strope, Ray Kurzweil</i><br>
<font size="3">
The scarcity of labeled training data across many languages is a significant roadblock for multilingual neural language processing. We approach the lack of in-language training data using sentence embeddings that map text written in different languages, but with similar meanings, to nearby embedding space representations. The representations are produced using a dual-encoder based model trained to maximize the representational similarity between sentence pairs drawn from parallel data. The representations are enhanced using multitask training and unsupervised monolingual corpora. The effectiveness of our multilingual sentence embeddings are assessed on a comprehensive collection of monolingual, cross-lingual, and zero-shot/few-shot learning tasks.</font>
<br>
</div>


<hr>
<div id="paper155"> <b>155. Improving Zero-shot Translation with Language-Independent Constraints</b>  <a href="https://www.aclweb.org/anthology/W19-5202.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title155" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. the Fourth Conference on Machine Translation (Volume 1: Research Papers)<br>&nbsp;&nbsp;<i>Ngoc-Quan Pham, Jan Niehues, Thanh-Le Ha, Alexander Waibel</i><br>
<font size="3">
An important concern in training multilingual neural machine translation (NMT) is to translate between language pairs unseen during training, i.e zero-shot translation. Improving this ability kills two birds with one stone by providing an alternative to pivot translation which also allows us to better understand how the model captures information between languages. In this work, we carried out an investigation on this capability of the multilingual NMT models. First, we intentionally create an encoder architecture which is independent with respect to the source language. Such experiments shed light on the ability of NMT encoders to learn multilingual representations, in general. Based on such proof of concept, we were able to design regularization methods into the standard Transformer model, so that the whole architecture becomes more robust in zero-shot conditions. We investigated the behaviour of such models on the standard IWSLT 2017 multilingual dataset. We achieved an average improvement of 2.23 BLEU points across 12 language pairs compared to the zero-shot performance of a state-of-the-art multilingual system. Additionally, we carry out further experiments in which the effect is confirmed even for language pairs with multiple intermediate pivots.</font>
<br>
</div>


<hr>
<div id="paper156"> <b>156. Evaluating the Supervised and Zero-shot Performance of Multi-lingual Translation Models</b>  <a href="https://www.aclweb.org/anthology/W19-5319.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title156" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2019. the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)<br>&nbsp;&nbsp;<i>Chris Hokamp, John Glover, Demian Gholipour Ghalandari</i><br>
<font size="3">
We study several methods for full or partial sharing of the decoder parameters of multi-lingual NMT models. Using only the WMT 2019 shared task parallel datasets for training, we evaluate both fully supervised and zero-shot translation performance in 110 unique translation directions. We use additional test sets and re-purpose evaluation methods recently used for unsupervised MT in order to evaluate zero-shot translation performance for language pairs where no gold-standard parallel data is available. To our knowledge, this is the largest evaluation of multi-lingual translation yet conducted in terms of the total size of the training data we use, and in terms of the number of zero-shot translation pairs we evaluate. We conduct an in-depth evaluation of the translation performance of different models, highlighting the trade-offs between methods of sharing decoder parameters. We find that models which have task-specific decoder parameters outperform models where decoder parameters are fully shared across all tasks.</font>
<br>
</div>


<hr>
<div id="paper157"> <b>157. Latent-Variable Generative Models for Data-Efficient Text Classification</b>  <a href="https://www.aclweb.org/anthology/D19-1048.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title157" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. <br>&nbsp;&nbsp;<i>Xiaoan Ding, Kevin Gimpel</i><br>
<font size="3">
Generative classifiers offer potential advantages over their discriminative counterparts, namely in the areas of data efficiency, robustness to data shift and adversarial examples, and zero-shot learning (Ng and Jordan,2002; Yogatama et al., 2017; Lewis and Fan,2019). In this paper, we improve generative text classifiers by introducing discrete latent variables into the generative story, and explore several graphical model configurations. We parameterize the distributions using standard neural architectures used in conditional language modeling and perform learning by directly maximizing the log marginal likelihood via gradient-based optimization, which avoids the need to do expectation-maximization. We empirically characterize the performance of our models on six text classification datasets. The choice of where to include the latent variable has a significant impact on performance, with the strongest results obtained when using the latent variable as an auxiliary conditioning variable in the generation of the textual input. This model consistently outperforms both the generative and discriminative classifiers in small-data settings. We analyze our model by finding that the latent variable captures interpretable properties of the data, even with very small training sets.</font>
<br>
</div>


<hr>
<div id="paper158"> <b>158. Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT</b>  <a href="https://www.aclweb.org/anthology/D19-1077.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title158" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. <br>&nbsp;&nbsp;<i>Shijie Wu, Mark Dredze</i><br>
<font size="3">
Pretrained contextual representation models (Peters et al., 2018; Devlin et al., 2018) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT (Devlin, 2018) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.</font>
<br>
</div>


<hr>
<div id="paper159"> <b>159. Pivot-based Transfer Learning for Neural Machine Translation between Non-English Languages</b>  <a href="https://www.aclweb.org/anthology/D19-1080.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title159" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. <br>&nbsp;&nbsp;<i>Yunsu Kim, Petre Petrov, Pavel Petrushkov, Shahram Khadivi, Hermann Ney</i><br>
<font size="3">
We present effective pre-training strategies for neural machine translation (NMT) using parallel corpora involving a pivot language, i.e., source-pivot and pivot-target, leading to a significant improvement in source-target translation. We propose three methods to increase the relation among source, pivot, and target languages in the pre-training: 1) step-wise training of a single model for different language pairs, 2) additional adapter component to smoothly connect pre-trained encoder and decoder, and 3) cross-lingual encoder training via autoencoding of the pivot language. Our methods greatly outperform multilingual models up to +2.6% BLEU in WMT 2019 French-German and German-Czech tasks. We show that our improvements are valid also in zero-shot/zero-resource scenarios.</font>
<br>
</div>


<hr>
<div id="paper160"> <b>160. Multilingual Neural Machine Translation with Language Clustering</b>  <a href="https://www.aclweb.org/anthology/D19-1089.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title160" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. <br>&nbsp;&nbsp;<i>Xu Tan, Jiale Chen, Di He, Yingce Xia, Tao Qin, Tie-Yan Liu</i><br>
<font size="3">
Multilingual neural machine translation (NMT), which translates multiple languages using a single model, is of great practical importance due to its advantages in simplifying the training process, reducing online maintenance costs, and enhancing low-resource and zero-shot translation. Given there are thousands of languages in the world and some of them are very different, it is extremely burdensome to handle them all in a single model or use a separate model for each language pair. Therefore, given a fixed resource budget, e.g., the number of models, how to determine which languages should be supported by one model is critical to multilingual NMT, which, unfortunately, has been ignored by previous work. In this work, we develop a framework that clusters languages into different groups and trains one multilingual model for each cluster. We study two methods for language clustering: (1) using prior knowledge, where we cluster languages according to language family, and (2) using language embedding, in which we represent each language by an embedding vector and cluster them in the embedding space. In particular, we obtain the embedding vectors of all the languages by training a universal neural machine translation model. Our experiments on 23 languages show that the first clustering method is simple and easy to understand but leading to suboptimal translation accuracy, while the second method sufficiently captures the relationship among languages well and improves the translation accuracy for almost all the languages over baseline methods.</font>
<br>
</div>


<hr>
<div id="paper161"> <b>161. Zero-shot Cross-lingual Dialogue Systems with Transferable Latent Variables</b>  <a href="https://www.aclweb.org/anthology/D19-1129.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title161" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. <br>&nbsp;&nbsp;<i>Zihan Liu, Jamin Shin, Yan Xu, Genta Indra Winata, Peng Xu, Andrea Madotto, Pascale Fung</i><br>
<font size="3">
Despite the surging demands for multilingual task-oriented dialog systems (e.g., Alexa, Google Home), there has been less research done in multilingual or cross-lingual scenarios. Hence, we propose a zero-shot adaptation of task-oriented dialogue system to low-resource languages. To tackle this challenge, we first use a set of very few parallel word pairs to refine the aligned cross-lingual word-level representations. We then employ a latent variable model to cope with the variance of similar sentences across different languages, which is induced by imperfect cross-lingual alignments and inherent differences in languages. Finally, the experimental results show that even though we utilize much less external resources, our model achieves better adaptation performance for natural language understanding task (i.e., the intent detection and slot filling) compared to the current state-of-the-art model in the zero-shot scenario.</font>
<br>
</div>


<hr>
<div id="paper162"> <b>162. DyKgChat: Benchmarking Dialogue Generation Grounding on Dynamic Knowledge Graphs</b>  <a href="https://www.aclweb.org/anthology/D19-1194.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title162" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. <br>&nbsp;&nbsp;<i>Yi-Lin Tuan, Yun-Nung Chen, Hung-yi Lee</i><br>
<font size="3">
Data-driven, knowledge-grounded neural conversation models are capable of generating more informative responses. However, these models have not yet demonstrated that they can zero-shot adapt to updated, unseen knowledge graphs. This paper proposes a new task about how to apply dynamic knowledge graphs in neural conversation model and presents a novel TV series conversation corpus (DyKgChat) for the task. Our new task and corpus aids in understanding the influence of dynamic knowledge graphs on responses generation. Also, we propose a preliminary model that selects an output from two networks at each time step: a sequence-to-sequence model (Seq2Seq) and a multi-hop reasoning model, in order to support dynamic knowledge graphs. To benchmark this new task and evaluate the capability of adaptation, we introduce several evaluation metrics and the experiments show that our proposed approach outperforms previous knowledge-grounded conversation models. The proposed corpus and model can motivate the future research directions.</font>
<br>
</div>


<hr>
<div id="paper163"> <b>163. 75 Languages, 1 Model: Parsing Universal Dependencies Universally</b>  <a href="https://www.aclweb.org/anthology/D19-1279.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title163" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. <br>&nbsp;&nbsp;<i>Dan Kondratyuk, Milan Straka</i><br>
<font size="3">
We present UDify, a multilingual multi-task model capable of accurately predicting universal part-of-speech, morphological features, lemmas, and dependency trees simultaneously for all 124 Universal Dependencies treebanks across 75 languages. By leveraging a multilingual BERT self-attention model pretrained on 104 languages, we found that fine-tuning it on all datasets concatenated together with simple softmax classifiers for each UD task can meet or exceed state-of-the-art UPOS, UFeats, Lemmas, (and especially) UAS, and LAS scores, without requiring any recurrent or language-specific components. We evaluate UDify for multilingual learning, showing that low-resource languages benefit the most from cross-linguistic annotations. We also evaluate for zero-shot learning, with results suggesting that multilingual training provides strong UD predictions even for languages that neither UDify nor BERT have ever been trained on.</font>
<br>
</div>


<hr>
<div id="paper164"> <b>164. Towards Zero-shot Language Modeling</b>  <a href="https://www.aclweb.org/anthology/D19-1288.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title164" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. <br>&nbsp;&nbsp;<i>Edoardo Maria Ponti, Ivan Vulić, Ryan Cotterell, Roi Reichart, Anna Korhonen</i><br>
<font size="3">
Can we construct a neural language model which is inductively biased towards learning human language? Motivated by this question, we aim at constructing an informative prior for held-out languages on the task of character-level, open-vocabulary language modelling. We obtain this prior as the posterior over network weights conditioned on the data from a sample of training languages, which is approximated through Laplace’s method. Based on a large and diverse sample of languages, the use of our prior outperforms baseline models with an uninformative prior in both zero-shot and few-shot settings, showing that the prior is imbued with universal linguistic knowledge. Moreover, we harness broad language-specific information available for most languages of the world, i.e., features from typological databases, as distant supervision for held-out languages. We explore several language modelling conditioning techniques, including concatenation and meta-networks for parameter generation. They appear beneficial in the few-shot setting, but ineffective in the zero-shot setting. Since the paucity of even plain digital text affects the majority of the world’s languages, we hope that these insights will broaden the scope of applications for language technology.</font>
<br>
</div>


<hr>
<div id="paper165"> <b>165. Out-of-Domain Detection for Low-Resource Text Classification Tasks</b>  <a href="https://www.aclweb.org/anthology/D19-1364.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title165" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. <br>&nbsp;&nbsp;<i>Ming Tan, Yang Yu, Haoyu Wang, Dakuo Wang, Saloni Potdar, Shiyu Chang, Mo Yu</i><br>
<font size="3">
Out-of-domain (OOD) detection for low-resource text classification is a realistic but understudied task. The goal is to detect the OOD cases with limited in-domain (ID) training data, since in machine learning applications we observe that training data is often insufficient. In this work, we propose an OOD-resistant Prototypical Network to tackle this zero-shot OOD detection and few-shot ID classification task. Evaluations on real-world datasets show that the proposed solution outperforms state-of-the-art methods in zero-shot OOD detection task, while maintaining a competitive performance on ID classification task.</font>
<br>
</div>


<hr>
<div id="paper166"> <b>166. Global Reasoning over Database Structures for Text-to-SQL Parsing</b>  <a href="https://www.aclweb.org/anthology/D19-1378.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title166" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. <br>&nbsp;&nbsp;<i>Ben Bogin, Matt Gardner, Jonathan Berant</i><br>
<font size="3">
State-of-the-art semantic parsers rely on auto-regressive decoding, emitting one symbol at a time. When tested against complex databases that are unobserved at training time (zero-shot), the parser often struggles to select the correct set of database constants in the new database, due to the local nature of decoding. %since their decisions are based on weak, local information only. In this work, we propose a semantic parser that globally reasons about the structure of the output query to make a more contextually-informed selection of database constants. We use message-passing through a graph neural network to softly select a subset of database constants for the output query, conditioned on the question. Moreover, we train a model to rank queries based on the global alignment of database constants to question words. We apply our techniques to the current state-of-the-art model for Spider, a zero-shot semantic parsing dataset with complex databases, increasing accuracy from 39.4% to 47.4%.</font>
<br>
</div>


<hr>
<div id="paper167"> <b>167. Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach</b>  <a href="https://www.aclweb.org/anthology/D19-1404.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title167" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. <br>&nbsp;&nbsp;<i>Wenpeng Yin, Jamaal Hay, Dan Roth</i><br>
<font size="3">
Zero-shot text classification (0Shot-TC) is a challenging NLU problem to which little attention has been paid by the research community. 0Shot-TC aims to associate an appropriate label with a piece of text, irrespective of the text domain and the aspect (e.g., topic, emotion, event, etc.) described by the label. And there are only a few articles studying 0Shot-TC, all focusing only on topical categorization which, we argue, is just the tip of the iceberg in 0Shot-TC. In addition, the chaotic experiments in literature make no uniform comparison, which blurs the progress. This work benchmarks the 0Shot-TC problem by providing unified datasets, standardized evaluations, and state-of-the-art baselines. Our contributions include: i) The datasets we provide facilitate studying 0Shot-TC relative to conceptually different and diverse aspects: the “topic” aspect includes “sports” and “politics” as labels; the “emotion” aspect includes “joy” and “anger”; the “situation” aspect includes “medical assistance” and “water shortage”. ii) We extend the existing evaluation setup (label-partially-unseen) – given a dataset, train on some labels, test on all labels – to include a more challenging yet realistic evaluation label-fully-unseen 0Shot-TC (Chang et al., 2008), aiming at classifying text snippets without seeing task specific training data at all. iii) We unify the 0Shot-TC of diverse aspects within a textual entailment formulation and study it this way.</font>
<br>
</div>


<hr>
<div id="paper168"> <b>168. Reconstructing Capsule Networks for Zero-shot Intent Classification</b>  <a href="https://www.aclweb.org/anthology/D19-1486.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title168" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. <br>&nbsp;&nbsp;<i>Han Liu, Xiaotong Zhang, Lu Fan, Xuandi Fu, Qimai Li, Xiao-Ming Wu, Albert Y.S. Lam</i><br>
<font size="3">
Intent classification is an important building block of dialogue systems. With the burgeoning of conversational AI, existing systems are not capable of handling numerous fast-emerging intents, which motivates zero-shot intent classification. Nevertheless, research on this problem is still in the incipient stage and few methods are available. A recently proposed zero-shot intent classification method, IntentCapsNet, has been shown to achieve state-of-the-art performance. However, it has two unaddressed limitations: (1) it cannot deal with polysemy when extracting semantic capsules; (2) it hardly recognizes the utterances of unseen intents in the generalized zero-shot intent classification setting. To overcome these limitations, we propose to reconstruct capsule networks for zero-shot intent classification. First, we introduce a dimensional attention mechanism to fight against polysemy. Second, we reconstruct the transformation matrices for unseen intents by utilizing abundant latent information of the labeled utterances, which significantly improves the model generalization ability. Experimental results on two task-oriented dialogue datasets in different languages show that our proposed method outperforms IntentCapsNet and other strong baselines.</font>
<br>
</div>


<hr>
<div id="paper169"> <b>169. Open Domain Web Keyphrase Extraction Beyond Language Modeling</b>  <a href="https://www.aclweb.org/anthology/D19-1521.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title169" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. <br>&nbsp;&nbsp;<i>Lee Xiong, Chuan Hu, Chenyan Xiong, Daniel Campos, Arnold Overwijk</i><br>
<font size="3">
This paper studies keyphrase extraction in real-world scenarios where documents are from diverse domains and have variant content quality. We curate and release OpenKP, a large scale open domain keyphrase extraction dataset with near one hundred thousand web documents and expert keyphrase annotations. To handle the variations of domain and content quality, we develop BLING-KPE, a neural keyphrase extraction model that goes beyond language understanding using visual presentations of documents and weak supervision from search queries. Experimental results on OpenKP confirm the effectiveness of BLING-KPE and the contributions of its neural architecture, visual features, and search log weak supervision. Zero-shot evaluations on DUC-2001 demonstrate the improved generalization ability of learning from the open domain data compared to a specific domain.</font>
<br>
</div>


<hr>
<div id="paper170"> <b>170. MultiFiT: Efficient Multi-lingual Language Model Fine-tuning</b>  <a href="https://www.aclweb.org/anthology/D19-1572.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title170" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. <br>&nbsp;&nbsp;<i>Julian Eisenschlos, Sebastian Ruder, Piotr Czapla, Marcin Kadras, Sylvain Gugger, Jeremy Howard</i><br>
<font size="3">
Pretrained language models are promising particularly for low-resource languages as they only require unlabelled data. However, training existing models requires huge amounts of compute, while pretrained cross-lingual models often underperform on low-resource languages. We propose Multi-lingual language model Fine-Tuning (MultiFiT) to enable practitioners to train and fine-tune language models efficiently in their own language. In addition, we propose a zero-shot method using an existing pretrained cross-lingual model. We evaluate our methods on two widely used cross-lingual classification datasets where they outperform models pretrained on orders of magnitude more data and compute. We release all models and code.</font>
<br>
</div>


<hr>
<div id="paper171"> <b>171. Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing</b>  <a href="https://www.aclweb.org/anthology/D19-1575.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title171" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. <br>&nbsp;&nbsp;<i>Yuxuan Wang, Wanxiang Che, Jiang Guo, Yijia Liu, Ting Liu</i><br>
<font size="3">
This paper investigates the problem of learning cross-lingual representations in a contextual space. We propose Cross-Lingual BERT Transformation (CLBT), a simple and efficient approach to generate cross-lingual contextualized word embeddings based on publicly available pre-trained BERT models (Devlin et al., 2018). In this approach, a linear transformation is learned from contextual word alignments to align the contextualized embeddings independently trained in different languages. We demonstrate the effectiveness of this approach on zero-shot cross-lingual transfer parsing. Experiments show that our embeddings substantially outperform the previous state-of-the-art that uses static embeddings. We further compare our approach with XLM (Lample and Conneau, 2019), a recently proposed cross-lingual language model trained with massive parallel data, and achieve highly competitive results.</font>
<br>
</div>


<hr>
<div id="paper172"> <b>172. Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model</b>  <a href="https://www.aclweb.org/anthology/D19-1607.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title172" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. <br>&nbsp;&nbsp;<i>Tsung-Yuan Hsu, Chi-Liang Liu, Hung-yi Lee</i><br>
<font size="3">
Because it is not feasible to collect training data for every language, there is a growing interest in cross-lingual transfer learning. In this paper, we systematically explore zero-shot cross-lingual transfer learning on reading comprehension tasks with language representation model pre-trained on multi-lingual corpus. The experimental results show that with pre-trained language representation zero-shot learning is feasible, and translating the source data into the target language is not necessary and even degrades the performance. We further explore what does the model learn in zero-shot setting.</font>
<br>
</div>


<hr>
<div id="paper173"> <b>173. Zero-Resource Neural Machine Translation with Monolingual Pivot Data</b>  <a href="https://www.aclweb.org/anthology/D19-5610.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title173" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. the 3rd Workshop on Neural Generation and Translation<br>&nbsp;&nbsp;<i>Anna Currey, Kenneth Heafield</i><br>
<font size="3">
Zero-shot neural machine translation (NMT) is a framework that uses source-pivot and target-pivot parallel data to train a source-target NMT system. An extension to zero-shot NMT is zero-resource NMT, which generates pseudo-parallel corpora using a zero-shot system and further trains the zero-shot system on that data. In this paper, we expand on zero-resource NMT by incorporating monolingual data in the pivot language into training; since the pivot language is usually the highest-resource language of the three, we expect monolingual pivot-language data to be most abundant. We propose methods for generating pseudo-parallel corpora using pivot-language monolingual data and for leveraging the pseudo-parallel corpora to improve the zero-shot NMT system. We evaluate these methods for a high-resource language pair (German-Russian) using English as the pivot. We show that our proposed methods yield consistent improvements over strong zero-shot and zero-resource baselines and even catch up to pivot-based models in BLEU (while not requiring the two-pass inference that pivot models require).</font>
<br>
</div>


<hr>
<div id="paper174"> <b>174. Domain Adaptation with BERT-based Domain Classification and Data Selection</b>  <a href="https://www.aclweb.org/anthology/D19-6109.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title174" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)<br>&nbsp;&nbsp;<i>Xiaofei Ma, Peng Xu, Zhiguo Wang, Ramesh Nallapati, Bing Xiang</i><br>
<font size="3">
The performance of deep neural models can deteriorate substantially when there is a domain shift between training and test data. For example, the pre-trained BERT model can be easily fine-tuned with just one additional output layer to create a state-of-the-art model for a wide range of tasks. However, the fine-tuned BERT model suffers considerably at zero-shot when applied to a different domain. In this paper, we present a novel two-step domain adaptation framework based on curriculum learning and domain-discriminative data selection. The domain adaptation is conducted in a mostly unsupervised manner using a small target domain validation set for hyper-parameter tuning. We tested the framework on four large public datasets with different domain similarities and task types. Our framework outperforms a popular discrepancy-based domain adaptation method on most transfer tasks while consuming only a fraction of the training budget.</font>
<br>
</div>


<hr>
<div id="paper175"> <b>175. Few-Shot and Zero-Shot Learning for Historical Text Normalization</b>  <a href="https://www.aclweb.org/anthology/D19-6112.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title175" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)<br>&nbsp;&nbsp;<i>Marcel Bollmann, Natalia Korchagina, Anders Søgaard</i><br>
<font size="3">
Historical text normalization often relies on small training datasets. Recent work has shown that multi-task learning can lead to significant improvements by exploiting synergies with related datasets, but there has been no systematic study of different multi-task learning architectures. This paper evaluates 63 multi-task learning configurations for sequence-to-sequence-based historical text normalization across ten datasets from eight languages, using autoencoding, grapheme-to-phoneme mapping, and lemmatization as auxiliary tasks. We observe consistent, significant improvements across languages when training data for the target task is limited, but minimal or no improvements when training data is abundant. We also show that zero-shot learning outperforms the simple, but relatively strong, identity baseline.</font>
<br>
</div>


<hr>
<div id="paper176"> <b>176. From Monolingual to Multilingual FAQ Assistant using Multilingual Co-training</b>  <a href="https://www.aclweb.org/anthology/D19-6113.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title176" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)<br>&nbsp;&nbsp;<i>Mayur Patidar, Surabhi Kumari, Manasi Patwardhan, Shirish Karande, Puneet Agarwal, Lovekesh Vig, Gautam Shroff</i><br>
<font size="3">
Recent research on cross-lingual transfer show state-of-the-art results on benchmark datasets using pre-trained language representation models (PLRM) like BERT. These results are achieved with the traditional training approaches, such as Zero-shot with no data, Translate-train or Translate-test with machine translated data. In this work, we propose an approach of “Multilingual Co-training” (MCT) where we augment the expert annotated dataset in the source language (English) with the corresponding machine translations in the target languages (e.g. Arabic, Spanish) and fine-tune the PLRM jointly. We observe that the proposed approach provides consistent gains in the performance of BERT for multiple benchmark datasets (e.g. 1.0% gain on MLDocs, and 1.2% gain on XNLI over translate-train with BERT), while requiring a single model for multiple languages. We further consider a FAQ dataset where the available English test dataset is translated by experts into Arabic and Spanish. On such a dataset, we observe an average gain of 4.9% over all other cross-lingual transfer protocols with BERT. We further observe that domain-specific joint pre-training of the PLRM using HR policy documents in English along with the machine translations in the target languages, followed by the joint finetuning, provides a further improvement of 2.8% in average accuracy.</font>
<br>
</div>


<hr>
<div id="paper177"> <b>177. X-WikiRE: A Large, Multilingual Resource for Relation Extraction as Machine Comprehension</b>  <a href="https://www.aclweb.org/anthology/D19-6130.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title177" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)<br>&nbsp;&nbsp;<i>Mostafa Abdou, Cezar Sas, Rahul Aralikatte, Isabelle Augenstein, Anders Søgaard</i><br>
<font size="3">
Although the vast majority of knowledge bases (KBs) are heavily biased towards English, Wikipedias do cover very different topics in different languages. Exploiting this, we introduce a new multilingual dataset (X-WikiRE), framing relation extraction as a multilingual machine reading problem. We show that by leveraging this resource it is possible to robustly transfer models cross-lingually and that multilingual support significantly improves (zero-shot) relation extraction, enabling the population of low-resourced KBs from their well-populated counterparts.</font>
<br>
</div>


<hr>
<div id="paper178"> <b>178. Zero-Shot Cross-lingual Name Retrieval for Low-Resource Languages</b>  <a href="https://www.aclweb.org/anthology/D19-6131.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title178" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)<br>&nbsp;&nbsp;<i>Kevin Blissett, Heng Ji</i><br>
<font size="3">
In this paper we address a challenging cross-lingual name retrieval task. Given an English named entity query, we aim to find all name mentions in documents in low-resource languages. We present a novel method which relies on zero annotation or resources from the target language. By leveraging freely available, cross-lingual resources and a small amount of training data from another language, we are able to perform name retrieval on a new language without any additional training data. Our method proceeds in a multi-step process: first, we pre-train a language-independent orthographic encoder using Wikipedia inter-lingual links from dozens of languages. Next, we gather user expectations about important entities in an English comparable document and compare those expected entities with actual spans of the target language text in order to perform name finding. Our method shows 11.6% absolute F-score improvement over state-of-the-art methods.</font>
<br>
</div>


<hr>
<div id="paper179"> <b>179. Zero-shot Dependency Parsing with Pre-trained Multilingual Sentence Representations</b>  <a href="https://www.aclweb.org/anthology/D19-6132.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title179" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2019. the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)<br>&nbsp;&nbsp;<i>Ke Tran, Arianna Bisazza</i><br>
<font size="3">
We investigate whether off-the-shelf deep bidirectional sentence representations (Devlin et al., 2019) trained on a massively multilingual corpus (multilingual BERT) enable the development of an unsupervised universal dependency parser. This approach only leverages a mix of monolingual corpora in many languages and does not require any translation data making it applicable to low-resource languages. In our experiments we outperform the best CoNLL 2018 language-specific systems in all of the shared task’s six truly low-resource languages while using a single system. However, we also find that (i) parsing accuracy still varies dramatically when changing the training languages and (ii) in some target languages zero-shot transfer fails under all tested conditions, raising concerns on the ‘universality’ of the whole approach.</font>
<br>
</div>


<hr>
<div id="paper180"> <b>180. Dimensionality Reduction for Representing the Knowledge of Probabilistic Models</b>  <a href="https://openreview.net/pdf?id=SygD-hCcF7" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title180" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICLR 2019. <br>&nbsp;&nbsp;<i>Marc T. Law, Jake Snell, Amir-massoud Farahmand, Raquel Urtasun, Richard S. Zemel</i><br>
<font size="3">
Most deep learning models rely on expressive high-dimensional representations to achieve good performance on tasks such as classification. However, the high dimensionality of these representations makes them difficult to interpret and prone to over-fitting. We propose a simple, intuitive and scalable dimension reduction framework that takes into account the soft probabilistic interpretation of standard deep models for classification. When applying our framework to visualization, our representations more accurately reflect inter-class distances than standard visualization techniques such as t-SNE. We show experimentally that our framework improves generalization performance to unseen categories in zero-shot learning. We also provide a finite sample error upper bound guarantee for the method.</font>
<br>
</div>


<hr>
<div id="paper181"> <b>181. Efficient Amortised Bayesian Inference for Hierarchical and Nonlinear Dynamical Systems</b>  <a href="http://proceedings.mlr.press/v97/meeds19a/meeds19a.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title181" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICML 2019. <br>&nbsp;&nbsp;<i>Geoffrey Roeder, Paul K. Grant, Andrew Phillips, Neil Dalchau, Edward Meeds</i><br>
<font size="3">
We introduce a flexible, scalable Bayesian inference framework for nonlinear dynamical systems characterised by distinct and hierarchical variability at the individual, group, and population levels. Our model class is a generalisation of nonlinear mixed-effects (NLME) dynamical systems, the statistical workhorse for many experimental sciences. We cast parameter inference as stochastic optimisation of an end-to-end differentiable, block-conditional variational autoencoder. We specify the dynamics of the data-generating process as an ordinary differential equation (ODE) such that both the ODE and its solver are fully differentiable. This model class is highly flexible: the ODE right-hand sides can be a mixture of user-prescribed or "white-box" sub-components and neural network or "black-box" sub-components. Using stochastic optimisation, our amortised inference algorithm could seamlessly scale up to massive data collection pipelines (common in labs with robotic automation). Finally, our framework supports interpretability with respect to the underlying dynamics, as well as predictive generalization to unseen combinations of group components (also called “zero-shot" learning). We empirically validate our method by predicting the dynamic behaviour of bacteria that were genetically engineered to function as biosensors.</font>
<br>
</div>


<hr>
<div id="paper182"> <b>182. Zero-Shot Knowledge Distillation in Deep Networks</b>  <a href="http://proceedings.mlr.press/v97/nayak19a/nayak19a.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title182" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICML 2019. <br>&nbsp;&nbsp;<i>Gaurav Kumar Nayak, Konda Reddy Mopuri, Vaisakh Shaj, Venkatesh Babu Radhakrishnan, Anirban Chakraborty</i><br>
<font size="3">
Knowledge distillation deals with the problem of training a smaller model (Student) from a high capacity source model (Teacher) so as to retain most of its performance. Existing approaches use either the training data or meta-data extracted from it in order to train the Student. However, accessing the dataset on which the Teacher has been trained may not always be feasible if the dataset is very large or it poses privacy or safety concerns (e.g., bio-metric or medical data). Hence, in this paper, we propose a novel data-free method to train the Student from the Teacher. Without even using any meta-data, we synthesize the Data Impressions from the complex Teacher model and utilize these as surrogates for the original training data samples to transfer its learning to Student via knowledge distillation. We, therefore, dub our method “Zero-Shot Knowledge Distillation" and demonstrate that our framework results in competitive generalization performance as achieved by distillation using the actual training data samples on multiple benchmark datasets.</font>
<br>
</div>


<hr>
<div id="paper183"> <b>183. AutoVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss</b>  <a href="http://proceedings.mlr.press/v97/qian19c/qian19c.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title183" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICML 2019. <br>&nbsp;&nbsp;<i>Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, Mark Hasegawa-Johnson</i><br>
<font size="3">
Despite the progress in voice conversion, many-to-many voice conversion trained on non-parallel data, as well as zero-shot voice conversion, remains under-explored. Deep style transfer algorithms, generative adversarial networks (GAN) in particular, are being applied as new solutions in this field. However, GAN training is very sophisticated and difficult, and there is no strong evidence that its generated speech is of good perceptual quality. In this paper, we propose a new style transfer scheme that involves only an autoencoder with a carefully designed bottleneck. We formally show that this scheme can achieve distribution-matching style transfer by training only on self-reconstruction loss. Based on this scheme, we proposed AutoVC, which achieves state-of-the-art results in many-to-many voice conversion with non-parallel data, and which is the first to perform zero-shot voice conversion.</font>
<br>
</div>


<hr>
<div id="paper184"> <b>184. Context-Aware Zero-Shot Learning for Object Recognition</b>  <a href="http://proceedings.mlr.press/v97/zablocki19a/zablocki19a.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title184" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICML 2019. <br>&nbsp;&nbsp;<i>Eloi Zablocki, Patrick Bordes, Laure Soulier, Benjamin Piwowarski, Patrick Gallinari</i><br>
<font size="3">
Zero-Shot Learning (ZSL) aims at classifying unlabeled objects by leveraging auxiliary knowledge, such as semantic representations. A limitation of previous approaches is that only intrinsic properties of objects, e.g. their visual appearance, are taken into account while their context, e.g. the surrounding objects in the image, is ignored. Following the intuitive principle that objects tend to be found in certain contexts but not others, we propose a new and challenging approach, context-aware ZSL, that leverages semantic representations in a new way to model the conditional likelihood of an object to appear in a given context. Finally, through extensive experiments conducted on Visual Genome, we show that contextual information can substantially improve the standard ZSL approach and is robust to unbalanced classes.</font>
<br>
</div>


<hr>
<div id="paper185"> <b>185. Co-Representation Network for Generalized Zero-Shot Learning</b>  <a href="http://proceedings.mlr.press/v97/zhang19l/zhang19l.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title185" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICML 2019. <br>&nbsp;&nbsp;<i>Fei Zhang, Guangming Shi</i><br>
<font size="3">
Generalized zero-shot learning is a significant topic but faced with bias problem, which leads to unseen classes being easily misclassified into seen classes. Hence we propose a embedding model called co-representation network to learn a more uniform visual embedding space that effectively alleviates the bias problem and helps with classification. We mathematically analyze our model and find it learns a projection with high local linearity, which is proved to cause less bias problem. The network consists of a cooperation module for representation and a relation module for classification, it is simple in structure and can be easily trained in an end-to-end manner. Experiments show that our method outperforms existing generalized zero-shot learning methods on several benchmark datasets.</font>
<br>
</div>


<hr>
<div id="paper186"> <b>186. Learning Classifiers for Target Domain with Limited or No Labels</b>  <a href="http://proceedings.mlr.press/v97/zhu19d/zhu19d.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title186" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICML 2019. <br>&nbsp;&nbsp;<i>Pengkai Zhu, Hanxiao Wang, Venkatesh Saligrama</i><br>
<font size="3">
In computer vision applications, such as domain adaptation (DA), few shot learning (FSL) and zero-shot learning (ZSL), we encounter new objects and environments, for which insufficient examples exist to allow for training “models from scratch,” and methods that adapt existing models, trained on the presented training environment, to the new scenario are required. We propose a novel visual attribute encoding method that encodes each image as a low-dimensional probability vector composed of prototypical part-type probabilities. The prototypes are learnt to be representative of all training data. At test-time we utilize this encoding as an input to a classifier. At test-time we freeze the encoder and only learn/adapt the classifier component to limited annotated labels in FSL; new semantic attributes in ZSL. We conduct extensive experiments on benchmark datasets. Our method outperforms state-of-art methods trained for the specific contexts (ZSL, FSL, DA).</font>
<br>
</div>


<hr>
<div id="paper187"> <b>187. Generalized Zero-Shot Vehicle Detection in Remote Sensing Imagery via Coarse-to-Fine Framework</b>  <a href="https://www.ijcai.org/proceedings/2019/0097.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title187" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2019. <br>&nbsp;&nbsp;<i>Hong Chen, Yongtan Luo, Liujuan Cao, Baochang Zhang, Guodong Guo, Cheng Wang, Jonathan Li, Rongrong Ji</i><br>
<font size="3">
Vehicle detection and recognition in remote sensing images are challenging, especially when only limited training data are available to accommodate various target categories. In this paper, we introduce a novel coarse-to-fine framework, which decomposes vehicle detection into segmentation-based vehicle localization and generalized zero-shot vehicle classification. Particularly, the proposed framework can well handle the problem of generalized zero-shot vehicle detection, which is challenging due to the requirement of recognizing vehicles that are even unseen during training. Specifically, a hierarchical DeepLab v3 model is proposed in the framework, which fully exploits fine-grained features to locate the target on a pixel-wise level, then recognizes vehicles in a coarse-grained manner. Additionally, the hierarchical DeepLab v3 model is beneficially compatible to combine the generalized zero-shot recognition. To the best of our knowledge, there is no publically available dataset to test comparative methods, we therefore construct a new dataset to fill this gap of evaluation. The experimental results show that the proposed framework yields promising results on the imperative yet difficult task of zero-shot vehicle detection and recognition.</font>
<br>
</div>


<hr>
<div id="paper188"> <b>188. Zero-shot Learning with Many Classes by High-rank Deep Embedding Networks</b>  <a href="https://www.ijcai.org/proceedings/2019/0337.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title188" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2019. <br>&nbsp;&nbsp;<i>Yuchen Guo, Guiguang Ding, Jungong Han, Hang Shao, Xin Lou, Qionghai Dai</i><br>
<font size="3">
Zero-shot learning (ZSL) is a recently emerging research topic which aims to build classification models for unseen classes with knowledge from auxiliary seen classes. Though many ZSL works have shown promising results on small-scale datasets by utilizing a bilinear compatibility function, the ZSL performance on large-scale datasets with many classes (say, ImageNet) is still unsatisfactory. We argue that the bilinear compatibility function is a low-rank approximation of the true compatibility function such that it is not expressive enough especially when there are a large number of classes because of the rank limitation. To address this issue, we propose a novel approach, termed as High-rank Deep Embedding Networks (GREEN), for ZSL with many classes. In particular, we propose a feature-dependent mixture of softmaxes as the image-class compatibility function, which is a simple extension of the bilinear compatibility function, but yields much better results. It utilizes a mixture of non-linear transformations with feature-dependent latent variables to approximate the true function in a high-rank way, which makes GREEN more expressive. Experiments on several datasets including ImageNet demonstrate GREEN significantly outperforms the state-of-the-art approaches.</font>
<br>
</div>


<hr>
<div id="paper189"> <b>189. Landmark Selection for Zero-shot Learning</b>  <a href="https://www.ijcai.org/proceedings/2019/0338.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title189" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2019. <br>&nbsp;&nbsp;<i>Yuchen Guo, Guiguang Ding, Jungong Han, Chenggang Yan, Jiyong Zhang, Qionghai Dai</i><br>
<font size="3">
Zero-shot learning (ZSL) is an emerging research topic whose goal is to build recognition models for previously unseen classes. The basic idea of ZSL is based on heterogeneous feature matching which learns a compatibility function between image and class features using seen classes. The function is constructed based on one-vs-all training in which each class has only one class feature and many image features. Existing ZSL works mostly treat all image features equivalently. However, in this paper we argue that it is more reasonable to use some representative cross-domain data instead of all. Motivated by this idea, we propose a novel approach, termed as Landmark Selection(LAST) for ZSL. LAST is able to identify representative cross-domain features which further lead to better image-class compatibility function. Experiments on several ZSL datasets including ImageNet demonstrate the superiority of LAST to the state-of-the-arts.</font>
<br>
</div>


<hr>
<div id="paper190"> <b>190. Graph and Autoencoder Based Feature Extraction for Zero-shot Learning</b>  <a href="https://www.ijcai.org/proceedings/2019/0421.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title190" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2019. <br>&nbsp;&nbsp;<i>Yang Liu, De-Yan Xie, Quanxue Gao, Jungong Han, Shujian Wang, Xinbo Gao</i><br>
<font size="3">
Zero-shot learning (ZSL) aims to build models to recognize novel visual categories that have no associated labelled training samples. The basic framework is to transfer knowledge from seen classes to unseen classes by learning the visual-semantic embedding. However, most of approaches do not preserve the underlying sub-manifold of samples in the embedding space. In addition, whether the mapping can precisely reconstruct the original visual feature is not investigated in-depth. In order to solve these problems, we formulate a novel framework named Graph and Autoencoder Based Feature Extraction (GAFE) to seek a low-rank mapping to preserve the sub-manifold of samples. Taking the encoder-decoder paradigm, the encoder part learns a mapping from the visual feature to the semantic space, while decoder part reconstructs the original features with the learned mapping. In addition, a graph is constructed to guarantee the learned mapping can preserve the local intrinsic structure of the data. To this end, an L21 norm sparsity constraint is imposed on the mapping to identify features relevant to the target domain. Extensive experiments on five attribute datasets demonstrate the effectiveness of the proposed model.</font>
<br>
</div>


<hr>
<div id="paper191"> <b>191. Learning Image-Specific Attributes by Hyperbolic Neighborhood Graph Propagation</b>  <a href="https://www.ijcai.org/proceedings/2019/0554.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title191" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2019. <br>&nbsp;&nbsp;<i>Xiaofeng Xu, Ivor W. Tsang, Xiaofeng Cao, Ruiheng Zhang, Chuancai Liu</i><br>
<font size="3">
As a kind of semantic representation of visual object descriptions, attributes are widely used in various computer vision tasks. In most of existing attribute-based research, class-specific attributes (CSA), which are class-level annotations, are usually adopted due to its low annotation cost for each class instead of each individual image. However, class-specific attributes are usually noisy because of annotation errors and diversity of individual images. Therefore, it is desirable to obtain image-specific attributes (ISA), which are image-level annotations, from the original class-specific attributes. In this paper, we propose to learn image-specific attributes by graph-based attribute propagation. Considering the intrinsic property of hyperbolic geometry that its distance expands exponentially, hyperbolic neighborhood graph (HNG) is constructed to characterize the relationship between samples. Based on HNG, we define neighborhood consistency for each sample to identify inconsistent samples. Subsequently, inconsistent samples are refined based on their neighbors in HNG. Extensive experiments on five benchmark datasets demonstrate the significant superiority of the learned image-specific attributes over the original class-specific attributes in the zero-shot object classification task.</font>
<br>
</div>


<hr>
<div id="paper192"> <b>192. Zero-shot Metric Learning</b>  <a href="https://www.ijcai.org/proceedings/2019/0555.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title192" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2019. <br>&nbsp;&nbsp;<i>Xinyi Xu, Huanhuan Cao, Yanhua Yang, Erkun Yang, Cheng Deng</i><br>
<font size="3">
In this work, we tackle the zero-shot metric learning problem and propose a novel method abbreviated as ZSML, with the purpose to learn a distance metric that measures the similarity of unseen categories (even unseen datasets). ZSML achieves strong transferability by capturing multi-nonlinear yet continuous relation among data. It is motivated by two facts: 1) relations can be essentially described from various perspectives; and 2) traditional binary supervision is insufficient to represent continuous visual similarity. Specifically, we first reformulate a collection of specific-shaped convolutional kernels to combine data pairs and generate multiple relation vectors. Furthermore, we design a new cross-update regression loss to discover continuous similarity. Extensive experiments including intra-dataset transfer and inter-dataset transfer on four benchmark datasets demonstrate that ZSML can achieve state-of-the-art performance.</font>
<br>
</div>


<hr>
<div id="paper193"> <b>193. Phylogenic Multi-Lingual Dependency Parsing</b>  <a href="https://www.aclweb.org/anthology/N19-1017.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title193" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NAACL 2019. <br>&nbsp;&nbsp;<i>Mathieu Dehouck, Pascal Denis</i><br>
<font size="3">
Languages evolve and diverge over time. Their evolutionary history is often depicted in the shape of a phylogenetic tree. Assuming parsing models are representations of their languages grammars, their evolution should follow a structure similar to that of the phylogenetic tree. In this paper, drawing inspiration from multi-task learning, we make use of the phylogenetic tree to guide the learning of multi-lingual dependency parsers leveraging languages structural similarities. Experiments on data from the Universal Dependency project show that phylogenetic training is beneficial to low resourced languages and to well furnished languages families. As a side product of phylogenetic training, our model is able to perform zero-shot parsing of previously unseen languages.</font>
<br>
</div>


<hr>
<div id="paper194"> <b>194. Description-Based Zero-shot Fine-Grained Entity Typing</b>  <a href="https://www.aclweb.org/anthology/N19-1087.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title194" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NAACL 2019. <br>&nbsp;&nbsp;<i>Rasha Obeidat, Xiaoli Fern, Hamed Shahbazi, Prasad Tadepalli</i><br>
<font size="3">
Fine-grained Entity typing (FGET) is the task of assigning a fine-grained type from a hierarchy to entity mentions in the text. As the taxonomy of types evolves continuously, it is desirable for an entity typing system to be able to recognize novel types without additional training. This work proposes a zero-shot entity typing approach that utilizes the type description available from Wikipedia to build a distributed semantic representation of the types. During training, our system learns to align the entity mentions and their corresponding type representations on the known types. At test time, any new type can be incorporated into the system given its Wikipedia descriptions. We evaluate our approach on FIGER, a public benchmark entity tying dataset. Because the existing test set of FIGER covers only a small portion of the fine-grained types, we create a new test set by manually annotating a portion of the noisy training data. Our experiments demonstrate the effectiveness of the proposed method in recognizing novel types that are not present in the training data.</font>
<br>
</div>


<hr>
<div id="paper195"> <b>195. Generating Token-Level Explanations for Natural Language Inference</b>  <a href="https://www.aclweb.org/anthology/N19-1101.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title195" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NAACL 2019. <br>&nbsp;&nbsp;<i>James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal</i><br>
<font size="3">
The task of Natural Language Inference (NLI) is widely modeled as supervised sentence pair classification. While there has been a lot of work recently on generating explanations of the predictions of classifiers on a single piece of text, there have been no attempts to generate explanations of classifiers operating on pairs of sentences. In this paper, we show that it is possible to generate token-level explanations for NLI without the need for training data explicitly annotated for this purpose. We use a simple LSTM architecture and evaluate both LIME and Anchor explanations for this task. We compare these to a Multiple Instance Learning (MIL) method that uses thresholded attention make token-level predictions. The approach we present in this paper is a novel extension of zero-shot single-sentence tagging to sentence pairs for NLI. We conduct our experiments on the well-studied SNLI dataset that was recently augmented with manually annotation of the tokens that explain the entailment relation. We find that our white-box MIL-based method, while orders of magnitude faster, does not reach the same accuracy as the black-box methods.</font>
<br>
</div>


<hr>
<div id="paper196"> <b>196. Integrating Semantic Knowledge to Tackle Zero-shot Text Classification</b>  <a href="https://www.aclweb.org/anthology/N19-1108.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title196" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NAACL 2019. <br>&nbsp;&nbsp;<i>Jingqing Zhang, Piyawat Lertvittayakumjorn, Yike Guo</i><br>
<font size="3">
Insufficient or even unavailable training data of emerging classes is a big challenge of many classification tasks, including text classification. Recognising text documents of classes that have never been seen in the learning stage, so-called zero-shot text classification, is therefore difficult and only limited previous works tackled this problem. In this paper, we propose a two-phase framework together with data augmentation and feature augmentation to solve this problem. Four kinds of semantic knowledge (word embeddings, class descriptions, class hierarchy, and a general knowledge graph) are incorporated into the proposed framework to deal with instances of unseen classes effectively. Experimental results show that each and the combination of the two phases achieve the best overall accuracy compared with baselines and recent approaches in classifying real-world texts under the zero-shot scenario.</font>
<br>
</div>


<hr>
<div id="paper197"> <b>197. Consistency by Agreement in Zero-Shot Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/N19-1121.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title197" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NAACL 2019. <br>&nbsp;&nbsp;<i>Maruan Al-Shedivat, Ankur Parikh</i><br>
<font size="3">
Generalization and reliability of multilingual translation often highly depend on the amount of available parallel data for each language pair of interest. In this paper, we focus on zero-shot generalization—a challenging setup that tests models on translation directions they have not been optimized for at training time. To solve the problem, we (i) reformulate multilingual translation as probabilistic inference, (ii) define the notion of zero-shot consistency and show why standard training often results in models unsuitable for zero-shot tasks, and (iii) introduce a consistent agreement-based training method that encourages the model to produce equivalent translations of parallel sentences in auxiliary languages. We test our multilingual NMT models on multiple public zero-shot translation benchmarks (IWSLT17, UN corpus, Europarl) and show that agreement-based learning often results in 2-3 BLEU zero-shot improvement over strong baselines without any loss in performance on supervised translation directions.</font>
<br>
</div>


<hr>
<div id="paper198"> <b>198. Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing</b>  <a href="https://www.aclweb.org/anthology/N19-1162.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title198" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NAACL 2019. <br>&nbsp;&nbsp;<i>Tal Schuster, Ori Ram, Regina Barzilay, Amir Globerson</i><br>
<font size="3">
We introduce a novel method for multilingual transfer that utilizes deep contextual embeddings, pretrained in an unsupervised fashion. While contextual embeddings have been shown to yield richer representations of meaning compared to their static counterparts, aligning them poses a challenge due to their dynamic nature. To this end, we construct context-independent variants of the original monolingual spaces and utilize their mapping to derive an alignment for the context-dependent spaces. This mapping readily supports processing of a target language, improving transfer by context-aware embeddings. Our experimental results demonstrate the effectiveness of this approach for zero-shot and few-shot learning of dependency parsing. Specifically, our method consistently outperforms the previous state-of-the-art on 6 tested languages, yielding an improvement of 6.8 LAS points on average.</font>
<br>
</div>


<hr>
<div id="paper199"> <b>199. Measuring Immediate Adaptation Performance for Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/N19-1206.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title199" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NAACL 2019. <br>&nbsp;&nbsp;<i>Patrick Simianer, Joern Wuebker, John DeNero</i><br>
<font size="3">
Incremental domain adaptation, in which a system learns from the correct output for each input immediately after making its prediction for that input, can dramatically improve system performance for interactive machine translation. Users of interactive systems are sensitive to the speed of adaptation and how often a system repeats mistakes, despite being corrected. Adaptation is most commonly assessed using corpus-level BLEU- or TER-derived metrics that do not explicitly take adaptation speed into account. We find that these metrics often do not capture immediate adaptation effects, such as zero-shot and one-shot learning of domain-specific lexical items. To this end, we propose new metrics that directly evaluate immediate adaptation performance for machine translation. We use these metrics to choose the most suitable adaptation method from a range of different adaptation techniques for neural machine translation systems.</font>
<br>
</div>


<hr>
<div id="paper200"> <b>200. Zero-Shot Cross-Lingual Opinion Target Extraction</b>  <a href="https://www.aclweb.org/anthology/N19-1257.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title200" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NAACL 2019. <br>&nbsp;&nbsp;<i>Soufian Jebbara, Philipp Cimiano</i><br>
<font size="3">
Aspect-based sentiment analysis involves the recognition of so called opinion target expressions (OTEs). To automatically extract OTEs, supervised learning algorithms are usually employed which are trained on manually annotated corpora. The creation of these corpora is labor-intensive and sufficiently large datasets are therefore usually only available for a very narrow selection of languages and domains. In this work, we address the lack of available annotated data for specific languages by proposing a zero-shot cross-lingual approach for the extraction of opinion target expressions. We leverage multilingual word embeddings that share a common vector space across various languages and incorporate these into a convolutional neural network architecture for OTE extraction. Our experiments with 5 languages give promising results: We can successfully train a model on annotated data of a source language and perform accurate prediction on a target language without ever using any annotated samples in that target language. Depending on the source and target language pairs, we reach performances in a zero-shot regime of up to 77% of a model trained on target language data. Furthermore, we can increase this performance up to 87% of a baseline model trained on target language data by performing cross-lingual learning from multiple source languages.</font>
<br>
</div>


<hr>
<div id="paper201"> <b>201. Cross-lingual Annotation Projection Is Effective for Neural Part-of-Speech Tagging</b>  <a href="https://www.aclweb.org/anthology/W19-1425.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title201" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NAACL 2019. the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects<br>&nbsp;&nbsp;<i>Matthias Huck, Diana Dutka, Alexander Fraser</i><br>
<font size="3">
We tackle the important task of part-of-speech tagging using a neural model in the zero-resource scenario, where we have no access to gold-standard POS training data. We compare this scenario with the low-resource scenario, where we have access to a small amount of gold-standard POS training data. Our experiments focus on Ukrainian as a representative of under-resourced languages. Russian is highly related to Ukrainian, so we exploit gold-standard Russian POS tags. We consider four techniques to perform Ukrainian POS tagging: zero-shot tagging and cross-lingual annotation projection (for the zero-resource scenario), and compare these with self-training and multilingual learning (for the low-resource scenario). We find that cross-lingual annotation projection works particularly well in the zero-resource scenario.</font>
<br>
</div>


<hr>
<div id="paper202"> <b>202. Extreme Multi-Label Legal Text Classification: A Case Study in EU Legislation</b>  <a href="https://www.aclweb.org/anthology/W19-2209.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title202" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NAACL 2019. the Natural Legal Language Processing Workshop 2019<br>&nbsp;&nbsp;<i>Ilias Chalkidis, Emmanouil Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, Ion Androutsopoulos</i><br>
<font size="3">
We consider the task of Extreme Multi-Label Text Classification (XMTC) in the legal domain. We release a new dataset of 57k legislative documents from EURLEX, the European Union’s public document database, annotated with concepts from EUROVOC, a multidisciplinary thesaurus. The dataset is substantially larger than previous EURLEX datasets and suitable for XMTC, few-shot and zero-shot learning. Experimenting with several neural classifiers, we show that BIGRUs with self-attention outperform the current multi-label state-of-the-art methods, which employ label-wise attention. Replacing CNNs with BIGRUs in label-wise attention networks leads to the best overall performance.</font>
<br>
</div>


<hr>
<div id="paper203"> <b>203. Dataset Mention Extraction and Classification</b>  <a href="https://www.aclweb.org/anthology/W19-2604.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title203" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NAACL 2019. the Workshop on Extracting Structured Knowledge from Scientific Publications<br>&nbsp;&nbsp;<i>Animesh Prasad, Chenglei Si, Min-Yen Kan</i><br>
<font size="3">
Datasets are integral artifacts of empirical scientific research. However, due to natural language variation, their recognition can be difficult and even when identified, can often be inconsistently referred across and within publications. We report our approach to the Coleridge Initiative’s Rich Context Competition, which tasks participants with identifying dataset surface forms (dataset mention extraction) and associating the extracted mention to its referred dataset (dataset classification). In this work, we propose various neural baselines and evaluate these model on one-plus and zero-shot classification scenarios. We further explore various joint learning approaches - exploring the synergy between the tasks - and report the issues with such techniques.</font>
<br>
</div>


<hr>
<div id="paper204"> <b>204. Zero-shot Learning via Simultaneous Generating and Learning</b>  <a href="https://papers.nips.cc/paper/8300-zero-shot-learning-via-simultaneous-generating-and-learning.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title204" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NeurIPS 2019. <br>&nbsp;&nbsp;<i>Hyeonwoo Yu, Beomhee Lee</i><br>
<font size="3">
To overcome the absence of training data for unseen classes, conventional zero-shot learning approaches mainly train their model on seen datapoints and leverage the semantic descriptions for both seen and unseen classes.Beyond exploiting relations between classes of seen and unseen, we present a deep generative model to provide the model with experience about both seen and unseen classes.Based on the variational auto-encoder with class-specific multi-modal prior, the proposed method learns the conditional distribution of seen and unseen classes.In order to circumvent the need for samples of unseen classes, we treat the non-existing data as missing examples.That is, our network aims to find optimal unseen datapoints and model parameters, by iteratively following the generating and learning strategy.Since we obtain the conditional generative model for both seen and unseen classes, classification as well as generation can be performed directly without any off-the-shell classifiers.In experimental results, we demonstrate that the proposed generating and learning strategy makes the model achieve the outperforming results compared to that trained only on the seen classes, and also to the several state-of-the-art methods.</font>
<br>
</div>


<hr>
<div id="paper205"> <b>205. Zero-Shot Semantic Segmentation</b>  <a href="https://papers.nips.cc/paper/8338-zero-shot-semantic-segmentation.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title205" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NeurIPS 2019. <br>&nbsp;&nbsp;<i>Maxime Bucher, Tuan-Hung VU, Matthieu Cord, Patrick Pérez</i><br>
<font size="3">
Semantic segmentation models are limited in their ability to scale to large numbers of object classes. In this paper, we introduce the new task of zero-shot semantic segmentation: learning pixel-wise classifiers for never-seen object categories with zero training examples. To this end, we present a novel architecture, ZS3Net, combining a deep visual  segmentation model with an approach to generate visual representations from semantic word embeddings. By this way, ZS3Net addresses pixel classification tasks where both seen and unseen categories are faced at test time (so called generalized zero-shot classification). Performance is further improved by a self-training step that relies on automatic pseudo-labeling of pixels from unseen classes. On the two standard segmentation datasets, Pascal-VOC and Pascal-Context, we propose zero-shot benchmarks and set competitive baselines. For complex scenes as ones in the Pascal-Context dataset, we extend our approach by using a graph-context encoding to fully leverage spatial context priors coming from class-wise segmentation maps.</font>
<br>
</div>


<hr>
<div id="paper206"> <b>206. Dual Adversarial Semantics-Consistent Network for Generalized Zero-Shot Learning</b>  <a href="https://papers.nips.cc/paper/8846-dual-adversarial-semantics-consistent-network-for-generalized-zero-shot-learning.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title206" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NeurIPS 2019. <br>&nbsp;&nbsp;<i>Jian Ni, Shanghang Zhang, Haiyong Xie</i><br>
<font size="3">
Generalized zero-shot learning (GZSL) is a challenging class of vision and knowledge transfer problems in which both seen and unseen classes appear during testing. Existing GZSL approaches either suffer from semantic loss and discard discriminative information at the embedding stage, or cannot guarantee the visual-semantic interactions. To address these limitations, we propose a Dual Adversarial Semantics-Consistent Network (referred to as DASCN), which learns both primal and dual Generative Adversarial Networks (GANs) in a unified framework for GZSL. In DASCN, the primal GAN learns to synthesize inter-class discriminative and semantics-preserving visual features from both the semantic representations of seen/unseen classes and the ones reconstructed by the dual GAN. The dual GAN enforces the synthetic visual features to represent prior semantic knowledge well via semantics-consistent adversarial learning. To the best of our knowledge, this is the first work that employs a novel dual-GAN mechanism for GZSL. Extensive experiments show that our approach achieves significant improvements over the state-of-the-art approaches.</font>
<br>
</div>


<hr>
<div id="paper207"> <b>207. Non-Stationary Markov Decision Processes, a Worst-Case Approach using Model-Based Reinforcement Learning</b>  <a href="https://papers.nips.cc/paper/8942-non-stationary-markov-decision-processes-a-worst-case-approach-using-model-based-reinforcement-learning.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title207" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NeurIPS 2019. <br>&nbsp;&nbsp;<i>Erwan Lecarpentier, Emmanuel Rachelson</i><br>
<font size="3">
This work tackles the problem of robust zero-shot planning in non-stationary stochastic environments. We study Markov Decision Processes (MDPs) evolving over time and consider Model-Based Reinforcement Learning algorithms in this setting. We make two hypotheses: 1) the environment evolves continuously with a bounded evolution rate; 2) a current model is known at each decision epoch but not its evolution. Our contribution can be presented in four points. 1) we define a specific class of MDPs that we call Non-Stationary MDPs (NSMDPs). We introduce the notion of regular evolution by making an hypothesis of Lipschitz-Continuity on the transition and reward functions w.r.t. time; 2) we consider a planning agent using the current model of the environment but unaware of its future evolution. This leads us to consider a worst-case method where the environment is seen as an adversarial agent; 3) following this approach, we propose the Risk-Averse Tree-Search (RATS) algorithm, a zero-shot Model-Based method similar to Minimax search; 4) we illustrate the benefits brought by RATS empirically and compare its performance with reference Model-Based algorithms.</font>
<br>
</div>


<hr>
<div id="paper208"> <b>208. A Structured Prediction Approach for Generalization in Cooperative Multi-Agent Reinforcement Learning</b>  <a href="https://papers.nips.cc/paper/9024-a-structured-prediction-approach-for-generalization-in-cooperative-multi-agent-reinforcement-learning.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title208" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NeurIPS 2019. <br>&nbsp;&nbsp;<i>Nicolas Carion, Nicolas Usunier, Gabriel Synnaeve, Alessandro Lazaric</i><br>
<font size="3">
Effective coordination is crucial to solve multi-agent collaborative (MAC) problems. While centralized reinforcement learning methods can optimally solve small MAC instances, they do not scale to large problems and they fail to generalize to scenarios different from those seen during training. In this paper, we consider MAC problems with some intrinsic notion of locality (e.g., geographic proximity) such that interactions between agents and tasks are locally limited. By leveraging this property, we introduce a novel structured prediction approach to assign agents to tasks. At each step, the assignment is obtained by solving a centralized optimization problem (the inference procedure) whose objective function is parameterized by a learned scoring model. We propose different combinations of inference procedures and scoring models able to represent coordination patterns of increasing complexity. The resulting assignment policy can be efficiently learned on small problem instances and readily reused in problems with more agents and tasks (i.e., zero-shot generalization). We report experimental results on a toy search and rescue problem and on several target selection scenarios in StarCraft: Brood War, in which our model significantly outperforms strong rule-based baselines on instances with 5 times more agents and tasks than those seen during training.</font>
<br>
</div>


<hr>
<div id="paper209"> <b>209. Zero-shot Knowledge Transfer via Adversarial Belief Matching</b>  <a href="https://papers.nips.cc/paper/9151-zero-shot-knowledge-transfer-via-adversarial-belief-matching.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title209" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NeurIPS 2019. <br>&nbsp;&nbsp;<i>Paul Micaelli, Amos J. Storkey</i><br>
<font size="3">
Performing knowledge transfer from a large teacher network to a smaller student is a popular task in modern deep learning applications. However, due to growing dataset sizes and stricter privacy regulations, it is increasingly common not to have access to the data that was used to train the teacher. We propose a novel method which trains a student to match the predictions of its teacher without using any data or metadata. We achieve this by training an adversarial generator to search for images on which the student poorly matches the teacher, and then using them to train the student. Our resulting student closely approximates its teacher for simple datasets like SVHN, and on CIFAR10 we improve on the state-of-the-art for few-shot distillation (with $100$ images per class), despite using no data. Finally, we also propose a metric to quantify the degree of belief matching between teacher and student in the vicinity of decision boundaries, and observe a significantly higher match between our zero-shot student and the teacher, than between a student distilled with real data and the teacher. Code is available at: https://github.com/polo5/ZeroShotKnowledgeTransfer</font>
<br>
</div>


<hr>
<div id="paper210"> <b>210. Transductive Zero-Shot Learning with Visual Structure Constraint</b>  <a href="https://papers.nips.cc/paper/9188-transductive-zero-shot-learning-with-visual-structure-constraint.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title210" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NeurIPS 2019. <br>&nbsp;&nbsp;<i>Ziyu Wan, Dongdong Chen, Yan Li, Xingguang Yan, Junge Zhang, Yizhou Yu, Jing Liao</i><br>
<font size="3">
To recognize objects of the unseen classes, most existing Zero-Shot Learning (ZSL) methods first learn a compatible projection function between the common semantic space and the visual space based on the data of source seen classes, then directly apply it to the target unseen classes. However, in real scenarios, the data distribution between the source and target domain might not match well, thus causing the well-known domain shift problem. Based on the observation that visual features of test instances can be separated into different clusters, we propose a new visual structure constraint on class centers for transductive ZSL, to improve the generality of the projection function (\ie alleviate the above domain shift problem). Specifically, three different strategies (symmetric Chamfer-distance,Bipartite matching distance, and Wasserstein distance) are adopted to align the projected unseen semantic centers and visual cluster centers of test instances. We also propose a new training strategy to handle the real cases where many unrelated images exist in the test dataset, which is not considered in previous methods. Experiments on many widely used datasets demonstrate that the proposed visual structure constraint can bring substantial performance gain consistently and achieve state-of-the-art results.</font>
<br>
</div>


<hr>
<div id="paper211"> <b>211. Semantic-Guided Multi-Attention Localization for Zero-Shot Learning</b>  <a href="https://papers.nips.cc/paper/9632-semantic-guided-multi-attention-localization-for-zero-shot-learning.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title211" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NeurIPS 2019. <br>&nbsp;&nbsp;<i>Yizhe Zhu, Jianwen Xie, Zhiqiang Tang, Xi Peng, Ahmed Elgammal</i><br>
<font size="3">
Zero-shot learning extends the conventional object classification to the unseen class recognition by introducing semantic representations of classes. Existing approaches predominantly focus on learning the proper mapping function for visual-semantic embedding, while neglecting the effect of learning discriminative visual features. In this paper, we study the significance of the discriminative region localization. We propose a semantic-guided multi-attention localization model, which automatically discovers the most discriminative parts of objects for zero-shot learning without any human annotations. Our model jointly learns cooperative global and local features from the whole object as well as the detected parts to categorize objects based on semantic descriptions. Moreover, with the joint supervision of embedding softmax loss and class-center triplet loss, the model is encouraged to learn features with high inter-class dispersion and intra-class compactness. Through comprehensive experiments on three widely used zero-shot learning benchmarks, we show the efficacy of the multi-attention localization and our proposed approach improves the state-of-the-art results by a considerable margin.</font>
<br>
</div>


<hr>
<div id="paper212"> <b>212. Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond</b>  <a href="https://www.aclweb.org/anthology/Q19-1038.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title212" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;TACL 2019. <br>&nbsp;&nbsp;<i>Mikel Artetxe, Holger Schwenk</i><br>
<font size="3">
We introduce an architecture to learn joint multilingual sentence representations for 93 languages, belonging to more than 30 different families and written in 28 different scripts. Our system uses a single BiLSTM encoder with a shared byte-pair encoding vocabulary for all languages, which is coupled with an auxiliary decoder and trained on publicly available parallel corpora. This enables us to learn a classifier on top of the resulting embeddings using English annotated data only, and transfer it to any of the 93 languages without any modification. Our experiments in cross-lingual natural language inference (XNLI data set), cross-lingual document classification (MLDoc data set), and parallel corpus mining (BUCC data set) show the effectiveness of our approach. We also introduce a new test set of aligned sentences in 112 languages, and show that our sentence embeddings obtain strong results in multilingual similarity search even for low- resource languages. Our implementation, the pre-trained encoder, and the multilingual test set are available at https://github.com/facebookresearch/LASER.</font>
<br>
</div>


<hr>
<div id="paper213"> <b>213. Adversarial Zero-shot Learning With Semantic Augmentation</b>  <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16805/15965" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title213" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2018. AAAI18 - Machine Learning Applications<br>&nbsp;&nbsp;<i>Bin Tong, Martin Klinkigt, Junwen Chen, Xiankun Cui, Quan Kong, Tomokazu Murakami, Yoshiyuki Kobayashi</i><br>
<font size="3">
In situations in which labels are expensive or difficult to obtain, deep neural networks for object recognition often suffer to achieve fair performance. Zero-shot learning is dedicated to this problem. It aims to recognize objects of unseen classes by transferring knowledge from seen classes via a shared intermediate representation. Using the manifold structure of seen training samples is widely regarded as important to learn a robust mapping between samples and the intermediate representation, which is crucial for transferring the knowledge. However, their irregular structures, such as the lack in variation of samples for certain classes and highly overlapping clusters of different classes, may result in an inappropriate mapping. Additionally, in a high dimensional mapping space, the hubness problem may arise, in which one of the unseen classes has a high possibility to be assigned to samples of different classes. To mitigate such problems, we use a generative adversarial network to synthesize samples with specified semantics to cover a higher diversity of given classes and interpolated semantics of pairs of classes. We propose a simple yet effective method for applying the augmented semantics to the hinge loss functions to learn a robust mapping. The proposed method was extensively evaluated on small- and large-scale datasets, showing a significant improvement over state-of-the-art methods.</font>
<br>
</div>


<hr>
<div id="paper214"> <b>214. Joint Dictionaries for Zero-Shot Learning</b>  <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16404/16723" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title214" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2018. AAAI18 - Machine Learning Methods<br>&nbsp;&nbsp;<i>Soheil Kolouri, Mohammad Rostami, Yuri Owechko, Kyungnam Kim</i><br>
<font size="3">
A classic approach toward zero-shot learning (ZSL) is to map the input domain to a set of semantically meaningful attributes that could be used later on to classify unseen classes of data (e.g. visual data). In this paper, we propose to learn a visual feature dictionary that has semantically meaningful atoms. Such a dictionary is learned via joint dictionary learning for the visual domain and the attribute domain, while enforcing the same sparse coding for both dictionaries. Our novel attribute aware formulation provides an algorithmic solution to the domain shift/hubness problem in ZSL. Upon learning the joint dictionaries, images from unseen classes can be mapped into the attribute space by finding the attribute aware joint sparse representation using solely the visual data. We demonstrate that our approach provides superior or comparable performance to that of the state of the art on benchmark datasets.</font>
<br>
</div>


<hr>
<div id="paper215"> <b>215. FiLM: Visual Reasoning with a General Conditioning Layer</b>  <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16528/16646" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title215" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2018. AAAI18 - Machine Learning Methods<br>&nbsp;&nbsp;<i>Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, Aaron C. Courville</i><br>
<font size="3">
We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.</font>
<br>
</div>


<hr>
<div id="paper216"> <b>216. Zero-Shot Learning via Class-Conditioned Deep Generative Models</b>  <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16087/16709" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title216" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2018. AAAI18 - Machine Learning Methods<br>&nbsp;&nbsp;<i>Wenlin Wang, Yunchen Pu, Vinay Kumar Verma, Kai Fan, Yizhe Zhang, Changyou Chen, Piyush Rai, Lawrence Carin</i><br>
<font size="3">
We present a deep generative model for Zero-Shot Learning (ZSL). Unlike most existing methods for this problem, that represent each class as a point (via a semantic embedding), we represent each seen/unseen class using a class-specific latent-space distribution, conditioned on class attributes. We use these latent-space distributions as a prior for a supervised variational autoencoder (VAE), which also facilitates learning highly discriminative feature representations for the inputs. The entire framework is learned end-to-end using only the seen-class training data. At test time, the label for an unseen-class test input is the class that maximizes the VAE lower bound. We further extend the model to a (i) semi-supervised/transductive setting by leveraging unlabeled unseen-class data via an unsupervised learning module, and (ii) few-shot learning where we also have a small number of labeled inputs from the unseen classes. We compare our model with several state-of-the-art methods through a comprehensive set of experiments on a variety of benchmark data sets.</font>
<br>
</div>


<hr>
<div id="paper217"> <b>217. Generalizing and Improving Bilingual Word Embedding Mappings with a Multi-Step Framework of Linear Transformations</b>  <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16935/16781" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title217" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2018. AAAI18 - NLP and Machine Learning<br>&nbsp;&nbsp;<i>Mikel Artetxe, Gorka Labaka, Eneko Agirre</i><br>
<font size="3">
Using a dictionary to map independently trained word embeddings to a shared space has shown to be an effective approach to learn bilingual word embeddings. In this work, we propose a multi-step framework of linear transformations that generalizes a substantial body of previous work. The core step of the framework is an orthogonal transformation, and existing methods can be explained in terms of the additional normalization, whitening, re-weighting, de-whitening and dimensionality reduction steps. This allows us to gain new insights into the behavior of existing methods, including the effectiveness of inverse regression, and design a novel variant that obtains the best published results in zero-shot bilingual lexicon extraction. The corresponding software is released as an open source project.</font>
<br>
</div>


<hr>
<div id="paper218"> <b>218. Neural Cross-Lingual Entity Linking</b>  <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16501/16101" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title218" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2018. AAAI18 - NLP and Machine Learning<br>&nbsp;&nbsp;<i>Avirup Sil, Gourab Kundu, Radu Florian, Wael Hamza</i><br>
<font size="3">
A major challenge in Entity Linking (EL) is making effective use of contextual information to disambiguate mentions to Wikipedia that might refer to different entities in different contexts. The problem exacerbates with cross-lingual EL which involves linking mentions written in non-English documents to entries in the English Wikipedia: to compare textual clues across languages we need to compute similarity between textual fragments across languages. In this paper, we propose a neural EL model that trains fine-grained similarities and dissimilarities between the query and candidate document from multiple perspectives, combined with convolution and tensor networks. Further, we show that this English-trained system can be applied, in zero-shot learning, to other languages by making surprisingly effective use of multi-lingual embeddings. The proposed system has strong empirical evidence yielding state-of-the-art results in English as well as cross-lingual: Spanish and Chinese TAC 2015 datasets.</font>
<br>
</div>


<hr>
<div id="paper219"> <b>219. Zero-Shot Learning With Attribute Selection</b>  <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16350/16272" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title219" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2018. AAAI18 - Vision<br>&nbsp;&nbsp;<i>Yuchen Guo, Guiguang Ding, Jungong Han, Sheng Tang</i><br>
<font size="3">
Zero-shot learning (ZSL) is regarded as an effective way to construct classification models for target classes which have no labeled samples available. The basic framework is to transfer knowledge from (different) auxiliary source classes having sufficient labeled samples with some attributes shared by target and source classes as bridge. Attributes play an important role in ZSL but they have not gained sufficient attention in recent years. Previous works mostly assume attributes are perfect and treat each attribute equally. However, as shown in this paper, different attributes have different properties, such as their class distribution, variance, and entropy, which may have considerable impact on ZSL accuracy if treated equally. Based on this observation, in this paper we propose to use a subset of attributes, instead of the whole set, for building ZSL models. The attribute selection is conducted by considering the information amount and predictability under a novel joint optimization framework. To our knowledge, this is the first work that notices the influence of attributes themselves and proposes to use a refined attribute set for ZSL. Since our approach focuses on selecting good attributes for ZSL, it can be combined to any attribute based ZSL approaches so as to augment their performance. Experiments on four ZSL benchmarks demonstrate that our approach can improve zero-shot classification accuracy and yield state-of-the-art results.</font>
<br>
</div>


<hr>
<div id="paper220"> <b>220. Deep Semantic Structural Constraints for Zero-Shot Learning</b>  <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16309/16294" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title220" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2018. AAAI18 - Vision<br>&nbsp;&nbsp;<i>Yan Li, Zhen Jia, Junge Zhang, Kaiqi Huang, Tieniu Tan</i><br>
<font size="3">
Zero-shot learning aims to classify unseen image categories by learning a visual-semantic embedding space. In most cases, the traditional methods adopt a separated two-step pipeline that extracts image features are utilized to learn the embedding space. It leads to the lack of specific structural semantic information of image features for zero-shot learning task. In this paper, we propose an end-to-end trainable Deep Semantic Structural Constraints model to address this issue. The proposed model contains the Image Feature Structure constraint and the Semantic Embedding Structure constraint, which aim to learn structure-preserving image features and endue the learned embedding space with stronger generalization ability respectively. With the assistance of semantic structural information, the model gains more auxiliary clues for zero-shot learning. The state-of-the-art performance certifies the effectiveness of our proposed method.</font>
<br>
</div>


<hr>
<div id="paper221"> <b>221. Visual Relationship Detection With Deep Structural Ranking</b>  <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16491/16300" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title221" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2018. AAAI18 - Vision<br>&nbsp;&nbsp;<i>Kongming Liang, Yuhong Guo, Hong Chang, Xilin Chen</i><br>
<font size="3">
Visual relationship detection aims to describe the interactions between pairs of objects. Different from individual object learning tasks, the number of possible relationships are much larger, which makes it hard to explore only based on the visual appearance of objects. In addition, due to the limited human effort, the annotations for visual relationships are usually incomplete which increases the difficulty of model training and evaluation. In this paper, we propose a novel framework, called Deep Structural Ranking, for visual relationship detection. To complement the representation ability of visual appearance, we integrate multiple cues for predicting the relationships contained in an input image. Moreover, we design a new ranking objective function by enforcing the annotated relationships to have higher relevance scores. Unlike previous works, our proposed method can both facilitate the co-occurrence of relationships and mitigate the incompleteness problem. Experimental results show that our proposed method outperforms the state-of-the-art on the two widely used datasets. We also demonstrate its superiority in detecting zero-shot relationships.</font>
<br>
</div>


<hr>
<div id="paper222"> <b>222. Towards Affordable Semantic Searching: Zero-Shot Retrieval via Dominant Attributes</b>  <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16626/16314" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title222" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2018. AAAI18 - Vision<br>&nbsp;&nbsp;<i>Yang Long, Li Liu, Yuming Shen, Ling Shao</i><br>
<font size="3">
Instance-level retrieval has become an essential paradigm to index and retrieves images from large-scale databases. Conventional instance search requires at least an example of the query image to retrieve images that contain the same object instance. Existing semantic retrieval can only search semantically-related images, such as those sharing the same category or a set of tags, not the exact instances. Meanwhile, the unrealistic assumption is that all categories or tags are known beforehand. Training models for these semantic concepts highly rely on instance-level attributes or human captions which are expensive to acquire. Given the above challenges, this paper studies the Zero-shot Retrieval problem that aims for instance-level image search using only a few dominant attributes. The contributions are: 1) we utilise automatic word embedding to infer class-level attributes to circumvent expensive human labelling; 2) the inferred class-attributes can be extended into discriminative instance attributes through our proposed Latent Instance Attributes Discovery (LIAD) algorithm; 3) our method is not restricted to complete attribute signatures, query of dominant attributes can also be dealt with. On two benchmarks, CUB and SUN, extensive experiments demonstrate that our method can achieve promising performance for the problem. Moreover, our approach can also benefit conventional ZSL tasks.</font>
<br>
</div>


<hr>
<div id="paper223"> <b>223. HCVRD: A Benchmark for Large-Scale Human-Centered Visual Relationship Detection</b>  <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16444/16362" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title223" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2018. AAAI18 - Vision<br>&nbsp;&nbsp;<i>Bohan Zhuang, Qi Wu, Chunhua Shen, Ian D. Reid, Anton van den Hengel</i><br>
<font size="3">
Visual relationship detection aims to capture interactions between pairs of objects in images. Relationships between objects and humans represent a particularly important subset of this problem, with implications for challenges such as understanding human behavior, and identifying affordances, amongst others. In addressing this problem we first construct a large-scale human-centric visual relationship detection dataset (HCVRD), which provides many more types of relationship annotations (nearly 10K categories) than the previous released datasets. This large label space better reflects the reality of human-object interactions, but gives rise to a long-tail distribution problem, which in turn demands a zero-shot approach to labels appearing only in the test set.  This is the first time this issue has been addressed. We propose a webly-supervised approach to these problems and demonstrate that the proposed model provides a strong baseline on our HCVRD dataset.</font>
<br>
</div>


<hr>
<div id="paper224"> <b>224. Zero-shot Learning of Classifiers from Natural Language Quantification</b>  <a href="https://www.aclweb.org/anthology/P18-1029.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title224" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2018. Long Papers<br>&nbsp;&nbsp;<i>Shashank Srivastava, Igor Labutov, Tom Mitchell</i><br>
<font size="3">
Humans can efficiently learn new concepts using language. We present a framework through which a set of explanations of a concept can be used to learn a classifier without access to any labeled examples. We use semantic parsing to map explanations to probabilistic assertions grounded in latent class labels and observed attributes of unlabeled data, and leverage the differential semantics of linguistic quantifiers (e.g., ‘usually’ vs ‘always’) to drive model training. Experiments on three domains show that the learned classifiers outperform previous approaches for learning with limited data, and are comparable with fully supervised classifiers trained from a small number of labeled examples.</font>
<br>
</div>


<hr>
<div id="paper225"> <b>225. Zero-Shot Transfer Learning for Event Extraction</b>  <a href="https://www.aclweb.org/anthology/P18-1201.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title225" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2018. Long Papers<br>&nbsp;&nbsp;<i>Lifu Huang, Heng Ji, Kyunghyun Cho, Ido Dagan, Sebastian Riedel, Clare Voss</i><br>
<font size="3">
Most previous supervised event extraction methods have relied on features derived from manual annotations, and thus cannot be applied to new event types without extra annotation effort. We take a fresh look at event extraction and model it as a generic grounding problem: mapping each event mention to a specific type in a target event ontology. We design a transferable architecture of structural and compositional neural networks to jointly represent and map event mentions and types into a shared semantic space. Based on this new framework, we can select, for each event mention, the event type which is semantically closest in this space as its type. By leveraging manual annotations available for a small set of existing event types, our framework can be applied to new unseen event types without additional manual annotations. When tested on 23 unseen event types, our zero-shot framework, without manual annotations, achieved performance comparable to a supervised model trained from 3,000 sentences annotated with 500 event mentions.</font>
<br>
</div>


<hr>
<div id="paper226"> <b>226. A Deep Relevance Model for Zero-Shot Document Filtering</b>  <a href="https://www.aclweb.org/anthology/P18-1214.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title226" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2018. Long Papers<br>&nbsp;&nbsp;<i>Chenliang Li, Wei Zhou, Feng Ji, Yu Duan, Haiqing Chen</i><br>
<font size="3">
In the era of big data, focused analysis for diverse topics with a short response time becomes an urgent demand. As a fundamental task, information filtering therefore becomes a critical necessity. In this paper, we propose a novel deep relevance model for zero-shot document filtering, named DAZER. DAZER estimates the relevance between a document and a category by taking a small set of seed words relevant to the category. With pre-trained word embeddings from a large external corpus, DAZER is devised to extract the relevance signals by modeling the hidden feature interactions in the word embedding space. The relevance signals are extracted through a gated convolutional process. The gate mechanism controls which convolution filters output the relevance signals in a category dependent manner. Experiments on two document collections of two different tasks (i.e., topic categorization and sentiment analysis) demonstrate that DAZER significantly outperforms the existing alternative solutions, including the state-of-the-art deep relevance ranking models.</font>
<br>
</div>


<hr>
<div id="paper227"> <b>227. Learning Simplifications for Specific Target Audiences</b>  <a href="https://www.aclweb.org/anthology/P18-2113.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title227" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2018. Short Papers<br>&nbsp;&nbsp;<i>Carolina Scarton, Lucia Specia</i><br>
<font size="3">
Text simplification (TS) is a monolingual text-to-text transformation task where an original (complex) text is transformed into a target (simpler) text. Most recent work is based on sequence-to-sequence neural models similar to those used for machine translation (MT). Different from MT, TS data comprises more elaborate transformations, such as sentence splitting. It can also contain multiple simplifications of the same original text targeting different audiences, such as school grade levels. We explore these two features of TS to build models tailored for specific grade levels. Our approach uses a standard sequence-to-sequence architecture where the original sequence is annotated with information about the target audience and/or the (predicted) type of simplification operation. We show that it outperforms state-of-the-art TS approaches (up to 3 and 12 BLEU and SARI points, respectively), including when training data for the specific complex-simple combination of grade levels is not available, i.e. zero-shot learning.</font>
<br>
</div>


<hr>
<div id="paper228"> <b>228. A Comparison of Transformer and Recurrent Neural Networks on Multilingual Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/C18-1054.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title228" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;COLING 2018. <br>&nbsp;&nbsp;<i>Surafel Melaku Lakew, Mauro Cettolo, Marcello Federico</i><br>
<font size="3">
Recently, neural machine translation (NMT) has been extended to multilinguality, that is to handle more than one translation direction with a single system. Multilingual NMT showed competitive performance against pure bilingual systems. Notably, in low-resource settings, it proved to work effectively and efficiently, thanks to shared representation space that is forced across languages and induces a sort of transfer-learning. Furthermore, multilingual NMT enables so-called zero-shot inference across language pairs never seen at training time. Despite the increasing interest in this framework, an in-depth analysis of what a multilingual NMT model is capable of and what it is not is still missing. Motivated by this, our work (i) provides a quantitative and comparative analysis of the translations produced by bilingual, multilingual and zero-shot systems; (ii) investigates the translation quality of two of the currently dominant neural architectures in MT, which are the Recurrent and the Transformer ones; and (iii) quantitatively explores how the closeness between languages influences the zero-shot translation. Our analysis leverages multiple professional post-edits of automatic translations by several different systems and focuses both on automatic standard metrics (BLEU and TER) and on widely used error categories, which are lexical, morphology, and word order errors.</font>
<br>
</div>


<hr>
<div id="paper229"> <b>229. Multilingual Neural Machine Translation with Task-Specific Attention</b>  <a href="https://www.aclweb.org/anthology/C18-1263.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title229" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;COLING 2018. <br>&nbsp;&nbsp;<i>Graeme Blackwood, Miguel Ballesteros, Todd Ward</i><br>
<font size="3">
Multilingual machine translation addresses the task of translating between multiple source and target languages. We propose task-specific attention models, a simple but effective technique for improving the quality of sequence-to-sequence neural multilingual translation. Our approach seeks to retain as much of the parameter sharing generalization of NMT models as possible, while still allowing for language-specific specialization of the attention model to a particular language-pair or task. Our experiments on four languages of the Europarl corpus show that using a target-specific model of attention provides consistent gains in translation quality for all possible translation directions, compared to a model in which all parameters are shared. We observe improved translation quality even in the (extreme) low-resource zero-shot translation directions for which the model never saw explicitly paired parallel data.</font>
<br>
</div>


<hr>
<div id="paper230"> <b>230. Adversarial Propagation and Zero-Shot Cross-Lingual Transfer of Word Vector Specialization</b>  <a href="https://www.aclweb.org/anthology/D18-1026.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title230" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2018. <br>&nbsp;&nbsp;<i>Edoardo Maria Ponti, Ivan Vulić, Goran Glavaš, Nikola Mrkšić, Anna Korhonen</i><br>
<font size="3">
Semantic specialization is a process of fine-tuning pre-trained distributional word vectors using external lexical knowledge (e.g., WordNet) to accentuate a particular semantic relation in the specialized vector space. While post-processing specialization methods are applicable to arbitrary distributional vectors, they are limited to updating only the vectors of words occurring in external lexicons (i.e., seen words), leaving the vectors of all other words unchanged. We propose a novel approach to specializing the full distributional vocabulary. Our adversarial post-specialization method propagates the external lexical knowledge to the full distributional space. We exploit words seen in the resources as training examples for learning a global specialization function. This function is learned by combining a standard L2-distance loss with a adversarial loss: the adversarial component produces more realistic output vectors. We show the effectiveness and robustness of the proposed method across three languages and on three tasks: word similarity, dialog state tracking, and lexical simplification. We report consistent improvements over distributional word vectors and vectors specialized by other state-of-the-art specialization frameworks. Finally, we also propose a cross-lingual transfer method for zero-shot specialization which successfully specializes a full target distributional space without any lexical knowledge in the target language and without any bilingual data.</font>
<br>
</div>


<hr>
<div id="paper231"> <b>231. Contextual Parameter Generation for Universal Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/D18-1039.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title231" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2018. <br>&nbsp;&nbsp;<i>Emmanouil Antonios Platanios, Mrinmaya Sachan, Graham Neubig, Tom Mitchell</i><br>
<font size="3">
We propose a simple modification to existing neural machine translation (NMT) models that enables using a single universal model to translate between multiple languages while allowing for language specific parameterization, and that can also be used for domain adaptation. Our approach requires no changes to the model architecture of a standard NMT system, but instead introduces a new component, the contextual parameter generator (CPG), that generates the parameters of the system (e.g., weights in a neural network). This parameter generator accepts source and target language embeddings as input, and generates the parameters for the encoder and the decoder, respectively. The rest of the model remains unchanged and is shared across all languages. We show how this simple modification enables the system to use monolingual data for training and also perform zero-shot translation. We further show it is able to surpass state-of-the-art performance for both the IWSLT-15 and IWSLT-17 datasets and that the learned language embeddings are able to uncover interesting relationships between languages.</font>
<br>
</div>


<hr>
<div id="paper232"> <b>232. Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing</b>  <a href="https://www.aclweb.org/anthology/D18-1190.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title232" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2018. <br>&nbsp;&nbsp;<i>Jonathan Herzig, Jonathan Berant</i><br>
<font size="3">
Building a semantic parser quickly in a new domain is a fundamental challenge for conversational interfaces, as current semantic parsers require expensive supervision and lack the ability to generalize to new domains. In this paper, we introduce a zero-shot approach to semantic parsing that can parse utterances in unseen domains while only being trained on examples in other source domains. First, we map an utterance to an abstract, domain independent, logical form that represents the structure of the logical form, but contains slots instead of KB constants. Then, we replace slots with KB constants via lexical alignment scores and global inference. Our model reaches an average accuracy of 53.4% on 7 domains in the OVERNIGHT dataset, substantially better than other zero-shot baselines, and performs as good as a parser trained on over 30% of the target domain examples.</font>
<br>
</div>


<hr>
<div id="paper233"> <b>233. Zero-Shot Open Entity Typing as Type-Compatible Grounding</b>  <a href="https://www.aclweb.org/anthology/D18-1231.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title233" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2018. <br>&nbsp;&nbsp;<i>Ben Zhou, Daniel Khashabi, Chen-Tse Tsai, Dan Roth</i><br>
<font size="3">
The problem of entity-typing has been studied predominantly as a supervised learning problems, mostly with task-specific annotations (for coarse types) and sometimes with distant supervision (for fine types). While such approaches have strong performance within datasets they often lack the flexibility to transfer across text genres and to generalize to new type taxonomies. In this work we propose a zero-shot entity typing approach that requires no annotated data and can flexibly identify newly defined types. Given a type taxonomy, the entries of which we define as Boolean functions of freebase “types,” we ground a given mention to a set of type-compatible Wikipedia entries, and then infer the target mention’s type using an inference algorithm that makes use of the types of these entries. We evaluate our system on a broad range of datasets, including standard fine-grained and coarse-grained entity typing datasets, and on a dataset in the biological domain. Our system is shown to be competitive with state-of-the-art supervised NER systems, and to outperform them on out-of-training datasets. We also show that our system significantly outperforms other zero-shot fine typing systems.</font>
<br>
</div>


<hr>
<div id="paper234"> <b>234. Joint Multilingual Supervision for Cross-lingual Entity Linking</b>  <a href="https://www.aclweb.org/anthology/D18-1270.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title234" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2018. <br>&nbsp;&nbsp;<i>Shyam Upadhyay, Nitish Gupta, Dan Roth</i><br>
<font size="3">
Cross-lingual Entity Linking (XEL) aims to ground entity mentions written in any language to an English Knowledge Base (KB), such as Wikipedia. XEL for most languages is challenging, owing to limited availability of resources as supervision. We address this challenge by developing the first XEL approach that combines supervision from multiple languages jointly. This enables our approach to: (a) augment the limited supervision in the target language with additional supervision from a high-resource language (like English), and (b) train a single entity linking model for multiple languages, improving upon individually trained models for each language. Extensive evaluation on three benchmark datasets across 8 languages shows that our approach significantly improves over the current state-of-the-art. We also provide analyses in two limited resource settings: (a) zero-shot setting, when no supervision in the target language is available, and in (b) low-resource setting, when some supervision in the target language is available. Our analysis provides insights into the limitations of zero-shot XEL approaches in realistic scenarios, and shows the value of joint supervision in low-resource settings.</font>
<br>
</div>


<hr>
<div id="paper235"> <b>235. Zero-shot User Intent Detection via Capsule Neural Networks</b>  <a href="https://www.aclweb.org/anthology/D18-1348.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title235" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2018. <br>&nbsp;&nbsp;<i>Congying Xia, Chenwei Zhang, Xiaohui Yan, Yi Chang, Philip Yu</i><br>
<font size="3">
User intent detection plays a critical role in question-answering and dialog systems. Most previous works treat intent detection as a classification problem where utterances are labeled with predefined intents. However, it is labor-intensive and time-consuming to label users’ utterances as intents are diversely expressed and novel intents will continually be involved. Instead, we study the zero-shot intent detection problem, which aims to detect emerging user intents where no labeled utterances are currently available. We propose two capsule-based architectures: IntentCapsNet that extracts semantic features from utterances and aggregates them to discriminate existing intents, and IntentCapsNet-ZSL which gives IntentCapsNet the zero-shot learning ability to discriminate emerging intents via knowledge transfer from existing intents. Experiments on two real-world datasets show that our model not only can better discriminate diversely expressed existing intents, but is also able to discriminate emerging intents when no labeled utterances are available.</font>
<br>
</div>


<hr>
<div id="paper236"> <b>236. Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces</b>  <a href="https://www.aclweb.org/anthology/D18-1352.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title236" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2018. <br>&nbsp;&nbsp;<i>Anthony Rios, Ramakanth Kavuluru</i><br>
<font size="3">
Large multi-label datasets contain labels that occur thousands of times (frequent group), those that occur only a few times (few-shot group), and labels that never appear in the training dataset (zero-shot group). Multi-label few- and zero-shot label prediction is mostly unexplored on datasets with large label spaces, especially for text classification. In this paper, we perform a fine-grained evaluation to understand how state-of-the-art methods perform on infrequent labels. Furthermore, we develop few- and zero-shot methods for multi-label text classification when there is a known structure over the label space, and evaluate them on two publicly available medical text datasets: MIMIC II and MIMIC III. For few-shot labels we achieve improvements of 6.2% and 4.8% in R@10 for MIMIC II and MIMIC III, respectively, over prior efforts; the corresponding R@10 improvements for zero-shot labels are 17.3% and 19%.</font>
<br>
</div>


<hr>
<div id="paper237"> <b>237. Zero-shot Relation Classification as Textual Entailment</b>  <a href="https://www.aclweb.org/anthology/W18-5511.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title237" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2018. the First Workshop on Fact Extraction and VERification (FEVER)<br>&nbsp;&nbsp;<i>Abiola Obamuyide, Andreas Vlachos</i><br>
<font size="3">
We consider the task of relation classification, and pose this task as one of textual entailment. We show that this formulation leads to several advantages, including the ability to (i) perform zero-shot relation classification by exploiting relation descriptions, (ii) utilize existing textual entailment models, and (iii) leverage readily available textual entailment datasets, to enhance the performance of relation classification systems. Our experiments show that the proposed approach achieves 20.16% and 61.32% in F1 zero-shot classification performance on two datasets, which further improved to 22.80% and 64.78% respectively with the use of conditional encoding.</font>
<br>
</div>


<hr>
<div id="paper238"> <b>238. A neural interlingua for multilingual machine translation</b>  <a href="https://www.aclweb.org/anthology/W18-6309.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title238" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2018. the Third Conference on Machine Translation: Research Papers<br>&nbsp;&nbsp;<i>Yichao Lu, Phillip Keung, Faisal Ladhak, Vikas Bhardwaj, Shaonan Zhang, Jason Sun</i><br>
<font size="3">
We incorporate an explicit neural interlingua into a multilingual encoder-decoder neural machine translation (NMT) architecture. We demonstrate that our model learns a language-independent representation by performing direct zero-shot translation (without using pivot translation), and by using the source sentence embeddings to create an English Yelp review classifier that, through the mediation of the neural interlingua, can also classify French and German reviews. Furthermore, we show that, despite using a smaller number of parameters than a pairwise collection of bilingual NMT models, our approach produces comparable BLEU scores for each language pair in WMT15.</font>
<br>
</div>


<hr>
<div id="paper239"> <b>239. Zero-Shot Visual Imitation</b>  <a href="https://openreview.net/pdf?id=BkisuzWRW" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title239" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICLR 2018. <br>&nbsp;&nbsp;<i>Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A. Efros, Trevor Darrell</i><br>
<font size="3">
The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both 'what' and 'how' to imitate. We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss. In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference. The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task. Our method is 'zero-shot' in the sense that the agent never has access to expert actions during training or for the task demonstration at inference. We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot. Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance. Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/.</font>
<br>
</div>


<hr>
<div id="paper240"> <b>240. Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models</b>  <a href="https://openreview.net/pdf?id=Sy8XvGb0-" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title240" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICLR 2018. <br>&nbsp;&nbsp;<i>Jesse H. Engel, Matthew Hoffman, Adam Roberts</i><br>
<font size="3">
Deep generative neural networks have proven effective at both conditional and unconditional modeling of complex data distributions. Conditional generation enables interactive control, but creating new controls often requires expensive retraining. In this paper, we develop a method to condition generation without retraining the model. By post-hoc learning latent constraints, value functions identify regions in latent space that generate outputs with desired attributes, we can conditionally sample from these regions with gradient-based optimization or amortized actor functions. Combining attribute constraints with a universal “realism” constraint, which enforces similarity to the data distribution, we generate realistic conditional images from an unconditional variational autoencoder. Further, using gradient-based optimization, we demonstrate identity-preserving transformations that make the minimal adjustment in latent space to modify the attributes of an image. Finally, with discrete sequences of musical notes, we demonstrate zero-shot conditional generation, learning latent constraints in the absence of labeled data or a differentiable reward function.</font>
<br>
</div>


<hr>
<div id="paper241"> <b>241. NerveNet: Learning Structured Policy with Graph Neural Networks</b>  <a href="https://openreview.net/pdf?id=S1sqHMZCb" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title241" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICLR 2018. <br>&nbsp;&nbsp;<i>Tingwu Wang, Renjie Liao, Jimmy Ba, Sanja Fidler</i><br>
<font size="3">
We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.</font>
<br>
</div>


<hr>
<div id="paper242"> <b>242. Interactive Grounded Language Acquisition and Generalization in a 2D World</b>  <a href="https://openreview.net/pdf?id=H1UOm4gA-" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title242" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICLR 2018. <br>&nbsp;&nbsp;<i>Haonan Yu, Haichao Zhang, Wei Xu</i><br>
<font size="3">
We build a virtual agent for learning language in a 2D maze-like world. The agent sees images of the surrounding environment, listens to a virtual teacher, and takes actions to receive rewards. It interactively learns the teacher’s language from scratch based on two language use cases: sentence-directed navigation and question answering. It learns simultaneously the visual representations of the world, the language, and the action control. By disentangling language grounding from other computational routines and sharing a concept detection function between language grounding and prediction, the agent reliably interpolates and extrapolates to interpret sentences that contain new word combinations or new words missing from training sentences. The new words are transferred from the answers of language prediction. Such a language ability is trained and evaluated on a population of over 1.6 million distinct sentences consisting of 119 object words, 8 color words, 9 spatial-relation words, and 50 grammatical words. The proposed model significantly outperforms five comparison methods for interpreting zero-shot sentences. In addition, we demonstrate human-interpretable intermediate outputs of the model in the appendix.</font>
<br>
</div>


<hr>
<div id="paper243"> <b>243. Compositional Obverter Communication Learning from Raw Visual Input</b>  <a href="https://openreview.net/pdf?id=rknt2Be0-" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title243" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICLR 2018. <br>&nbsp;&nbsp;<i>Edward Choi, Angeliki Lazaridou, Nando de Freitas</i><br>
<font size="3">
One of the distinguishing aspects of human language is its compositionality, which allows us to describe complex environments with limited vocabulary. Previously, it has been shown that neural network agents can learn to communicate in a highly structured, possibly compositional language based on disentangled input (e.g. hand- engineered features). Humans, however, do not learn to communicate based on well-summarized features. In this work, we train neural agents to simultaneously develop visual perception from raw image pixels, and learn to communicate with a sequence of discrete symbols. The agents play an image description game where the image contains factors such as colors and shapes. We train the agents using the obverter technique where an agent introspects to generate messages that maximize its own understanding. Through qualitative analysis, visualization and a zero-shot test, we show that the agents can develop, out of raw image pixels, a language with compositional properties, given a proper pressure from the environment.</font>
<br>
</div>


<hr>
<div id="paper244"> <b>244. Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks</b>  <a href="http://proceedings.mlr.press/v80/lake18a/lake18a.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title244" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICML 2018. <br>&nbsp;&nbsp;<i>Brenden M. Lake, Marco Baroni</i><br>
<font size="3">
Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks’ notorious training data thirst.</font>
<br>
</div>


<hr>
<div id="paper245"> <b>245. MSplit LBI: Realizing Feature Selection and Dense Estimation Simultaneously in Few-shot and Zero-shot Learning</b>  <a href="http://proceedings.mlr.press/v80/zhao18c/zhao18c.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title245" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICML 2018. <br>&nbsp;&nbsp;<i>Bo Zhao, Xinwei Sun, Yanwei Fu, Yuan Yao, Yizhou Wang</i><br>
<font size="3">
It is one typical and general topic of learning a good embedding model to efficiently learn the representation coefficients between two spaces/subspaces. To solve this task, $L_{1}$ regularization is widely used for the pursuit of feature selection and avoiding overfitting, and yet the sparse estimation of features in $L_{1}$ regularization may cause the underfitting of training data. $L_{2}$ regularization is also frequently used, but it is a biased estimator. In this paper, we propose the idea that the features consist of three orthogonal parts, namely sparse strong signals, dense weak signals and random noise, in which both strong and weak signals contribute to the fitting of data. To facilitate such novel decomposition, MSplit LBI is for the first time proposed to realize feature selection and dense estimation simultaneously. We provide theoretical and simulational verification that our method exceeds $L_{1}$ and $L_{2}$ regularization, and extensive experimental results show that our method achieves state-of-the-art performance in the few-shot and zero-shot learning.</font>
<br>
</div>


<hr>
<div id="paper246"> <b>246. Dual Adversarial Networks for Zero-shot Cross-media Retrieval</b>  <a href="https://www.ijcai.org/proceedings/2018/0092.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title246" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2018. <br>&nbsp;&nbsp;<i>Jingze Chi, Yuxin Peng</i><br>
<font size="3">
Existing cross-media retrieval methods usually require that testing categories remain the same with training categories, which cannot support the retrieval of increasing new categories. Inspired by zero-shot learning, this paper proposes zeroshot cross-media retrieval for addressing the above problem, which aims to retrieve data of new categories across different media types. It is challenging that zero-shot cross-media retrieval has to handle not only the inconsistent semantics across new and known categories, but also the heterogeneous distributions across different media types. To address the above challenges, this paper proposes Dual Adversarial Networks for Zero-shot Crossmedia Retrieval (DANZCR), which is the first approach to address zero-shot cross-media retrieval to the best of our knowledge. Our DANZCR approach consists of two GANs in a dual structure for common representation generation and original representation reconstruction respectively, which capture the underlying data structures as well as strengthen relations between input data and semantic space to generalize across seen and unseen categories. Our DANZCR approach exploits word embeddings to learn common representations in semantic space via an adversarial learning method, which preserves the inherent cross-media correlation and enhances the knowledge transfer to new categories. Experiments on three widely-used cross-media retrieval datasets show the effectiveness of our approach.</font>
<br>
</div>


<hr>
<div id="paper247"> <b>247. Visual Data Synthesis via GAN for Zero-Shot Video Classification</b>  <a href="https://www.ijcai.org/proceedings/2018/0157.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title247" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2018. <br>&nbsp;&nbsp;<i>Chenrui Zhang, Yuxin Peng</i><br>
<font size="3">
Zero-Shot Learning (ZSL) in video classificationis a promising research direction, which aims totackle the challenge from explosive growth of videocategories. Most existing methods exploit seento-unseen correlation via learning a projection betweenvisual and semantic spaces. However, suchprojection-based paradigms cannot fully utilize thediscriminative information implied in data distribution,and commonly suffer from the informationdegradation issue caused by "heterogeneity gap".In this paper, we propose a visual data synthesisframework via GAN to address these problems.Specifically, both semantic knowledge and visualdistribution are leveraged to synthesize video featureof unseen categories, and ZSL can be turnedinto typical supervised problem with the syntheticfeatures. First, we propose multi-level semanticinference to boost video feature synthesis, whichcaptures the discriminative information implied injoint visual-semantic distribution via feature-leveland label-level semantic inference. Second, wepropose Matching-aware Mutual Information Correlationto overcome information degradation issue,which captures seen-to-unseen correlation inmatched and mismatched visual-semantic pairs bymutual information, providing the zero-shot synthesisprocedure with robust guidance signals. Experimentalresults on four video datasets demonstratethat our approach can improve the zero-shotvideo classification performance significantly.</font>
<br>
</div>


<hr>
<div id="paper248"> <b>248. Zero Shot Learning via Low-rank Embedded Semantic AutoEncoder</b>  <a href="https://www.ijcai.org/proceedings/2018/0345.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title248" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2018. <br>&nbsp;&nbsp;<i>Yang Liu, Quanxue Gao, Jin Li, Jungong Han, Ling Shao</i><br>
<font size="3">
Zero-shot learning (ZSL) has been widely researched and get successful in machine learning. Most existing ZSL methods aim to accurately recognize objects of unseen classes by learning a shared mapping from the feature space to a semantic space. However, such methods did not investigate in-depth whether the mapping can precisely reconstruct the original visual feature. Motivated by the fact that the data have low intrinsic dimensionality e.g. low-dimensional subspace. In this paper, we formulate a novel framework named Low-rank Embedded Semantic AutoEncoder (LESAE) to jointly seek a low-rank mapping to link visual features with their semantic representations. Taking the encoder-decoder paradigm, the encoder part aims to learn a low-rank mapping from the visual feature to the semantic space, while decoder part manages to reconstruct the original data with the learned mapping. In addition, a non-greedy iterative algorithm is adopted to solve our model. Extensive experiments on six benchmark datasets demonstrate its superiority over several state-of-the-art algorithms.</font>
<br>
</div>


<hr>
<div id="paper249"> <b>249. Adaptive Graph Guided Embedding for Multi-label Annotation</b>  <a href="https://www.ijcai.org/proceedings/2018/0388.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title249" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2018. <br>&nbsp;&nbsp;<i>Lichen Wang, Zhengming Ding, Yun Fu</i><br>
<font size="3">
Multi-label annotation is challenging since a large amount of well-labeled training data are required to achieve promising performance. However, providing such data is expensive while unlabeled data are widely available. To this end, we propose a novel Adaptive Graph Guided Embedding (AG2E) approach for multi-label annotation in a semi-supervised fashion, which utilizes limited labeled data associating with large-scale unlabeled data to facilitate learning performance. Specifically, a multi-label propagation scheme and an effective embedding are jointly learned to seek a latent space where unlabeled instances tend to be well assigned multiple labels. Furthermore, a locality structure regularizer is designed to preserve the intrinsic structure and enhance the multi-label annotation. We evaluate our model in both conventional multi-label learning and zero-shot learning scenario. Experimental results demonstrate that our approach outperforms other compared state-of-the-art methods.</font>
<br>
</div>


<hr>
<div id="paper250"> <b>250. Co-training Embeddings of Knowledge Graphs and Entity Descriptions for Cross-lingual Entity Alignment</b>  <a href="https://www.ijcai.org/proceedings/2018/0556.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title250" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2018. <br>&nbsp;&nbsp;<i>Muhao Chen, Yingtao Tian, Kai-Wei Chang, Steven Skiena, Carlo Zaniolo</i><br>
<font size="3">
Multilingual knowledge graph (KG) embeddings provide latent semantic representations of entities and structured knowledge with cross-lingual inferences, which benefit various knowledge-driven cross-lingual NLP tasks. However, precisely learning such cross-lingual inferences is usually hindered by the low coverage of entity alignment in many KGs. Since many multilingual KGs also provide literal descriptions of entities, in this paper, we introduce an embedding-based approach which leverages a weakly aligned multilingual KG for semi-supervised cross-lingual learning using entity descriptions. Our approach performs co-training of two embedding models, i.e. a multilingual KG embedding model and a multilingual literal description embedding model. The models are trained on a large Wikipedia-based trilingual dataset where most entity alignment is unknown to training. Experimental results show that the performance of the proposed approach on the entity alignment task improves at each iteration of co-training, and eventually reaches a stage at which it significantly surpasses previous approaches. We also show that our approach has promising abilities for zero-shot entity alignment, and cross-lingual KG completion.</font>
<br>
</div>


<hr>
<div id="paper251"> <b>251. Implicit Non-linear Similarity Scoring for Recognizing Unseen Classes</b>  <a href="https://www.ijcai.org/proceedings/2018/0680.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title251" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2018. <br>&nbsp;&nbsp;<i>Yuchen Guo, Guiguang Ding, Jungong Han, Sicheng Zhao, Bin Wang</i><br>
<font size="3">
Recognizing unseen classes is an important task for real-world applications, due to: 1) it is common that some classes in reality have no labeled image exemplar for training; and 2) novel classes emerge rapidly. Recently, to address this task many zero-shot learning (ZSL) approaches have been proposed where explicit linear scores, like inner product score, are employed to measure the similarity between a class and an image. We argue that explicit linear scoring (ELS) seems too weak to capture complicated image-class correspondence. We propose a simple yet effective framework, called Implicit Non-linear Similarity Scoring (ICINESS). In particular, we train a scoring network which uses image and class features as input, fuses them by hidden layers, and outputs the similarity. Based on the universal approximation theorem, it can approximate the true similarity function between images and classes if a proper structure is used in an implicit non-linear way, which is more flexible and powerful. With ICINESS framework, we implement ZSL algorithms by shallow and deep networks, which yield consistently superior results.</font>
<br>
</div>


<hr>
<div id="paper252"> <b>252. Robust Multi-view Representation: A Unified Perspective from Multi-view Learning to Domain Adaption</b>  <a href="https://www.ijcai.org/proceedings/2018/0767.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title252" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2018. <br>&nbsp;&nbsp;<i>Zhengming Ding, Ming Shao, Yun Fu</i><br>
<font size="3">
Multi-view data are extensively accessible nowadays thanks to various types of features, different view-points and sensors which tend to facilitate better representation in many key applications. This survey covers the topic of robust multi-view data representation, centered around several major visual applications. First of all, we formulate a unified learning framework which is able to model most existing multi-view learning and domain adaptation in this line. Following this, we conduct a comprehensive discussion across these two problems by reviewing the algorithms along these two topics, including multi-view clustering, multi-view classification, zero-shot learning, and domain adaption. We further present more practical challenges in multi-view data analysis. Finally, we discuss future research including incomplete, unbalance, large-scale multi-view learning. This would benefit AI community from literature review to future direction.</font>
<br>
</div>


<hr>
<div id="paper253"> <b>253. Zero-Shot Question Generation from Knowledge Graphs for Unseen Predicates and Entity Types</b>  <a href="https://www.aclweb.org/anthology/N18-1020.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title253" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NAACL 2018. Long Papers<br>&nbsp;&nbsp;<i>Hady Elsahar, Christophe Gravier, Frederique Laforest</i><br>
<font size="3">
We present a neural model for question generation from knowledge graphs triples in a “Zero-shot” setup, that is generating questions for predicate, subject types or object types that were not seen at training time. Our model leverages triples occurrences in the natural language corpus in a encoder-decoder architecture, paired with an original part-of-speech copy action mechanism to generate questions. Benchmark and human evaluation show that our model outperforms state-of-the-art on this task.</font>
<br>
</div>


<hr>
<div id="paper254"> <b>254. Zero-Shot Sequence Labeling: Transferring Knowledge from Sentences to Tokens</b>  <a href="https://www.aclweb.org/anthology/N18-1027.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title254" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NAACL 2018. Long Papers<br>&nbsp;&nbsp;<i>Marek Rei, Anders Søgaard</i><br>
<font size="3">
Can attention- or gradient-based visualization techniques be used to infer token-level labels for binary sequence tagging problems, using networks trained only on sentence-level labels? We construct a neural network architecture based on soft attention, train it as a binary sentence classifier and evaluate against token-level annotation on four different datasets. Inferring token labels from a network provides a method for quantitatively evaluating what the model is learning, along with generating useful feedback in assistance systems. Our results indicate that attention-based methods are able to predict token-level labels more accurately, compared to gradient-based methods, sometimes even rivaling the supervised oracle network.</font>
<br>
</div>


<hr>
<div id="paper255"> <b>255. Universal Neural Machine Translation for Extremely Low Resource Languages</b>  <a href="https://www.aclweb.org/anthology/N18-1032.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title255" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NAACL 2018. Long Papers<br>&nbsp;&nbsp;<i>Jiatao Gu, Hany Hassan, Jacob Devlin, Victor O.K. Li</i><br>
<font size="3">
In this paper, we propose a new universal machine translation approach focusing on languages with a limited amount of parallel data. Our proposed approach utilizes a transfer-learning approach to share lexical and sentence level representations across multiple source languages into one target language. The lexical part is shared through a Universal Lexical Representation to support multi-lingual word-level sharing. The sentence-level sharing is represented by a model of experts from all source languages that share the source encoders with all other languages. This enables the low-resource language to utilize the lexical and sentence representations of the higher resource languages. Our approach is able to achieve 23 BLEU on Romanian-English WMT2016 using a tiny parallel corpus of 6k sentences, compared to the 18 BLEU of strong baseline system which uses multi-lingual training and back-translation. Furthermore, we show that the proposed approach can achieve almost 20 BLEU on the same dataset through fine-tuning a pre-trained multi-lingual system in a zero-shot setting.</font>
<br>
</div>


<hr>
<div id="paper256"> <b>256. Domain-Invariant Projection Learning for Zero-Shot Recognition</b>  <a href="https://papers.nips.cc/paper/7380-domain-invariant-projection-learning-for-zero-shot-recognition.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title256" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NeurIPS 2018. <br>&nbsp;&nbsp;<i>An Zhao, Mingyu Ding, Jiechao Guan, Zhiwu Lu, Tao Xiang, Ji-Rong Wen</i><br>
<font size="3">
Zero-shot learning (ZSL) aims to recognize unseen object classes without any training samples, which can be regarded as a form of transfer learning from seen classes to unseen ones. This is made possible by learning a projection between a feature space and a semantic space (e.g. attribute space). Key to ZSL is thus to learn a projection function that is robust against the often large domain gap between the seen and unseen classes. In this paper, we propose a novel ZSL model termed domain-invariant projection learning (DIPL). Our model has two novel components: (1) A domain-invariant feature self-reconstruction task is introduced to the seen/unseen class data, resulting in a simple linear formulation that casts ZSL into a min-min optimization problem. Solving the problem is non-trivial, and a novel iterative algorithm is formulated as the solver, with rigorous theoretic algorithm analysis provided. (2) To further align the two domains via the learned projection, shared semantic structure among seen and unseen classes is explored via forming superclasses in the semantic space. Extensive experiments show that our model outperforms the state-of-the-art alternatives by significant margins.</font>
<br>
</div>


<hr>
<div id="paper257"> <b>257. Generalized Zero-Shot Learning with Deep Calibration Network</b>  <a href="https://papers.nips.cc/paper/7471-generalized-zero-shot-learning-with-deep-calibration-network.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title257" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NeurIPS 2018. <br>&nbsp;&nbsp;<i>Shichen Liu, Mingsheng Long, Jianmin Wang, Michael I. Jordan</i><br>
<font size="3">
A technical challenge of deep learning is recognizing target classes without seen data. Zero-shot learning leverages semantic representations such as attributes or class prototypes to bridge source and target classes. Existing standard zero-shot learning methods may be prone to overfitting the seen data of source classes as they are blind to the semantic representations of target classes. In this paper, we study generalized zero-shot learning that assumes accessible to target classes for unseen data during training, and prediction on unseen data is made by searching on both source and target classes. We propose a novel Deep Calibration Network (DCN) approach towards this generalized zero-shot learning paradigm, which enables simultaneous calibration of deep networks on the confidence of source classes and uncertainty of target classes. Our approach maps visual features of images and semantic representations of class prototypes to a common embedding space such that the compatibility of seen data to both source and target classes are maximized. We show superior accuracy of our approach over the state of the art on benchmark datasets for generalized zero-shot learning, including AwA, CUB, SUN, and aPY.</font>
<br>
</div>


<hr>
<div id="paper258"> <b>258. Zero-Shot Transfer with Deictic Object-Oriented Representation in Reinforcement Learning</b>  <a href="https://papers.nips.cc/paper/7497-zero-shot-transfer-with-deictic-object-oriented-representation-in-reinforcement-learning.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title258" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NeurIPS 2018. <br>&nbsp;&nbsp;<i>Ofir Marom, Benjamin Rosman</i><br>
<font size="3">
Object-oriented representations in reinforcement learning have shown promise in transfer learning, with previous research introducing a propositional object-oriented framework that has provably efficient learning bounds with respect to sample complexity. However, this framework has limitations in terms of the classes of tasks it can efficiently learn. In this paper we introduce a novel deictic object-oriented framework that has provably efficient learning bounds and can solve a broader range of tasks. Additionally, we show that this framework is capable of zero-shot transfer of transition dynamics across tasks and demonstrate this empirically for the Taxi and Sokoban domains.</font>
<br>
</div>


<hr>
<div id="paper259"> <b>259. Supervising Unsupervised Learning</b>  <a href="https://papers.nips.cc/paper/7747-supervising-unsupervised-learning.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title259" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NeurIPS 2018. <br>&nbsp;&nbsp;<i>Vikas Garg, Adam T. Kalai</i><br>
<font size="3">
We introduce a framework to transfer knowledge acquired from a repository of (heterogeneous) supervised datasets to new unsupervised datasets. Our perspective avoids the subjectivity inherent in unsupervised learning by reducing it to supervised learning, and  provides a principled way to evaluate unsupervised algorithms. We demonstrate the versatility of our framework via rigorous agnostic bounds on a variety of unsupervised problems. In the context of clustering, our approach helps choose the number of clusters and the clustering algorithm,  remove the outliers, and provably circumvent Kleinberg's  impossibility result.  Experiments across hundreds of problems demonstrate improvements in performance on unsupervised data with simple algorithms despite the fact our problems come from heterogeneous domains. Additionally, our framework lets us leverage deep networks to learn common features across many small datasets, and perform zero shot learning.</font>
<br>
</div>


<hr>
<div id="paper260"> <b>260. Stacked Semantics-Guided Attention Model for Fine-Grained Zero-Shot Learning</b>  <a href="https://papers.nips.cc/paper/7839-stacked-semantics-guided-attention-model-for-fine-grained-zero-shot-learning.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title260" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NeurIPS 2018. <br>&nbsp;&nbsp;<i>yunlong yu, Zhong Ji, Yanwei Fu, Jichang Guo, Yanwei Pang, Zhongfei (Mark) Zhang</i><br>
<font size="3">
Zero-Shot Learning (ZSL) is generally achieved via aligning the semantic relationships between the visual features and the corresponding class semantic descriptions. However, using the global features to represent fine-grained images may lead to sub-optimal results since they neglect the discriminative differences of local regions. Besides, different regions contain distinct discriminative information. The important regions should contribute more to the prediction. To this end, we propose a novel stacked semantics-guided attention (S2GA) model to obtain semantic relevant features by using individual class semantic features to progressively guide the visual features to generate an attention map for weighting the importance of different local regions. Feeding both the integrated visual features and the class semantic features into a multi-class classification architecture, the proposed framework can be trained end-to-end. Extensive experimental results on CUB and NABird datasets show that the proposed approach has a consistent improvement on both fine-grained zero-shot classification and retrieval tasks.</font>
<br>
</div>


<hr>
<div id="paper261"> <b>261. Hierarchical Reinforcement Learning for Zero-shot Generalization with Subtask Dependencies</b>  <a href="https://papers.nips.cc/paper/7946-hierarchical-reinforcement-learning-for-zero-shot-generalization-with-subtask-dependencies.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title261" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NeurIPS 2018. <br>&nbsp;&nbsp;<i>Sungryull Sohn, Junhyuk Oh, Honglak Lee</i><br>
<font size="3">
We introduce a new RL problem where the agent is required to generalize to a previously-unseen environment characterized by a subtask graph which describes a set of subtasks and their dependencies. Unlike existing hierarchical multitask RL approaches that explicitly describe what the agent should do at a high level, our problem only describes properties of subtasks and relationships among them, which requires the agent to perform complex reasoning to find the optimal subtask to execute. To solve this problem, we propose a neural subtask graph solver (NSGS) which encodes the subtask graph using a recursive neural network embedding. To overcome the difficulty of training, we propose a novel non-parametric gradient-based policy, graph reward propagation, to pre-train our NSGS agent and further finetune it through actor-critic method. The experimental results on two 2D visual domains show that our agent can perform complex reasoning to find a near-optimal way of executing the subtask graph and generalize well to the unseen subtask graphs. In addition, we compare our agent with a Monte-Carlo tree search (MCTS) method showing that our method is much more efficient than MCTS, and the performance of NSGS can be further improved by combining it with MCTS.</font>
<br>
</div>


<hr>
<div id="paper262"> <b>262. Hardware Conditioned Policies for Multi-Robot Transfer Learning</b>  <a href="https://papers.nips.cc/paper/8145-hardware-conditioned-policies-for-multi-robot-transfer-learning.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title262" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NeurIPS 2018. <br>&nbsp;&nbsp;<i>Tao Chen, Adithyavairavan Murali, Abhinav Gupta</i><br>
<font size="3">
Deep reinforcement learning could be used to learn dexterous robotic policies but it is challenging to transfer them to new robots with vastly different hardware properties. It is also prohibitively expensive to learn a new policy from scratch for each robot hardware due to the high sample complexity of modern state-of-the-art algorithms. We propose a novel approach called Hardware Conditioned Policies where we train a universal policy conditioned on a vector representation of robot hardware. We considered robots in simulation with varied dynamics, kinematic structure, kinematic lengths and degrees-of-freedom. First, we use the kinematic structure directly as the hardware encoding and show great zero-shot transfer to completely novel robots not seen during training. For robots with lower zero-shot success rate, we also demonstrate that fine-tuning the policy network is significantly more sample-efficient than training a model from scratch. In tasks where knowing the agent dynamics is important for success, we learn an embedding for robot hardware and show that policies conditioned on the encoding of hardware tend to generalize and transfer well. Videos of experiments are available at: https://sites.google.com/view/robot-transfer-hcp.</font>
<br>
</div>


<hr>
<div id="paper263"> <b>263. Transfer of Deep Reactive Policies for MDP Planning</b>  <a href="https://papers.nips.cc/paper/8293-transfer-of-deep-reactive-policies-for-mdp-planning.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title263" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NeurIPS 2018. <br>&nbsp;&nbsp;<i>Aniket (Nick) Bajpai, Sankalp Garg, Mausam</i><br>
<font size="3">
Domain-independent probabilistic planners input an MDP description in a factored representation language such as PPDDL or RDDL, and exploit the specifics of the representation for faster planning. Traditional algorithms operate on each problem instance independently, and good methods for transferring experience from policies of other instances of a domain to a new instance do not exist.  Recently, researchers have begun exploring the use of deep reactive policies, trained via deep reinforcement learning (RL), for MDP planning domains. One advantage of deep reactive policies is that they are more amenable to transfer learning.  In this paper, we present the first domain-independent transfer algorithm for MDP planning domains expressed in an RDDL representation. Our architecture exploits the symbolic state configuration and transition function of the domain (available via RDDL) to learn a shared embedding space for states and state-action pairs for all problem instances of a domain. We then learn an RL agent in the embedding space, making a near zero-shot transfer possible, i.e., without much training on the new instance, and without using the domain simulator at all. Experiments on three different benchmark domains underscore the value of our transfer algorithm. Compared against planning from scratch, and a state-of-the-art RL transfer algorithm, our transfer solution has significantly superior learning curves.</font>
<br>
</div>


<hr>
<div id="paper264"> <b>264. Learning Attributes from the Crowdsourced Relative Labels</b>  <a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14260/13951" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title264" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2017. Machine Learning Applications<br>&nbsp;&nbsp;<i>Tian Tian, Ning Chen, Jun Zhu</i><br>
<font size="3">
Finding semantic attributes to describe related concepts is typically a hard problem. The commonly used attributes in most fields are designed by domain experts, which is expensive and time-consuming. In this paper we propose an efficient method to learn human comprehensible attributes with crowdsourcing. We first design an analogical interface to collect relative labels from the crowds. Then we propose a hierarchical Bayesian model, as well as an efficient initialization strategy, to aggregate labels and extract concise attributes. Our experimental results demonstrate promise on discovering diverse and convincing attributes, which significantly improve the performance of the challenging zero-shot learning tasks.</font>
<br>
</div>


<hr>
<div id="paper265"> <b>265. DECK: Discovering Event Composition Knowledge from Web Images for Zero-Shot Event Detection and Recounting in Videos</b>  <a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14624/14277" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title265" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2017. Vision<br>&nbsp;&nbsp;<i>Chuang Gan, Chen Sun, Ram Nevatia</i><br>
<font size="3">
We address the problem of zero-shot event recognition in consumer videos. An event usually consists of multiple human-human and human-object interactions over a relative long period of time. A common approach proceeds by representing videos with banks of object and action concepts, but requires additional user inputs to specify the desired concepts per event. In this paper, we provide a fully automatic algorithm to select representative and reliable concepts for event queries. This is achieved by discovering event composition knowledge (DECK) from web images. To evaluate our proposed method, we use the standard zero-shot event detection protocol (ZeroMED), but also introduce a novel zero-shot event recounting (ZeroMER) problem to select supporting evidence of the events. Our ZeroMER formulation aims to select video snippets that are relevant and diverse. Evaluation on the challenging TRECVID MED dataset show that our proposed method achieves promising results on both tasks.</font>
<br>
</div>


<hr>
<div id="paper266"> <b>266. Zero-Shot Recognition via Direct Classifier Learning with Transferred Samples and Pseudo Labels</b>  <a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14160/14281" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title266" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2017. Vision<br>&nbsp;&nbsp;<i>Yuchen Guo, Guiguang Ding, Jungong Han, Yue Gao</i><br>
<font size="3">
As an interesting and emerging topic, zero-shot recognition (ZSR) makes it possible to train a recognition model by specifying the category's attributes when there are no labeled exemplars available. The fundamental idea for ZSR is to transfer knowledge from the abundant labeled data in different but related source classes via the class attributes. Conventional ZSR approaches adopt a two-step strategy in test stage, where the samples are projected into the attribute space in the first step, and then the recognition is carried out based on considering the relationship between samples and classes in the attribute space. Due to this intermediate transformation, information loss is unavoidable, thus degrading the performance of the overall system. Rather than following this two-step strategy, in this paper, we propose a novel one-step approach that is able to perform ZSR in the original feature space by using directly trained classifiers. To tackle the problem that no labeled samples of target classes are available, we propose to assign pseudo labels to samples based on the reliability and diversity, which in turn will be used to train the classifiers. Moreover,  we adopt a robust SVM that accounts for the unreliability of pseudo labels. Extensive experiments on four datasets demonstrate consistent performance gains of our approach over the state-of-the-art two-step ZSR approaches.</font>
<br>
</div>


<hr>
<div id="paper267"> <b>267. Problems in Large-Scale Image Classification</b>  <a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14167/14007" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title267" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2017. Doctoral Consortium<br>&nbsp;&nbsp;<i>Yuchen Guo</i><br>
<font size="3">
The number of images is growing rapidly in recent years because of development of Internet, especially the social networks like Facebook, and the popularization of portable image capture devices like smart phone. Annotating them with semantically meaningful words to describe them, i.e., classification, is a useful way to manage these images. However, the huge number of images and classes brings several challenges to classification, of which two are 1) how to measure the similarity efficiently between large-scale images, for example, measuring similarity between samples is the building block for SVM and kNN classifiers, and 2) how to train supervised classification models for newly emerging classes with only a few or even no labeled samples because new concepts appear every day in the Web, like Tesla's Model S. The research of my Ph. D. thesis focuses on the two problems in large-scale image classification mentioned above. Formally, these two problems are termed as large-scale similarity search which focuses on the large scale of samples/images and zero-shot/few-shots learning which focuses on the large scale of classes. Specifically, my research considers the following three aspects: 1) hashing based large-scale similarity search which adopts hashing to improve the efficiency; 2) cross-class transfer active learning which simultaneously transfers knowledge from the abundant labeled samples in the Web and selects the most informative samples for expert labeling such that we can construct effective classifiers for novel classes with only a few labeled samples; and 3) zero-shot learning which utilizes no labeled samples for novel classes at all to build supervised classifiers for them by transferring knowledge from the related classes.</font>
<br>
</div>


<hr>
<div id="paper268"> <b>268. Transfer of Knowledge through Collective Learning</b>  <a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14698/14013" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title268" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2017. Doctoral Consortium<br>&nbsp;&nbsp;<i>Mohammad Rostami</i><br>
<font size="3">
Learning fast and efficiently using minimal  data has been consistently a challenge in machine learning. In my thesis,  I explore this problem for  knowledge transfer  for multi-agent multi-task learning  in a life-long learning paradigm. My goal is to demonstrate that by sharing knowledge between  agents and similar tasks, efficient algorithms can be designed that can increase the speed of learning as well as improve performance. Moreover, this would allow for handling hard tasks through collective learning of multiple agents that share knowledge. As an initial step,  I study the problem of incorporating task descriptors into lifelong learning of related tasks to perform zero-shot knowledge transfer. Zero-shot learning is highly desirable because it leads to considerable speedup in handling similar sequential tasks. Then I focus on a multi-agent learning setting, where related tasks are learned collectively and/or address privacy concerns.</font>
<br>
</div>


<hr>
<div id="paper269"> <b>269. Obtaining referential word meanings from visual and distributional information: Experiments on object naming</b>  <a href="https://www.aclweb.org/anthology/P17-1023.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title269" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2017. Long Papers<br>&nbsp;&nbsp;<i>Sina Zarrieß, David Schlangen</i><br>
<font size="3">
We investigate object naming, which is an important sub-task of referring expression generation on real-world images. As opposed to mutually exclusive labels used in object recognition, object names are more flexible, subject to communicative preferences and semantically related to each other. Therefore, we investigate models of referential word meaning that link visual to lexical information which we assume to be given through distributional word embeddings. We present a model that learns individual predictors for object names that link visual and distributional aspects of word meaning during training. We show that this is particularly beneficial for zero-shot learning, as compared to projecting visual objects directly into the distributional space. In a standard object naming task, we find that different ways of combining lexical and visual information achieve very similar performance, though experiments on model combination suggest that they capture complementary aspects of referential meaning.</font>
<br>
</div>


<hr>
<div id="paper270"> <b>270. One-Shot Neural Cross-Lingual Transfer for Paradigm Completion</b>  <a href="https://www.aclweb.org/anthology/P17-1182.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title270" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2017. Long Papers<br>&nbsp;&nbsp;<i>Katharina Kann, Ryan Cotterell, Hinrich Schütze</i><br>
<font size="3">
We present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task. We use labeled data from a high-resource language to increase performance on a low-resource language. In experiments on 21 language pairs from four different language families, we obtain up to 58% higher accuracy than without transfer and show that even zero-shot and one-shot learning are possible. We further find that the degree of language relatedness strongly influences the ability to transfer morphological knowledge.</font>
<br>
</div>


<hr>
<div id="paper271"> <b>271. Zero-Shot Activity Recognition with Verb Attribute Induction</b>  <a href="https://www.aclweb.org/anthology/D17-1099.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title271" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2017. <br>&nbsp;&nbsp;<i>Rowan Zellers, Yejin Choi</i><br>
<font size="3">
In this paper, we investigate large-scale zero-shot activity recognition by modeling the visual and linguistic attributes of action verbs. For example, the verb “salute” has several properties, such as being a light movement, a social act, and short in duration. We use these attributes as the internal mapping between visual and textual representations to reason about a previously unseen action. In contrast to much prior work that assumes access to gold standard attributes for zero-shot classes and focuses primarily on object attributes, our model uniquely learns to infer action attributes from dictionary definitions and distributed word representations. Experimental results confirm that action attributes inferred from language can provide a predictive signal for zero-shot prediction of previously unseen activities.</font>
<br>
</div>


<hr>
<div id="paper272"> <b>272. Knowledge Distillation for Bilingual Dictionary Induction</b>  <a href="https://www.aclweb.org/anthology/D17-1264.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title272" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2017. <br>&nbsp;&nbsp;<i>Ndapandula Nakashole, Raphael Flauger</i><br>
<font size="3">
Leveraging zero-shot learning to learn mapping functions between vector spaces of different languages is a promising approach to bilingual dictionary induction. However, methods using this approach have not yet achieved high accuracy on the task. In this paper, we propose a bridging approach, where our main contribution is a knowledge distillation training objective. As teachers, rich resource translation paths are exploited in this role. And as learners, translation paths involving low resource languages learn from the teachers. Our training objective allows seamless addition of teacher translation paths for any given low resource pair. Since our approach relies on the quality of monolingual word embeddings, we also propose to enhance vector representations of both the source and target language with linguistic information. Our experiments on various languages show large performance gains from our distillation training objective, obtaining as high as 17% accuracy improvements.</font>
<br>
</div>


<hr>
<div id="paper273"> <b>273. DARLA: Improving Zero-Shot Transfer in Reinforcement Learning</b>  <a href="http://proceedings.mlr.press/v70/higgins17a/higgins17a.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title273" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICML 2017. <br>&nbsp;&nbsp;<i>Irina Higgins, Arka Pal, Andrei A. Rusu, Loïc Matthey, Christopher Burgess, Alexander Pritzel, Matthew Botvinick, Charles Blundell, Alexander Lerchner</i><br>
<font size="3">
Domain adaptation is an important open problem in deep reinforcement learning (RL). In many scenarios of interest data is hard to obtain, so agents may learn a source policy in a setting where data is readily available, with the hope that it generalises well to the target domain. We propose a new multi-stage RL agent, DARLA (DisentAngled Representation Learning Agent), which learns to see before learning to act. DARLA’s vision is based on learning a disentangled representation of the observed environment. Once DARLA can see, it is able to acquire source policies that are robust to many domain shifts – even with no access to the target domain. DARLA significantly outperforms conventional baselines in zero-shot domain adaptation scenarios, an effect that holds across a variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms (DQN, A3C and EC).</font>
<br>
</div>


<hr>
<div id="paper274"> <b>274. Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics</b>  <a href="http://proceedings.mlr.press/v70/kansky17a/kansky17a.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title274" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICML 2017. <br>&nbsp;&nbsp;<i>Ken Kansky, Tom Silver, David A. Mély, Mohamed Eldawy, Miguel Lázaro-Gredilla, Xinghua Lou, Nimrod Dorfman, Szymon Sidor, D. Scott Phoenix, Dileep George</i><br>
<font size="3">
The recent adaptation of deep neural network-based methods to reinforcement learning and planning domains has yielded remarkable progress on individual tasks. Nonetheless, progress on task-to-task transfer remains limited. In pursuit of efficient and robust generalization, we introduce the Schema Network, an object-oriented generative physics simulator capable of disentangling multiple causes of events and reasoning backward through causes to achieve goals. The richly structured architecture of the Schema Network can learn the dynamics of an environment directly from data. We compare Schema Networks with Asynchronous Advantage Actor-Critic and Progressive Networks on a suite of Breakout variations, reporting results on training efficiency and zero-shot generalization, consistently demonstrating faster, more robust learning and better transfer. We argue that generalizing from limited data and learning causal relationships are essential abilities on the path toward generally intelligent systems.</font>
<br>
</div>


<hr>
<div id="paper275"> <b>275. Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning</b>  <a href="http://proceedings.mlr.press/v70/oh17a/oh17a.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title275" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICML 2017. <br>&nbsp;&nbsp;<i>Junhyuk Oh, Satinder P. Singh, Honglak Lee, Pushmeet Kohli</i><br>
<font size="3">
As a step towards developing zero-shot task generalization capabilities in reinforcement learning (RL), we introduce a new RL problem where the agent should learn to execute sequences of instructions after learning useful skills that solve subtasks. In this problem, we consider two types of generalizations: to previously unseen instructions and to longer sequences of instructions. For generalization over unseen instructions, we propose a new objective which encourages learning correspondences between similar subtasks by making analogies. For generalization over sequential instructions, we present a hierarchical architecture where a meta controller learns to use the acquired skills for executing the instructions. To deal with delayed reward, we propose a new neural architecture in the meta controller that learns when to update the subtask, which makes learning more efficient. Experimental results on a stochastic 3D domain show that the proposed ideas are crucial for generalization to longer instructions as well as unseen instructions.</font>
<br>
</div>


<hr>
<div id="paper276"> <b>276. SitNet: Discrete Similarity Transfer Network for Zero-shot Hashing</b>  <a href="https://www.ijcai.org/proceedings/2017/0245.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title276" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2017. <br>&nbsp;&nbsp;<i>Yuchen Guo, Guiguang Ding, Jungong Han, Yue Gao</i><br>
<font size="3">
Hashing has been widely utilized for fast image retrieval recently. With semantic information as supervision, hashing approaches perform much better, especially when combined with deep convolution neural network(CNN). However, in practice, new concepts emerge every day, making collecting supervised information for re-training hashing model infeasible. In this paper, we propose a novel zero-shot hashing approach, called Discrete Similarity Transfer Network (SitNet), to preserve the semantic similarity between images from both ``seen'' concepts and new ``unseen'' concepts. Motivated by zero-shot learning, the semantic vectors of concepts are adopted to capture the similarity structures among classes, making the model trained with seen concepts generalize well for unseen ones benefiting from the transferability of the semantic vector space. We adopt a multi-task architecture to exploit the supervised information for seen concepts and the semantic vectors simultaneously. Moreover, a discrete hashing layer is integrated into the network for hashcode generating to avoid the information loss caused by real-value relaxation in training phase, which is a critical problem in existing works. Experiments on three benchmarks validate the superiority of SitNet to the state-of-the-arts.</font>
<br>
</div>


<hr>
<div id="paper277"> <b>277. Synthesizing Samples for Zero-shot Learning</b>  <a href="https://www.ijcai.org/proceedings/2017/0246.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title277" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2017. <br>&nbsp;&nbsp;<i>Yuchen Guo, Guiguang Ding, Jungong Han, Yue Gao</i><br>
<font size="3">
Zero-shot learning (ZSL) is to construct recognition models for unseen target classes that have no labeled samples for training. It utilizes the class attributes or semantic vectors as side information and transfers supervision information from related source classes with abundant labeled samples. Existing ZSL approaches adopt an intermediary embedding space to measure the similarity between a sample and the attributes of a target class to perform zero-shot classification. However, this way may suffer from the information loss caused by the embedding process and the similarity measure cannot fully make use of the data distribution. In this paper, we propose a novel approach which turns the ZSL problem into a conventional supervised learning problem by synthesizing samples for the unseen classes. Firstly, the probability distribution of an unseen class is estimated by using the knowledge from seen classes and the class attributes. Secondly, the samples are synthesized based on the distribution for the unseen class. Finally, we can train any supervised classifiers based on the synthesized samples. Extensive experiments on benchmarks demonstrate the superiority of the proposed approach to the state-of-the-art ZSL approaches.</font>
<br>
</div>


<hr>
<div id="paper278"> <b>278. Ordinal Zero-Shot Learning</b>  <a href="https://www.ijcai.org/proceedings/2017/0266.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title278" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2017. <br>&nbsp;&nbsp;<i>Zeng-Wei Huo, Xin Geng</i><br>
<font size="3">
Zero-shot learning predicts new class even if no training data is available for that class. The solution to conventional zero-shot learning usually depends on side information such as attribute or text corpora. But these side information is not easy to obtain or use. Fortunately in many classification tasks, the class labels are ordered, and therefore closely related to each other. This paper deals with zero-shot learning for ordinal classification. The key idea is using label relevance to expand supervision information from seen labels to unseen labels. The proposed method SIDL generates a supervision intensity distribution (SID) that contains each label's supervision intensity, and then learns a mapping from instance to SID. Experiments on two typical ordinal classification problems, i.e., head pose estimation and age estimation, show that SIDL performs significantly better than the compared regression methods. Furthermore, SIDL appears much more robust against the increase of unseen labels than other compared baselines.</font>
<br>
</div>


<hr>
<div id="paper279"> <b>279. Boosted Zero-Shot Learning with Semantic Correlation Regularization</b>  <a href="https://www.ijcai.org/proceedings/2017/0362.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title279" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2017. <br>&nbsp;&nbsp;<i>Te Pi, Xi Li, Zhongfei (Mark) Zhang</i><br>
<font size="3">
We study zero-shot learning (ZSL) as a transfer learning problem, and focus on the two key aspects of ZSL, model effectiveness and model adaptation. For effective modeling, we adopt the boosting strategy to learn a zero-shot classifier from weak models to a strong model. For adaptable knowledge transfer, we devise a Semantic Correlation Regularization (SCR) approach to regularize the boosted model to be consistent with the inter-class semantic correlations. With SCR embedded in the boosting objective, and with a self-controlled sample selection for learning robustness, we propose a unified framework, Boosted Zero-shot classification with Semantic Correlation Regularization (BZ-SCR). By balancing the SCR-regularized boosted model selection and the self-controlled sample selection, BZ-SCR is capable of capturing both discriminative and adaptable feature-to-class semantic alignments, while ensuring the reliability and adaptability of the learned samples. The experiments on two ZSL datasets show the superiority of BZ-SCR over the state-of-the-arts.</font>
<br>
</div>


<hr>
<div id="paper280"> <b>280. Prototypical Networks for Few-shot Learning</b>  <a href="https://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title280" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NeurIPS 2017. <br>&nbsp;&nbsp;<i>Jake Snell, Kevin Swersky, Richard Zemel</i><br>
<font size="3">
We propose Prototypical Networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical Networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend Prototypical Networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.</font>
<br>
</div>


<hr>
<div id="paper281"> <b>281. Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation</b>  <a href="https://www.aclweb.org/anthology/Q17-1024.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title281" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;TACL 2017. <br>&nbsp;&nbsp;<i>Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, Jeffrey Dean</i><br>
<font size="3">
We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT systems using a single model. On the WMT’14 benchmarks, a single multilingual model achieves comparable performance for English→French and surpasses state-of-theart results for English→German. Similarly, a single multilingual model surpasses state-of-the-art results for French→English and German→English on WMT’14 and WMT’15 benchmarks, respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. Our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and also show some interesting examples when mixing languages.</font>
<br>
</div>


<hr>
<div id="paper282"> <b>282. All-in Text: Learning Document, Label, and Word Representations Jointly</b>  <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12058/11827" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title282" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2016. Technical Papers: Machine Learning Methods<br>&nbsp;&nbsp;<i>Jinseok Nam, Eneldo Loza Mencía, Johannes Fürnkranz</i><br>
<font size="3">
Conventional multi-label classification algorithms treat the target labels of the classification task as mere symbols that are void of an inherent semantics. However, in many cases textual descriptions of these labels are available or can be easily constructed from public document sources such as Wikipedia. In this paper, we investigate an approach for embedding documents and labels into a joint space while sharing word representations between documents and labels. For finding such embeddings, we rely on the text of documents as well as descriptions for the labels. The use of such label descriptions not only lets us expect an increased performance on conventional multi-label text classification tasks, but can also be used to make predictions for labels that have not been seen during the training phase. The potential of our method is demonstrated on the multi-label classification task of assigning keywords from the Medical Subject Headings (MeSH) to publications in biomedical research, both in a conventional and in a zero-shot learning setting.</font>
<br>
</div>


<hr>
<div id="paper283"> <b>283. Relational Knowledge Transfer for Zero-Shot Learning</b>  <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11802/11854" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title283" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2016. Technical Papers: Machine Learning Methods<br>&nbsp;&nbsp;<i>Donghui Wang, Yanan Li, Yuetan Lin, Yueting Zhuang</i><br>
<font size="3">
General zero-shot learning (ZSL) approaches exploit transfer learning via semantic knowledge space. In this paper, we reveal a novel relational knowledge transfer (RKT) mechanism for ZSL, which is simple, generic and effective. RKT resolves the inherent semantic shift problem existing in ZSL through restoring the missing manifold structure of unseen categories via optimizing semantic mapping. It extracts the relational knowledge from data manifold structure in semantic knowledge space based on sparse coding theory. The extracted knowledge is then transferred backwards to generate virtual data for unseen categories in the feature space. On the one hand, the generalizing ability of the semantic mapping function can be enhanced with the added data. On the other hand, the mapping function for unseen categories can be learned directly from only these generated data, achieving inspiring performance. Incorporated with RKT, even simple baseline methods can achieve good results. Extensive experiments on three challenging datasets show prominent performance obtained by RKT, and we obtain 82.43% accuracy on the Animals with Attributes dataset.</font>
<br>
</div>


<hr>
<div id="paper284"> <b>284. Representation Learning of Knowledge Graphs with Entity Descriptions</b>  <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12216/12004" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title284" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2016. Technical Papers: NLP and Knowledge Representation<br>&nbsp;&nbsp;<i>Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, Maosong Sun</i><br>
<font size="3">
Representation learning (RL) of knowledge graphs aims to project both entities and relations into a continuous low-dimensional space. Most methods concentrate on learning representations with knowledge triples indicating relations between entities. In fact, in most knowledge graphs there are usually concise descriptions for entities, which cannot be well utilized by existing methods. In this paper, we propose a novel RL method for knowledge graphs taking advantages of entity descriptions. More specifically, we explore two encoders, including continuous bag-of-words and deep convolutional neural models to encode semantics of entity descriptions. We further learn knowledge representations with both triples and descriptions. We evaluate our method on two tasks, including knowledge graph completion and entity classification. Experimental results on real-world datasets show that, our method outperforms other baselines on the two tasks, especially under the zero-shot setting, which indicates that our method is capable of building representations for novel entities according to their descriptions. The source code of this paper can be obtained from https://github.com/xrb92/DKRL.</font>
<br>
</div>


<hr>
<div id="paper285"> <b>285. Dynamic Concept Composition for Zero-Example Event Detection</b>  <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12499/12116" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title285" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2016. Technical Papers: Vision<br>&nbsp;&nbsp;<i>Xiaojun Chang, Yi Yang, Guodong Long, Chengqi Zhang, Alexander G. Hauptmann</i><br>
<font size="3">
In this paper, we focus on automatically detecting events in unconstrained videos without the use of any visual training exemplars. In principle, zero-shot learning makes it possible to train an event detection model based on the assumption that events (e.g. birthday party) can be described by multiple mid-level semantic concepts (e.g. ``blowing candle'', ``birthday cake''). Towards this goal, we first pre-train a bundle of concept classifiers using data from other sources. Then we evaluate the semantic correlation of each concept w.r.t. the event of interest and pick up the relevant concept classifiers, which are applied on all test videos to get multiple prediction score vectors. While most existing systems combine the predictions of the concept classifiers with fixed weights, we propose to learn the optimal weights of the concept classifiers for each testing video by exploring a set of online available videos with free-form text descriptions of their content. To validate the effectiveness of the proposed approach, we have conducted extensive experiments on the latest TRECVID MEDTest 2014, MEDTest 2013 and CCV dataset. The experimental results confirm the superiority of the proposed approach.</font>
<br>
</div>


<hr>
<div id="paper286"> <b>286. Zero-Shot Event Detection by Multimodal Distributional Semantic Embedding of Videos</b>  <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12165/12118" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title286" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2016. Technical Papers: Vision<br>&nbsp;&nbsp;<i>Mohamed Elhoseiny, Jingen Liu, Hui Cheng, Harpreet S. Sawhney, Ahmed M. Elgammal</i><br>
<font size="3">
We propose a new zero-shot Event-Detection method by Multi-modal Distributional Semantic embedding of videos. Our model embeds object and action concepts as well as other available modalities from videos into a  distributional semantic space. To our knowledge, this is the first Zero-Shot event detection model that is built on top of distributional semantics and extends it in the following directions: (a) semantic embedding of multimodal information in videos (with focus on the visual modalities), (b) semantic embedding of concepts definitions, and (c) retrieve videos by free text event query (e.g., "changing a vehicle tire") based on their content.  We first embed the video into the multi-modal  semantic space and then measure the similarity between videos with the event query in free text form. We validated our method on the large TRECVID MED  (Multimedia Event Detection) challenge.  Using only the event title as a query, our method outperformed the state-the-art that uses big descriptions from 12.6\% to 13.5\% with MAP metric and from 0.73 to 0.83 with ROC-AUC metric. It is also an order of magnitude faster.</font>
<br>
</div>


<hr>
<div id="paper287"> <b>287. Concepts Not Alone: Exploring Pairwise Relationships for Zero-Shot Video Activity Recognition</b>  <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12259/12119" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title287" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2016. Technical Papers: Vision<br>&nbsp;&nbsp;<i>Chuang Gan, Ming Lin, Yi Yang, Gerard de Melo, Alexander G. Hauptmann</i><br>
<font size="3">
Vast quantities of videos are now being captured at astonishing rates, but the majority of these are not labelled.  To cope with such data, we consider the task of content-based activity recognition in videos without any manually labelled examples, also known as zero-shot video recognition. To achieve this, videos are represented in  terms of detected visual concepts, which are then scored as relevant or irrelevant according to their similarity with a given textual query.  In this paper, we propose a more robust approach for scoring concepts in order to alleviate many of the brittleness and low precision problems of previous work. Not only do we jointly consider semantic relatedness, visual reliability, and discriminative power. To handle noise and non-linearities in the ranking scores of the selected concepts, we propose a novel pairwise order matrix approach for score aggregation. Extensive experiments on the large-scale TRECVID Multimedia Event Detection data show the superiority of our approach.</font>
<br>
</div>


<hr>
<div id="paper288"> <b>288. Transductive Zero-Shot Recognition via Shared Model Space Learning</b>  <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11975/12120" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title288" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2016. Technical Papers: Vision<br>&nbsp;&nbsp;<i>Yuchen Guo, Guiguang Ding, Xiaoming Jin, Jianmin Wang</i><br>
<font size="3">
Zero-shot Recognition (ZSR) is to learn recognition models for novel classes without labeled data. It is a challenging task and has drawn considerable attention in recent years. The basic idea is to transfer knowledge from seen classes via the shared attributes. This paper focus on the transductive ZSR, i.e., we have unlabeled data for novel classes. Instead of learning models for seen and novel classes separately as in existing works, we put forward a novel joint learning approach which learns the shared model space (SMS) for models such that the knowledge can be effectively transferred between classes using the attributes. An effective algorithm is proposed for optimization. We conduct comprehensive experiments on three benchmark datasets for ZSR. The results demonstrates that the proposed SMS can significantly outperform the state-of-the-art related approaches which validates its efficacy for the ZSR task.</font>
<br>
</div>


<hr>
<div id="paper289"> <b>289. Exploiting View-Specific Appearance Similarities Across Classes for Zero-Shot Pose Prediction: A Metric Learning Approach</b>  <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12438/12125" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title289" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2016. Technical Papers: Vision<br>&nbsp;&nbsp;<i>Alina Kuznetsova, Sung Ju Hwang, Bodo Rosenhahn, Leonid Sigal</i><br>
<font size="3">
Viewpoint estimation, especially in case of multiple object classes, remains an important and challenging problem. First, objects under different views undergo extreme appearance variations, often making within-class variance larger than between-class variance. Second, obtaining precise ground truth for real-world images, necessary for training supervised viewpoint estimation models, is extremely difficult and time consuming. As a result, annotated data is often available only for a limited number of classes. Hence it is desirable to share viewpoint information across classes. Additional complexity arises from unaligned pose labels between classes, i.e. a side view of a car might look more like a frontal view of a toaster, than its side view. To address these problems, we propose a metric learning approach for joint class prediction and pose estimation. Our approach allows to circumvent the problem of viewpoint alignment across multiple classes, and does not require dense viewpoint labels. Moreover, we show, that the learned metric generalizes to new classes, for which the pose labels are not available, and therefore makes it possible to use only partially annotated training sets, relying on the intrinsic similarities in the viewpoint manifolds. We evaluate our approach on two challenging multi-class datasets, 3DObjects and PASCAL3D+.</font>
<br>
</div>


<hr>
<div id="paper290"> <b>290. Label Embedding for Zero-shot Fine-grained Named Entity Typing</b>  <a href="https://www.aclweb.org/anthology/C16-1017.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title290" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;COLING 2016. <br>&nbsp;&nbsp;<i>Yukun Ma, Erik Cambria, Sa Gao</i><br>
<font size="3">
Named entity typing is the task of detecting the types of a named entity in context. For instance, given “Eric is giving a presentation”, our goal is to infer that ‘Eric’ is a speaker or a presenter and a person. Existing approaches to named entity typing cannot work with a growing type set and fails to recognize entity mentions of unseen types. In this paper, we present a label embedding method that incorporates prototypical and hierarchical information to learn pre-trained label embeddings. In addition, we adapt a zero-shot learning framework that can predict both seen and previously unseen entity types. We perform evaluation on three benchmark datasets with two settings: 1) few-shots recognition where all types are covered by the training set; and 2) zero-shot recognition where fine-grained types are assumed absent from training set. Results show that prior knowledge encoded using our label embedding methods can significantly boost the performance of classification for both cases.</font>
<br>
</div>


<hr>
<div id="paper291"> <b>291. Exploring Distributional Representations and Machine Translation for Aspect-based Cross-lingual Sentiment Classification</b>  <a href="https://www.aclweb.org/anthology/C16-1152.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title291" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;COLING 2016. <br>&nbsp;&nbsp;<i>Jeremy Barnes, Patrik Lambert, Toni Badia</i><br>
<font size="3">
Cross-lingual sentiment classification (CLSC) seeks to use resources from a source language in order to detect sentiment and classify text in a target language. Almost all research into CLSC has been carried out at sentence and document level, although this level of granularity is often less useful. This paper explores methods for performing aspect-based cross-lingual sentiment classification (aspect-based CLSC) for under-resourced languages. Given the limited nature of parallel data for many languages, we would like to make the most of this resource for our task. We compare zero-shot learning, bilingual word embeddings, stacked denoising autoencoder representations and machine translation techniques for aspect-based CLSC. Each of these approaches requires differing amounts of parallel data. We show that models based on distributed semantics can achieve comparable results to machine translation on aspect-based CLSC and give an analysis of the errors found for each method.</font>
<br>
</div>


<hr>
<div id="paper292"> <b>292. Gaussian Visual-Linguistic Embedding for Zero-Shot Recognition</b>  <a href="https://www.aclweb.org/anthology/D16-1089.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title292" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2016. <br>&nbsp;&nbsp;<i>Tanmoy Mukherjee, Timothy Hospedales</i><br>
<font size="3">
 </font>
<br>
</div>


<hr>
<div id="paper293"> <b>293. Using Task Features for Zero-Shot Knowledge Transfer in Lifelong Learning</b>  <a href="https://www.ijcai.org/Proceedings/16/Papers/232.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title293" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2016. <br>&nbsp;&nbsp;<i>David Isele, Mohammad Rostami, Eric Eaton</i><br>
<font size="3">
Knowledge transfer between tasks can improve the performance of learned models, but requires an accurate estimate of the inter-task relationships to identify the relevant knowledge to transfer. These inter-task relationships are typically estimated based on training data for each task, which is inefficient in lifelong learning settings where the goal is to learn each consecutive task rapidly from as little data as possible. To reduce this burden, we develop a lifelong reinforcement learning method based on coupled dictionary learning that incorporates high-level task descriptors to model the inter-task relationships. We show that using task descriptors improves the performance of the learned task policies, providing both theoretical justification for the benefit and empirical demonstration of the improvement across a variety of dynamical control problems. Given only the descriptor for a new task, the lifelong learner is also able to accurately predict the task policy through zero-shot learning using the coupled dictionary, eliminating the need to pause to gather training data before addressing the task.</font>
<br>
</div>


<hr>
<div id="paper294"> <b>294. Unsupervised Learning on Neural Network Outputs: With Application in Zero-Shot Learning</b>  <a href="https://www.ijcai.org/Proceedings/16/Papers/485.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title294" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2016. <br>&nbsp;&nbsp;<i>Yao Lu</i><br>
<font size="3">
The outputs of a trained neural network contain much richer information than just a one-hot classifier. For example, a neural network might give an image of a dog the probability of one in a million of being a cat but it is still much larger than the probability of being a car. To reveal the hidden structure in them, we apply two unsupervised learning algorithms, PCA and ICA, to the outputs of a deep Convolutional Neural Network trained on the ImageNet of 1000 classes. The PCA/ICA embedding of the object classes reveals their visual similarity and the PCA/ICA components can be interpreted as common visual features shared by similar object classes. For an application, we proposed a new zero-shot learning method, in which the visual features learned by PCA/ICA are employed. Our zero-shot learning method achieves the state-of-the-art results on the ImageNet of over 20000 classes.</font>
<br>
</div>


<hr>
<div id="paper295"> <b>295. Local Similarity-Aware Deep Feature Embedding</b>  <a href="https://papers.nips.cc/paper/6368-local-similarity-aware-deep-feature-embedding.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title295" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NeurIPS 2016. <br>&nbsp;&nbsp;<i>Chen Huang, Chen Change Loy, Xiaoou Tang</i><br>
<font size="3">
Existing deep embedding methods in vision tasks are capable of learning a compact Euclidean space from images, where Euclidean distances correspond to a similarity metric. To make learning more effective and efficient, hard sample mining is usually employed, with samples identified through computing the Euclidean feature distance. However, the global Euclidean distance cannot faithfully characterize the true feature similarity in a complex visual feature space, where the intraclass distance in a high-density region may be larger than the interclass distance in low-density regions. In this paper, we introduce a Position-Dependent Deep Metric (PDDM) unit, which is capable of learning a similarity metric adaptive to local feature structure. The metric can be used to select genuinely hard samples in a local neighborhood to guide the deep embedding learning in an online and robust manner. The new layer is appealing in that it is pluggable to any convolutional networks and is trained end-to-end. Our local similarity-aware feature embedding not only demonstrates faster convergence and boosted performance on two complex image retrieval datasets, its large margin nature also leads to superior generalization results under the large and open set scenarios of transfer learning and zero-shot learning on ImageNet 2010 and ImageNet-10K datasets.</font>
<br>
</div>


<hr>
<div id="paper296"> <b>296. Learning Deep Parsimonious Representations</b>  <a href="https://papers.nips.cc/paper/6263-learning-deep-parsimonious-representations.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title296" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NeurIPS 2016. <br>&nbsp;&nbsp;<i>Renjie Liao, Alex Schwing, Richard Zemel, Raquel Urtasun</i><br>
<font size="3">
In this paper we aim at facilitating generalization for deep networks while supporting interpretability of the learned representations. Towards this goal, we propose a clustering based regularization that encourages parsimonious representations. Our k-means style objective is easy to optimize and flexible  supporting various forms of clustering, including sample and spatial clustering as well as co-clustering. We demonstrate the effectiveness of our approach on the tasks of unsupervised learning, classification, fine grained categorization and zero-shot learning.</font>
<br>
</div>


<hr>
<div id="paper297"> <b>297. Exploring Semantic Inter-Class Relationships (SIR) for Zero-Shot Action Recognition</b>  <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9929/9810" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title297" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;AAAI 2015. Vision<br>&nbsp;&nbsp;<i>Chuang Gan, Ming Lin, Yi Yang, Yueting Zhuang, Alexander G. Hauptmann</i><br>
<font size="3">
Automatically recognizing a large number of action categories from videos is of significant importance for video understanding. Most existing works focused on the design of more discriminative feature representation, and have achieved promising results when the positive samples are enough. However, very limited efforts were spent on recognizing a novel action without any positive exemplars, which is often the case in the real settings due to the large amount of action classes and the users' queries dramatic variations. To address this issue, we propose to perform action recognition when no positive exemplars of that class are provided, which is often known as the zero-shot learning. Different from other zero-shot learning approaches, which exploit attributes as the intermediate layer for the knowledge transfer, our main contribution is SIR, which directly leverages the semantic inter-class relationships between the known and unknown actions followed by label transfer learning. The inter-class semantic relationships are automatically measured by continuous word vectors, which learned by the skip-gram model using the large-scale text corpus. Extensive experiments on the UCF101 dataset validate the superiority of our method over fully-supervised approaches using few positive exemplars.</font>
<br>
</div>


<hr>
<div id="paper298"> <b>298. Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning</b>  <a href="https://www.aclweb.org/anthology/P15-1027.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title298" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2015. Long Papers<br>&nbsp;&nbsp;<i>Angeliki Lazaridou, Georgiana Dinu, Marco Baroni</i><br>
<font size="3">
 </font>
<br>
</div>


<hr>
<div id="paper299"> <b>299. Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning</b>  <a href="https://www.aclweb.org/anthology/P15-1027.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title299" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2015. Long Papers<br>&nbsp;&nbsp;<i>Angeliki Lazaridou, Georgiana Dinu, Marco Baroni</i><br>
<font size="3">
 </font>
<br>
</div>


<hr>
<div id="paper300"> <b>300. A Model of Zero-Shot Learning of Spoken Language Understanding</b>  <a href="https://www.aclweb.org/anthology/D15-1027.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title300" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2015. <br>&nbsp;&nbsp;<i>Majid Yazdani, James Henderson</i><br>
<font size="3">
 </font>
<br>
</div>


<hr>
<div id="paper301"> <b>301. Image-Mediated Learning for Zero-Shot Cross-Lingual Document Retrieval</b>  <a href="https://www.aclweb.org/anthology/D15-1070.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title301" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;EMNLP 2015. <br>&nbsp;&nbsp;<i>Ruka Funaki, Hideki Nakayama</i><br>
<font size="3">
 </font>
<br>
</div>


<hr>
<div id="paper302"> <b>302. A Unified Perspective on Multi-Domain and Multi-Task Learning</b>  <a href="https://arxiv.org/pdf/1412.7489.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title302" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICLR 2015. <br>&nbsp;&nbsp;<i>Yongxin Yang, Timothy M. Hospedales</i><br>
<font size="3">
In this paper, we provide a new neural-network based perspective onmulti-task learning (MTL) and multi-domain learning (MDL). By introducing theconcept of a semantic descriptor, this framework unifies MDL and MTL as well asencompassing various classic and recent MTL/MDL algorithms by interpreting themas different ways of constructing semantic descriptors. Our interpretationprovides an alternative pipeline for zero-shot learning (ZSL), where a modelfor a novel class can be constructed without training data. Moreover, it leadsto a new and practically relevant problem setting of zero-shot domainadaptation (ZSDA), which is the analogous to ZSL but for novel domains: A modelfor an unseen domain can be generated by its semantic descriptor. Experimentsacross this range of problems demonstrate that our framework outperforms avariety of alternatives.</font>
<br>
</div>


<hr>
<div id="paper303"> <b>303. An embarrassingly simple approach to zero-shot learning</b>  <a href="http://proceedings.mlr.press/v37/romera-paredes15.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title303" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICML 2015. <br>&nbsp;&nbsp;<i>Bernardino Romera-Paredes, Philip H. S. Torr</i><br>
<font size="3">
Zero-shot learning consists in learning how to recognize new concepts by just having a description of them. Many sophisticated approaches have been proposed to address the challenges this problem comprises. In this paper we describe a zero-shot learning approach that can be implemented in just one line of code, yet it is able to outperform state of the art approaches on standard datasets. The approach is based on a more general framework which models the relationships between features, attributes, and classes as a two linear layers network, where the weights of the top layer are not learned but are given by the environment. We further provide a learning bound on the generalization error of this kind of approaches, by casting them as domain adaptation methods. In experiments carried out on three standard real datasets, we found that our approach is able to perform significantly better than the state of art on all of them, obtaining a ratio of improvement up to 17%.</font>
<br>
</div>


<hr>
<div id="paper304"> <b>304. Semantic Concept Discovery for Large-Scale Zero-Shot Event Detection</b>  <a href="https://www.ijcai.org/Proceedings/15/Papers/316.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title304" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;IJCAI 2015. <br>&nbsp;&nbsp;<i>Xiaojun Chang, Yi Yang, Alexander G. Hauptmann, Eric P. Xing, Yaoliang Yu</i><br>
<font size="3">
https://www.ijcai.org/We focus on detecting complex events in unconstrained Internet videos. While most existing works rely on the abundance of labeled training data, we consider a more difficult zero-shot setting where no training data is supplied. We first pre-train a number of concept classifiers using data from other sources. Then we evaluate the semantic correlation of each concept w.r.t. the event of interest. After further refinement to take prediction inaccuracy and discriminative power into account, we apply the discovered concept classifiers on all test videos and obtain multiple score vectors. These distinct score vectors are converted into pairwise comparison matrices and the nuclear norm rank aggregation framework is adopted to seek consensus. To address the challenging optimization formulation, we propose an efficient, highly scalable algorithm that is an order of magnitude faster than existing alternatives. Experiments on recent TRECVID datasets verify the superiority of the proposed approach. We focus on detecting complex events in unconstrained Internet videos. While most existing works rely on the abundance of labeled training data, we consider a more difficult zero-shot setting where no training data is supplied.We first pre-train a number of concept classifiers using data from other sources. Then we evaluate the semantic correlation of each concept w.r.t. the event of interest. After further refinement to take prediction inaccuracy and discriminative power into account, we apply the discovered concept classifiers on all test videos and obtain multiple score vectors. These distinct score vectors are converted into pairwise comparison matrices and the nuclear norm rank aggregation framework is adopted to seek consensus. To address the challenging optimization formulation, we propose an efficient, highly scalable algorithm that is an order of magnitude faster than existing alternatives. Experiments on recent TRECVID datasets verify the superiority of the proposed approach.</font>
<br>
</div>


<hr>
<div id="paper305"> <b>305. From Visual Attributes to Adjectives through Decompositional Distributional Semantics</b>  <a href="https://www.aclweb.org/anthology/Q15-1014.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title305" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;TACL 2015. <br>&nbsp;&nbsp;<i>Angeliki Lazaridou, Georgiana Dinu, Adam Liska, Marco Baroni</i><br>
<font size="3">
As automated image analysis progresses, there is increasing interest in richer linguistic annotation of pictures, with attributes of objects (e.g., furry, brown…) attracting most attention. By building on the recent “zero-shot learning” approach, and paying attention to the linguistic nature of attributes as noun modifiers, and specifically adjectives, we show that it is possible to tag images with attribute-denoting adjectives even when no training data containing the relevant annotation are available. Our approach relies on two key observations. First, objects can be seen as bundles of attributes, typically expressed as adjectival modifiers (a dog is something furry, brown, etc.), and thus a function trained to map visual representations of objects to nominal labels can implicitly learn to map attributes to adjectives. Second, objects and attributes come together in pictures (the same thing is a dog and it is brown). We can thus achieve better attribute (and object) label retrieval by treating images as “visual phrases”, and decomposing their linguistic representation into an attribute-denoting adjective and an object-denoting noun. Our approach performs comparably to a method exploiting manual attribute annotation, it out-performs various competitive alternatives in both attribute and object annotation, and it automatically constructs attribute-centric representations that significantly improve performance in supervised object recognition.</font>
<br>
</div>


<hr>
<div id="paper306"> <b>306. Zero-shot Entity Extraction from Web Pages</b>  <a href="https://www.aclweb.org/anthology/P14-1037.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title306" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2014. Long Papers<br>&nbsp;&nbsp;<i>Panupong Pasupat, Percy Liang</i><br>
<font size="3">
 </font>
<br>
</div>


<hr>
<div id="paper307"> <b>307. Zero-Shot Learning by Convex Combination of Semantic Embeddings</b>  <a href="https://arxiv.org/pdf/1312.5650.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title307" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICLR 2014. <br>&nbsp;&nbsp;<i>Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram Singer, Jonathon Shlens, Andrea Frome, Greg Corrado, Jeffrey Dean</i><br>
<font size="3">
Several recent publications have proposed methods for mapping images intocontinuous semantic embedding spaces. In some cases the embedding space istrained jointly with the image transformation. In other cases the semanticembedding space is established by an independent natural language processingtask, and then the image transformation into that space is learned in a secondstage. Proponents of these image embedding systems have stressed theiradvantages over the traditional \nway{} classification framing of imageunderstanding, particularly in terms of the promise for zero-shot learning --the ability to correctly annotate images of previously unseen objectcategories. In this paper, we propose a simple method for constructing an imageembedding system from any existing \nway{} image classifier and a semantic wordembedding model, which contains the $\n$ class labels in its vocabulary. Ourmethod maps images into the semantic embedding space via convex combination ofthe class label embedding vectors, and requires no additional training. We showthat this simple and direct method confers many of the advantages associatedwith more complex image embedding schemes, and indeed outperforms state of theart methods on the ImageNet zero-shot learning task.</font>
<br>
</div>


<hr>
<div id="paper308"> <b>308. Zero-Shot Learning and Clustering for Semantic Utterance Classification</b>  <a href="https://arxiv.org/pdf/1401.0509.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title308" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ICLR 2014. <br>&nbsp;&nbsp;<i>Yann N. Dauphin, Gökhan Tür, Dilek Hakkani-Tür, Larry P. Heck</i><br>
<font size="3">
We propose a novel zero-shot learning method for semantic utteranceclassification (SUC). It learns a classifier $f: X \to Y$ for problems wherenone of the semantic categories $Y$ are present in the training set. Theframework uncovers the link between categories and utterances using a semanticspace. We show that this semantic space can be learned by deep neural networkstrained on large amounts of search engine query log data. More precisely, wepropose a novel method that can learn discriminative semantic features withoutsupervision. It uses the zero-shot learning framework to guide the learning ofthe semantic features. We demonstrate the effectiveness of the zero-shotsemantic learning algorithm on the SUC dataset collected by (Tur, 2012).Furthermore, we achieve state-of-the-art results by combining the semanticfeatures with a supervised method.</font>
<br>
</div>


<hr>
<div id="paper309"> <b>309. Zero-shot recognition with unreliable attributes</b>  <a href="https://papers.nips.cc/paper/5290-zero-shot-recognition-with-unreliable-attributes.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title309" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;NeurIPS 2014. <br>&nbsp;&nbsp;<i>Dinesh Jayaraman, Kristen Grauman</i><br>
<font size="3">
In principle, zero-shot learning makes it possible to train an object recognition model simply by specifying the category's attributes. For example, with classifiers for generic attributes like striped and four-legged, one can construct a classifier for the zebra category by enumerating which properties it possesses --- even without providing zebra training images. In practice, however, the standard zero-shot paradigm suffers because attribute predictions in novel images are hard to get right. We propose a novel random forest approach to train zero-shot models that explicitly accounts for the unreliability of attribute predictions. By leveraging statistics about each attribute’s error tendencies, our method obtains more robust discriminative models for the unseen classes. We further devise extensions to handle the few-shot scenario and unreliable attribute descriptions. On three datasets, we demonstrate the benefit for visual category learning with zero or few training examples, a critical domain for rare categories or categories defined on the fly.</font>
<br>
</div>


<hr>
<p><font style="color:red;">注：论文列表使用<a href="https://zhuanlan.zhihu.com/p/282844968" target="_blank" rel="noopener">AC论文搜索器</a>整理！</font></p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>PROCJX
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://procjx.github.io/2020/12/09/%E3%80%90NLP%E3%80%912014-2020%20Zero-Shot%20%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E6%95%B4%E7%90%86/" title="【NLP】 2014-2020 Zero-Shot 相关论文整理">https://procjx.github.io/2020/12/09/%E3%80%90NLP%E3%80%912014-2020%20Zero-Shot%20%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E6%95%B4%E7%90%86/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">

        
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2020/12/09/%E3%80%90NLP%E3%80%912015-2020%20Knowledge%20Distillation%20%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E6%95%B4%E7%90%86/" rel="next" title="【NLP】 2015-2020 Knowledge Distillation 知识蒸馏相关论文整理">
                  <i class="fa fa-chevron-left"></i> 【NLP】 2015-2020 Knowledge Distillation 知识蒸馏相关论文整理
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/2020/12/09/%E3%80%90NLP%E3%80%91%202014-2020%20Entity%20Linking%20%E5%AE%9E%E4%BD%93%E9%93%BE%E6%8E%A5%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E6%95%B4%E7%90%86/" rel="prev" title="【NLP】 2014-2020 Entity Linking 实体链接相关论文整理">
                  【NLP】 2014-2020 Entity Linking 实体链接相关论文整理 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>

        
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang -->
<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9197824246"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="gitalk-container"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#目录"><span class="nav-number">1.</span> <span class="nav-text">目录</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#摘要"><span class="nav-number">2.</span> <span class="nav-text">摘要</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="PROCJX"
    src="/images/procjx.png">
  <p class="site-author-name" itemprop="name">PROCJX</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">422</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">71</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/procjx" title="GitHub &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;procjx" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:procjx@gmail.com" title="E-Mail &amp;rarr; mailto:procjx@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>


<!--
      
        <script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
        <script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
        <div class="widget-wrap">
            <h3 class="widget-title">标签云</h3>
            <div id="myCanvasContainer" class="widget tagcloud">
                <canvas width="250" height="250" id="resCanvas" style="width=100%">
                    <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AAAI/" rel="tag">AAAI</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ACL/" rel="tag">ACL</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Accepted-Papers/" rel="tag">Accepted Papers</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ArXiv/" rel="tag">ArXiv</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/" rel="tag">BERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS20SI/" rel="tag">CS20SI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS224d/" rel="tag">CS224d</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/" rel="tag">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CVPR/" rel="tag">CVPR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Context/" rel="tag">Context</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cross-Lingual/" rel="tag">Cross Lingual</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dialog-System/" rel="tag">Dialog System</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse/" rel="tag">Discourse</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Ranking/" rel="tag">Discourse Ranking</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Structure/" rel="tag">Discourse Structure</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Document-NMT/" rel="tag">Document NMT</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EMNLP/" rel="tag">EMNLP</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Extractive/" rel="tag">Extractive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICLR/" rel="tag">ICLR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICML/" rel="tag">ICML</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IJCAI/" rel="tag">IJCAI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Inter-Sentence/" rel="tag">Inter-Sentence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Keyphrase-Generation/" rel="tag">Keyphrase Generation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NAACL/" rel="tag">NAACL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIPS/" rel="tag">NIPS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NMT/" rel="tag">NMT</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Relation-Extraction/" rel="tag">Neural Relation Extraction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RST/" rel="tag">RST</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Relation-Constraints/" rel="tag">Relation Constraints</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Summarization/" rel="tag">Summarization</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Translation/" rel="tag">Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Word-Translation/" rel="tag">Word Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/alias/" rel="tag">alias</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/git/" rel="tag">git</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pip/" rel="tag">pip</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/screen/" rel="tag">screen</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/" rel="tag">shell</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tgz/" rel="tag">tgz</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tts/" rel="tag">tts</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1/" rel="tag">冒泡</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/" rel="tag">冒泡排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%99%E4%BD%9C%E5%8A%A9%E6%89%8B/" rel="tag">写作助手</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8E%8B%E7%BC%A9/" rel="tag">压缩</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/" rel="tag">发送邮件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%88%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">合并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%8E%E5%8F%B0/" rel="tag">后台</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/" rel="tag">基数排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F/" rel="tag">希尔排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6/" rel="tag">归并</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">归并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/" rel="tag">快速排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F/" rel="tag">批量</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4/" rel="tag">批量删除</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F/" rel="tag">排序</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5/" rel="tag">插入</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/" rel="tag">插入排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%99%E7%A8%8B/" rel="tag">教程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/" rel="tag">斐波那契数列</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9D%80%E6%AD%BB%E8%BF%9B%E7%A8%8B/" rel="tag">杀死进程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B1%89%E8%AF%BA%E5%A1%94/" rel="tag">汉诺塔</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%A7%A3%E5%8E%8B/" rel="tag">解压</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%B0%B7%E6%AD%8C%E7%BF%BB%E8%AF%91/" rel="tag">谷歌翻译</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BF%AD%E4%BB%A3%E5%9B%9E%E7%BF%BB/" rel="tag">迭代回翻</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9/" rel="tag">选择</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/" rel="tag">选择排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%99%84%E4%BB%B6/" rel="tag">附件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3/" rel="tag">非监督</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%A2%86%E5%9F%9F%E9%80%82%E5%BA%94/" rel="tag">领域适应</a><span class="tag-list-count">1</span></li></ul>
                </canvas>
            </div>
        </div>
        
-->
        
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-hengfu -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9879871597"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-chuizhi -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="1662238719"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-zhengfangxing -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="6699421902"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PROCJX</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.4.2
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>












        
      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  <script src="/js/local-search.js"></script>













  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '2286ab64f5194d9d79ce',
      clientSecret: 'f912492bec2391664b40478f50f2f943376768d6',
      repo: 'procjx.github.io',
      owner: 'procjx',
      admin: ['procjx'],
      id: 'da381a0b650ec18dea6f77b5503c268a',
        language: 'zh-CN',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
