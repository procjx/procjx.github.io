<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/procjx.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/procjxfavicon32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/procjxfavicon16x16.ico">
  <link rel="mask-icon" href="/images/procjx.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.4.2',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

<!-- Google Adsense -->
<!--
<script async src="//pagead2.googlesyndication.com/
pagead/js/adsbygoogle.js"></script>
<script>
(adsbygoogle = window.adsbygoogle || []).push({
google_ad_client: "pub-1179774715076800",
enable_page_level_ads: true
});
</script>
-->

<script data-ad-client="ca-pub-1179774715076800" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>


<meta name="google-site-verification" content="cEiGwg0T8Rj5msmuEcGYZTh5nnf05EhCXy0gp2Ml5BI" />
<meta name="baidu-site-verification" content="noSKHe8MJs" />

  <meta name="description" content="目录  1. Content Word Aware Neural Machine Translation, ACL 2020 [PDF] 摘要  2. Evaluating Explanation Methods for Neural Machine Translation, ACL 2020 [PDF] 摘要  3. Jointly Masked Sequence-to-Sequence Mod">
<meta property="og:type" content="article">
<meta property="og:title" content="【NLP】 ACL 2020 Neural Machine Translation 机器翻译相关论文整理">
<meta property="og:url" content="https:&#x2F;&#x2F;procjx.github.io&#x2F;2020&#x2F;12&#x2F;09&#x2F;%E3%80%90NLP%E3%80%91%20ACL%202020%20Neural%20Machine%20Translation%20%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E6%95%B4%E7%90%86&#x2F;index.html">
<meta property="og:site_name" content="PROCJX&#39;s BLOGS">
<meta property="og:description" content="目录  1. Content Word Aware Neural Machine Translation, ACL 2020 [PDF] 摘要  2. Evaluating Explanation Methods for Neural Machine Translation, ACL 2020 [PDF] 摘要  3. Jointly Masked Sequence-to-Sequence Mod">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2020-12-09T14:26:13.472Z">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://procjx.github.io/2020/12/09/%E3%80%90NLP%E3%80%91%20ACL%202020%20Neural%20Machine%20Translation%20%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E6%95%B4%E7%90%86/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>【NLP】 ACL 2020 Neural Machine Translation 机器翻译相关论文整理 | PROCJX's BLOGS</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">PROCJX's BLOGS</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">WITH LOVE OF WORLD</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-fw fa-download"></i>资源</a>

  </li>
        <li class="menu-item menu-item-arxiv">

    <a href="/arxiv/" rel="section"><i class="fa fa-fw fa-file-pdf-o"></i>arxiv论文</a>

  </li>
        <li class="menu-item menu-item-deadline">

    <a href="/deadline/" rel="section"><i class="fa fa-fw fa-calendar"></i>会议截稿</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://procjx.github.io/2020/12/09/%E3%80%90NLP%E3%80%91%20ACL%202020%20Neural%20Machine%20Translation%20%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E6%95%B4%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/procjx.png">
      <meta itemprop="name" content="PROCJX">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PROCJX's BLOGS">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          【NLP】 ACL 2020 Neural Machine Translation 机器翻译相关论文整理
        </h2>

        <div class="post-meta">
        
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-12-09 22:25:42 / 修改时间：22:26:13" itemprop="dateCreated datePublished" datetime="2020-12-09T22:25:42+08:00">2020-12-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AC%E8%AE%BA%E6%96%87/" itemprop="url" rel="index">
                    <span itemprop="name">AC论文</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AC%E8%AE%BA%E6%96%87/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
              <span>139k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
              <span>3:52</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Content Word Aware Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.34.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Evaluating Explanation Methods for Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.35.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.36.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Learning Source Phrase Representations for Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.37.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Multiscale Collaborative Deep Models for Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.40.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Norm-Based Curriculum Learning for Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.41.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Opportunistic Decoding with Timely Correction for Simultaneous Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.42.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Multi-Hypothesis Machine Translation Evaluation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.113.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Multimodal Quality Estimation for Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.114.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Attend, Translate and Summarize: An Efficient Method for Neural Cross-Lingual Summarization, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.121.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Bilingual Dictionary Based Neural Machine Translation without Using Parallel Sentences, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.143.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Boosting Neural Machine Translation with Similar Translations, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.144.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Character-Level Translation with Self-attention, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.145.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Enhancing Machine Translation with Dependency-Aware Self-Attention, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.147.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.148.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> It’s Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.149.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Language-aware Interlingua for Multilingual Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.150.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.151.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> “You Sound Just Like Your Father” Commercial Machine Translation Systems Include Stylistic Biases, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.154.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> MMPE: A Multi-Modal Interface for Post-Editing Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.155.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.165.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Improving Non-autoregressive Neural Machine Translation with Monolingual Data, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.171.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Phone Features Improve Speech Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.217.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.251.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.252.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> On The Evaluation of Machine Translation Systems Trained With Back-Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.253.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Simultaneous Translation Policies: From Fixed to Adaptive, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.254.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.273.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.275.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.277.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> On the Inference Calibration of Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.278.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> A Reinforced Generation of Adversarial Examples for Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.319.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> A Retrieve-and-Rewrite Initialization Method for Unsupervised Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.320.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
<div id="title34">
<b>34.</b> A Simple and Effective Unified Encoder for Document-Level Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.321.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper34" style="color:#0000EE;">摘要</a><br></div>
<div id="title35">
<b>35.</b> Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.322.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper35" style="color:#0000EE;">摘要</a><br></div>
<div id="title36">
<b>36.</b> Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.324.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper36" style="color:#0000EE;">摘要</a><br></div>
<div id="title37">
<b>37.</b> Lexically Constrained Neural Machine Translation with Levenshtein Transformer, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.325.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper37" style="color:#0000EE;">摘要</a><br></div>
<div id="title38">
<b>38.</b> On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.326.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper38" style="color:#0000EE;">摘要</a><br></div>
<div id="title39">
<b>39.</b> Automatic Machine Translation Evaluation using Source Language Inputs and Cross-lingual Language Model, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.327.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper39" style="color:#0000EE;">摘要</a><br></div>
<div id="title40">
<b>40.</b> Curriculum Pre-training for End-to-End Speech Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.344.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper40" style="color:#0000EE;">摘要</a><br></div>
<div id="title41">
<b>41.</b> SimulSpeech: End-to-End Simultaneous Speech to Text Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.350.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper41" style="color:#0000EE;">摘要</a><br></div>
<div id="title42">
<b>42.</b> Selecting Backtranslated Data from Multiple Sources for Improved Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.359.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper42" style="color:#0000EE;">摘要</a><br></div>
<div id="title43">
<b>43.</b> Modeling Word Formation in English–German Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.389.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper43" style="color:#0000EE;">摘要</a><br></div>
<div id="title44">
<b>44.</b> Multimodal Transformer for Multimodal Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.400.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper44" style="color:#0000EE;">摘要</a><br></div>
<div id="title45">
<b>45.</b> Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.448.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper45" style="color:#0000EE;">摘要</a><br></div>
<div id="title46">
<b>46.</b> AdvAug: Robust Adversarial Augmentation for Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.529.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper46" style="color:#0000EE;">摘要</a><br></div>
<div id="title47">
<b>47.</b> Contextual Neural Machine Translation Improves Translation of Cataphoric Pronouns, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.530.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper47" style="color:#0000EE;">摘要</a><br></div>
<div id="title48">
<b>48.</b> Improving Neural Machine Translation with Soft Template Prediction, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.531.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper48" style="color:#0000EE;">摘要</a><br></div>
<div id="title49">
<b>49.</b> Tagged Back-translation Revisited: Why Does It Really Work?, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.532.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper49" style="color:#0000EE;">摘要</a><br></div>
<div id="title50">
<b>50.</b> Worse WER, but Better BLEU? Leveraging Word Embedding as Intermediate in Multitask End-to-End Speech Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.533.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper50" style="color:#0000EE;">摘要</a><br></div>
<div id="title51">
<b>51.</b> Are we Estimating or Guesstimating Translation Quality?, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.558.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper51" style="color:#0000EE;">摘要</a><br></div>
<div id="title52">
<b>52.</b> Document Translation vs. Query Translation for Cross-Lingual Information Retrieval in the Medical Domain, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.613.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper52" style="color:#0000EE;">摘要</a><br></div>
<div id="title53">
<b>53.</b> Gender in Danger? Evaluating Speech Translation Technology on the MuST-SHE Corpus, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.619.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper53" style="color:#0000EE;">摘要</a><br></div>
<div id="title54">
<b>54.</b> Uncertainty-Aware Curriculum Learning for Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.620.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper54" style="color:#0000EE;">摘要</a><br></div>
<div id="title55">
<b>55.</b> Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.627.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper55" style="color:#0000EE;">摘要</a><br></div>
<div id="title56">
<b>56.</b> Speech Translation and the End-to-End Promise: Taking Stock of Where We Are, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.661.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper56" style="color:#0000EE;">摘要</a><br></div>
<div id="title57">
<b>57.</b> Hard-Coded Gaussian Attention for Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.687.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper57" style="color:#0000EE;">摘要</a><br></div>
<div id="title58">
<b>58.</b> In Neural Machine Translation, What Does Transfer Learning Transfer?, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.688.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper58" style="color:#0000EE;">摘要</a><br></div>
<div id="title59">
<b>59.</b> Learning a Multi-Domain Curriculum for Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.689.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper59" style="color:#0000EE;">摘要</a><br></div>
<div id="title60">
<b>60.</b> Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.690.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper60" style="color:#0000EE;">摘要</a><br></div>
<div id="title61">
<b>61.</b> Translationese as a Language in “Multilingual” NMT, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.691.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper61" style="color:#0000EE;">摘要</a><br></div>
<div id="title62">
<b>62.</b> Using Context in Neural Machine Translation Training Objectives, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.693.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper62" style="color:#0000EE;">摘要</a><br></div>
<div id="title63">
<b>63.</b> Variational Neural Machine Translation with Normalizing Flows, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.694.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper63" style="color:#0000EE;">摘要</a><br></div>
<div id="title64">
<b>64.</b> BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.703.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper64" style="color:#0000EE;">摘要</a><br></div>
<div id="title65">
<b>65.</b> Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.731.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper65" style="color:#0000EE;">摘要</a><br></div>
<div id="title66">
<b>66.</b> Addressing Posterior Collapse with Mutual Information for Improved Variational Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.753.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper66" style="color:#0000EE;">摘要</a><br></div>
<div id="title67">
<b>67.</b> Balancing Training for Multilingual Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.754.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper67" style="color:#0000EE;">摘要</a><br></div>
<div id="title68">
<b>68.</b> Evaluating Robustness to Input Perturbations for Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.755.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper68" style="color:#0000EE;">摘要</a><br></div>
<div id="title69">
<b>69.</b> Regularized Context Gates on Transformer for Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-main.757.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper69" style="color:#0000EE;">摘要</a><br></div>
<div id="title70">
<b>70.</b> CLIReval: Evaluating Machine Translation as a Cross-Lingual Information Retrieval Task, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-demos.18.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper70" style="color:#0000EE;">摘要</a><br></div>
<div id="title71">
<b>71.</b> ESPnet-ST: All-in-One Speech Translation Toolkit, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-demos.34.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper71" style="color:#0000EE;">摘要</a><br></div>
<div id="title72">
<b>72.</b> MMPE: A Multi-Modal Interface using Handwriting, Touch Reordering, and Speech Commands for Post-Editing Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-demos.37.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper72" style="color:#0000EE;">摘要</a><br></div>
<div id="title73">
<b>73.</b> Zero-shot North Korean to English Neural Machine Translation by Character Tokenization and Phoneme Decomposition, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-srw.11.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper73" style="color:#0000EE;">摘要</a><br></div>
<div id="title74">
<b>74.</b> Multi-Task Neural Model for Agglutinative Language Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-srw.15.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper74" style="color:#0000EE;">摘要</a><br></div>
<div id="title75">
<b>75.</b> Efficient Neural Machine Translation for Low-Resource Languages via Exploiting Related Languages, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-srw.22.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper75" style="color:#0000EE;">摘要</a><br></div>
<div id="title76">
<b>76.</b> Pre-training via Leveraging Assisting Languages for Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-srw.37.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper76" style="color:#0000EE;">摘要</a><br></div>
<div id="title77">
<b>77.</b> Checkpoint Reranking: An Approach to Select Better Hypothesis for Neural Machine Translation Systems, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-srw.38.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper77" style="color:#0000EE;">摘要</a><br></div>
<div id="title78">
<b>78.</b> Compositional Generalization by Factorizing Alignment and Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.acl-srw.42.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper78" style="color:#0000EE;">摘要</a><br></div>
<div id="title79">
<b>79.</b> Proceedings of the First Workshop on Automatic Simultaneous Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.autosimtrans-1.0.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper79" style="color:#0000EE;">摘要</a><br></div>
<div id="title80">
<b>80.</b> Dynamic Sentence Boundary Detection for Simultaneous Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.autosimtrans-1.1.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper80" style="color:#0000EE;">摘要</a><br></div>
<div id="title81">
<b>81.</b> End-to-End Speech Translation with Adversarial Training, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.autosimtrans-1.2.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper81" style="color:#0000EE;">摘要</a><br></div>
<div id="title82">
<b>82.</b> Robust Neural Machine Translation with ASR Errors, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.autosimtrans-1.3.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper82" style="color:#0000EE;">摘要</a><br></div>
<div id="title83">
<b>83.</b> Modeling Discourse Structure for Document-level Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.autosimtrans-1.5.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper83" style="color:#0000EE;">摘要</a><br></div>
<div id="title84">
<b>84.</b> Proceedings of the 17th International Conference on Spoken Language Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.0.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper84" style="color:#0000EE;">摘要</a><br></div>
<div id="title85">
<b>85.</b> ON-TRAC Consortium for End-to-End and Simultaneous Speech Translation Challenge Tasks at IWSLT 2020, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.2.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper85" style="color:#0000EE;">摘要</a><br></div>
<div id="title86">
<b>86.</b> Start-Before-End and End-to-End: Neural Speech Translation by AppTek and RWTH Aachen University, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.3.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper86" style="color:#0000EE;">摘要</a><br></div>
<div id="title87">
<b>87.</b> KIT’s IWSLT 2020 SLT Translation System, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.4.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper87" style="color:#0000EE;">摘要</a><br></div>
<div id="title88">
<b>88.</b> End-to-End Simultaneous Translation System for IWSLT2020 Using Modality Agnostic Meta-Learning, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.5.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper88" style="color:#0000EE;">摘要</a><br></div>
<div id="title89">
<b>89.</b> DiDi Labs’ End-to-end System for the IWSLT 2020 Offline Speech TranslationTask, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.6.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper89" style="color:#0000EE;">摘要</a><br></div>
<div id="title90">
<b>90.</b> End-to-End Offline Speech Translation System for IWSLT 2020 using Modality Agnostic Meta-Learning, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.7.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper90" style="color:#0000EE;">摘要</a><br></div>
<div id="title91">
<b>91.</b> End-to-End Speech-Translation with Knowledge Distillation: FBK@IWSLT2020, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.8.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper91" style="color:#0000EE;">摘要</a><br></div>
<div id="title92">
<b>92.</b> SRPOL’s System for the IWSLT 2020 End-to-End Speech Translation Task, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.9.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper92" style="color:#0000EE;">摘要</a><br></div>
<div id="title93">
<b>93.</b> The University of Helsinki Submission to the IWSLT2020 Offline SpeechTranslation Task, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.10.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper93" style="color:#0000EE;">摘要</a><br></div>
<div id="title94">
<b>94.</b> LIT Team’s System Description for Japanese-Chinese Machine Translation Task in IWSLT 2020, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.12.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper94" style="color:#0000EE;">摘要</a><br></div>
<div id="title95">
<b>95.</b> OPPO’s Machine Translation System for the IWSLT 2020 Open Domain Translation Task, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.13.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper95" style="color:#0000EE;">摘要</a><br></div>
<div id="title96">
<b>96.</b> Character Mapping and Ad-hoc Adaptation: Edinburgh’s IWSLT 2020 Open Domain Translation System, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.14.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper96" style="color:#0000EE;">摘要</a><br></div>
<div id="title97">
<b>97.</b> CASIA’s System for IWSLT 2020 Open Domain Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.15.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper97" style="color:#0000EE;">摘要</a><br></div>
<div id="title98">
<b>98.</b> Deep Blue Sonics’ Submission to IWSLT 2020 Open Domain Translation Task, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.16.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper98" style="color:#0000EE;">摘要</a><br></div>
<div id="title99">
<b>99.</b> University of Tsukuba’s Machine Translation System for IWSLT20 Open Domain Translation Task, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.17.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper99" style="color:#0000EE;">摘要</a><br></div>
<div id="title100">
<b>100.</b> Xiaomi’s Submissions for IWSLT 2020 Open Domain Translation Task, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.18.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper100" style="color:#0000EE;">摘要</a><br></div>
<div id="title101">
<b>101.</b> ISTIC’s Neural Machine Translation System for IWSLT’2020, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.19.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper101" style="color:#0000EE;">摘要</a><br></div>
<div id="title102">
<b>102.</b> Octanove Labs’ Japanese-Chinese Open Domain Translation System, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.20.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper102" style="color:#0000EE;">摘要</a><br></div>
<div id="title103">
<b>103.</b> NAIST’s Machine Translation Systems for IWSLT 2020 Conversational Speech Translation Task, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.21.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper103" style="color:#0000EE;">摘要</a><br></div>
<div id="title104">
<b>104.</b> Generating Fluent Translations from Disfluent Text Without Access to Fluent References: IIT Bombay@IWSLT2020, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.22.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper104" style="color:#0000EE;">摘要</a><br></div>
<div id="title105">
<b>105.</b> The HW-TSC Video Speech Translation System at IWSLT 2020, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.23.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper105" style="color:#0000EE;">摘要</a><br></div>
<div id="title106">
<b>106.</b> ELITR Non-Native Speech Translation at IWSLT 2020, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.25.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper106" style="color:#0000EE;">摘要</a><br></div>
<div id="title107">
<b>107.</b> Is 42 the Answer to Everything in Subtitling-oriented Speech Translation?, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.26.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper107" style="color:#0000EE;">摘要</a><br></div>
<div id="title108">
<b>108.</b> Re-translation versus Streaming for Simultaneous Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.27.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper108" style="color:#0000EE;">摘要</a><br></div>
<div id="title109">
<b>109.</b> Towards Stream Translation: Adaptive Computation Time for Simultaneous Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.28.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper109" style="color:#0000EE;">摘要</a><br></div>
<div id="title110">
<b>110.</b> Neural Simultaneous Speech Translation Using Alignment-Based Chunking, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.29.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper110" style="color:#0000EE;">摘要</a><br></div>
<div id="title111">
<b>111.</b> From Speech-to-Speech Translation to Automatic Dubbing, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.31.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper111" style="color:#0000EE;">摘要</a><br></div>
<div id="title112">
<b>112.</b> Joint Translation and Unit Conversion for End-to-end Localization, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.32.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper112" style="color:#0000EE;">摘要</a><br></div>
<div id="title113">
<b>113.</b> How Human is Machine Translationese? Comparing Human and Machine Translations of Text and Speech, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.iwslt-1.34.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper113" style="color:#0000EE;">摘要</a><br></div>
<div id="title114">
<b>114.</b> Proceedings of the Fourth Workshop on Neural Generation and Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.ngt-1.0.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper114" style="color:#0000EE;">摘要</a><br></div>
<div id="title115">
<b>115.</b> Findings of the Fourth Workshop on Neural Generation and Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.ngt-1.1.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper115" style="color:#0000EE;">摘要</a><br></div>
<div id="title116">
<b>116.</b> Compressing Neural Machine Translation Models with 4-bit Precision, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.ngt-1.4.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper116" style="color:#0000EE;">摘要</a><br></div>
<div id="title117">
<b>117.</b> The Unreasonable Volatility of Neural Machine Translation Models, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.ngt-1.10.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper117" style="color:#0000EE;">摘要</a><br></div>
<div id="title118">
<b>118.</b> Distill, Adapt, Distill: Training Small, In-Domain Models for Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.ngt-1.12.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper118" style="color:#0000EE;">摘要</a><br></div>
<div id="title119">
<b>119.</b> Training and Inference Methods for High-Coverage Neural Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.ngt-1.13.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper119" style="color:#0000EE;">摘要</a><br></div>
<div id="title120">
<b>120.</b> English-to-Japanese Diverse Translation by Combining Forward and Backward Outputs, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.ngt-1.15.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper120" style="color:#0000EE;">摘要</a><br></div>
<div id="title121">
<b>121.</b> The ADAPT System Description for the STAPLE 2020 English-to-Portuguese Translation Task, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.ngt-1.17.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper121" style="color:#0000EE;">摘要</a><br></div>
<div id="title122">
<b>122.</b> Exploring Model Consensus to Generate Translation Paraphrases, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.ngt-1.19.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper122" style="color:#0000EE;">摘要</a><br></div>
<div id="title123">
<b>123.</b> Growing Together: Modeling Human Language Learning With n-Best Multi-Checkpoint Machine Translation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.ngt-1.20.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper123" style="color:#0000EE;">摘要</a><br></div>
<div id="title124">
<b>124.</b> Generating Diverse Translations via Weighted Fine-tuning and Hypotheses Filtering for the Duolingo STAPLE Task, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.ngt-1.21.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper124" style="color:#0000EE;">摘要</a><br></div>
<div id="title125">
<b>125.</b> The JHU Submission to the 2020 Duolingo Shared Task on Simultaneous Translation and Paraphrase for Language Education, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.ngt-1.22.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper125" style="color:#0000EE;">摘要</a><br></div>
<div id="title126">
<b>126.</b> Simultaneous paraphrasing and translation by fine-tuning Transformer models, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.ngt-1.23.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper126" style="color:#0000EE;">摘要</a><br></div>
<div id="title127">
<b>127.</b> Efficient and High-Quality Neural Machine Translation with OpenNMT, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.ngt-1.25.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper127" style="color:#0000EE;">摘要</a><br></div>
<div id="title128">
<b>128.</b> Edinburgh’s Submissions to the 2020 Machine Translation Efficiency Task, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.ngt-1.26.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper128" style="color:#0000EE;">摘要</a><br></div>
<div id="title129">
<b>129.</b> Improving Document-Level Neural Machine Translation with Domain Adaptation, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.ngt-1.27.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper129" style="color:#0000EE;">摘要</a><br></div>
<div id="title130">
<b>130.</b> Simultaneous Translation and Paraphrase for Language Education, ACL 2020 <a href="https://www.aclweb.org/anthology/2020.ngt-1.28.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper130" style="color:#0000EE;">摘要</a><br></div>
<div id="title131">
<b>131.</b> A Translation-Based Approach to Morphology Learning for Low Resource Languages, ACL 2020 <a href style="color:#0000EE;">[PDF]</a> <a href="#paper131" style="color:#0000EE;">摘要</a><br></div>
<div id="title132">
<b>132.</b> FFR v1.1: Fon-French Neural Machine Translation, ACL 2020 <a href style="color:#0000EE;">[PDF]</a> <a href="#paper132" style="color:#0000EE;">摘要</a><br></div>
<div id="title133">
<b>133.</b> Translating Natural Language Instructions for Behavioral Robot Navigation with a Multi-Head Attention Mechanism, ACL 2020 <a href style="color:#0000EE;">[PDF]</a> <a href="#paper133" style="color:#0000EE;">摘要</a><br></div>
<div id="title134">
<b>134.</b> Towards Mitigating Gender Bias in a decoder-based Neural Machine Translation model by Adding Contextual Information, ACL 2020 <a href style="color:#0000EE;">[PDF]</a> <a href="#paper134" style="color:#0000EE;">摘要</a><br></div>
<div id="title135">
<b>135.</b> Multitask Models for Controlling the Complexity of Neural Machine Translation, ACL 2020 <a href style="color:#0000EE;">[PDF]</a> <a href="#paper135" style="color:#0000EE;">摘要</a><br></div>
<div id="title136">
<b>136.</b> HausaMT v1.0: Towards English–Hausa Neural Machine Translation, ACL 2020 <a href style="color:#0000EE;">[PDF]</a> <a href="#paper136" style="color:#0000EE;">摘要</a><br></div>
<div id="title137">
<b>137.</b> An Evaluation of Subword Segmentation Strategies for Neural Machine Translation of Morphologically Rich Languages, ACL 2020 <a href style="color:#0000EE;">[PDF]</a> <a href="#paper137" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>                    <!-- procjx-wenzhang2 -->                     <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins>                     <script>                         (adsbygoogle = window.adsbygoogle || []).push({});                     </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Content Word Aware Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.34.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita</i><br>
<font size="3">
Neural machine translation (NMT) encodes the source sentence in a universal way to generate the target sentence word-by-word. However, NMT does not consider the importance of word in the sentence meaning, for example, some words (i.e., content words) express more important meaning than others (i.e., function words). To address this limitation, we first utilize word frequency information to distinguish between content and function words in a sentence, and then design a content word-aware NMT to improve translation performance. Empirical results on the WMT14 English-to-German, WMT14 English-to-French, and WMT17 Chinese-to-English translation tasks show that the proposed methods can significantly improve the performance of Transformer-based NMT.</font>
<br>
</div>


<hr>
<div id="paper2"> <b>2. Evaluating Explanation Methods for Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.35.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Jierui Li, Lemao Liu, Huayang Li, Guanlin Li, Guoping Huang, Shuming Shi</i><br>
<font size="3">
Recently many efforts have been devoted to interpreting the black-box NMT models, but little progress has been made on metrics to evaluate explanation methods. Word Alignment Error Rate can be used as such a metric that matches human understanding, however, it can not measure explanation methods on those target words that are not aligned to any source word. This paper thereby makes an initial attempt to evaluate explanation methods from an alternative viewpoint. To this end, it proposes a principled metric based on fidelity in regard to the predictive behavior of the NMT model. As the exact computation for this metric is intractable, we employ an efficient approach as its approximation. On six standard translation tasks, we quantitatively evaluate several explanation methods in terms of the proposed metric and we reveal some valuable findings for these explanation methods in our experiments.</font>
<br>
</div>


<hr>
<div id="paper3"> <b>3. Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.36.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Junliang Guo, Linli Xu, Enhong Chen</i><br>
<font size="3">
The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks. However, few works have adopted this technique in the sequence-to-sequence models. In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation~(NAT). Specifically, we first empirically study the functionalities of the encoder and the decoder in NAT models, and find that the encoder takes a more important role than the decoder regarding the translation quality. Therefore, we propose to train the encoder more rigorously by masking the encoder input while training. As for the decoder, we propose to train it based on the consecutive masking of the decoder input with an n-gram loss function to alleviate the problem of translating duplicate words. The two types of masks are applied to the model jointly at the training stage. We conduct experiments on five benchmark machine translation tasks, and our model can achieve 27.69/32.24 BLEU scores on WMT14 English-German/German-English tasks with 5+ times speed up compared with an autoregressive model.</font>
<br>
</div>


<hr>
<div id="paper4"> <b>4. Learning Source Phrase Representations for Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.37.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Hongfei Xu, Josef van Genabith, Deyi Xiong, Qiuhui Liu, Jingyi Zhang</i><br>
<font size="3">
The Transformer translation model (Vaswani et al., 2017) based on a multi-head attention mechanism can be computed effectively in parallel and has significantly pushed forward the performance of Neural Machine Translation (NMT). Though intuitively the attentional network can connect distant words via shorter network paths than RNNs, empirical analysis demonstrates that it still has difficulty in fully capturing long-distance dependencies (Tang et al., 2018). Considering that modeling phrases instead of words has significantly improved the Statistical Machine Translation (SMT) approach through the use of larger translation blocks (“phrases”) and its reordering ability, modeling NMT at phrase level is an intuitive proposal to help the model capture long-distance relationships. In this paper, we first propose an attentive phrase representation generation mechanism which is able to generate phrase representations from corresponding token representations. In addition, we incorporate the generated phrase representations into the Transformer translation model to enhance its ability to capture long-distance relationships. In our experiments, we obtain significant improvements on the WMT 14 English-German and English-French tasks on top of the strong Transformer baseline, which shows the effectiveness of our approach. Our approach helps Transformer Base models perform at the level of Transformer Big models, and even significantly better for long sentences, but with substantially fewer parameters and training steps. The fact that phrase representations help even in the big setting further supports our conjecture that they make a valuable contribution to long-distance relations.</font>
<br>
</div>


<hr>
<div id="paper5"> <b>5. Multiscale Collaborative Deep Models for Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.40.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Xiangpeng Wei, Heng Yu, Yue Hu, Yue Zhang, Rongxiang Weng, Weihua Luo</i><br>
<font size="3">
Recent evidence reveals that Neural Machine Translation (NMT) models with deeper neural networks can be more effective but are difficult to train. In this paper, we present a MultiScale Collaborative (MSC) framework to ease the training of NMT models that are substantially deeper than those used previously. We explicitly boost the gradient back-propagation from top to bottom levels by introducing a block-scale collaboration mechanism into deep NMT models. Then, instead of forcing the whole encoder stack directly learns a desired representation, we let each encoder block learns a fine-grained representation and enhance it by encoding spatial dependencies using a context-scale collaboration. We provide empirical evidence showing that the MSC nets are easy to optimize and can obtain improvements of translation quality from considerably increased depth. On IWSLT translation tasks with three translation directions, our extremely deep models (with 72-layer encoders) surpass strong baselines by +2.2~+3.1 BLEU points. In addition, our deep MSC achieves a BLEU score of 30.56 on WMT14 English-to-German task that significantly outperforms state-of-the-art deep NMT models. We have included the source code in supplementary materials.</font>
<br>
</div>


<hr>
<div id="paper6"> <b>6. Norm-Based Curriculum Learning for Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.41.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Xuebo Liu, Houtim Lai, Derek F. Wong, Lidia S. Chao</i><br>
<font size="3">
A neural machine translation (NMT) system is expensive to train, especially with high-resource settings. As the NMT architectures become deeper and wider, this issue gets worse and worse. In this paper, we aim to improve the efficiency of training an NMT by introducing a novel norm-based curriculum learning method. We use the norm (aka length or module) of a word embedding as a measure of 1) the difficulty of the sentence, 2) the competence of the model, and 3) the weight of the sentence. The norm-based sentence difficulty takes the advantages of both linguistically motivated and model-based sentence difficulties. It is easy to determine and contains learning-dependent features. The norm-based model competence makes NMT learn the curriculum in a fully automated way, while the norm-based sentence weight further enhances the learning of the vector representation of the NMT. Experimental results for the WMT’14 English-German and WMT’17 Chinese-English translation tasks demonstrate that the proposed method outperforms strong baselines in terms of BLEU score (+1.17/+1.56) and training speedup (2.22x/3.33x).</font>
<br>
</div>


<hr>
<div id="paper7"> <b>7. Opportunistic Decoding with Timely Correction for Simultaneous Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.42.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Renjie Zheng, Mingbo Ma, Baigong Zheng, Kaibo Liu, Liang Huang</i><br>
<font size="3">
Simultaneous translation has many important application scenarios and attracts much attention from both academia and industry recently. Most existing frameworks, however, have difficulties in balancing between the translation quality and latency, i.e., the decoding policy is usually either too aggressive or too conservative. We propose an opportunistic decoding technique with timely correction ability, which always (over-)generates a certain mount of extra words at each step to keep the audience on track with the latest information. At the same time, it also corrects, in a timely fashion, the mistakes in the former overgenerated words when observing more source context to ensure high translation quality. Experiments show our technique achieves substantial reduction in latency and up to +3.1 increase in BLEU, with revision rate under 8% in Chinese-to-English and English-to-Chinese translation.</font>
<br>
</div>


<hr>
<div id="paper8"> <b>8. Multi-Hypothesis Machine Translation Evaluation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.113.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Marina Fomicheva, Lucia Specia, Francisco Guzmán</i><br>
<font size="3">
Reliably evaluating Machine Translation (MT) through automated metrics is a long-standing problem. One of the main challenges is the fact that multiple outputs can be equally valid. Attempts to minimise this issue include metrics that relax the matching of MT output and reference strings, and the use of multiple references. The latter has been shown to significantly improve the performance of evaluation metrics. However, collecting multiple references is expensive and in practice a single reference is generally used. In this paper, we propose an alternative approach: instead of modelling linguistic variation in human reference we exploit the MT model uncertainty to generate multiple diverse translations and use these: (i) as surrogates to reference translations; (ii) to obtain a quantification of translation variability to either complement existing metric scores or (iii) replace references altogether. We show that for a number of popular evaluation metrics our variability estimates lead to substantial improvements in correlation with human judgements of quality by up 15%.</font>
<br>
</div>


<hr>
<div id="paper9"> <b>9. Multimodal Quality Estimation for Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.114.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Shu Okabe, Frédéric Blain, Lucia Specia</i><br>
<font size="3">
We propose approaches to Quality Estimation (QE) for Machine Translation that explore both text and visual modalities for Multimodal QE. We compare various multimodality integration and fusion strategies. For both sentence-level and document-level predictions, we show that state-of-the-art neural and feature-based QE frameworks obtain better results when using the additional modality.</font>
<br>
</div>


<hr>
<div id="paper10"> <b>10. Attend, Translate and Summarize: An Efficient Method for Neural Cross-Lingual Summarization</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.121.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Junnan Zhu, Yu Zhou, Jiajun Zhang, Chengqing Zong</i><br>
<font size="3">
Cross-lingual summarization aims at summarizing a document in one language (e.g., Chinese) into another language (e.g., English). In this paper, we propose a novel method inspired by the translation pattern in the process of obtaining a cross-lingual summary. We first attend to some words in the source text, then translate them into the target language, and summarize to get the final summary. Specifically, we first employ the encoder-decoder attention distribution to attend to the source words. Second, we present three strategies to acquire the translation probability, which helps obtain the translation candidates for each source word. Finally, each summary word is generated either from the neural distribution or from the translation candidates of source words. Experimental results on Chinese-to-English and English-to-Chinese summarization tasks have shown that our proposed method can significantly outperform the baselines, achieving comparable performance with the state-of-the-art.</font>
<br>
</div>


<hr>
<div id="paper11"> <b>11. Bilingual Dictionary Based Neural Machine Translation without Using Parallel Sentences</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.143.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Xiangyu Duan, Baijun Ji, Hao Jia, Min Tan, Min Zhang, Boxing Chen, Weihua Luo, Yue Zhang</i><br>
<font size="3">
In this paper, we propose a new task of machine translation (MT), which is based on no parallel sentences but can refer to a ground-truth bilingual dictionary. Motivated by the ability of a monolingual speaker learning to translate via looking up the bilingual dictionary, we propose the task to see how much potential an MT system can attain using the bilingual dictionary and large scale monolingual corpora, while is independent on parallel sentences. We propose anchored training (AT) to tackle the task. AT uses the bilingual dictionary to establish anchoring points for closing the gap between source language and target language. Experiments on various language pairs show that our approaches are significantly better than various baselines, including dictionary-based word-by-word translation, dictionary-supervised cross-lingual word embedding transformation, and unsupervised MT. On distant language pairs that are hard for unsupervised MT to perform well, AT performs remarkably better, achieving performances comparable to supervised SMT trained on more than 4M parallel sentences.</font>
<br>
</div>


<hr>
<div id="paper12"> <b>12. Boosting Neural Machine Translation with Similar Translations</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.144.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Jitao Xu, Josep Crego, Jean Senellart</i><br>
<font size="3">
This paper explores data augmentation methods for training Neural Machine Translation to make use of similar translations, in a comparable way a human translator employs fuzzy matches. In particular, we show how we can simply present the neural model with information of both source and target sides of the fuzzy matches, we also extend the similarity to include semantically related translations retrieved using sentence distributed representations. We show that translations based on fuzzy matching provide the model with “copy” information while translations based on embedding similarities tend to extend the translation “context”. Results indicate that the effect from both similar sentences are adding up to further boost accuracy, combine naturally with model fine-tuning and are providing dynamic adaptation for unseen translation pairs. Tests on multiple data sets and domains show consistent accuracy improvements. To foster research around these techniques, we also release an Open-Source toolkit with efficient and flexible fuzzy-match implementation.</font>
<br>
</div>


<hr>
<div id="paper13"> <b>13. Character-Level Translation with Self-attention</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.145.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Yingqiang Gao, Nikola I. Nikolov, Yuhuang Hu, Richard H.R. Hahnloser</i><br>
<font size="3">
We explore the suitability of self-attention models for character-level neural machine translation. We test the standard transformer model, as well as a novel variant in which the encoder block combines information from nearby characters using convolutions. We perform extensive experiments on WMT and UN datasets, testing both bilingual and multilingual translation to English using up to three input languages (French, Spanish, and Chinese). Our transformer variant consistently outperforms the standard transformer at the character-level and converges faster while learning more robust character-level alignments.</font>
<br>
</div>


<hr>
<div id="paper14"> <b>14. Enhancing Machine Translation with Dependency-Aware Self-Attention</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.147.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Emanuele Bugliarello, Naoaki Okazaki</i><br>
<font size="3">
Most neural machine translation models only rely on pairs of parallel sentences, assuming syntactic information is automatically learned by an attention mechanism. In this work, we investigate different approaches to incorporate syntactic knowledge in the Transformer model and also propose a novel, parameter-free, dependency-aware self-attention mechanism that improves its translation quality, especially for long sentences and in low-resource scenarios. We show the efficacy of each approach on WMT English-German and English-Turkish, and WAT English-Japanese translation tasks.</font>
<br>
</div>


<hr>
<div id="paper15"> <b>15. Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.148.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Biao Zhang, Philip Williams, Ivan Titov, Rico Sennrich</i><br>
<font size="3">
Massively multilingual models for neural machine translation (NMT) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations. In this paper, we explore ways to improve them. We argue that multilingual NMT requires stronger modeling capacity to support language pairs with varying typological characteristics, and overcome this bottleneck via language-specific components and deepening NMT architectures. We identify the off-target translation issue (i.e. translating into a wrong target language) as the major source of the inferior zero-shot performance, and propose random online backtranslation to enforce the translation of unseen training language pairs. Experiments on OPUS-100 (a novel multilingual dataset with 100 languages) show that our approach substantially narrows the performance gap with bilingual models in both one-to-many and many-to-many settings, and improves zero-shot performance by ~10 BLEU, approaching conventional pivot-based methods.</font>
<br>
</div>


<hr>
<div id="paper16"> <b>16. It’s Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.149.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Emanuele Bugliarello, Sabrina J. Mielke, Antonios Anastasopoulos, Ryan Cotterell, Naoaki Okazaki</i><br>
<font size="3">
The performance of neural machine translation systems is commonly evaluated in terms of BLEU. However, due to its reliance on target language properties and generation, the BLEU metric does not allow an assessment of which translation directions are more difficult to model. In this paper, we propose cross-mutual information (XMI): an asymmetric information-theoretic metric of machine translation difficulty that exploits the probabilistic nature of most neural machine translation models. XMI allows us to better evaluate the difficulty of translating text into the target language while controlling for the difficulty of the target-side generation component independent of the translation task. We then present the first systematic and controlled study of cross-lingual translation difficulties using modern neural translation systems. Code for replicating our experiments is available online at https://github.com/e-bug/nmt-difficulty.</font>
<br>
</div>


<hr>
<div id="paper17"> <b>17. Language-aware Interlingua for Multilingual Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.150.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Changfeng Zhu, Heng Yu, Shanbo Cheng, Weihua Luo</i><br>
<font size="3">
Multilingual neural machine translation (NMT) has led to impressive accuracy improvements in low-resource scenarios by sharing common linguistic information across languages. However, the traditional multilingual model fails to capture the diversity and specificity of different languages, resulting in inferior performance compared with individual models that are sufficiently trained. In this paper, we incorporate a language-aware interlingua into the Encoder-Decoder architecture. The interlingual network enables the model to learn a language-independent representation from the semantic spaces of different languages, while still allowing for language-specific specialization of a particular language-pair. Experiments show that our proposed method achieves remarkable improvements over state-of-the-art multilingual NMT baselines and produces comparable performance with strong individual models.</font>
<br>
</div>


<hr>
<div id="paper18"> <b>18. On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.151.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Wei Zhao, Goran Glavaš, Maxime Peyrard, Yang Gao, Robert West, Steffen Eger</i><br>
<font size="3">
Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity. In this paper, we concern ourselves with reference-free machine translation (MT) evaluation where we directly compare source texts to (sometimes low-quality) system translations, which represents a natural adversarial setup for multilingual encoders. Reference-free evaluation holds the promise of web-scale comparison of MT systems. We systematically investigate a range of metrics based on state-of-the-art cross-lingual semantic representations obtained with pretrained M-BERT and LASER. We find that they perform poorly as semantic encoders for reference-free MT evaluation and identify their two key limitations, namely, (a) a semantic mismatch between representations of mutual translations and, more prominently, (b) the inability to punish “translationese”, i.e., low-quality literal translations. We propose two partial remedies: (1) post-hoc re-alignment of the vector spaces and (2) coupling of semantic-similarity based metrics with target-side language modeling. In segment-level MT evaluation, our best metric surpasses reference-based BLEU by 5.7 correlation points.</font>
<br>
</div>


<hr>
<div id="paper19"> <b>19. “You Sound Just Like Your Father” Commercial Machine Translation Systems Include Stylistic Biases</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.154.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Dirk Hovy, Federico Bianchi, Tommaso Fornaciari</i><br>
<font size="3">
The main goal of machine translation has been to convey the correct content. Stylistic considerations have been at best secondary. We show that as a consequence, the output of three commercial machine translation systems (Bing, DeepL, Google) make demographically diverse samples from five languages “sound” older and more male than the original. Our findings suggest that translation models reflect demographic bias in the training data. This opens up interesting new research avenues in machine translation to take stylistic considerations into account.</font>
<br>
</div>


<hr>
<div id="paper20"> <b>20. MMPE: A Multi-Modal Interface for Post-Editing Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.155.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Nico Herbig, Tim Düwel, Santanu Pal, Kalliopi Meladaki, Mahsa Monshizadeh, Antonio Krüger, Josef van Genabith</i><br>
<font size="3">
Current advances in machine translation (MT) increase the need for translators to switch from traditional translation to post-editing (PE) of machine-translated text, a process that saves time and reduces errors. This affects the design of translation interfaces, as the task changes from mainly generating text to correcting errors within otherwise helpful translation proposals. Since this paradigm shift offers potential for modalities other than mouse and keyboard, we present MMPE, the first prototype to combine traditional input modes with pen, touch, and speech modalities for PE of MT. The results of an evaluation with professional translators suggest that pen and touch interaction are suitable for deletion and reordering tasks, while they are of limited use for longer insertions. On the other hand, speech and multi-modal combinations of select & speech are considered suitable for replacements and insertions but offer less potential for deletion and reordering. Overall, participants were enthusiastic about the new modalities and saw them as good extensions to mouse & keyboard, but not as a complete substitute.</font>
<br>
</div>


<hr>
<div id="paper21"> <b>21. Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.165.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Haoming Jiang, Chen Liang, Chong Wang, Tuo Zhao</i><br>
<font size="3">
Many multi-domain neural machine translation (NMT) models achieve knowledge transfer by enforcing one encoder to learn shared embedding across domains. However, this design lacks adaptation to individual domains. To overcome this limitation, we propose a novel multi-domain NMT model using individual modules for each domain, on which we apply word-level, adaptive and layer-wise domain mixing. We first observe that words in a sentence are often related to multiple domains. Hence, we assume each word has a domain proportion, which indicates its domain preference. Then word representations are obtained by mixing their embedding in individual domains based on their domain proportions. We show this can be achieved by carefully designing multi-head dot-product attention modules for different domains, and eventually taking weighted averages of their parameters by word-level layer-wise domain proportions. Through this, we can achieve effective domain knowledge sharing and capture fine-grained domain-specific knowledge as well. Our experiments show that our proposed model outperforms existing ones in several NMT tasks.</font>
<br>
</div>


<hr>
<div id="paper22"> <b>22. Improving Non-autoregressive Neural Machine Translation with Monolingual Data</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.171.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Jiawei Zhou, Phillip Keung</i><br>
<font size="3">
Non-autoregressive (NAR) neural machine translation is usually done via knowledge distillation from an autoregressive (AR) model. Under this framework, we leverage large monolingual corpora to improve the NAR model’s performance, with the goal of transferring the AR model’s generalization ability while preventing overfitting. On top of a strong NAR baseline, our experimental results on the WMT14 En-De and WMT16 En-Ro news translation tasks confirm that monolingual data augmentation consistently improves the performance of the NAR model to approach the teacher AR model’s performance, yields comparable or better results than the best non-iterative NAR methods in the literature and helps reduce overfitting in the training process.</font>
<br>
</div>


<hr>
<div id="paper23"> <b>23. Phone Features Improve Speech Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.217.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Elizabeth Salesky, Alan W Black</i><br>
<font size="3">
End-to-end models for speech translation (ST) more tightly couple speech recognition (ASR) and machine translation (MT) than a traditional cascade of separate ASR and MT models, with simpler model architectures and the potential for reduced error propagation. Their performance is often assumed to be superior, though in many conditions this is not yet the case. We compare cascaded and end-to-end models across high, medium, and low-resource conditions, and show that cascades remain stronger baselines. Further, we introduce two methods to incorporate phone features into ST models. We show that these features improve both architectures, closing the gap between end-to-end models and cascades, and outperforming previous academic work – by up to 9 BLEU on our low-resource setting.</font>
<br>
</div>


<hr>
<div id="paper24"> <b>24. ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.251.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Lifu Tu, Richard Yuanzhe Pang, Sam Wiseman, Kevin Gimpel</i><br>
<font size="3">
We propose to train a non-autoregressive machine translation model to minimize the energy defined by a pretrained autoregressive model. In particular, we view our non-autoregressive translation system as an inference network (Tu and Gimpel, 2018) trained to minimize the autoregressive teacher energy. This contrasts with the popular approach of training a non-autoregressive model on a distilled corpus consisting of the beam-searched outputs of such a teacher model. Our approach, which we call ENGINE (ENerGy-based Inference NEtworks), achieves state-of-the-art non-autoregressive results on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, approaching the performance of autoregressive models.</font>
<br>
</div>


<hr>
<div id="paper25"> <b>25. Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.252.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Aditya Siddhant, Ankur Bapna, Yuan Cao, Orhan Firat, Mia Chen, Sneha Kudugunta, Naveen Arivazhagan, Yonghui Wu</i><br>
<font size="3">
Over the last few years two promising research directions in low-resource neural machine translation (NMT) have emerged. The first focuses on utilizing high-resource languages to improve the quality of low-resource languages via multilingual NMT. The second direction employs monolingual data with self-supervision to pre-train translation models, followed by fine-tuning on small amounts of supervised data. In this work, we join these two lines of research and demonstrate the efficacy of monolingual data with self-supervision in multilingual NMT. We offer three major results: (i) Using monolingual data significantly boosts the translation quality of low-resource languages in multilingual models. (ii) Self-supervision improves zero-shot translation quality in multilingual models. (iii) Leveraging monolingual data with self-supervision provides a viable path towards adding new languages to multilingual models, getting up to 33 BLEU on ro-en translation without any parallel data or back-translation.</font>
<br>
</div>


<hr>
<div id="paper26"> <b>26. On The Evaluation of Machine Translation Systems Trained With Back-Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.253.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Sergey Edunov, Myle Ott, Marc’Aurelio Ranzato, Michael Auli</i><br>
<font size="3">
Back-translation is a widely used data augmentation technique which leverages target monolingual data. However, its effectiveness has been challenged since automatic metrics such as BLEU only show significant improvements for test examples where the source itself is a translation, or translationese. This is believed to be due to translationese inputs better matching the back-translated training data. In this work, we show that this conjecture is not empirically supported and that back-translation improves translation quality of both naturally occurring text as well as translationese according to professional human translators. We provide empirical evidence to support the view that back-translation is preferred by humans because it produces more fluent outputs. BLEU cannot capture human preferences because references are translationese when source sentences are natural text. We recommend complementing BLEU with a language model score to measure fluency.</font>
<br>
</div>


<hr>
<div id="paper27"> <b>27. Simultaneous Translation Policies: From Fixed to Adaptive</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.254.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Baigong Zheng, Kaibo Liu, Renjie Zheng, Mingbo Ma, Hairong Liu, Liang Huang</i><br>
<font size="3">
Adaptive policies are better than fixed policies for simultaneous translation, since they can flexibly balance the tradeoff between translation quality and latency based on the current context information. But previous methods on obtaining adaptive policies either rely on complicated training process, or underperform simple fixed policies. We design an algorithm to achieve adaptive policies via a simple heuristic composition of a set of fixed policies. Experiments on Chinese -> English and German -> English show that our adaptive policies can outperform fixed ones by up to 4 BLEU points for the same latency, and more surprisingly, it even surpasses the BLEU score of full-sentence translation in the greedy mode (and very close to beam mode), but with much lower latency.</font>
<br>
</div>


<hr>
<div id="paper28"> <b>28. A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.273.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Yongjing Yin, Fandong Meng, Jinsong Su, Chulun Zhou, Zhengyuan Yang, Jie Zhou, Jiebo Luo</i><br>
<font size="3">
Multi-modal neural machine translation (NMT) aims to translate source sentences into a target language paired with images. However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning. To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT. Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects). We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations. Finally, these representations provide an attention-based context vector for the decoder. We evaluate our proposed encoder on the Multi30K datasets. Experimental results and in-depth analysis show the superiority of our multi-modal NMT model.</font>
<br>
</div>


<hr>
<div id="paper29"> <b>29. Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.275.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Xuanli He, Gholamreza Haffari, Mohammad Norouzi</i><br>
<font size="3">
This paper introduces Dynamic Programming Encoding (DPE), a new segmentation algorithm for tokenizing sentences into subword units. We view the subword segmentation of output sentences as a latent variable that should be marginalized out for learning and inference. A mixed character-subword transformer is proposed, which enables exact log marginal likelihood estimation and exact MAP inference to find target segmentations with maximum posterior probability. DPE uses a lightweight mixed character-subword transformer as a means of pre-processing parallel data to segment output sentences using dynamic programming. Empirical results on machine translation suggest that DPE is effective for segmenting output sentences and can be combined with BPE dropout for stochastic segmentation of source sentences. DPE achieves an average improvement of 0.9 BLEU over BPE (Sennrich et al., 2016) and an average improvement of 0.55 BLEU over BPE dropout (Provilkov et al., 2019) on several WMT datasets including English <=> (German, Romanian, Estonian, Finnish, Hungarian).</=></font>
<br>
</div>


<hr>
<div id="paper30"> <b>30. Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.277.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Qiu Ran, Yankai Lin, Peng Li, Jie Zhou</i><br>
<font size="3">
Non-autoregressive neural machine translation (NAT) predicts the entire target sequence simultaneously and significantly accelerates inference process. However, NAT discards the dependency information in a sentence, and thus inevitably suffers from the multi-modality problem: the target tokens may be provided by different possible translations, often causing token repetitions or missing. To alleviate this problem, we propose a novel semi-autoregressive model RecoverSAT in this work, which generates a translation as a sequence of segments. The segments are generated simultaneously while each segment is predicted token-by-token. By dynamically determining segment length and deleting repetitive segments, RecoverSAT is capable of recovering from repetitive and missing token errors. Experimental results on three widely-used benchmark datasets show that our proposed model achieves more than 4 times speedup while maintaining comparable performance compared with the corresponding autoregressive model.</font>
<br>
</div>


<hr>
<div id="paper31"> <b>31. On the Inference Calibration of Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.278.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Shuo Wang, Zhaopeng Tu, Shuming Shi, Yang Liu</i><br>
<font size="3">
Confidence calibration, which aims to make model predictions equal to the true correctness measures, is important for neural machine translation (NMT) because it is able to offer useful indicators of translation errors in the generated output. While prior studies have shown that NMT models trained with label smoothing are well-calibrated on the ground-truth training data, we find that miscalibration still remains a severe challenge for NMT during inference due to the discrepancy between training and inference. By carefully designing experiments on three language pairs, our work provides in-depth analyses of the correlation between calibration and translation performance as well as linguistic properties of miscalibration and reports a number of interesting findings that might help humans better analyze, understand and improve NMT models. Based on these observations, we further propose a new graduated label smoothing method that can improve both inference calibration and translation performance.</font>
<br>
</div>


<hr>
<div id="paper32"> <b>32. A Reinforced Generation of Adversarial Examples for Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.319.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>wei zou, Shujian Huang, Jun Xie, Xinyu Dai, Jiajun Chen</i><br>
<font size="3">
Neural machine translation systems tend to fail on less decent inputs despite its significant efficacy, which may significantly harm the credibility of these systems—fathoming how and when neural-based systems fail in such cases is critical for industrial maintenance. Instead of collecting and analyzing bad cases using limited handcrafted error features, here we investigate this issue by generating adversarial examples via a new paradigm based on reinforcement learning. Our paradigm could expose pitfalls for a given performance metric, e.g., BLEU, and could target any given neural machine translation architecture. We conduct experiments of adversarial attacks on two mainstream neural machine translation architectures, RNN-search, and Transformer. The results show that our method efficiently produces stable attacks with meaning-preserving adversarial examples. We also present a qualitative and quantitative analysis for the preference pattern of the attack, demonstrating its capability of pitfall exposure.</font>
<br>
</div>


<hr>
<div id="paper33"> <b>33. A Retrieve-and-Rewrite Initialization Method for Unsupervised Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.320.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title33" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Shuo Ren, Yu Wu, Shujie Liu, Ming Zhou, Shuai Ma</i><br>
<font size="3">
The commonly used framework for unsupervised machine translation builds initial translation models of both translation directions, and then performs iterative back-translation to jointly boost their translation performance. The initialization stage is very important since bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance. In this paper, we propose a novel retrieval and rewriting based method to better initialize unsupervised translation models. We first retrieve semantically comparable sentences from monolingual corpora of two languages and then rewrite the target side to minimize the semantic gap between the source and retrieved targets with a designed rewriting model. The rewritten sentence pairs are used to initialize SMT models which are used to generate pseudo data for two NMT models, followed by the iterative back-translation. Experiments show that our method can build better initial unsupervised translation models and improve the final translation performance by over 4 BLEU scores. Our code is released at https://github.com/Imagist-Shuo/RRforUNMT.git.</font>
<br>
</div>


<hr>
<div id="paper34"> <b>34. A Simple and Effective Unified Encoder for Document-Level Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.321.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title34" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Shuming Ma, Dongdong Zhang, Ming Zhou</i><br>
<font size="3">
Most of the existing models for document-level machine translation adopt dual-encoder structures. The representation of the source sentences and the document-level contexts are modeled with two separate encoders. Although these models can make use of the document-level contexts, they do not fully model the interaction between the contexts and the source sentences, and can not directly adapt to the recent pre-training models (e.g., BERT) which encodes multiple sentences with a single encoder. In this work, we propose a simple and effective unified encoder that can outperform the baseline models of dual-encoder models in terms of BLEU and METEOR scores. Moreover, the pre-training models can further boost the performance of our proposed model.</font>
<br>
</div>


<hr>
<div id="paper35"> <b>35. Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.322.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title35" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Bei Li, Hui Liu, Ziyang Wang, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, changliang Li</i><br>
<font size="3">
In encoder-decoder neural models, multiple encoders are in general used to represent the contextual information in addition to the individual sentence. In this paper, we investigate multi-encoder approaches in document-level neural machine translation (NMT). Surprisingly, we find that the context encoder does not only encode the surrounding sentences but also behaves as a noise generator. This makes us rethink the real benefits of multi-encoder in context-aware translation - some of the improvements come from robust training. We compare several methods that introduce noise and/or well-tuned dropout setup into the training of these encoders. Experimental results show that noisy training plays an important role in multi-encoder-based NMT, especially when the training data is small. Also, we establish a new state-of-the-art on IWSLT Fr-En task by careful use of noise generation and dropout methods.</font>
<br>
</div>


<hr>
<div id="paper36"> <b>36. Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.324.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title36" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Haipeng Sun, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita, Tiejun Zhao</i><br>
<font size="3">
Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.</font>
<br>
</div>


<hr>
<div id="paper37"> <b>37. Lexically Constrained Neural Machine Translation with Levenshtein Transformer</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.325.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title37" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Raymond Hendy Susanto, Shamil Chollampatt, Liling Tan</i><br>
<font size="3">
This paper proposes a simple and effective algorithm for incorporating lexical constraints in neural machine translation. Previous work either required re-training existing models with the lexical constraints or incorporating them during beam search decoding with significantly higher computational overheads. Leveraging the flexibility and speed of a recently proposed Levenshtein Transformer model (Gu et al., 2019), our method injects terminology constraints at inference time without any impact on decoding speed. Our method does not require any modification to the training procedure and can be easily applied at runtime with custom dictionaries. Experiments on English-German WMT datasets show that our approach improves an unconstrained baseline and previous approaches.</font>
<br>
</div>


<hr>
<div id="paper38"> <b>38. On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.326.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title38" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Chaojun Wang, Rico Sennrich</i><br>
<font size="3">
The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.</font>
<br>
</div>


<hr>
<div id="paper39"> <b>39. Automatic Machine Translation Evaluation using Source Language Inputs and Cross-lingual Language Model</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.327.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title39" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Kosuke Takahashi, Katsuhito Sudoh, Satoshi Nakamura</i><br>
<font size="3">
We propose an automatic evaluation method of machine translation that uses source language sentences regarded as additional pseudo references. The proposed method evaluates a translation hypothesis in a regression model. The model takes the paired source, reference, and hypothesis sentence all together as an input. A pretrained large scale cross-lingual language model encodes the input to sentence-pair vectors, and the model predicts a human evaluation score with those vectors. Our experiments show that our proposed method using Cross-lingual Language Model (XLM) trained with a translation language modeling (TLM) objective achieves a higher correlation with human judgments than a baseline method that uses only hypothesis and reference sentences. Additionally, using source sentences in our proposed method is confirmed to improve the evaluation performance.</font>
<br>
</div>


<hr>
<div id="paper40"> <b>40. Curriculum Pre-training for End-to-End Speech Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.344.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title40" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Chengyi Wang, Yu Wu, Shujie Liu, Ming Zhou, Zhenglu Yang</i><br>
<font size="3">
End-to-end speech translation poses a heavy burden on the encoder because it has to transcribe, understand, and learn cross-lingual semantics simultaneously. To obtain a powerful encoder, traditional methods pre-train it on ASR data to capture speech features. However, we argue that pre-training the encoder only through simple speech recognition is not enough, and high-level linguistic knowledge should be considered. Inspired by this, we propose a curriculum pre-training method that includes an elementary course for transcription learning and two advanced courses for understanding the utterance and mapping words in two languages. The difficulty of these courses is gradually increasing. Experiments show that our curriculum pre-training method leads to significant improvements on En-De and En-Fr speech translation benchmarks.</font>
<br>
</div>


<hr>
<div id="paper41"> <b>41. SimulSpeech: End-to-End Simultaneous Speech to Text Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.350.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title41" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Yi Ren, Jinglin Liu, Xu Tan, Chen Zhang, Tao Qin, Zhou Zhao, Tie-Yan Liu</i><br>
<font size="3">
In this work, we develop SimulSpeech, an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently. SimulSpeech consists of a speech encoder, a speech segmenter and a text decoder, where 1) the segmenter builds upon the encoder and leverages a connectionist temporal classification (CTC) loss to split the input streaming speech in real time, 2) the encoder-decoder attention adopts a wait-k strategy for simultaneous translation. SimulSpeech is more challenging than previous cascaded systems (with simultaneous automatic speech recognition (ASR) and simultaneous neural machine translation (NMT)). We introduce two novel knowledge distillation methods to ensure the performance: 1) Attention-level knowledge distillation transfers the knowledge from the multiplication of the attention matrices of simultaneous NMT and ASR models to help the training of the attention mechanism in SimulSpeech; 2) Data-level knowledge distillation transfers the knowledge from the full-sentence NMT model and also reduces the complexity of data distribution to help on the optimization of SimulSpeech. Experiments on MuST-C English-Spanish and English-German spoken language translation datasets show that SimulSpeech achieves reasonable BLEU scores and lower delay compared to full-sentence end-to-end speech to text translation (without simultaneous translation), and better performance than the two-stage cascaded simultaneous translation model in terms of BLEU scores and translation delay.</font>
<br>
</div>


<hr>
<div id="paper42"> <b>42. Selecting Backtranslated Data from Multiple Sources for Improved Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.359.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title42" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Xabier Soto, Dimitar Shterionov, Alberto Poncelas, Andy Way</i><br>
<font size="3">
Machine translation (MT) has benefited from using synthetic training data originating from translating monolingual corpora, a technique known as backtranslation. Combining backtranslated data from different sources has led to better results than when using such data in isolation. In this work we analyse the impact that data translated with rule-based, phrase-based statistical and neural MT systems has on new MT systems. We use a real-world low-resource use-case (Basque-to-Spanish in the clinical domain) as well as a high-resource language pair (German-to-English) to test different scenarios with backtranslation and employ data selection to optimise the synthetic corpora. We exploit different data selection strategies in order to reduce the amount of data used, while at the same time maintaining high-quality MT systems. We further tune the data selection method by taking into account the quality of the MT systems used for backtranslation and lexical diversity of the resulting corpora. Our experiments show that incorporating backtranslated data from different sources can be beneficial, and that availing of data selection can yield improved performance.</font>
<br>
</div>


<hr>
<div id="paper43"> <b>43. Modeling Word Formation in English–German Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.389.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title43" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Marion Weller-Di Marco, Alexander Fraser</i><br>
<font size="3">
This paper studies strategies to model word formation in NMT using rich linguistic information, namely a word segmentation approach that goes beyond splitting into substrings by considering fusional morphology. Our linguistically sound segmentation is combined with a method for target-side inflection to accommodate modeling word formation. The best system variants employ source-side morphological analysis and model complex target-side words, improving over a standard system.</font>
<br>
</div>


<hr>
<div id="paper44"> <b>44. Multimodal Transformer for Multimodal Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.400.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title44" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Shaowei Yao, Xiaojun Wan</i><br>
<font size="3">
Multimodal Machine Translation (MMT) aims to introduce information from other modality, generally static images, to improve the translation quality. Previous works propose various incorporation methods, but most of them do not consider the relative importance of multiple modalities. Equally treating all modalities may encode too much useless information from less important modalities. In this paper, we introduce the multimodal self-attention in Transformer to solve the issues above in MMT. The proposed method learns the representation of images based on the text, which avoids encoding irrelevant information in images. Experiments and visualization analysis demonstrate that our model benefits from visual information and substantially outperforms previous works and competitive baselines in terms of various metrics.</font>
<br>
</div>


<hr>
<div id="paper45"> <b>45. Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.448.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title45" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Nitika Mathur, Timothy Baldwin, Trevor Cohn</i><br>
<font size="3">
Automatic metrics are fundamental for the development and evaluation of machine translation systems. Judging whether, and to what extent, automatic metrics concur with the gold standard of human evaluation is not a straightforward problem. We show that current methods for judging metrics are highly sensitive to the translations used for assessment, particularly the presence of outliers, which often leads to falsely confident conclusions about a metric’s efficacy. Finally, we turn to pairwise system ranking, developing a method for thresholding performance improvement under an automatic metric against human judgements, which allows quantification of type I versus type II errors incurred, i.e., insignificant human differences in system quality that are accepted, and significant human differences that are rejected. Together, these findings suggest improvements to the protocols for metric evaluation and system performance evaluation in machine translation.</font>
<br>
</div>


<hr>
<div id="paper46"> <b>46. AdvAug: Robust Adversarial Augmentation for Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.529.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title46" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Yong Cheng, Lu Jiang, Wolfgang Macherey, Jacob Eisenstein</i><br>
<font size="3">
In this paper, we propose a new adversarial augmentation method for Neural Machine Translation (NMT). The main idea is to minimize the vicinal risk over virtual sentences sampled from two vicinity distributions, in which the crucial one is a novel vicinity distribution for adversarial sentences that describes a smooth interpolated embedding space centered around observed training sentence pairs. We then discuss our approach, AdvAug, to train NMT models using the embeddings of virtual sentences in sequence-to-sequence learning. Experiments on Chinese-English, English-French, and English-German translation benchmarks show that AdvAug achieves significant improvements over theTransformer (up to 4.9 BLEU points), and substantially outperforms other data augmentation techniques (e.g.back-translation) without using extra corpora.</font>
<br>
</div>


<hr>
<div id="paper47"> <b>47. Contextual Neural Machine Translation Improves Translation of Cataphoric Pronouns</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.530.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title47" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>KayYen Wong, Sameen Maruf, Gholamreza Haffari</i><br>
<font size="3">
The advent of context-aware NMT has resulted in promising improvements in the overall translation quality and specifically in the translation of discourse phenomena such as pronouns. Previous works have mainly focused on the use of past sentences as context with a focus on anaphora translation. In this work, we investigate the effect of future sentences as context by comparing the performance of a contextual NMT model trained with the future context to the one trained with the past context. Our experiments and evaluation, using generic and pronoun-focused automatic metrics, show that the use of future context not only achieves significant improvements over the context-agnostic Transformer, but also demonstrates comparable and in some cases improved performance over its counterpart trained on past context. We also perform an evaluation on a targeted cataphora test suite and report significant gains over the context-agnostic Transformer in terms of BLEU.</font>
<br>
</div>


<hr>
<div id="paper48"> <b>48. Improving Neural Machine Translation with Soft Template Prediction</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.531.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title48" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Jian Yang, Shuming Ma, Dongdong Zhang, Zhoujun Li, Ming Zhou</i><br>
<font size="3">
Although neural machine translation (NMT) has achieved significant progress in recent years, most previous NMT models only depend on the source text to generate translation. Inspired by the success of template-based and syntax-based approaches in other fields, we propose to use extracted templates from tree structures as soft target templates to guide the translation procedure. In order to learn the syntactic structure of the target sentences, we adopt constituency-based parse tree to generate candidate templates. We incorporate the template information into the encoder-decoder framework to jointly utilize the templates and source text. Experiments show that our model significantly outperforms the baseline models on four benchmarks and demonstrates the effectiveness of soft target templates.</font>
<br>
</div>


<hr>
<div id="paper49"> <b>49. Tagged Back-translation Revisited: Why Does It Really Work?</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.532.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title49" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Benjamin Marie, Raphael Rubino, Atsushi Fujita</i><br>
<font size="3">
In this paper, we show that neural machine translation (NMT) systems trained on large back-translated data overfit some of the characteristics of machine-translated texts. Such NMT systems better translate human-produced translations, i.e., translationese, but may largely worsen the translation quality of original texts. Our analysis reveals that adding a simple tag to back-translations prevents this quality degradation and improves on average the overall translation quality by helping the NMT system to distinguish back-translated data from original parallel data during training. We also show that, in contrast to high-resource configurations, NMT systems trained in low-resource settings are much less vulnerable to overfit back-translations. We conclude that the back-translations in the training data should always be tagged especially when the origin of the text to be translated is unknown.</font>
<br>
</div>


<hr>
<div id="paper50"> <b>50. Worse WER, but Better BLEU? Leveraging Word Embedding as Intermediate in Multitask End-to-End Speech Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.533.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title50" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Shun-Po Chuang, Tzu-Wei Sung, Alexander H. Liu, Hung-yi Lee</i><br>
<font size="3">
Speech translation (ST) aims to learn transformations from speech in the source language to the text in the target language. Previous works show that multitask learning improves the ST performance, in which the recognition decoder generates the text of the source language, and the translation decoder obtains the final translations based on the output of the recognition decoder. Because whether the output of the recognition decoder has the correct semantics is more critical than its accuracy, we propose to improve the multitask ST model by utilizing word embedding as the intermediate.</font>
<br>
</div>


<hr>
<div id="paper51"> <b>51. Are we Estimating or Guesstimating Translation Quality?</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.558.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title51" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Shuo Sun, Francisco Guzmán, Lucia Specia</i><br>
<font size="3">
Recent advances in pre-trained multilingual language models lead to state-of-the-art results on the task of quality estimation (QE) for machine translation. A carefully engineered ensemble of such models won the QE shared task at WMT19. Our in-depth analysis, however, shows that the success of using pre-trained language models for QE is over-estimated due to three issues we observed in current QE datasets: (i) The distributions of quality scores are imbalanced and skewed towards good quality scores; (iii) QE models can perform well on these datasets while looking at only source or translated sentences; (iii) They contain statistical artifacts that correlate well with human-annotated QE labels. Our findings suggest that although QE models might capture fluency of translated sentences and complexity of source sentences, they cannot model adequacy of translations effectively.</font>
<br>
</div>


<hr>
<div id="paper52"> <b>52. Document Translation vs. Query Translation for Cross-Lingual Information Retrieval in the Medical Domain</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.613.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title52" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Shadi Saleh, Pavel Pecina</i><br>
<font size="3">
We present a thorough comparison of two principal approaches to Cross-Lingual Information Retrieval: document translation (DT) and query translation (QT). Our experiments are conducted using the cross-lingual test collection produced within the CLEF eHealth information retrieval tasks in 2013–2015 containing English documents and queries in several European languages. We exploit the Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) paradigms and train several domain-specific and task-specific machine translation systems to translate the non-English queries into English (for the QT approach) and the English documents to all the query languages (for the DT approach). The results show that the quality of QT by SMT is sufficient enough to outperform the retrieval results of the DT approach for all the languages. NMT then further boosts translation quality and retrieval quality for both QT and DT for most languages, but still, QT provides generally better retrieval results than DT.</font>
<br>
</div>


<hr>
<div id="paper53"> <b>53. Gender in Danger? Evaluating Speech Translation Technology on the MuST-SHE Corpus</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.619.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title53" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Luisa Bentivogli, Beatrice Savoldi, Matteo Negri, Mattia A. Di Gangi, Roldano Cattoni, Marco Turchi</i><br>
<font size="3">
Translating from languages without productive grammatical gender like English into gender-marked languages is a well-known difficulty for machines. This difficulty is also due to the fact that the training data on which models are built typically reflect the asymmetries of natural languages, gender bias included. Exclusively fed with textual data, machine translation is intrinsically constrained by the fact that the input sentence does not always contain clues about the gender identity of the referred human entities. But what happens with speech translation, where the input is an audio signal? Can audio provide additional information to reduce gender bias? We present the first thorough investigation of gender bias in speech translation, contributing with: i) the release of a benchmark useful for future studies, and ii) the comparison of different technologies (cascade and end-to-end) on two language directions (English-Italian/French).</font>
<br>
</div>


<hr>
<div id="paper54"> <b>54. Uncertainty-Aware Curriculum Learning for Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.620.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title54" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Yikai Zhou, Baosong Yang, Derek F. Wong, Yu Wan, Lidia S. Chao</i><br>
<font size="3">
Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule.</font>
<br>
</div>


<hr>
<div id="paper55"> <b>55. Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.627.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title55" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Hao Fei, Meishan Zhang, Donghong Ji</i><br>
<font size="3">
Many efforts of research are devoted to semantic role labeling (SRL) which is crucial for natural language understanding. Supervised approaches have achieved impressing performances when large-scale corpora are available for resource-rich languages such as English. While for the low-resource languages with no annotated SRL dataset, it is still challenging to obtain competitive performances. Cross-lingual SRL is one promising way to address the problem, which has achieved great advances with the help of model transferring and annotation projection. In this paper, we propose a novel alternative based on corpus translation, constructing high-quality training datasets for the target languages from the source gold-standard SRL annotations. Experimental results on Universal Proposition Bank show that the translation-based method is highly effective, and the automatic pseudo datasets can improve the target-language SRL performances significantly.</font>
<br>
</div>


<hr>
<div id="paper56"> <b>56. Speech Translation and the End-to-End Promise: Taking Stock of Where We Are</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.661.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title56" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Matthias Sperber, Matthias Paulik</i><br>
<font size="3">
Over its three decade history, speech translation has experienced several shifts in its primary research themes; moving from loosely coupled cascades of speech recognition and machine translation, to exploring questions of tight coupling, and finally to end-to-end models that have recently attracted much attention. This paper provides a brief survey of these developments, along with a discussion of the main challenges of traditional approaches which stem from committing to intermediate representations from the speech recognizer, and from training cascaded models separately towards different objectives. Recent end-to-end modeling techniques promise a principled way of overcoming these issues by allowing joint training of all model components and removing the need for explicit intermediate representations. However, a closer look reveals that many end-to-end models fall short of solving these issues, due to compromises made to address data scarcity. This paper provides a unifying categorization and nomenclature that covers both traditional and recent approaches and that may help researchers by highlighting both trade-offs and open research questions.</font>
<br>
</div>


<hr>
<div id="paper57"> <b>57. Hard-Coded Gaussian Attention for Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.687.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title57" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Weiqiu You, Simeng Sun, Mohit Iyyer</i><br>
<font size="3">
Recent work has questioned the importance of the Transformer’s multi-headed attention for achieving high translation quality. We push further in this direction by developing a “hard-coded” attention variant without any learned parameters. Surprisingly, replacing all learned self-attention heads in the encoder and decoder with fixed, input-agnostic Gaussian distributions minimally impacts BLEU scores across four different language pairs. However, additionally, hard-coding cross attention (which connects the decoder to the encoder) significantly lowers BLEU, suggesting that it is more important than self-attention. Much of this BLEU drop can be recovered by adding just a single learned cross attention head to an otherwise hard-coded Transformer. Taken as a whole, our results offer insight into which components of the Transformer are actually important, which we hope will guide future work into the development of simpler and more efficient attention-based models.</font>
<br>
</div>


<hr>
<div id="paper58"> <b>58. In Neural Machine Translation, What Does Transfer Learning Transfer?</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.688.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title58" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Alham Fikri Aji, Nikolay Bogoychev, Kenneth Heafield, Rico Sennrich</i><br>
<font size="3">
Transfer learning improves quality for low-resource machine translation, but it is unclear what exactly it transfers. We perform several ablation studies that limit information transfer, then measure the quality impact across three language pairs to gain a black-box understanding of transfer learning. Word embeddings play an important role in transfer learning, particularly if they are properly aligned. Although transfer learning can be performed without embeddings, results are sub-optimal. In contrast, transferring only the embeddings but nothing else yields catastrophic results. We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains. Finally, transfer learning can eliminate the need for a warm-up phase when training transformer models in high resource language pairs.</font>
<br>
</div>


<hr>
<div id="paper59"> <b>59. Learning a Multi-Domain Curriculum for Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.689.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title59" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Wei Wang, Ye Tian, Jiquan Ngiam, Yinfei Yang, Isaac Caswell, Zarana Parekh</i><br>
<font size="3">
Most data selection research in machine translation focuses on improving a single domain. We perform data selection for multiple domains at once. This is achieved by carefully introducing instance-level domain-relevance features and automatically constructing a training curriculum to gradually concentrate on multi-domain relevant and noise-reduced data batches. Both the choice of features and the use of curriculum are crucial for balancing and improving all domains, including out-of-domain. In large-scale experiments, the multi-domain curriculum simultaneously reaches or outperforms the individual performance and brings solid gains over no-curriculum training.</font>
<br>
</div>


<hr>
<div id="paper60"> <b>60. Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.690.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title60" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Danielle Saunders, Bill Byrne</i><br>
<font size="3">
Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men. In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al, 2019) Ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. Rather than attempt to create a ‘balanced’ dataset, we use transfer learning on a small set of trusted, gender-balanced examples. This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch. A known pitfall of transfer learning on new domains is ‘catastrophic forgetting’, which we address at adaptation and inference time. During adaptation we show that Elastic Weight Consolidation allows a performance trade-off between general translation quality and bias reduction. At inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky et al, 2019 on WinoMT with no degradation of general test set BLEU. We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability.</font>
<br>
</div>


<hr>
<div id="paper61"> <b>61. Translationese as a Language in “Multilingual” NMT</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.691.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title61" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Parker Riley, Isaac Caswell, Markus Freitag, David Grangier</i><br>
<font size="3">
Machine translation has an undesirable propensity to produce “translationese” artifacts, which can lead to higher BLEU scores while being liked less by human raters. Motivated by this, we model translationese and original (i.e. natural) text as separate languages in a multilingual model, and pose the question: can we perform zero-shot translation between original source text and original target text? There is no data with original source and original target, so we train a sentence-level classifier to distinguish translationese from original target text, and use this classifier to tag the training data for an NMT model. Using this technique we bias the model to produce more natural outputs at test time, yielding gains in human evaluation scores on both accuracy and fluency. Additionally, we demonstrate that it is possible to bias the model to produce translationese and game the BLEU score, increasing it while decreasing human-rated quality. We analyze these outputs using metrics measuring the degree of translationese, and present an analysis of the volatility of heuristic-based train-data tagging.</font>
<br>
</div>


<hr>
<div id="paper62"> <b>62. Using Context in Neural Machine Translation Training Objectives</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.693.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title62" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Danielle Saunders, Felix Stahlberg, Bill Byrne</i><br>
<font size="3">
We present Neural Machine Translation (NMT) training using document-level metrics with batch-level documents. Previous sequence-objective approaches to NMT training focus exclusively on sentence-level metrics like sentence BLEU which do not correspond to the desired evaluation metric, typically document BLEU. Meanwhile research into document-level NMT training focuses on data or model architecture rather than training procedure. We find that each of these lines of research has a clear space in it for the other, and propose merging them with a scheme that allows a document-level evaluation metric to be used in the NMT training objective. We first sample pseudo-documents from sentence samples. We then approximate the expected document BLEU gradient with Monte Carlo sampling for use as a cost function in Minimum Risk Training (MRT). This two-level sampling procedure gives NMT performance gains over sequence MRT and maximum-likelihood training. We demonstrate that training is more robust for document-level metrics than with sequence metrics. We further demonstrate improvements on NMT with TER and Grammatical Error Correction (GEC) using GLEU, both metrics used at the document level for evaluations.</font>
<br>
</div>


<hr>
<div id="paper63"> <b>63. Variational Neural Machine Translation with Normalizing Flows</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.694.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title63" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Hendra Setiawan, Matthias Sperber, Udhyakumar Nallasamy, Matthias Paulik</i><br>
<font size="3">
Variational Neural Machine Translation (VNMT) is an attractive framework for modeling the generation of target translations, conditioned not only on the source sentence but also on some latent random variables. The latent variable modeling may introduce useful statistical dependencies that can improve translation accuracy. Unfortunately, learning informative latent variables is non-trivial, as the latent space can be prohibitively large, and the latent codes are prone to be ignored by many translation models at training time. Previous works impose strong assumptions on the distribution of the latent code and limit the choice of the NMT architecture. In this paper, we propose to apply the VNMT framework to the state-of-the-art Transformer and introduce a more flexible approximate posterior based on normalizing flows. We demonstrate the efficacy of our proposal under both in-domain and out-of-domain conditions, significantly outperforming strong baselines.</font>
<br>
</div>


<hr>
<div id="paper64"> <b>64. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.703.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title64" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer</i><br>
<font size="3">
We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.</font>
<br>
</div>


<hr>
<div id="paper65"> <b>65. Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.731.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title65" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Po-Yao Huang, Junjie Hu, Xiaojun Chang, Alexander Hauptmann</i><br>
<font size="3">
Unsupervised machine translation (MT) has recently achieved impressive results with monolingual corpora only. However, it is still challenging to associate source-target sentences in the latent space. As people speak different languages biologically share similar visual systems, the potential of achieving better alignment through visual content is promising yet under-explored in unsupervised multimodal MT (MMT). In this paper, we investigate how to utilize visual content for disambiguation and promoting latent space alignment in unsupervised MMT. Our model employs multimodal back-translation and features pseudo visual pivoting in which we learn a shared multilingual visual-semantic embedding space and incorporate visually-pivoted captioning as additional weak supervision. The experimental results on the widely used Multi30K dataset show that the proposed model significantly improves over the state-of-the-art methods and generalizes well when images are not available at the testing time.</font>
<br>
</div>


<hr>
<div id="paper66"> <b>66. Addressing Posterior Collapse with Mutual Information for Improved Variational Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.753.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title66" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Arya D. McCarthy, Xian Li, Jiatao Gu, Ning Dong</i><br>
<font size="3">
This paper proposes a simple and effective approach to address the problem of posterior collapse in conditional variational autoencoders (CVAEs). It thus improves performance of machine translation models that use noisy or monolingual data, as well as in conventional settings. Extending Transformer and conditional VAEs, our proposed latent variable model measurably prevents posterior collapse by (1) using a modified evidence lower bound (ELBO) objective which promotes mutual information between the latent variable and the target, and (2) guiding the latent variable with an auxiliary bag-of-words prediction task. As a result, the proposed model yields improved translation quality compared to existing variational NMT models on WMT Ro↔En and De↔En. With latent variables being effectively utilized, our model demonstrates improved robustness over non-latent Transformer in handling uncertainty: exploiting noisy source-side monolingual data (up to +3.2 BLEU), and training with weakly aligned web-mined parallel data (up to +4.7 BLEU).</font>
<br>
</div>


<hr>
<div id="paper67"> <b>67. Balancing Training for Multilingual Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.754.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title67" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Xinyi Wang, Yulia Tsvetkov, Graham Neubig</i><br>
<font size="3">
When training multilingual machine translation (MT) models that can translate to/from multiple languages, we are faced with imbalanced training sets: some languages have much more training data than others. Standard practice is to up-sample less resourced languages to increase representation, and the degree of up-sampling has a large effect on the overall performance. In this paper, we propose a method that instead automatically learns how to weight training data through a data scorer that is optimized to maximize performance on all test languages. Experiments on two sets of languages under both one-to-many and many-to-one MT settings show our method not only consistently outperforms heuristic baselines in terms of average performance, but also offers flexible control over the performance of which languages are optimized.</font>
<br>
</div>


<hr>
<div id="paper68"> <b>68. Evaluating Robustness to Input Perturbations for Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.755.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title68" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Xing Niu, Prashant Mathur, Georgiana Dinu, Yaser Al-Onaizan</i><br>
<font size="3">
Neural Machine Translation (NMT) models are sensitive to small perturbations in the input. Robustness to such perturbations is typically measured using translation quality metrics such as BLEU on the noisy input. This paper proposes additional metrics which measure the relative degradation and changes in translation when small perturbations are added to the input. We focus on a class of models employing subword regularization to address robustness and perform extensive evaluations of these models using the robustness measures proposed. Results show that our proposed metrics reveal a clear trend of improved robustness to perturbations when subword regularization methods are used.</font>
<br>
</div>


<hr>
<div id="paper69"> <b>69. Regularized Context Gates on Transformer for Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-main.757.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title69" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Xintong Li, Lemao Liu, Rui Wang, Guoping Huang, Max Meng</i><br>
<font size="3">
Context gates are effective to control the contributions from the source and target contexts in the recurrent neural network (RNN) based neural machine translation (NMT). However, it is challenging to extend them into the advanced Transformer architecture, which is more complicated than RNN. This paper first provides a method to identify source and target contexts and then introduce a gate mechanism to control the source and target contributions in Transformer. In addition, to further reduce the bias problem in the gate mechanism, this paper proposes a regularization method to guide the learning of the gates with supervision automatically generated using pointwise mutual information. Extensive experiments on 4 translation datasets demonstrate that the proposed model obtains an averaged gain of 1.0 BLEU score over a strong Transformer baseline.</font>
<br>
</div>


<hr>
<div id="paper70"> <b>70. CLIReval: Evaluating Machine Translation as a Cross-Lingual Information Retrieval Task</b>  <a href="https://www.aclweb.org/anthology/2020.acl-demos.18.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title70" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. System Demonstrations<br>&nbsp;&nbsp;<i>Shuo Sun, Suzanna Sia, Kevin Duh</i><br>
<font size="3">
We present CLIReval, an easy-to-use toolkit for evaluating machine translation (MT) with the proxy task of cross-lingual information retrieval (CLIR). Contrary to what the project name might suggest, CLIReval does not actually require any annotated CLIR dataset. Instead, it automatically transforms translations and references used in MT evaluations into a synthetic CLIR dataset; it then sets up a standard search engine (Elasticsearch) and computes various information retrieval metrics (e.g., mean average precision) by treating the translations as documents to be retrieved. The idea is to gauge the quality of MT by its impact on the document translation approach to CLIR. As a case study, we run CLIReval on the “metrics shared task” of WMT2019; while this extrinsic metric is not intended to replace popular intrinsic metrics such as BLEU, results suggest CLIReval is competitive in many language pairs in terms of correlation to human judgments of quality. CLIReval is publicly available at https://github.com/ssun32/CLIReval.</font>
<br>
</div>


<hr>
<div id="paper71"> <b>71. ESPnet-ST: All-in-One Speech Translation Toolkit</b>  <a href="https://www.aclweb.org/anthology/2020.acl-demos.34.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title71" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. System Demonstrations<br>&nbsp;&nbsp;<i>Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Yalta, Tomoki Hayashi, Shinji Watanabe</i><br>
<font size="3">
We present ESPnet-ST, which is designed for the quick development of speech-to-speech translation systems in a single framework. ESPnet-ST is a new project inside end-to-end speech processing toolkit, ESPnet, which integrates or newly implements automatic speech recognition, machine translation, and text-to-speech functions for speech translation. We provide all-in-one recipes including data pre-processing, feature extraction, training, and decoding pipelines for a wide range of benchmark datasets. Our reproducible results can match or even outperform the current state-of-the-art performances; these pre-trained models are downloadable. The toolkit is publicly available at https://github.com/espnet/espnet.</font>
<br>
</div>


<hr>
<div id="paper72"> <b>72. MMPE: A Multi-Modal Interface using Handwriting, Touch Reordering, and Speech Commands for Post-Editing Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-demos.37.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title72" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. System Demonstrations<br>&nbsp;&nbsp;<i>Nico Herbig, Santanu Pal, Tim Düwel, Kalliopi Meladaki, Mahsa Monshizadeh, Vladislav Hnatovskiy, Antonio Krüger, Josef van Genabith</i><br>
<font size="3">
The shift from traditional translation to post-editing (PE) of machine-translated (MT) text can save time and reduce errors, but it also affects the design of translation interfaces, as the task changes from mainly generating text to correcting errors within otherwise helpful translation proposals. Since this paradigm shift offers potential for modalities other than mouse and keyboard, we present MMPE, the first prototype to combine traditional input modes with pen, touch, and speech modalities for PE of MT. Users can directly cross out or hand-write new text, drag and drop words for reordering, or use spoken commands to update the text in place. All text manipulations are logged in an easily interpretable format to simplify subsequent translation process research. The results of an evaluation with professional translators suggest that pen and touch interaction are suitable for deletion and reordering tasks, while speech and multi-modal combinations of select & speech are considered suitable for replacements and insertions. Overall, experiment participants were enthusiastic about the new modalities and saw them as useful extensions to mouse & keyboard, but not as a complete substitute.</font>
<br>
</div>


<hr>
<div id="paper73"> <b>73. Zero-shot North Korean to English Neural Machine Translation by Character Tokenization and Phoneme Decomposition</b>  <a href="https://www.aclweb.org/anthology/2020.acl-srw.11.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title73" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. Student Research Workshop<br>&nbsp;&nbsp;<i>Hwichan Kim, Tosho Hirasawa, Mamoru Komachi</i><br>
<font size="3">
The primary limitation of North Korean to English translation is the lack of a parallel corpus; therefore, high translation accuracy cannot be achieved. To address this problem, we propose a zero-shot approach using South Korean data, which are remarkably similar to North Korean data. We train a neural machine translation model after tokenizing a South Korean text at the character level and decomposing characters into phonemes.We demonstrate that our method can effectively learn North Korean to English translation and improve the BLEU scores by +1.01 points in comparison with the baseline.</font>
<br>
</div>


<hr>
<div id="paper74"> <b>74. Multi-Task Neural Model for Agglutinative Language Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-srw.15.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title74" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. Student Research Workshop<br>&nbsp;&nbsp;<i>Yirong Pan, Xiao Li, Yating Yang, Rui Dong</i><br>
<font size="3">
Neural machine translation (NMT) has achieved impressive performance recently by using large-scale parallel corpora. However, it struggles in the low-resource and morphologically-rich scenarios of agglutinative language translation task. Inspired by the finding that monolingual data can greatly improve the NMT performance, we propose a multi-task neural model that jointly learns to perform bi-directional translation and agglutinative language stemming. Our approach employs the shared encoder and decoder to train a single model without changing the standard NMT architecture but instead adding a token before each source-side sentence to specify the desired target outputs of the two different tasks. Experimental results on Turkish-English and Uyghur-Chinese show that our proposed approach can significantly improve the translation performance on agglutinative languages by using a small amount of monolingual data.</font>
<br>
</div>


<hr>
<div id="paper75"> <b>75. Efficient Neural Machine Translation for Low-Resource Languages via Exploiting Related Languages</b>  <a href="https://www.aclweb.org/anthology/2020.acl-srw.22.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title75" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. Student Research Workshop<br>&nbsp;&nbsp;<i>Vikrant Goyal, Sourav Kumar, Dipti Misra Sharma</i><br>
<font size="3">
A large percentage of the world’s population speaks a language of the Indian subcontinent, comprising languages from both Indo-Aryan (e.g. Hindi, Punjabi, Gujarati, etc.) and Dravidian (e.g. Tamil, Telugu, Malayalam, etc.) families. A universal characteristic of Indian languages is their complex morphology, which, when combined with the general lack of sufficient quantities of high-quality parallel data, can make developing machine translation (MT) systems for these languages difficult. Neural Machine Translation (NMT) is a rapidly advancing MT paradigm and has shown promising results for many language pairs, especially in large training data scenarios. Since the condition of large parallel corpora is not met for Indian-English language pairs, we present our efforts towards building efficient NMT systems between Indian languages (specifically Indo-Aryan languages) and English via efficiently exploiting parallel data from the related languages. We propose a technique called Unified Transliteration and Subword Segmentation to leverage language similarity while exploiting parallel data from related language pairs. We also propose a Multilingual Transfer Learning technique to leverage parallel data from multiple related languages to assist translation for low resource language pair of interest. Our experiments demonstrate an overall average improvement of 5 BLEU points over the standard Transformer-based NMT baselines.</font>
<br>
</div>


<hr>
<div id="paper76"> <b>76. Pre-training via Leveraging Assisting Languages for Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-srw.37.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title76" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. Student Research Workshop<br>&nbsp;&nbsp;<i>Haiyue Song, Raj Dabre, Zhuoyuan Mao, Fei Cheng, Sadao Kurohashi, Eiichiro Sumita</i><br>
<font size="3">
Sequence-to-sequence (S2S) pre-training using large monolingual data is known to improve performance for various S2S NLP tasks. However, large monolingual corpora might not always be available for the languages of interest (LOI). Thus, we propose to exploit monolingual corpora of other languages to complement the scarcity of monolingual corpora for the LOI. We utilize script mapping (Chinese to Japanese) to increase the similarity (number of cognates) between the monolingual corpora of helping languages and LOI. An empirical case study of low-resource Japanese-English neural machine translation (NMT) reveals that leveraging large Chinese and French monolingual corpora can help overcome the shortage of Japanese and English monolingual corpora, respectively, for S2S pre-training. Using only Chinese and French monolingual corpora, we were able to improve Japanese-English translation quality by up to 8.5 BLEU in low-resource scenarios.</font>
<br>
</div>


<hr>
<div id="paper77"> <b>77. Checkpoint Reranking: An Approach to Select Better Hypothesis for Neural Machine Translation Systems</b>  <a href="https://www.aclweb.org/anthology/2020.acl-srw.38.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title77" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. Student Research Workshop<br>&nbsp;&nbsp;<i>Vinay Pandramish, Dipti Misra Sharma</i><br>
<font size="3">
In this paper, we propose a method of re-ranking the outputs of Neural Machine Translation (NMT) systems. After the decoding process, we select a few last iteration outputs in the training process as the N-best list. After training a Neural Machine Translation (NMT) baseline system, it has been observed that these iteration outputs have an oracle score higher than baseline up to 1.01 BLEU points compared to the last iteration of the trained system.We come up with a ranking mechanism by solely focusing on the decoder’s ability to generate distinct tokens and without the usage of any language model or data. With this method, we achieved a translation improvement up to +0.16 BLEU points over baseline.We also evaluate our approach by applying the coverage penalty to the training process.In cases of moderate coverage penalty, the oracle scores are higher than the final iteration up to +0.99 BLEU points, and our algorithm gives an improvement up to +0.17 BLEU points.With excessive penalty, there is a decrease in translation quality compared to the baseline system. Still, an increase in oracle scores up to +1.30 is observed with the re-ranking algorithm giving an improvement up to +0.15 BLEU points is found in case of excessive penalty.The proposed re-ranking method is a generic one and can be extended to other language pairs as well.</font>
<br>
</div>


<hr>
<div id="paper78"> <b>78. Compositional Generalization by Factorizing Alignment and Translation</b>  <a href="https://www.aclweb.org/anthology/2020.acl-srw.42.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title78" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. Student Research Workshop<br>&nbsp;&nbsp;<i>Jacob Russin, Jason Jo, Randall O’Reilly, Yoshua Bengio</i><br>
<font size="3">
Standard methods in deep learning for natural language processing fail to capture the compositional structure of human language that allows for systematic generalization outside of the training distribution. However, human learners readily generalize in this way, e.g. by applying known grammatical rules to novel words. Inspired by work in cognitive science suggesting a functional distinction between systems for syntactic and semantic processing, we implement a modification to an existing approach in neural machine translation, imposing an analogous separation between alignment and translation. The resulting architecture substantially outperforms standard recurrent networks on the SCAN dataset, a compositional generalization task, without any additional supervision. Our work suggests that learning to align and to translate in separate modules may be a useful heuristic for capturing compositional structure.</font>
<br>
</div>


<hr>
<div id="paper79"> <b>79. Proceedings of the First Workshop on Automatic Simultaneous Translation</b>  <a href="https://www.aclweb.org/anthology/2020.autosimtrans-1.0.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title79" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the First Workshop on Automatic Simultaneous Translation<br>&nbsp;&nbsp;<i>Hua Wu, Collin Cherry, Liang Huang, Zhongjun He, Mark Liberman, James Cross, Yang Liu</i><br>
<font size="3">
 </font>
<br>
</div>


<hr>
<div id="paper80"> <b>80. Dynamic Sentence Boundary Detection for Simultaneous Translation</b>  <a href="https://www.aclweb.org/anthology/2020.autosimtrans-1.1.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title80" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the First Workshop on Automatic Simultaneous Translation<br>&nbsp;&nbsp;<i>Ruiqing Zhang, Chuanqiang Zhang</i><br>
<font size="3">
Simultaneous Translation is a great challenge in which translation starts before the source sentence finished. Most studies take transcription as input and focus on balancing translation quality and latency for each sentence. However, most ASR systems can not provide accurate sentence boundaries in realtime. Thus it is a key problem to segment sentences for the word streaming before translation. In this paper, we propose a novel method for sentence boundary detection that takes it as a multi-class classification task under the end-to-end pre-training framework. Experiments show significant improvements both in terms of translation quality and latency.</font>
<br>
</div>


<hr>
<div id="paper81"> <b>81. End-to-End Speech Translation with Adversarial Training</b>  <a href="https://www.aclweb.org/anthology/2020.autosimtrans-1.2.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title81" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the First Workshop on Automatic Simultaneous Translation<br>&nbsp;&nbsp;<i>Xuancai Li, Chen Kehai, Tiejun Zhao, Muyun Yang</i><br>
<font size="3">
End-to-End speech translation usually leverages audio-to-text parallel data to train an available speech translation model which has shown impressive results on various speech translation tasks. Due to the artificial cost of collecting audio-to-text parallel data, the speech translation is a natural low-resource translation scenario, which greatly hinders its improvement. In this paper, we proposed a new adversarial training method to leverage target monolingual data to relieve the low-resource shortcoming of speech translation. In our method, the existing speech translation model is considered as a Generator to gain a target language output, and another neural Discriminator is used to guide the distinction between outputs of speech translation model and true target monolingual sentences. Experimental results on the CCMT 2019-BSTC dataset speech translation task demonstrate that the proposed methods can significantly improve the performance of the End-to-End speech translation system.</font>
<br>
</div>


<hr>
<div id="paper82"> <b>82. Robust Neural Machine Translation with ASR Errors</b>  <a href="https://www.aclweb.org/anthology/2020.autosimtrans-1.3.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title82" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the First Workshop on Automatic Simultaneous Translation<br>&nbsp;&nbsp;<i>Haiyang Xue, Yang Feng, Shuhao Gu, Wei Chen</i><br>
<font size="3">
In many practical applications, neural machine translation systems have to deal with the input from automatic speech recognition (ASR) systems which may contain a certain number of errors. This leads to two problems which degrade translation performance. One is the discrepancy between the training and testing data and the other is the translation error caused by the input errors may ruin the whole translation. In this paper, we propose a method to handle the two problems so as to generate robust translation to ASR errors. First, we simulate ASR errors in the training data so that the data distribution in the training and test is consistent. Second, we focus on ASR errors on homophone words and words with similar pronunciation and make use of their pronunciation information to help the translation model to recover from the input errors. Experiments on two Chinese-English data sets show that our method is more robust to input errors and can outperform the strong Transformer baseline significantly.</font>
<br>
</div>


<hr>
<div id="paper83"> <b>83. Modeling Discourse Structure for Document-level Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.autosimtrans-1.5.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title83" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the First Workshop on Automatic Simultaneous Translation<br>&nbsp;&nbsp;<i>Junxuan Chen, Xiang Li, Jiarui Zhang, Chulun Zhou, Jianwei Cui, Bin Wang, Jinsong Su</i><br>
<font size="3">
Recently, document-level neural machine translation (NMT) has become a hot topic in the community of machine translation. Despite its success, most of existing studies ignored the discourse structure information of the input document to be translated, which has shown effective in other tasks. In this paper, we propose to improve document-level NMT with the aid of discourse structure information. Our encoder is based on a hierarchical attention network (HAN) (Miculicich et al., 2018). Specifically, we first parse the input document to obtain its discourse structure. Then, we introduce a Transformer-based path encoder to embed the discourse structure information of each word. Finally, we combine the discourse structure information with the word embedding before it is fed into the encoder. Experimental results on the English-to-German dataset show that our model can significantly outperform both Transformer and Transformer+HAN.</font>
<br>
</div>


<hr>
<div id="paper84"> <b>84. Proceedings of the 17th International Conference on Spoken Language Translation</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.0.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title84" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Marcello Federico, Alex Waibel, Kevin Knight, Satoshi Nakamura, Hermann Ney, Jan Niehues, Sebastian Stüker, Dekai Wu, Joseph Mariani, Francois Yvon</i><br>
<font size="3">
 </font>
<br>
</div>


<hr>
<div id="paper85"> <b>85. ON-TRAC Consortium for End-to-End and Simultaneous Speech Translation Challenge Tasks at IWSLT 2020</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.2.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title85" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Maha Elbayad, Ha Nguyen, Fethi Bougares, Natalia Tomashenko, Antoine Caubrière, Benjamin Lecouteux, Yannick Estève, Laurent Besacier</i><br>
<font size="3">
This paper describes the ON-TRAC Consortium translation systems developed for two challenge tracks featured in the Evaluation Campaign of IWSLT 2020, offline speech translation and simultaneous speech translation. ON-TRAC Consortium is composed of researchers from three French academic laboratories: LIA (Avignon Université), LIG (Université Grenoble Alpes), and LIUM (Le Mans Université). Attention-based encoder-decoder models, trained end-to-end, were used for our submissions to the offline speech translation track. Our contributions focused on data augmentation and ensembling of multiple models. In the simultaneous speech translation track, we build on Transformer-based wait-k models for the text-to-text subtask. For speech-to-text simultaneous translation, we attach a wait-k MT system to a hybrid ASR system. We propose an algorithm to control the latency of the ASR+MT cascade and achieve a good latency-quality trade-off on both subtasks.</font>
<br>
</div>


<hr>
<div id="paper86"> <b>86. Start-Before-End and End-to-End: Neural Speech Translation by AppTek and RWTH Aachen University</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.3.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title86" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Parnia Bahar, Patrick Wilken, Tamer Alkhouli, Andreas Guta, Pavel Golik, Evgeny Matusov, Christian Herold</i><br>
<font size="3">
AppTek and RWTH Aachen University team together to participate in the offline and simultaneous speech translation tracks of IWSLT 2020. For the offline task, we create both cascaded and end-to-end speech translation systems, paying attention to careful data selection and weighting. In the cascaded approach, we combine high-quality hybrid automatic speech recognition (ASR) with the Transformer-based neural machine translation (NMT). Our end-to-end direct speech translation systems benefit from pretraining of adapted encoder and decoder components, as well as synthetic data and fine-tuning and thus are able to compete with cascaded systems in terms of MT quality. For simultaneous translation, we utilize a novel architecture that makes dynamic decisions, learned from parallel data, to determine when to continue feeding on input or generate output words. Experiments with speech and text input show that even at low latency this architecture leads to superior translation results.</font>
<br>
</div>


<hr>
<div id="paper87"> <b>87. KIT’s IWSLT 2020 SLT Translation System</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.4.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title87" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Ngoc-Quan Pham, Felix Schneider, Tuan-Nam Nguyen, Thanh-Le Ha, Thai Son Nguyen, Maximilian Awiszus, Sebastian Stüker, Alexander Waibel</i><br>
<font size="3">
This paper describes KIT’s submissions to the IWSLT2020 Speech Translation evaluation campaign. We first participate in the simultaneous translation task, in which our simultaneous models are Transformer based and can be efficiently trained to obtain low latency with minimized compromise in quality. On the offline speech translation task, we applied our new Speech Transformer architecture to end-to-end speech translation. The obtained model can provide translation quality which is competitive to a complicated cascade. The latter still has the upper hand, thanks to the ability to transparently access to the transcription, and resegment the inputs to avoid fragmentation.</font>
<br>
</div>


<hr>
<div id="paper88"> <b>88. End-to-End Simultaneous Translation System for IWSLT2020 Using Modality Agnostic Meta-Learning</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.5.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title88" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Hou Jeung Han, Mohd Abbas Zaidi, Sathish Reddy Indurthi, Nikhil Kumar Lakumarapu, Beomseok Lee, Sangha Kim</i><br>
<font size="3">
In this paper, we describe end-to-end simultaneous speech-to-text and text-to-text translation systems submitted to IWSLT2020 online translation challenge. The systems are built by adding wait-k and meta-learning approaches to the Transformer architecture. The systems are evaluated on different latency regimes. The simultaneous text-to-text translation achieved a BLEU score of 26.38 compared to the competition baseline score of 14.17 on the low latency regime (Average latency ≤ 3). The simultaneous speech-to-text system improves the BLEU score by 7.7 points over the competition baseline for the low latency regime (Average Latency ≤ 1000).</font>
<br>
</div>


<hr>
<div id="paper89"> <b>89. DiDi Labs’ End-to-end System for the IWSLT 2020 Offline Speech TranslationTask</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.6.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title89" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Arkady Arkhangorodsky, Yiqi Huang, Amittai Axelrod</i><br>
<font size="3">
This paper describes the system that was submitted by DiDi Labs to the offline speech translation task for IWSLT 2020. We trained an end-to-end system that translates audio from English TED talks to German text, without producing intermediate English text. We use the S-Transformer architecture and train using the MuSTC dataset. We also describe several additional experiments that were attempted, but did not yield improved results.</font>
<br>
</div>


<hr>
<div id="paper90"> <b>90. End-to-End Offline Speech Translation System for IWSLT 2020 using Modality Agnostic Meta-Learning</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.7.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title90" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Nikhil Kumar Lakumarapu, Beomseok Lee, Sathish Reddy Indurthi, Hou Jeung Han, Mohd Abbas Zaidi, Sangha Kim</i><br>
<font size="3">
In this paper, we describe the system submitted to the IWSLT 2020 Offline Speech Translation Task. We adopt the Transformer architecture coupled with the meta-learning approach to build our end-to-end Speech-to-Text Translation (ST) system. Our meta-learning approach tackles the data scarcity of the ST task by leveraging the data available from Automatic Speech Recognition (ASR) and Machine Translation (MT) tasks. The meta-learning approach combined with synthetic data augmentation techniques improves the model performance significantly and achieves BLEU scores of 24.58, 27.51, and 27.61 on IWSLT test 2015, MuST-C test, and Europarl-ST test sets respectively.</font>
<br>
</div>


<hr>
<div id="paper91"> <b>91. End-to-End Speech-Translation with Knowledge Distillation: FBK@IWSLT2020</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.8.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title91" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Marco Gaido, Mattia A. Di Gangi, Matteo Negri, Marco Turchi</i><br>
<font size="3">
This paper describes FBK’s participation in the IWSLT 2020 offline speech translation (ST) task. The task evaluates systems’ ability to translate English TED talks audio into German texts. The test talks are provided in two versions: one contains the data already segmented with automatic tools and the other is the raw data without any segmentation. Participants can decide whether to work on custom segmentation or not. We used the provided segmentation. Our system is an end-to-end model based on an adaptation of the Transformer for speech data. Its training process is the main focus of this paper and it is based on: i) transfer learning (ASR pretraining and knowledge distillation), ii) data augmentation (SpecAugment, time stretch and synthetic data), iii)combining synthetic and real data marked as different domains, and iv) multi-task learning using the CTC loss. Finally, after the training with word-level knowledge distillation is complete, our ST models are fine-tuned using label smoothed cross entropy. Our best model scored 29 BLEU on the MuST-CEn-De test set, which is an excellent result compared to recent papers, and 23.7 BLEU on the same data segmented with VAD, showing the need for researching solutions addressing this specific data condition.</font>
<br>
</div>


<hr>
<div id="paper92"> <b>92. SRPOL’s System for the IWSLT 2020 End-to-End Speech Translation Task</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.9.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title92" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Tomasz Potapczyk, Pawel Przybysz</i><br>
<font size="3">
We took part in the offline End-to-End English to German TED lectures translation task. We based our solution on our last year’s submission. We used a slightly altered Transformer architecture with ResNet-like convolutional layer preparing the audio input to Transformer encoder. To improve the model’s quality of translation we introduced two regularization techniques and trained on machine translated Librispeech corpus in addition to iwslt-corpus, TEDLIUM2 andMust_C corpora. Our best model scored almost 3 BLEU higher than last year’s model. To segment 2020 test set we used exactly the same procedure as last year.</font>
<br>
</div>


<hr>
<div id="paper93"> <b>93. The University of Helsinki Submission to the IWSLT2020 Offline SpeechTranslation Task</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.10.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title93" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Raúl Vázquez, Mikko Aulamo, Umut Sulubacak, Jörg Tiedemann</i><br>
<font size="3">
This paper describes the University of Helsinki Language Technology group’s participation in the IWSLT 2020 offline speech translation task, addressing the translation of English audio into German text. In line with this year’s task objective, we train both cascade and end-to-end systems for spoken language translation. We opt for an end-to-end multitasking architecture with shared internal representations and a cascade approach that follows a standard procedure consisting of ASR, correction, and MT stages. We also describe the experiments that served as a basis for the submitted systems. Our experiments reveal that multitasking training with shared internal representations is not only possible but allows for knowledge-transfer across modalities.</font>
<br>
</div>


<hr>
<div id="paper94"> <b>94. LIT Team’s System Description for Japanese-Chinese Machine Translation Task in IWSLT 2020</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.12.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title94" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Yimeng Zhuang, Yuan Zhang, Lijie Wang</i><br>
<font size="3">
This paper describes the LIT Team’s submission to the IWSLT2020 open domain translation task, focusing primarily on Japanese-to-Chinese translation direction. Our system is based on the organizers’ baseline system, but we do more works on improving the Transform baseline system by elaborate data pre-processing. We manage to obtain significant improvements, and this paper aims to share some data processing experiences in this translation task. Large-scale back-translation on monolingual corpus is also investigated. In addition, we also try shared and exclusive word embeddings, compare different granularity of tokens like sub-word level. Our Japanese-to-Chinese translation system achieves a performance of BLEU=34.0 and ranks 2nd among all participating systems.</font>
<br>
</div>


<hr>
<div id="paper95"> <b>95. OPPO’s Machine Translation System for the IWSLT 2020 Open Domain Translation Task</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.13.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title95" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Qian Zhang, Xiaopu Li, Dawei Dang, Tingxun Shi, Di Ai, Zhengshan Xue, Jie Hao</i><br>
<font size="3">
In this paper, we demonstrate our machine translation system applied for the Chinese-Japanese bidirectional translation task (aka. open domain translation task) for the IWSLT 2020. Our model is based on Transformer (Vaswani et al., 2017), with the help of many popular, widely proved effective data preprocessing and augmentation methods. Experiments show that these methods can improve the baseline model steadily and significantly.</font>
<br>
</div>


<hr>
<div id="paper96"> <b>96. Character Mapping and Ad-hoc Adaptation: Edinburgh’s IWSLT 2020 Open Domain Translation System</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.14.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title96" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Pinzhen Chen, Nikolay Bogoychev, Ulrich Germann</i><br>
<font size="3">
This paper describes the University of Edinburgh’s neural machine translation systems submitted to the IWSLT 2020 open domain Japanese↔Chinese translation task. On top of commonplace techniques like tokenisation and corpus cleaning, we explore character mapping and unsupervised decoding-time adaptation. Our techniques focus on leveraging the provided data, and we show the positive impact of each technique through the gradual improvement of BLEU.</font>
<br>
</div>


<hr>
<div id="paper97"> <b>97. CASIA’s System for IWSLT 2020 Open Domain Translation</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.15.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title97" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Qian Wang, Yuchen Liu, Cong Ma, Yu Lu, Yining Wang, Long Zhou, Yang Zhao, Jiajun Zhang, Chengqing Zong</i><br>
<font size="3">
This paper describes the CASIA’s system for the IWSLT 2020 open domain translation task. This year we participate in both Chinese→Japanese and Japanese→Chinese translation tasks. Our system is neural machine translation system based on Transformer model. We augment the training data with knowledge distillation and back translation to improve the translation performance. Domain data classification and weighted domain model ensemble are introduced to generate the final translation result. We compare and analyze the performance on development data with different model settings and different data processing techniques.</font>
<br>
</div>


<hr>
<div id="paper98"> <b>98. Deep Blue Sonics’ Submission to IWSLT 2020 Open Domain Translation Task</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.16.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title98" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Enmin Su, Yi Ren</i><br>
<font size="3">
We present in this report our submission to IWSLT 2020 Open Domain Translation Task. We built a data pre-processing pipeline to efficiently handle large noisy web-crawled corpora, which boosts the BLEU score of a widely used transformer model in this translation task. To tackle the open-domain nature of this task, back- translation is applied to further improve the translation performance.</font>
<br>
</div>


<hr>
<div id="paper99"> <b>99. University of Tsukuba’s Machine Translation System for IWSLT20 Open Domain Translation Task</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.17.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title99" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Hongyi Cui, Yizhen Wei, Shohei Iida, Takehito Utsuro, Masaaki Nagata</i><br>
<font size="3">
In this paper, we introduce University of Tsukuba’s submission to the IWSLT20 Open Domain Translation Task. We participate in both Chinese→Japanese and Japanese→Chinese directions. For both directions, our machine translation systems are based on the Transformer architecture. Several techniques are integrated in order to boost the performance of our models: data filtering, large-scale noised training, model ensemble, reranking and postprocessing. Consequently, our efforts achieve 33.0 BLEU scores for Chinese→Japanese translation and 32.3 BLEU scores for Japanese→Chinese translation.</font>
<br>
</div>


<hr>
<div id="paper100"> <b>100. Xiaomi’s Submissions for IWSLT 2020 Open Domain Translation Task</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.18.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title100" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Yuhui Sun, Mengxue Guo, Xiang Li, Jianwei Cui, Bin Wang</i><br>
<font size="3">
This paper describes the Xiaomi’s submissions to the IWSLT20 shared open domain translation task for Chinese<->Japanese language pair. We explore different model ensembling strategies based on recent Transformer variants. We also further strengthen our systems via some effective techniques, such as data filtering, data selection, tagged back translation, domain adaptation, knowledge distillation, and re-ranking. Our resulting Chinese->Japanese primary system ranked second in terms of character-level BLEU score among all submissions. Our resulting Japanese->Chinese primary system also achieved a competitive performance.</-></font>
<br>
</div>


<hr>
<div id="paper101"> <b>101. ISTIC’s Neural Machine Translation System for IWSLT’2020</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.19.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title101" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>jiaze wei, wenbin liu, zhenfeng wu, you pan, yanqing he</i><br>
<font size="3">
This paper introduces technical details of machine translation system of Institute of Scientific and Technical Information of China (ISTIC) for the 17th International Conference on Spoken Language Translation (IWSLT 2020). ISTIC participated in both translation tasks of the Open Domain Translation track: Japanese-to-Chinese MT task and Chinese-to-Japanese MT task. The paper mainly elaborates on the model framework, data preprocessing methods and decoding strategies adopted in our system. In addition, the system performance on the development set are given under different settings.</font>
<br>
</div>


<hr>
<div id="paper102"> <b>102. Octanove Labs’ Japanese-Chinese Open Domain Translation System</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.20.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title102" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Masato Hagiwara</i><br>
<font size="3">
This paper describes Octanove Labs’ submission to the IWSLT 2020 open domain translation challenge. In order to build a high-quality Japanese-Chinese neural machine translation (NMT) system, we use a combination of 1) parallel corpus filtering and 2) back-translation. We have shown that, by using heuristic rules and learned classifiers, the size of the parallel data can be reduced by 70% to 90% without much impact on the final MT performance. We have also shown that including the artificially generated parallel data through back-translation further boosts the metric by 17% to 27%, while self-training contributes little. Aside from a small number of parallel sentences annotated for filtering, no external resources have been used to build our system.</font>
<br>
</div>


<hr>
<div id="paper103"> <b>103. NAIST’s Machine Translation Systems for IWSLT 2020 Conversational Speech Translation Task</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.21.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title103" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Ryo Fukuda, Katsuhito Sudoh, Satoshi Nakamura</i><br>
<font size="3">
This paper describes NAIST’s NMT system submitted to the IWSLT 2020 conversational speech translation task. We focus on the translation disfluent speech transcripts that include ASR errors and non-grammatical utterances. We tried a domain adaptation method by transferring the styles of out-of-domain data (United Nations Parallel Corpus) to be like in-domain data (Fisher transcripts). Our system results showed that the NMT model with domain adaptation outperformed a baseline. In addition, slight improvement by the style transfer was observed.</font>
<br>
</div>


<hr>
<div id="paper104"> <b>104. Generating Fluent Translations from Disfluent Text Without Access to Fluent References: IIT Bombay@IWSLT2020</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.22.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title104" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Nikhil Saini, Jyotsana Khatri, Preethi Jyothi, Pushpak Bhattacharyya</i><br>
<font size="3">
Machine translation systems perform reasonably well when the input is well-formed speech or text. Conversational speech is spontaneous and inherently consists of many disfluencies. Producing fluent translations of disfluent source text would typically require parallel disfluent to fluent training data. However, fluent translations of spontaneous speech are an additional resource that is tedious to obtain. This work describes the submission of IIT Bombay to the Conversational Speech Translation challenge at IWSLT 2020. We specifically tackle the problem of disfluency removal in disfluent-to-fluent text-to-text translation assuming no access to fluent references during training. Common patterns of disfluency are extracted from disfluent references and a noise induction model is used to simulate them starting from a clean monolingual corpus. This synthetically constructed dataset is then considered as a proxy for labeled data during training. We also make use of additional fluent text in the target language to help generate fluent translations. This work uses no fluent references during training and beats a baseline model by a margin of 4.21 and 3.11 BLEU points where the baseline uses disfluent and fluent references, respectively. Index Terms- disfluency removal, machine translation, noise induction, leveraging monolingual data, denoising for disfluency removal.</font>
<br>
</div>


<hr>
<div id="paper105"> <b>105. The HW-TSC Video Speech Translation System at IWSLT 2020</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.23.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title105" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Minghan Wang, Hao Yang, Yao Deng, Ying Qin, Lizhi Lei, Daimeng Wei, Hengchao Shang, Ning Xie, Xiaochun Li, Jiaxian Guo</i><br>
<font size="3">
The paper presents details of our system in the IWSLT Video Speech Translation evaluation. The system works in a cascade form, which contains three modules: 1) A proprietary ASR system. 2) A disfluency correction system aims to remove interregnums or other disfluent expressions with a fine-tuned BERT and a series of rule-based algorithms. 3) An NMT System based on the Transformer and trained with massive publicly available corpus.</font>
<br>
</div>


<hr>
<div id="paper106"> <b>106. ELITR Non-Native Speech Translation at IWSLT 2020</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.25.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title106" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Dominik Macháček, Jonáš Kratochvíl, Sangeet Sagar, Matúš Žilinec, Ondřej Bojar, Thai-Son Nguyen, Felix Schneider, Philip Williams, Yuekun Yao</i><br>
<font size="3">
This paper is an ELITR system submission for the non-native speech translation task at IWSLT 2020. We describe systems for offline ASR, real-time ASR, and our cascaded approach to offline SLT and real-time SLT. We select our primary candidates from a pool of pre-existing systems, develop a new end-to-end general ASR system, and a hybrid ASR trained on non-native speech. The provided small validation set prevents us from carrying out a complex validation, but we submit all the unselected candidates for contrastive evaluation on the test set.</font>
<br>
</div>


<hr>
<div id="paper107"> <b>107. Is 42 the Answer to Everything in Subtitling-oriented Speech Translation?</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.26.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title107" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Alina Karakanta, Matteo Negri, Marco Turchi</i><br>
<font size="3">
Subtitling is becoming increasingly important for disseminating information, given the enormous amounts of audiovisual content becoming available daily. Although Neural Machine Translation (NMT) can speed up the process of translating audiovisual content, large manual effort is still required for transcribing the source language, and for spotting and segmenting the text into proper subtitles. Creating proper subtitles in terms of timing and segmentation highly depends on information present in the audio (utterance duration, natural pauses). In this work, we explore two methods for applying Speech Translation (ST) to subtitling, a) a direct end-to-end and b) a classical cascade approach. We discuss the benefit of having access to the source language speech for improving the conformity of the generated subtitles to the spatial and temporal subtitling constraints and show that length is not the answer to everything in the case of subtitling-oriented ST.</font>
<br>
</div>


<hr>
<div id="paper108"> <b>108. Re-translation versus Streaming for Simultaneous Translation</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.27.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title108" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Naveen Arivazhagan, Colin Cherry, Wolfgang Macherey, George Foster</i><br>
<font size="3">
There has been great progress in improving streaming machine translation, a simultaneous paradigm where the system appends to a growing hypothesis as more source content becomes available. We study a related problem in which revisions to the hypothesis beyond strictly appending words are permitted. This is suitable for applications such as live captioning an audio feed. In this setting, we compare custom streaming approaches to re-translation, a straightforward strategy where each new source token triggers a distinct translation from scratch. We find re-translation to be as good or better than state-of-the-art streaming systems, even when operating under constraints that allow very few revisions. We attribute much of this success to a previously proposed data-augmentation technique that adds prefix-pairs to the training data, which alongside wait-k inference forms a strong baseline for streaming translation. We also highlight re-translation’s ability to wrap arbitrarily powerful MT systems with an experiment showing large improvements from an upgrade to its base model.</font>
<br>
</div>


<hr>
<div id="paper109"> <b>109. Towards Stream Translation: Adaptive Computation Time for Simultaneous Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.28.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title109" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Felix Schneider, Alexander Waibel</i><br>
<font size="3">
Simultaneous machine translation systems rely on a policy to schedule read and write operations in order to begin translating a source sentence before it is complete. In this paper, we demonstrate the use of Adaptive Computation Time (ACT) as an adaptive, learned policy for simultaneous machine translation using the transformer model and as a more numerically stable alternative to Monotonic Infinite Lookback Attention (MILk). We achieve state-of-the-art results in terms of latency-quality tradeoffs. We also propose a method to use our model on unsegmented input, i.e. without sentence boundaries, simulating the condition of translating output from automatic speech recognition. We present first benchmark results on this task.</font>
<br>
</div>


<hr>
<div id="paper110"> <b>110. Neural Simultaneous Speech Translation Using Alignment-Based Chunking</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.29.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title110" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Patrick Wilken, Tamer Alkhouli, Evgeny Matusov, Pavel Golik</i><br>
<font size="3">
In simultaneous machine translation, the objective is to determine when to produce a partial translation given a continuous stream of source words, with a trade-off between latency and quality. We propose a neural machine translation (NMT) model that makes dynamic decisions when to continue feeding on input or generate output words. The model is composed of two main components: one to dynamically decide on ending a source chunk, and another that translates the consumed chunk. We train the components jointly and in a manner consistent with the inference conditions. To generate chunked training data, we propose a method that utilizes word alignment while also preserving enough context. We compare models with bidirectional and unidirectional encoders of different depths, both on real speech and text input. Our results on the IWSLT 2020 English-to-German task outperform a wait-k baseline by 2.6 to 3.7% BLEU absolute.</font>
<br>
</div>


<hr>
<div id="paper111"> <b>111. From Speech-to-Speech Translation to Automatic Dubbing</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.31.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title111" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Marcello Federico, Robert Enyedi, Roberto Barra-Chicote, Ritwik Giri, Umut Isik, Arvindh Krishnaswamy, Hassan Sawaf</i><br>
<font size="3">
We present enhancements to a speech-to-speech translation pipeline in order to perform automatic dubbing. Our architecture features neural machine translation generating output of preferred length, prosodic alignment of the translation with the original speech segments, neural text-to-speech with fine tuning of the duration of each utterance, and, finally, audio rendering to enriches text-to-speech output with background noise and reverberation extracted from the original audio. We report and discuss results of a first subjective evaluation of automatic dubbing of excerpts of TED Talks from English into Italian, which measures the perceived naturalness of automatic dubbing and the relative importance of each proposed enhancement.</font>
<br>
</div>


<hr>
<div id="paper112"> <b>112. Joint Translation and Unit Conversion for End-to-end Localization</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.32.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title112" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Georgiana Dinu, Prashant Mathur, Marcello Federico, Stanislas Lauly, Yaser Al-Onaizan</i><br>
<font size="3">
A variety of natural language tasks require processing of textual data which contains a mix of natural language and formal languages such as mathematical expressions. In this paper, we take unit conversions as an example and propose a data augmentation technique which lead to models learning both translation and conversion tasks as well as how to adequately switch between them for end-to-end localization.</font>
<br>
</div>


<hr>
<div id="paper113"> <b>113. How Human is Machine Translationese? Comparing Human and Machine Translations of Text and Speech</b>  <a href="https://www.aclweb.org/anthology/2020.iwslt-1.34.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title113" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the 17th International Conference on Spoken Language Translation<br>&nbsp;&nbsp;<i>Yuri Bizzoni, Tom S Juzek, Cristina España-Bonet, Koel Dutta Chowdhury, Josef van Genabith, Elke Teich</i><br>
<font size="3">
Translationese is a phenomenon present in human translations, simultaneous interpreting, and even machine translations. Some translationese features tend to appear in simultaneous interpreting with higher frequency than in human text translation, but the reasons for this are unclear. This study analyzes translationese patterns in translation, interpreting, and machine translation outputs in order to explore possible reasons. In our analysis we – (i) detail two non-invasive ways of detecting translationese and (ii) compare translationese across human and machine translations from text and speech. We find that machine translation shows traces of translationese, but does not reproduce the patterns found in human translation, offering support to the hypothesis that such patterns are due to the model (human vs machine) rather than to the data (written vs spoken).</font>
<br>
</div>


<hr>
<div id="paper114"> <b>114. Proceedings of the Fourth Workshop on Neural Generation and Translation</b>  <a href="https://www.aclweb.org/anthology/2020.ngt-1.0.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title114" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the Fourth Workshop on Neural Generation and Translation<br>&nbsp;&nbsp;<i>Alexandra Birch, Andrew Finch, Hiroaki Hayashi, Kenneth Heafield, Marcin Junczys-Dowmunt, Ioannis Konstas, Xian Li, Graham Neubig, Yusuke Oda</i><br>
<font size="3">
 </font>
<br>
</div>


<hr>
<div id="paper115"> <b>115. Findings of the Fourth Workshop on Neural Generation and Translation</b>  <a href="https://www.aclweb.org/anthology/2020.ngt-1.1.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title115" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the Fourth Workshop on Neural Generation and Translation<br>&nbsp;&nbsp;<i>Kenneth Heafield, Hiroaki Hayashi, Yusuke Oda, Ioannis Konstas, Andrew Finch, Graham Neubig, Xian Li, Alexandra Birch</i><br>
<font size="3">
We describe the finding of the Fourth Workshop on Neural Generation and Translation, held in concert with the annual conference of the Association for Computational Linguistics (ACL 2020). First, we summarize the research trends of papers presented in the proceedings. Second, we describe the results of the three shared tasks 1) efficient neural machine translation (NMT) where participants were tasked with creating NMT systems that are both accurate and efficient, and 2) document-level generation and translation (DGT) where participants were tasked with developing systems that generate summaries from structured data, potentially with assistance from text in another language and 3) STAPLE task: creation of as many possible translations of a given input text. This last shared task was organised by Duolingo.</font>
<br>
</div>


<hr>
<div id="paper116"> <b>116. Compressing Neural Machine Translation Models with 4-bit Precision</b>  <a href="https://www.aclweb.org/anthology/2020.ngt-1.4.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title116" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the Fourth Workshop on Neural Generation and Translation<br>&nbsp;&nbsp;<i>Alham Fikri Aji, Kenneth Heafield</i><br>
<font size="3">
Neural Machine Translation (NMT) is resource-intensive. We design a quantization procedure to compress fit NMT models better for devices with limited hardware capability. We use logarithmic quantization, instead of the more commonly used fixed-point quantization, based on the empirical fact that parameters distribution is not uniform. We find that biases do not take a lot of memory and show that biases can be left uncompressed to improve the overall quality without affecting the compression rate. We also propose to use an error-feedback mechanism during retraining, to preserve the compressed model as a stale gradient. We empirically show that NMT models based on Transformer or RNN architecture can be compressed up to 4-bit precision without any noticeable quality degradation. Models can be compressed up to binary precision, albeit with lower quality. RNN architecture seems to be more robust towards compression, compared to the Transformer.</font>
<br>
</div>


<hr>
<div id="paper117"> <b>117. The Unreasonable Volatility of Neural Machine Translation Models</b>  <a href="https://www.aclweb.org/anthology/2020.ngt-1.10.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title117" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the Fourth Workshop on Neural Generation and Translation<br>&nbsp;&nbsp;<i>Marzieh Fadaee, Christof Monz</i><br>
<font size="3">
Recent works have shown that Neural Machine Translation (NMT) models achieve impressive performance, however, questions about understanding the behavior of these models remain unanswered. We investigate the unexpected volatility of NMT models where the input is semantically and syntactically correct. We discover that with trivial modifications of source sentences, we can identify cases where unexpected changes happen in the translation and in the worst case lead to mistranslations. This volatile behavior of translating extremely similar sentences in surprisingly different ways highlights the underlying generalization problem of current NMT models. We find that both RNN and Transformer models display volatile behavior in 26% and 19% of sentence variations, respectively.</font>
<br>
</div>


<hr>
<div id="paper118"> <b>118. Distill, Adapt, Distill: Training Small, In-Domain Models for Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.ngt-1.12.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title118" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the Fourth Workshop on Neural Generation and Translation<br>&nbsp;&nbsp;<i>Mitchell Gordon, Kevin Duh</i><br>
<font size="3">
We explore best practices for training small, memory efficient machine translation models with sequence-level knowledge distillation in the domain adaptation setting. While both domain adaptation and knowledge distillation are widely-used, their interaction remains little understood. Our large-scale empirical results in machine translation (on three language pairs with three domains each) suggest distilling twice for best performance: once using general-domain data and again using in-domain data with an adapted teacher.</font>
<br>
</div>


<hr>
<div id="paper119"> <b>119. Training and Inference Methods for High-Coverage Neural Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.ngt-1.13.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title119" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the Fourth Workshop on Neural Generation and Translation<br>&nbsp;&nbsp;<i>Michael Yang, Yixin Liu, Rahul Mayuranath</i><br>
<font size="3">
In this paper, we introduce a system built for the Duolingo Simultaneous Translation And Paraphrase for Language Education (STAPLE) shared task at the 4th Workshop on Neural Generation and Translation (WNGT 2020). We participated in the English-to-Japanese track with a Transformer model pretrained on the JParaCrawl corpus and fine-tuned in two steps on the JESC corpus and then the (smaller) Duolingo training corpus. First, during training, we find it is essential to deliberately expose the model to higher-quality translations more often during training for optimal translation performance. For inference, encouraging a small amount of diversity with Diverse Beam Search to improve translation coverage yielded marginal improvement over regular Beam Search. Finally, using an auxiliary filtering model to filter out unlikely candidates from Beam Search improves performance further. We achieve a weighted F1 score of 27.56% on our own test set, outperforming the STAPLE AWS translations baseline score of 4.31%.</font>
<br>
</div>


<hr>
<div id="paper120"> <b>120. English-to-Japanese Diverse Translation by Combining Forward and Backward Outputs</b>  <a href="https://www.aclweb.org/anthology/2020.ngt-1.15.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title120" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the Fourth Workshop on Neural Generation and Translation<br>&nbsp;&nbsp;<i>Masahiro Kaneko, Aizhan Imankulova, Tosho Hirasawa, Mamoru Komachi</i><br>
<font size="3">
We introduce our TMU system that is submitted to The 4th Workshop on Neural Generation and Translation (WNGT2020) to English-to-Japanese (En→Ja) track on Simultaneous Translation And Paraphrase for Language Education (STAPLE) shared task. In most cases machine translation systems generate a single output from the input sentence, however, in order to assist language learners in their journey with better and more diverse feedback, it is helpful to create a machine translation system that is able to produce diverse translations of each input sentence. However, creating such systems would require complex modifications in a model to ensure the diversity of outputs. In this paper, we investigated if it is possible to create such systems in a simple way and whether it can produce desired diverse outputs. In particular, we combined the outputs from forward and backward neural translation models (NMT). Our system achieved third place in En→Ja track, despite adopting only a simple approach.</font>
<br>
</div>


<hr>
<div id="paper121"> <b>121. The ADAPT System Description for the STAPLE 2020 English-to-Portuguese Translation Task</b>  <a href="https://www.aclweb.org/anthology/2020.ngt-1.17.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title121" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the Fourth Workshop on Neural Generation and Translation<br>&nbsp;&nbsp;<i>Rejwanul Haque, Yasmin Moslem, Andy Way</i><br>
<font size="3">
This paper describes the ADAPT Centre’s submission to STAPLE (Simultaneous Translation and Paraphrase for Language Education) 2020, a shared task of the 4th Workshop on Neural Generation and Translation (WNGT), for the English-to-Portuguese translation task. In this shared task, the participants were asked to produce high-coverage sets of plausible translations given English prompts (input source sentences). We present our English-to-Portuguese machine translation (MT) models that were built applying various strategies, e.g. data and sentence selection, monolingual MT for generating alternative translations, and combining multiple n-best translations. Our experiments show that adding the aforementioned techniques to the baseline yields an excellent performance in the English-to-Portuguese translation task.</font>
<br>
</div>


<hr>
<div id="paper122"> <b>122. Exploring Model Consensus to Generate Translation Paraphrases</b>  <a href="https://www.aclweb.org/anthology/2020.ngt-1.19.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title122" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the Fourth Workshop on Neural Generation and Translation<br>&nbsp;&nbsp;<i>Zhenhao Li, Marina Fomicheva, Lucia Specia</i><br>
<font size="3">
This paper describes our submission to the 2020 Duolingo Shared Task on Simultaneous Translation And Paraphrase for Language Education (STAPLE). This task focuses on improving the ability of neural MT systems to generate diverse translations. Our submission explores various methods, including N-best translation, Monte Carlo dropout, Diverse Beam Search, Mixture of Experts, Ensembling, and Lexical Substitution. Our main submission is based on the integration of multiple translations from multiple methods using Consensus Voting. Experiments show that the proposed approach achieves a considerable degree of diversity without introducing noisy translations. Our final submission achieves a 0.5510 weighted F1 score on the blind test set for the English-Portuguese track.</font>
<br>
</div>


<hr>
<div id="paper123"> <b>123. Growing Together: Modeling Human Language Learning With n-Best Multi-Checkpoint Machine Translation</b>  <a href="https://www.aclweb.org/anthology/2020.ngt-1.20.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title123" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the Fourth Workshop on Neural Generation and Translation<br>&nbsp;&nbsp;<i>El Moatez Billah Nagoudi, Muhammad Abdul-Mageed, Hasan Cavusoglu</i><br>
<font size="3">
We describe our submission to the 2020 Duolingo Shared Task on Simultaneous Translation And Paraphrase for Language Education (STAPLE). We view MT models at various training stages (i.e., checkpoints) as human learners at different levels. Hence, we employ an ensemble of multi-checkpoints from the same model to generate translation sequences with various levels of fluency. From each checkpoint, for our best model, we sample n-Best sequences (n=10) with a beam width =100. We achieve an 37.57 macro F1 with a 6 checkpoint model ensemble on the official shared task test data, outperforming a baseline Amazon translation system of 21.30 macro F1 and ultimately demonstrating the utility of our intuitive method.</font>
<br>
</div>


<hr>
<div id="paper124"> <b>124. Generating Diverse Translations via Weighted Fine-tuning and Hypotheses Filtering for the Duolingo STAPLE Task</b>  <a href="https://www.aclweb.org/anthology/2020.ngt-1.21.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title124" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the Fourth Workshop on Neural Generation and Translation<br>&nbsp;&nbsp;<i>Sweta Agrawal, Marine Carpuat</i><br>
<font size="3">
This paper describes the University of Maryland’s submission to the Duolingo Shared Task on Simultaneous Translation And Paraphrase for Language Education (STAPLE). Unlike the standard machine translation task, STAPLE requires generating a set of outputs for a given input sequence, aiming to cover the space of translations produced by language learners. We adapt neural machine translation models to this requirement by (a) generating n-best translation hypotheses from a model fine-tuned on learner translations, oversampled to reflect the distribution of learner responses, and (b) filtering hypotheses using a feature-rich binary classifier that directly optimizes a close approximation of the official evaluation metric. Combination of systems that use these two strategies achieves F1 scores of 53.9% and 52.5% on Vietnamese and Portuguese, respectively ranking 2nd and 4th on the leaderboard.</font>
<br>
</div>


<hr>
<div id="paper125"> <b>125. The JHU Submission to the 2020 Duolingo Shared Task on Simultaneous Translation and Paraphrase for Language Education</b>  <a href="https://www.aclweb.org/anthology/2020.ngt-1.22.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title125" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the Fourth Workshop on Neural Generation and Translation<br>&nbsp;&nbsp;<i>Huda Khayrallah, Jacob Bremerman, Arya D. McCarthy, Kenton Murray, Winston Wu, Matt Post</i><br>
<font size="3">
This paper presents the Johns Hopkins University submission to the 2020 Duolingo Shared Task on Simultaneous Translation and Paraphrase for Language Education (STAPLE). We participated in all five language tasks, placing first in each. Our approach involved a language-agnostic pipeline of three components: (1) building strong machine translation systems on general-domain data, (2) fine-tuning on Duolingo-provided data, and (3) generating n-best lists which are then filtered with various score-based techniques. In addi- tion to the language-agnostic pipeline, we attempted a number of linguistically-motivated approaches, with, unfortunately, little success. We also find that improving BLEU performance of the beam-search generated translation does not necessarily improve on the task metric—weighted macro F1 of an n-best list.</font>
<br>
</div>


<hr>
<div id="paper126"> <b>126. Simultaneous paraphrasing and translation by fine-tuning Transformer models</b>  <a href="https://www.aclweb.org/anthology/2020.ngt-1.23.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title126" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the Fourth Workshop on Neural Generation and Translation<br>&nbsp;&nbsp;<i>Rakesh Chada</i><br>
<font size="3">
This paper describes the third place submission to the shared task on simultaneous translation and paraphrasing for language education at the 4th workshop on Neural Generation and Translation (WNGT) for ACL 2020. The final system leverages pre-trained translation models and uses a Transformer architecture combined with an oversampling strategy to achieve a competitive performance. This system significantly outperforms the baseline on Hungarian (27% absolute improvement in Weighted Macro F1 score) and Portuguese (33% absolute improvement) languages.</font>
<br>
</div>


<hr>
<div id="paper127"> <b>127. Efficient and High-Quality Neural Machine Translation with OpenNMT</b>  <a href="https://www.aclweb.org/anthology/2020.ngt-1.25.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title127" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the Fourth Workshop on Neural Generation and Translation<br>&nbsp;&nbsp;<i>Guillaume Klein, Dakun Zhang, Clément Chouteau, Josep Crego, Jean Senellart</i><br>
<font size="3">
This paper describes the OpenNMT submissions to the WNGT 2020 efficiency shared task. We explore training and acceleration of Transformer models with various sizes that are trained in a teacher-student setup. We also present a custom and optimized C++ inference engine that enables fast CPU and GPU decoding with few dependencies. By combining additional optimizations and parallelization techniques, we create small, efficient, and high-quality neural machine translation models.</font>
<br>
</div>


<hr>
<div id="paper128"> <b>128. Edinburgh’s Submissions to the 2020 Machine Translation Efficiency Task</b>  <a href="https://www.aclweb.org/anthology/2020.ngt-1.26.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title128" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the Fourth Workshop on Neural Generation and Translation<br>&nbsp;&nbsp;<i>Nikolay Bogoychev, Roman Grundkiewicz, Alham Fikri Aji, Maximiliana Behnke, Kenneth Heafield, Sidharth Kashyap, Emmanouil-Ioannis Farsarakis, Mateusz Chudyk</i><br>
<font size="3">
We participated in all tracks of the Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU, and GPU. At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning. On GPUs, we used 16-bit floating-point tensor cores. On CPUs, we customized 8-bit quantization and multiple processes with affinity for the multi-core setting. To reduce model size, we experimented with 4-bit log quantization but use floats at runtime. In the shared task, most of our submissions were Pareto optimal with respect the trade-off between time and quality.</font>
<br>
</div>


<hr>
<div id="paper129"> <b>129. Improving Document-Level Neural Machine Translation with Domain Adaptation</b>  <a href="https://www.aclweb.org/anthology/2020.ngt-1.27.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title129" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the Fourth Workshop on Neural Generation and Translation<br>&nbsp;&nbsp;<i>Sami Ul Haq, Sadaf Abdul Rauf, Arslan Shoukat, Noor-e- Hira</i><br>
<font size="3">
Recent studies have shown that translation quality of NMT systems can be improved by providing document-level contextual information. In general sentence-based NMT models are extended to capture contextual information from large-scale document-level corpora which are difficult to acquire. Domain adaptation on the other hand promises adapting components of already developed systems by exploiting limited in-domain data. This paper presents FJWU’s system submission at WNGT, we specifically participated in Document level MT task for German-English translation. Our system is based on context-aware Transformer model developed on top of original NMT architecture by integrating contextual information using attention networks. Our experimental results show providing previous sentences as context significantly improves the BLEU score as compared to a strong NMT baseline. We also studied the impact of domain adaptation on document level translationand were able to improve results by adaptingthe systems according to the testing domain.</font>
<br>
</div>


<hr>
<div id="paper130"> <b>130. Simultaneous Translation and Paraphrase for Language Education</b>  <a href="https://www.aclweb.org/anthology/2020.ngt-1.28.pdf" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title130" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. the Fourth Workshop on Neural Generation and Translation<br>&nbsp;&nbsp;<i>Stephen Mayhew, Klinton Bicknell, Chris Brust, Bill McDowell, Will Monroe, Burr Settles</i><br>
<font size="3">
We present the task of Simultaneous Translation and Paraphrasing for Language Education (STAPLE). Given a prompt in one language, the goal is to generate a diverse set of correct translations that language learners are likely to produce. This is motivated by the need to create and maintain large, high-quality sets of acceptable translations for exercises in a language-learning application, and synthesizes work spanning machine translation, MT evaluation, automatic paraphrasing, and language education technology. We developed a novel corpus with unique properties for five languages (Hungarian, Japanese, Korean, Portuguese, and Vietnamese), and report on the results of a shared task challenge which attracted 20 teams to solve the task. In our meta-analysis, we focus on three aspects of the resulting systems: external training corpus selection, model architecture and training decisions, and decoding and filtering strategies. We find that strong systems start with a large amount of generic training data, and then fine-tune with in-domain data, sampled according to our provided learner response frequencies.</font>
<br>
</div>


<hr>
<div id="paper131"> <b>131. A Translation-Based Approach to Morphology Learning for Low Resource Languages</b>  <a href style="color:#0000EE;">[PDF]</a>  <a href="#title131" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Tewodros Gebreselassie, Amanuel Mersha, Michael Gasser</i><br>
<font size="3">
“Low resource languages” usually refers to languages that lack corpora and basic tools such as part-of-speech taggers. But a significant number of such languages do benefit from the availability of relatively complex linguistic descriptions of phonology, morphology, and syntax, as well as dictionaries. A further category, probably the majority of the world’s languages, suffers from the lack of even these resources. In this paper, we investigate the possibility of learning the morphology of such a language by relying on its close relationship to a language with more resources. Specifically, we use a transfer-based approach to learn the morphology of the severely under-resourced language Gofa, starting with a neural morphological generator for the closely related language, Wolaytta. Both languages are members of the Omotic family, spoken and southwestern Ethiopia, and, like other Omotic languages, both are morphologically complex. We first create a finite- state transducer for morphological analysis and generation for Wolaytta, based on relatively complete linguistic descriptions and lexicons for the language. Next, we train an encoder-decoder neural network on the task of morphological generation for Wolaytta, using data generated by the FST. Such a network takes a root and a set of grammatical features as input and generates a word form as output. We then elicit Gofa translations of a small set of Wolaytta words from bilingual speakers. Finally, we retrain the decoder of the Wolaytta network, using a small set of Gofa target words that are translations of the Wolaytta outputs of the original network. The evaluation shows that the transfer network performs better than a separate encoder-decoder network trained on a larger set of Gofa words. We conclude with implications for the learning of morphology for severely under-resourced languages in regions where there are related languages with more resources.</font>
<br>
</div>


<hr>
<div id="paper132"> <b>132. FFR v1.1: Fon-French Neural Machine Translation</b>  <a href style="color:#0000EE;">[PDF]</a>  <a href="#title132" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Chris Chinenye Emezue, Femi Pancrace Bonaventure Dossou</i><br>
<font size="3">
All over the world and especially in Africa, researchers are putting efforts into building Neural Machine Translation (NMT) systems to help tackle the language barriers in Africa, a continent of over 2000 different languages. However, the low-resourceness, diacritical, and tonal complexities of African languages are major issues being faced. The FFR project is a major step towards creating a robust translation model from Fon, a very low-resource and tonal language, to French, for research and public use. In this paper, we introduce FFR Dataset, a corpus of Fon-to-French translations, describe the diacritical encoding process, and introduce our FFR v1.1 model, trained on the dataset. The dataset and model are made publicly available, to promote collaboration and reproducibility.</font>
<br>
</div>


<hr>
<div id="paper133"> <b>133. Translating Natural Language Instructions for Behavioral Robot Navigation with a Multi-Head Attention Mechanism</b>  <a href style="color:#0000EE;">[PDF]</a>  <a href="#title133" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Patricio Cerda-Mardini, Vladimir Araujo, Álvaro Soto</i><br>
<font size="3">
We propose a multi-head attention mechanism as a blending layer in a neural network model that translates natural language to a high level behavioral language for indoor robot navigation. We follow the framework established by (Zang et al., 2018a) that proposes the use of a navigation graph as a knowledge base for the task. Our results show significant performance gains when translating instructions on previously unseen environments, therefore, improving the generalization capabilities of the model.</font>
<br>
</div>


<hr>
<div id="paper134"> <b>134. Towards Mitigating Gender Bias in a decoder-based Neural Machine Translation model by Adding Contextual Information</b>  <a href style="color:#0000EE;">[PDF]</a>  <a href="#title134" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Christine Basta, Marta R. Costa-jussà, José A. R. Fonollosa</i><br>
<font size="3">
Gender bias negatively impacts many natural language processing applications, including machine translation (MT). The motivation behind this work is to study whether recent proposed MT techniques are significantly contributing to attenuate biases in document-level and gender-balanced data. For the study, we consider approaches of adding the previous sentence and the speaker information, implemented in a decoder-based neural MT system. We show improvements both in translation quality (+1 BLEU point) as well as in gender bias mitigation on WinoMT (+5% accuracy).</font>
<br>
</div>


<hr>
<div id="paper135"> <b>135. Multitask Models for Controlling the Complexity of Neural Machine Translation</b>  <a href style="color:#0000EE;">[PDF]</a>  <a href="#title135" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Sweta Agrawal, Marine Carpuat</i><br>
<font size="3">
We introduce a machine translation task where the output is aimed at audiences of different levels of target language proficiency. We collect a novel dataset of news articles available in English and Spanish and written for diverse reading grade levels. We leverage this dataset to train multitask sequence to sequence models that translate Spanish into English targeted at an easier reading grade level than the original Spanish. We show that multitask models outperform pipeline approaches that translate and simplify text independently.</font>
<br>
</div>


<hr>
<div id="paper136"> <b>136. HausaMT v1.0: Towards English–Hausa Neural Machine Translation</b>  <a href style="color:#0000EE;">[PDF]</a>  <a href="#title136" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Adewale Akinfaderin</i><br>
<font size="3">
Neural Machine Translation (NMT) for low-resource languages suffers from low performance because of the lack of large amounts of parallel data and language diversity. To contribute to ameliorating this problem, we built a baseline model for English–Hausa machine translation, which is considered a task for low–resource language. The Hausa language is the second largest Afro–Asiatic language in the world after Arabic and it is the third largest language for trading across a larger swath of West Africa countries, after English and French. In this paper, we curated different datasets containing Hausa–English parallel corpus for our translation. We trained baseline models and evaluated the performance of our models using the Recurrent and Transformer encoder–decoder architecture with two tokenization approaches: standard word–level tokenization and Byte Pair Encoding (BPE) subword tokenization.</font>
<br>
</div>


<hr>
<div id="paper137"> <b>137. An Evaluation of Subword Segmentation Strategies for Neural Machine Translation of Morphologically Rich Languages</b>  <a href style="color:#0000EE;">[PDF]</a>  <a href="#title137" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;ACL 2020. <br>&nbsp;&nbsp;<i>Aquia Richburg, Ramy Eskander, Smaranda Muresan, Marine Carpuat</i><br>
<font size="3">
Byte-Pair Encoding (BPE) (Sennrich et al., 2016) has become a standard pre-processing step when building neural machine translation systems. However, it is not clear whether this is an optimal strategy in all settings. We conduct a controlled comparison of subword segmentation strategies for translating two low-resource morphologically rich languages (Swahili and Turkish) into English. We show that segmentations based on a unigram language model (Kudo, 2018) yield comparable BLEU and better recall for translating rare source words than BPE.</font>
<br>
</div>


<hr>
<p><font style="color:red;">注：论文列表使用<a href="https://zhuanlan.zhihu.com/p/282844968" target="_blank" rel="noopener">AC论文搜索器</a>整理！</font></p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>PROCJX
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://procjx.github.io/2020/12/09/%E3%80%90NLP%E3%80%91%20ACL%202020%20Neural%20Machine%20Translation%20%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E6%95%B4%E7%90%86/" title="【NLP】 ACL 2020 Neural Machine Translation 机器翻译相关论文整理">https://procjx.github.io/2020/12/09/%E3%80%90NLP%E3%80%91%20ACL%202020%20Neural%20Machine%20Translation%20%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E6%95%B4%E7%90%86/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">

        
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2020/12/09/%E3%80%90NLP%E3%80%91%20EMNLP%202020%20Neural%20Machine%20Translation%20%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E6%95%B4%E7%90%86/" rel="next" title="【NLP】 EMNLP 2020 Neural Machine Translation 机器翻译相关论文整理">
                  <i class="fa fa-chevron-left"></i> 【NLP】 EMNLP 2020 Neural Machine Translation 机器翻译相关论文整理
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
            </div>
          </div>

        
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang -->
<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9197824246"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="gitalk-container"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#目录"><span class="nav-number">1.</span> <span class="nav-text">目录</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#摘要"><span class="nav-number">2.</span> <span class="nav-text">摘要</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="PROCJX"
    src="/images/procjx.png">
  <p class="site-author-name" itemprop="name">PROCJX</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">429</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">71</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/procjx" title="GitHub &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;procjx" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:procjx@gmail.com" title="E-Mail &amp;rarr; mailto:procjx@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>


<!--
      
        <script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
        <script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
        <div class="widget-wrap">
            <h3 class="widget-title">标签云</h3>
            <div id="myCanvasContainer" class="widget tagcloud">
                <canvas width="250" height="250" id="resCanvas" style="width=100%">
                    <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AAAI/" rel="tag">AAAI</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ACL/" rel="tag">ACL</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Accepted-Papers/" rel="tag">Accepted Papers</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ArXiv/" rel="tag">ArXiv</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/" rel="tag">BERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS20SI/" rel="tag">CS20SI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS224d/" rel="tag">CS224d</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/" rel="tag">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CVPR/" rel="tag">CVPR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Context/" rel="tag">Context</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cross-Lingual/" rel="tag">Cross Lingual</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dialog-System/" rel="tag">Dialog System</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse/" rel="tag">Discourse</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Ranking/" rel="tag">Discourse Ranking</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Structure/" rel="tag">Discourse Structure</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Document-NMT/" rel="tag">Document NMT</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EMNLP/" rel="tag">EMNLP</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Extractive/" rel="tag">Extractive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICLR/" rel="tag">ICLR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICML/" rel="tag">ICML</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IJCAI/" rel="tag">IJCAI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Inter-Sentence/" rel="tag">Inter-Sentence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Keyphrase-Generation/" rel="tag">Keyphrase Generation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NAACL/" rel="tag">NAACL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIPS/" rel="tag">NIPS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NMT/" rel="tag">NMT</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Relation-Extraction/" rel="tag">Neural Relation Extraction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RST/" rel="tag">RST</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Relation-Constraints/" rel="tag">Relation Constraints</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Summarization/" rel="tag">Summarization</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Translation/" rel="tag">Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Word-Translation/" rel="tag">Word Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/alias/" rel="tag">alias</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/git/" rel="tag">git</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pip/" rel="tag">pip</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/screen/" rel="tag">screen</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/" rel="tag">shell</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tgz/" rel="tag">tgz</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tts/" rel="tag">tts</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1/" rel="tag">冒泡</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/" rel="tag">冒泡排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%99%E4%BD%9C%E5%8A%A9%E6%89%8B/" rel="tag">写作助手</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8E%8B%E7%BC%A9/" rel="tag">压缩</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/" rel="tag">发送邮件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%88%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">合并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%8E%E5%8F%B0/" rel="tag">后台</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/" rel="tag">基数排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F/" rel="tag">希尔排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6/" rel="tag">归并</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">归并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/" rel="tag">快速排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F/" rel="tag">批量</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4/" rel="tag">批量删除</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F/" rel="tag">排序</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5/" rel="tag">插入</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/" rel="tag">插入排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%99%E7%A8%8B/" rel="tag">教程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/" rel="tag">斐波那契数列</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9D%80%E6%AD%BB%E8%BF%9B%E7%A8%8B/" rel="tag">杀死进程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B1%89%E8%AF%BA%E5%A1%94/" rel="tag">汉诺塔</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%A7%A3%E5%8E%8B/" rel="tag">解压</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%B0%B7%E6%AD%8C%E7%BF%BB%E8%AF%91/" rel="tag">谷歌翻译</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BF%AD%E4%BB%A3%E5%9B%9E%E7%BF%BB/" rel="tag">迭代回翻</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9/" rel="tag">选择</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/" rel="tag">选择排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%99%84%E4%BB%B6/" rel="tag">附件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3/" rel="tag">非监督</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%A2%86%E5%9F%9F%E9%80%82%E5%BA%94/" rel="tag">领域适应</a><span class="tag-list-count">1</span></li></ul>
                </canvas>
            </div>
        </div>
        
-->
        
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-hengfu -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9879871597"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-chuizhi -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="1662238719"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-zhengfangxing -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="6699421902"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PROCJX</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.4.2
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>












        
      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  <script src="/js/local-search.js"></script>













  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '2286ab64f5194d9d79ce',
      clientSecret: 'f912492bec2391664b40478f50f2f943376768d6',
      repo: 'procjx.github.io',
      owner: 'procjx',
      admin: ['procjx'],
      id: '13fcd72438ec9eae6c89415875c67cd7',
        language: 'zh-CN',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
