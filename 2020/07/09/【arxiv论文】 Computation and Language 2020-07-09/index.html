<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/procjx.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/procjxfavicon32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/procjxfavicon16x16.ico">
  <link rel="mask-icon" href="/images/procjx.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.4.2',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

<!-- Google Adsense -->
<!--
<script async src="//pagead2.googlesyndication.com/
pagead/js/adsbygoogle.js"></script>
<script>
(adsbygoogle = window.adsbygoogle || []).push({
google_ad_client: "pub-1179774715076800",
enable_page_level_ads: true
});
</script>
-->

<script data-ad-client="ca-pub-1179774715076800" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>


<meta name="google-site-verification" content="cEiGwg0T8Rj5msmuEcGYZTh5nnf05EhCXy0gp2Ml5BI" />
<meta name="baidu-site-verification" content="noSKHe8MJs" />

  <meta name="description" content="目录  1. Unsupervised Online Grounding of Natural Language during Human-Robot  Interactions [PDF] 摘要  2. Tweets Sentiment Analysis via Word Embeddings and Machine Learning  Techniques [PDF] 摘要  3. A Nov">
<meta property="og:type" content="article">
<meta property="og:title" content="【arxiv论文】 Computation and Language 2020-07-09">
<meta property="og:url" content="https:&#x2F;&#x2F;procjx.github.io&#x2F;2020&#x2F;07&#x2F;09&#x2F;%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-07-09&#x2F;index.html">
<meta property="og:site_name" content="PROCJX&#39;s BLOGS">
<meta property="og:description" content="目录  1. Unsupervised Online Grounding of Natural Language during Human-Robot  Interactions [PDF] 摘要  2. Tweets Sentiment Analysis via Word Embeddings and Machine Learning  Techniques [PDF] 摘要  3. A Nov">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https:&#x2F;&#x2F;procjx.github.io&#x2F;images&#x2F;cl-2020-07-09.jpg">
<meta property="og:updated_time" content="2020-07-09T14:53:14.017Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;procjx.github.io&#x2F;images&#x2F;cl-2020-07-09.jpg">

<link rel="canonical" href="https://procjx.github.io/2020/07/09/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-07-09/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>【arxiv论文】 Computation and Language 2020-07-09 | PROCJX's BLOGS</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">PROCJX's BLOGS</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">WITH LOVE OF WORLD</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-fw fa-download"></i>资源</a>

  </li>
        <li class="menu-item menu-item-arxiv">

    <a href="/arxiv/" rel="section"><i class="fa fa-fw fa-file-pdf-o"></i>arxiv论文</a>

  </li>
        <li class="menu-item menu-item-deadline">

    <a href="/deadline/" rel="section"><i class="fa fa-fw fa-calendar"></i>会议截稿</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://procjx.github.io/2020/07/09/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-07-09/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/procjx.png">
      <meta itemprop="name" content="PROCJX">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="PROCJX's BLOGS">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          【arxiv论文】 Computation and Language 2020-07-09
        </h2>

        <div class="post-meta">
        
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-07-09 11:33:00 / 修改时间：22:53:14" itemprop="dateCreated datePublished" datetime="2020-07-09T11:33:00+08:00">2020-07-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/arxiv/" itemprop="url" rel="index">
                    <span itemprop="name">arxiv</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/arxiv/CL/" itemprop="url" rel="index">
                    <span itemprop="name">CL</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
              <span>46k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
              <span>1:17</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="/images/cl-2020-07-09.jpg" alt></p><h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Unsupervised Online Grounding of Natural Language during Human-Robot  Interactions <a href="https://arxiv.org/pdf/2007.04304" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Tweets Sentiment Analysis via Word Embeddings and Machine Learning  Techniques <a href="https://arxiv.org/pdf/2007.04303" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> A Novel BGCapsule Network for Text Classification <a href="https://arxiv.org/pdf/2007.04302" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Segmentation Approach for Coreference Resolution Task <a href="https://arxiv.org/pdf/2007.04301" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Normalizador Neural de Datas e Endereços <a href="https://arxiv.org/pdf/2007.04300" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Interpreting Hierarchical Linguistic Interactions in DNNs <a href="https://arxiv.org/pdf/2007.04298" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Open Domain Suggestion Mining Leveraging Fine-Grained Analysis <a href="https://arxiv.org/pdf/2007.04297" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Cooking Is All About People: Comment Classification On Cookery Channels  Using BERT and Classification Models (Malayalam-English Mix-Code) <a href="https://arxiv.org/pdf/2007.04249" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Chatbot: A Conversational Agent employed with Named Entity Recognition  Model using Artificial Neural Network <a href="https://arxiv.org/pdf/2007.04248" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Neural relation extraction: a survey <a href="https://arxiv.org/pdf/2007.04247" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Understanding Object Affordances Through Verb Usage Patterns <a href="https://arxiv.org/pdf/2007.04245" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> A Survey on Transfer Learning in Natural Language Processing <a href="https://arxiv.org/pdf/2007.04239" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Analysis of Predictive Coding Models for Phonemic Representation  Learning in Small Datasets <a href="https://arxiv.org/pdf/2007.04205" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Automatic Detection of Sexist Statements Commonly Used at the Workplace <a href="https://arxiv.org/pdf/2007.04181" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Learning Neural Textual Representations for Citation Recommendation <a href="https://arxiv.org/pdf/2007.04070" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Improving Conversational Recommender Systems via Knowledge Graph based  Semantic Fusion <a href="https://arxiv.org/pdf/2007.04032" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Generalizing Tensor Decomposition for N-ary Relational Knowledge Bases <a href="https://arxiv.org/pdf/2007.03988" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Best-First Beam Search <a href="https://arxiv.org/pdf/2007.03909" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Audio-Visual Understanding of Passenger Intents for In-Cabin  Conversational Agents <a href="https://arxiv.org/pdf/2007.03876" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> KQA Pro: A Large Diagnostic Dataset for Complex Question Answering over  Knowledge Base <a href="https://arxiv.org/pdf/2007.03875" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Research on multi-dimensional end-to-end phrase recognition algorithm  based on background knowledge <a href="https://arxiv.org/pdf/2007.03860" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Language Modeling with Reduced Densities <a href="https://arxiv.org/pdf/2007.03834" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> ISA: An Intelligent Shopping Assistant <a href="https://arxiv.org/pdf/2007.03805" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> The curious case of developmental BERTology: On sparsity, transfer  learning, generalization and the brain <a href="https://arxiv.org/pdf/2007.03774" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Cross-lingual Inductive Transfer to Detect Offensive Language <a href="https://arxiv.org/pdf/2007.03771" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> Evaluating German Transformer Language Models with Syntactic Agreement  Tests <a href="https://arxiv.org/pdf/2007.03765" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Learning Speech Representations from Raw Audio by Joint Audiovisual  Self-Supervision <a href="https://arxiv.org/pdf/2007.04134" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> Streaming End-to-End Bilingual ASR Systems with Joint Language  Identification <a href="https://arxiv.org/pdf/2007.03900" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Spatio-Temporal Scene Graphs for Video Dialog <a href="https://arxiv.org/pdf/2007.03848" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> Expressive Interviewing: A Conversational System for Coping with  COVID-19 <a href="https://arxiv.org/pdf/2007.03819" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> Placepedia: Comprehensive Place Understanding with Multi-Faceted  Annotations <a href="https://arxiv.org/pdf/2007.03777" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>



<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Unsupervised Online Grounding of Natural Language during Human-Robot  Interactions</b>  <a href="https://arxiv.org/pdf/2007.04304" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Roesler%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Oliver Roesler</a><br>
<font size="3">
 Abstract: Allowing humans to communicate through natural language with robots requires connections between words and percepts. The process of creating these connections is called symbol grounding and has been studied for nearly three decades. Although many studies have been conducted, not many considered grounding of synonyms and the employed algorithms either work only offline or in a supervised manner. In this paper, a cross-situational learning based grounding framework is proposed that allows grounding of words and phrases through corresponding percepts without human supervision and online, i.e. it does not require any explicit training phase, but instead updates the obtained mappings for every new encountered situation. The proposed framework is evaluated through an interaction experiment between a human tutor and a robot, and compared to an existing unsupervised grounding framework. The results show that the proposed framework is able to ground words through their corresponding percepts online and in an unsupervised manner, while outperforming the baseline framework.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：让人类通过与机器人的自然语言沟通需要的单词和知觉之间的联系。建立这些连接的过程被称为符号接地，并已研究了近三十年。虽然许多研究已进行同义词和所采用的算法，无论是工作，而不是很多人认为的接地仅脱机或监督的方式。在本文中，跨情境学习基础接地框架被提出，允许单词和短语的接地通过相应无需人工监控和在线知觉，即它不需要任何明确的训练阶段，而是更新获得每个新遇到的映射情况。所提出的架构是通过人的导师和机器人之间的交互实验评估，并且相比于现有的无监督的接地框架。结果表明，该框架能够通过设置相应的知觉在线和无监督的方式对地的话，而跑赢基准框架。</font>
</div>


<hr>
<div id="paper2"> <b>2. Tweets Sentiment Analysis via Word Embeddings and Machine Learning  Techniques</b>  <a href="https://arxiv.org/pdf/2007.04303" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sharma%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aditya Sharma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Daniels%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alex Daniels</a><br>
<font size="3">
 Abstract: Sentiment analysis of social media data consists of attitudes, assessments, and emotions which can be considered a way human think. Understanding and classifying the large collection of documents into positive and negative aspects are a very difficult task. Social networks such as Twitter, Facebook, and Instagram provide a platform in order to gather information about peoples sentiments and opinions. Considering the fact that people spend hours daily on social media and share their opinion on various different topics helps us analyze sentiments better. More and more companies are using social media tools to provide various services and interact with customers. Sentiment Analysis (SA) classifies the polarity of given tweets to positive and negative tweets in order to understand the sentiments of the public. This paper aims to perform sentiment analysis of real-time 2019 election twitter data using the feature selection model word2vec and the machine learning algorithm random forest for sentiment classification. Word2vec with Random Forest improves the accuracy of sentiment analysis significantly compared to traditional methods such as BOW and TF-IDF. Word2vec improves the quality of features by considering contextual semantics of words in a text hence improving the accuracy of machine learning and sentiment analysis.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：社交媒体数据的情感分析包括态度，评估和情感可以被视为人类的方式思考。了解和大集合的文件转化为积极和消极方面进行分类是一个非常艰巨的任务。社交网络如Twitter，Facebook的，和Instagram以收集关于民族主义情绪和意见的信息提供了一个平台。考虑到这一事实，人们每天花在社交媒体小时并分享各种不同的主题他们的意见可以帮助我们分析情绪更好。越来越多的公司使用社交媒体工具来提供各种服务和互动与客户。情感分析（SA），以了解公众的情绪分类给予鸣叫正面和负面的鸣叫的极性。本文旨在执行使用特征选择模型word2vec和机器学习的情感分类算法随机森林实时2019大选的Twitter数据的情感分析。 Word2vec与随机森林改善情绪分析的准确性显著相比传统方法，如弓和TF-IDF。 Word2vec改善的特点，通过考虑从而改善机器学习和情感分析的准确度文本中的字的上下文语义的质量。</font>
</div>


<hr>
<div id="paper3"> <b>3. A Novel BGCapsule Network for Text Classification</b>  <a href="https://arxiv.org/pdf/2007.04302" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gangwar%2C+A+K" target="_blank" rel="noopener" style="color:#0000EE;">Akhilesh Kumar Gangwar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ravi%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vadlamani Ravi</a><br>
<font size="3">
 Abstract: Several text classification tasks such as sentiment analysis, news categorization, multi-label classification and opinion classification are challenging problems even for modern deep learning networks. Recently, Capsule Networks (CapsNets) are proposed for image classification. It has been shown that CapsNets have several advantages over Convolutional Neural Networks (CNNs), while their validity in the domain of text has been less explored. In this paper, we propose a novel hybrid architecture viz., BGCapsule, which is a Capsule model preceded by an ensemble of Bidirectional Gated Recurrent Units (BiGRU) for several text classification tasks. We employed an ensemble of Bidirectional GRUs for feature extraction layer preceding the primary capsule layer. The hybrid architecture, after performing basic pre-processing steps, consists of five layers: an embedding layer based on GloVe, a BiGRU based ensemble layer, a primary capsule layer, a flatten layer and fully connected ReLU layer followed by a fully connected softmax layer. In order to evaluate the effectiveness of BGCapsule, we conducted extensive experiments on five benchmark datasets (ranging from 10,000 records to 700,000 records) including Movie Review (MR Imdb 2005), AG News dataset, Dbpedia ontology dataset, Yelp Review Full dataset and Yelp review polarity dataset. These benchmarks cover several text classification tasks such as news categorization, sentiment analysis, multiclass classification, multi-label classification and opinion classification. We found that our proposed architecture (BGCapsule) achieves better accuracy compared to the existing methods without the help of any external linguistic knowledge such as positive sentiment keywords and negative sentiment keywords. Further, BGCapsule converged faster compared to other extant techniques.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：一些文本分类的任务，如情感分析，新闻分类，多标签分类和意见分类提出了挑战，甚至对现代深学习网络问题。近日，胶囊网络（CapsNets）提出的图像分类。它已经表明，CapsNets有超过卷积神经网络（细胞神经网络）几个优点，而他们在文本域中有效性已经得到更少的探讨。在本文中，我们提出了一种新颖的混合体系结构即，BGCapsule，其是胶囊模型通过双向门控复发性单位（BiGRU）的集合数文本分类任务之前。我们采用双向越冬的集合用于主要胶囊层前述特征提取层。的混合体系结构，执行基本预处理步骤之后，由五层组成：后面是完全连接SOFTMAX层基于手套，基于BiGRU合奏层，主胶囊层，平化层和完全连接RELU层包埋层。为了评估BGCapsule的有效性，我们在五个基准数据集（10,000记录到700,000记录范围），包括电影回顾（MR IMDB 2005），AG新闻数据集，DBpedia的本体数据集，Yelp的评论完整数据集和Yelp的评论进行了广泛的实验极性数据集。这些基准测试涵盖多个文本分类的任务，如新闻分类，情感分析，多分类，多标签分类和意见分类。我们发现，我们提出的架构（BGCapsule）实现了比没有任何外部的语言知识的帮助下，现有的方法更准确，如积极情绪关键字和负面情绪关键字。此外，BGCapsule相比其他现存技术的融合速度更快。</font>
</div>


<hr>
<div id="paper4"> <b>4. Segmentation Approach for Coreference Resolution Task</b>  <a href="https://arxiv.org/pdf/2007.04301" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jafari%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aref Jafari</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ghodsi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ali Ghodsi</a><br>
<font size="3">
 Abstract: In coreference resolution, it is important to consider all members of a coreference cluster and decide about all of them at once. This technique can help to avoid losing precision and also in finding long-distance relations. The presented paper is a report of an ongoing study on an idea which proposes a new approach for coreference resolution which can resolve all coreference mentions to a given mention in the document in one pass. This has been accomplished by defining an embedding method for the position of all members of a coreference cluster in a document and resolving all of them for a given mention. In the proposed method, the BERT model has been used for encoding the documents and a head network designed to capture the relations between the embedded tokens. These are then converted to the proposed span position embedding matrix which embeds the position of all coreference mentions in the document. We tested this idea on CoNLL 2012 dataset and although the preliminary results from this method do not quite meet the state-of-the-art results, they are promising and they can capture features like long-distance relations better than the other approaches.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在指代消解，它考虑的共参照组的所有成员，并同时决定对所有的人是很重要的。这种技术可以帮助避免失去精度，也找到长途关系。所提出的论文是一个正在进行的研究上提出了指代消解能解决所有的共参照提到在一个通文档中给定的提新方法的思路的报告。这已通过限定嵌入方法用于共参照集群的所有成员的一个文档中的位置和解决对给定提所有这些实现。在该方法中，BERT模型已被用于编码的文件和一个头网络设计来捕捉嵌入标记之间的关系。然后这些转换成嵌入所有共参照的位置的文档中提到所提出的跨度位置嵌入基质。我们测试CoNLL这个想法2012集，虽然此方法的初步结果不太符合国家的最先进的成果，他们是有希望的，他们可以更好地捕捉像长途关系的功能比其他方法。</font>
</div>


<hr>
<div id="paper5"> <b>5. Normalizador Neural de Datas e Endereços</b>  <a href="https://arxiv.org/pdf/2007.04300" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Plensack%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gustavo Plensack</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Finardi%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Paulo Finardi</a><br>
<font size="3">
 Abstract: Documents of any kind present a wide variety of date and address formats, in some cases dates can be written entirely in full or even have different types of separators. The pattern disorder in addresses is even greater due to the greater possibility of interchanging between streets, neighborhoods, cities and states. In the context of natural language processing, problems of this nature are handled by rigid tools such as ReGex or DateParser, which are efficient as long as the expected input is pre-configured. When these algorithms are given an unexpected format, errors and unwanted outputs happen. To circumvent this challenge, we present a solution with deep neural networks state of art T5 that treats non-preconfigured formats of dates and addresses with accuracy above 90% in some cases. With this model, our proposal brings generalization to the task of normalizing dates and addresses. We also deal with this problem with noisy data that simulates possible errors in the text.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：任何一种存在的文件各种各样的日期和地址格式，在某些情况下，红枣能完全的全文写，甚至有不同类型的分离。在地址模式障碍是更大的，由于街道，社区，城市和国家之间交换的可能性更大。在自然语言处理的上下文中，这种性质的问题是由刚性工具，如正则表达式或DateParser，这是有效的，只要预期输入被预先配置处理。当这些算法给出一个意外的格式，错误和不必要的输出发生。为了克服这一挑战，我们提出用艺术T5的深层神经网络状态的解决方案，日期和地址，在某些情况下，精度在90％以上的对待非预先配置的格式。在这种模式下，我们的建议带来的泛化正火日期和地址的任务。我们还处理与噪声数据这一问题在文本中模拟可能出现的错误。</font>
</div>


<hr>
<div id="paper6"> <b>6. Interpreting Hierarchical Linguistic Interactions in DNNs</b>  <a href="https://arxiv.org/pdf/2007.04298" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Die Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huilin Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoyi Bao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huo%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Da Huo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruizhao Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xu Cheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mengyue Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Quanshi Zhang</a><br>
<font size="3">
 Abstract: This paper proposes a method to disentangle and quantify interactions among words that are encoded inside a DNN for natural language processing. We construct a tree to encode salient interactions extracted by the DNN. Six metrics are proposed to analyze properties of interactions between constituents in a sentence. The interaction is defined based on Shapley values of words, which are considered as an unbiased estimation of word contributions to the network prediction. Our method is used to quantify word interactions encoded inside the BERT, ELMo, LSTM, CNN, and Transformer networks. Experimental results have provided a new perspective to understand these DNNs, and have demonstrated the effectiveness of our method.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了被编码的DNN内用于自然语言处理的单词中解开和量化的相互作用的方法。我们构建一个树由DNN提取编码显着的相互作用。六个指标提出来分析句子成分之间的相互作用的性质。的相互作用是基于话，这被认为是到网络预测字捐款无偏估计的沙普利值定义。我们的方法用于将BERT，毛毛，LSTM，CNN，和变压器网络内编码字进行量化的相互作用。实验结果提供了一个新的角度来理解这些DNNs，并证明了该方法的有效性。</font>
</div>


<hr>
<div id="paper7"> <b>7. Open Domain Suggestion Mining Leveraging Fine-Grained Analysis</b>  <a href="https://arxiv.org/pdf/2007.04297" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Singal%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shreya Singal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Goel%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tanishq Goel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chopra%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shivang Chopra</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dahiya%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sonika Dahiya</a><br>
<font size="3">
 Abstract: Suggestion mining tasks are often semantically complex and lack sophisticated methodologies that can be applied to real-world data. The presence of suggestions across a large diversity of domains and the absence of large labelled and balanced datasets render this task particularly challenging to deal with. In an attempt to overcome these challenges, we propose a two-tier pipeline that leverages Discourse Marker based oversampling and fine-grained suggestion mining techniques to retrieve suggestions from online forums. Through extensive comparison on a real-world open-domain suggestion dataset, we demonstrate how the oversampling technique combined with transformer based fine-grained analysis can beat the state of the art. Additionally, we perform extensive qualitative and qualitative analysis to give construct validity to our proposed pipeline. Finally, we discuss the practical, computational and reproducibility aspects of the deployment of our pipeline across the web.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：建议挖掘任务往往是语义复杂，缺乏可应用到真实世界的数据完善的方法。的建议在大型多元化域以及缺乏大型标记和平衡数据集的存在使得这一任务尤其具有挑战性的处理。在试图克服这些挑战，我们提出了一个两层的管道，它利用基于话语标记过采样和细粒度建议挖掘技术来从网上论坛的建议。通过对现实世界的开放领域建议集广泛的比较，我们展示过采样技术与基于变压器的细粒度分析相结合，如何能击败领域的状态。此外，我们进行大量的定性和定量分析给构想效度，以我们提出的管道。最后，我们讨论了我们在网络上的管道部署的实际，计算和可重复性方面。</font>
</div>


<hr>
<div id="paper8"> <b>8. Cooking Is All About People: Comment Classification On Cookery Channels  Using BERT and Classification Models (Malayalam-English Mix-Code)</b>  <a href="https://arxiv.org/pdf/2007.04249" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kazhuparambil%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Subramaniam Kazhuparambil</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kaushik%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Abhishek Kaushik</a><br>
<font size="3">
 Abstract: The scope of a lucrative career promoted by Google through its video distribution platform YouTube has attracted a large number of users to become content creators. An important aspect of this line of work is the feedback received in the form of comments which show how well the content is being received by the audience. However, volume of comments coupled with spam and limited tools for comment classification makes it virtually impossible for a creator to go through each and every comment and gather constructive feedback. Automatic classification of comments is a challenge even for established classification models, since comments are often of variable lengths riddled with slang, symbols and abbreviations. This is a greater challenge where comments are multilingual as the messages are often rife with the respective vernacular. In this work, we have evaluated top-performing classification models and four different vectorizers, for classifying comments which are a mix of different combinations of English and Malayalam (only English, only Malayalam and Mix of English and Malayalam). The statistical analysis of results indicates that Multinomial Naive Bayes, K-Nearest Neighbors (KNN), Support Vector Machine (SVM), Random Forest and Decision Trees offer similar level of accuracy in comment classification. Further, we have also evaluated 3 multilingual sub-types of the novel NLP language model, BERT and compared its performance to the conventional machine learning classification techniques. XLM was the top-performing BERT model with an accuracy of 67.31. Random Forest with Term Frequency Vectorizer was the best the top-performing model out of all the traditional classification models with an accuracy of 63.59.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：通过视频分发平台的YouTube由谷歌推动一个有利可图的职业的范围，吸引了大量用户成为内容创造者。这条线工作的一个重要方面是展示如何做好内容受到观众收到的意见的形式收到的反馈意见。然而，加之垃圾邮件和有限的工具评论分类注释体积使得它几乎不可能的创作者要经过的每一个意见，并收集建设性的反馈意见。注释自动分类甚至对于建立分类模型是一个挑战，因为评论经常用俚语，符号和缩写充斥可变长度。这是一个更大的挑战，其中的意见是多语言的信息往往充斥着各自的白话。在这项工作中，我们已经评估了顶级表现分类模式和四种不同的vectorizers，进行分类这是英语和马来亚（只有英文，只有马来亚和英语和马拉雅拉姆语的混合）的不同组合的混合意见。结果的统计分析表明，多项朴素贝叶斯，K最近邻（KNN），支持向量机（SVM），随机森林和决策树报价在评论分类准确度的水平相近。此外，我们还评估3多语言的子类型小说NLP语言模型，BERT和比较其性能与传统的机器学习分类技术。 XLM是表现最出色BERT模型的67.31的精度。随机森林与词频矢量器是最好的所有传统的分类模型的表现最出色的模型只是一支由63.59的精度。</font>
</div>


<hr>
<div id="paper9"> <b>9. Chatbot: A Conversational Agent employed with Named Entity Recognition  Model using Artificial Neural Network</b>  <a href="https://arxiv.org/pdf/2007.04248" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ali%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nazakat Ali</a><br>
<font size="3">
 Abstract: Chatbot is a technology that is used to mimic human behavior using natural language. There are different types of Chatbot that can be used as conversational agent in various business domains in order to increase the customer service and satisfaction. For any business domain, it requires a knowledge base to be built for that domain and design an information retrieval based system that can respond the user with a piece of documentation or generated sentences. The core component of a Chatbot is Natural Language Understanding (NLU) which has been impressively improved by deep learning methods. But we often lack such properly built NLU modules and requires more time to build it from scratch for high quality conversations. This may encourage fresh learners to build a Chatbot from scratch with simple architecture and using small dataset, although it may have reduced functionality, rather than building high quality data driven methods. This research focuses on Named Entity Recognition (NER) and Intent Classification models which can be integrated into NLU service of a Chatbot. Named entities will be inserted manually in the knowledge base and automatically detected in a given sentence. The NER model in the proposed architecture is based on artificial neural network which is trained on manually created entities and evaluated using CoNLL-2003 dataset.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：聊天机器人是用自然语言来模仿人类行为的技术。有迹象表明，在以提高客户服务和满意度作为在各个业务领域的会话代理不同类型的聊天机器人的。对于任何业务领域，它需要一个知识库，该域将建成并设计了一个信息检索基础的系统，可以用一块文档或生成的句子的响应用户。一个聊天机器人的核心部件是自然语言理解（NLU），其具有通过深学习方法得到令人印象深刻的改进。但是，我们往往缺乏这样的正确建NLU模块，需要更多的时间来从头高质量通话建立它。这可能会鼓励新鲜学习者建立从简单的架构，并使用小数据集从头开始聊天机器人，但它可能会降低的功能性，而不是建立驱动方法，高质量的数据。本研究以命名实体识别（NER）和意图分类模型可以被集成到一个聊天机器人的NLU服务。命名实体将手动知识库中的插入，并在给定的句子自动检测。在所提出的架构的NER模型是基于它是在手动创建实体的培训和评估使用CoNLL-2003数据集的人工神经网络。</font>
</div>


<hr>
<div id="paper10"> <b>10. Neural relation extraction: a survey</b>  <a href="https://arxiv.org/pdf/2007.04247" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Aydar%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mehmet Aydar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bozal%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Ozge Bozal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ozbay%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Furkan Ozbay</a><br>
<font size="3">
 Abstract: Neural relation extraction discovers semantic relations between entities from unstructured text using deep learning methods. In this study, we present a comprehensive review of methods on neural network based relation extraction. We discuss advantageous and incompetent sides of existing studies and investigate additional research directions and improvement ideas in this field.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：采用深学习方法从非结构化文本实体之间的神经关系抽取发现的语义关系。在这项研究中，我们提出了对基于神经网络的关系抽取方法进行全面审查。我们讨论了现有研究的有利和不称职的两侧和探讨在这一领域进一步研究的方向和改进意见。</font>
</div>


<hr>
<div id="paper11"> <b>11. Understanding Object Affordances Through Verb Usage Patterns</b>  <a href="https://arxiv.org/pdf/2007.04245" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lam%2C+K+C" target="_blank" rel="noopener" style="color:#0000EE;">Ka Chun Lam</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pereira%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Francisco Pereira</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vaziri-Pashkam%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maryam Vaziri-Pashkam</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Woodard%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kristin Woodard</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=McMahon%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Emalie McMahon</a><br>
<font size="3">
 Abstract: In order to interact with objects in our environment, we rely on an understanding of the actions that can be performed on them, and the extent to which they rely or have an effect on the properties of the object. This knowledge is called the object "affordance". We propose an approach for creating an embedding of objects in an affordance space, in which each dimension corresponds to an aspect of meaning shared by many actions, using text corpora. This embedding makes it possible to predict which verbs will be applicable to a given object, as captured in human judgments of affordance. We show that the dimensions learned are interpretable, and that they correspond to patterns of interaction with objects. Finally, we show that they can be used to predict other dimensions of object representation that have been shown to underpin human judgments of object similarity.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：为了在我们的环境中的对象进行交互，我们依靠的是可以对它们进行到它们所依赖或对对象的属性产生影响的行动，以及在何种程度的理解。这方面的知识被称为对象“启示”。我们提出的方法在一个启示空间创建对象的嵌入，其中每个维度对应的许多行动意思共享，利用语料库的一个方面。该嵌入使得能够预测哪些动词将适用于给定的对象，如在启示的人工判断捕获。我们发现，学到的尺寸是可解释的，并且它们对应于与对象的交互模式。最后，我们表明，它们可以被用于预测已显示对象相似的托换人为判断对象表示的其它尺寸。</font>
</div>


<hr>
<div id="paper12"> <b>12. A Survey on Transfer Learning in Natural Language Processing</b>  <a href="https://arxiv.org/pdf/2007.04239" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Alyafeai%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zaid Alyafeai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=AlShaibani%2C+M+S" target="_blank" rel="noopener" style="color:#0000EE;">Maged Saeed AlShaibani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ahmad%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Irfan Ahmad</a><br>
<font size="3">
 Abstract: Deep learning models usually require a huge amount of data. However, these large datasets are not always attainable. This is common in many challenging NLP tasks. Consider Neural Machine Translation, for instance, where curating such large datasets may not be possible specially for low resource languages. Another limitation of deep learning models is the demand for huge computing resources. These obstacles motivate research to question the possibility of knowledge transfer using large trained models. The demand for transfer learning is increasing as many large models are emerging. In this survey, we feature the recent transfer learning advances in the field of NLP. We also provide a taxonomy for categorizing different transfer learning approaches from the literature.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深学习模式通常需要庞大的数据量。然而，这些大型数据集并不总是可以实现的。这是在许多具有挑战性的任务NLP常见。考虑神经机器翻译，例如，在那里策划这样的大型数据集可能不适合低资源语言是可能的特别。深学习模型的另一个局限性是巨大计算资源的需求。这些障碍激励研究质疑知识转移的使用大训练的模型的可能性。为众多大型的模式正在兴起的迁移学习的需求不断增加。在本次调查中，我们拥有在自然语言处理领域的最新传输的学习进展。我们还提供了不同的分类迁移学习的分类法从文献的方法。</font>
</div>


<hr>
<div id="paper13"> <b>13. Analysis of Predictive Coding Models for Phonemic Representation  Learning in Small Datasets</b>  <a href="https://arxiv.org/pdf/2007.04205" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bland%C3%B3n%2C+M+A+C" target="_blank" rel="noopener" style="color:#0000EE;">María Andrea Cruz Blandón</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=R%C3%A4s%C3%A4nen%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Okko Räsänen</a><br>
<font size="3">
 Abstract: Neural network models using predictive coding are interesting from the viewpoint of computational modelling of human language acquisition, where the objective is to understand how linguistic units could be learned from speech without any labels. Even though several promising predictive coding -based learning algorithms have been proposed in the literature, it is currently unclear how well they generalise to different languages and training dataset sizes. In addition, despite that such models have shown to be effective phonemic feature learners, it is unclear whether minimisation of the predictive loss functions of these models also leads to optimal phoneme-like representations. The present study investigates the behaviour of two predictive coding models, Autoregressive Predictive Coding and Contrastive Predictive Coding, in a phoneme discrimination task (ABX task) for two languages with different dataset sizes. Our experiments show a strong correlation between the autoregressive loss and the phoneme discrimination scores with the two datasets. However, to our surprise, the CPC model shows rapid convergence already after one pass over the training data, and, on average, its representations outperform those of APC on both languages.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：使用预测编码的神经网络模型是从人类语言习得的计算模型的角度来看有趣如果目标是要了解如何单位可以语言学从语音没有任何标签来学习。虽然几个有前途的预测编码为基础的学习算法已经在文献中提出，目前还不清楚他们如何推广到不同的语言和训练数据集的大小。此外，尽管这些模型已经证明是有效的音位功能学习者，目前还不清楚这些车型也导致的损失预测功能最小化，以最优的音素般的表示是否。本研究探讨两个预测编码模型的行为，自回归预测编码和对比预测编码，在音素识别任务（ABX任务）对两种语言不同的数据集的大小。我们的实验表明自回归的损失，并用两个数据集的音素识别分数之间的强相关性。然而，出乎我们的意料，中共模特表演快速收敛已经一个传过来的训练数据，以及平均后，其表示上优于两种语言的APC的。</font>
</div>


<hr>
<div id="paper14"> <b>14. Automatic Detection of Sexist Statements Commonly Used at the Workplace</b>  <a href="https://arxiv.org/pdf/2007.04181" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Grosz%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dylan Grosz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Conde-Cespedes%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Patricia Conde-Cespedes</a><br>
<font size="3">
 Abstract: Detecting hate speech in the workplace is a unique classification task, as the underlying social context implies a subtler version of conventional hate speech. Applications regarding a state-of the-art workplace sexism detection model include aids for Human Resources departments, AI chatbots and sentiment analysis. Most existing hate speech detection methods, although robust and accurate, focus on hate speech found on social media, specifically Twitter. The context of social media is much more anonymous than the workplace, therefore it tends to lend itself to more aggressive and "hostile" versions of sexism. Therefore, datasets with large amounts of "hostile" sexism have a slightly easier detection task since "hostile" sexist statements can hinge on a couple words that, regardless of context, tip the model off that a statement is sexist. In this paper we present a dataset of sexist statements that are more likely to be said in the workplace as well as a deep learning model that can achieve state-of-the art results. Previous research has created state-of-the-art models to distinguish "hostile" and "benevolent" sexism based simply on aggregated Twitter data. Our deep learning methods, initialized with GloVe or random word embeddings, use LSTMs with attention mechanisms to outperform those models on a more diverse, filtered dataset that is more targeted towards workplace sexism, leading to an F1 score of 0.88.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在工作场所检测仇恨言论是一个独特的分类任务，作为潜在的社会背景下意味着传统的仇恨言论的一个微妙的版本。关于先进国家的职场性别歧视检测模型应用包括人力资源部门，人工智能聊天机器人和情感分析的辅助工具。大多数现有的仇恨言论的检测方法，尽管需要稳定和准确，对社交媒体发现专注于仇恨言论，特别是Twitter的。社交媒体的背景是比工作更匿名的，因此它倾向于借给自己的性别歧视更积极和“敌对”的版本。因此，大量的“敌对”性别歧视的数据集有一个稍微容易检测的任务，因为“敌对”性别歧视语句可对一对夫妇的话，无论背景下，小费模型关闭该声明是性别歧视的铰链。在本文中，我们目前更可能的性别歧视报表的数据集在工作场所，以及一个深度学习模式，可以实现国家的艺术效果可说的。以前的研究已经创造国家的最先进的车型来区分“敌对”，并简单地基于Twitter的汇总数据“仁者”性别歧视。我们深厚的学习方法，用手套或随机字的嵌入初始化，注重机制使用LSTMs跑赢上更加多样化，过滤数据集更针对朝职场性别歧视的模式，导致F1得分0.88。</font>
</div>


<hr>
<div id="paper15"> <b>15. Learning Neural Textual Representations for Citation Recommendation</b>  <a href="https://arxiv.org/pdf/2007.04070" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kieu%2C+B+T" target="_blank" rel="noopener" style="color:#0000EE;">Binh Thanh Kieu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Unanue%2C+I+J" target="_blank" rel="noopener" style="color:#0000EE;">Inigo Jauregi Unanue</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pham%2C+S+B" target="_blank" rel="noopener" style="color:#0000EE;">Son Bao Pham</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Phan%2C+H+X" target="_blank" rel="noopener" style="color:#0000EE;">Hieu Xuan Phan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Piccardi%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Massimo Piccardi</a><br>
<font size="3">
 Abstract: With the rapid growth of the scientific literature, manually selecting appropriate citations for a paper is becoming increasingly challenging and time-consuming. While several approaches for automated citation recommendation have been proposed in the recent years, effective document representations for citation recommendation are still elusive to a large extent. For this reason, in this paper we propose a novel approach to citation recommendation which leverages a deep sequential representation of the documents (Sentence-BERT) cascaded with Siamese and triplet networks in a submodular scoring function. To the best of our knowledge, this is the first approach to combine deep representations and submodular selection for a task of citation recommendation. Experiments have been carried out using a popular benchmark dataset - the ACL Anthology Network corpus - and evaluated against baselines and a state-of-the-art approach using metrics such as the MRR and F1-at-k score. The results show that the proposed approach has been able to outperform all the compared approaches in every measured metric.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着科学文献的快速增长，手动地选择用于造纸适当引用变得越来越具有挑战性的并且耗时。虽然自动引用推荐几种方法已经在近几年被提出，有效的文件表示了引用推荐仍然是难以捉摸的，在很大程度上。为此，本文提出了一种新的方法，以充分利用其在子模的计分函数与连体和三线网络级联的文件（句子-BERT）的深顺序表示引文建议。据我们所知，这是深交涉和子模块的选择结合起来，引用的推荐任务的第一种方法。使用度量如MRR和F1-在-K得分国家的最先进的方法，并评价针对基线和 - 实验已经进行了使用一个流行的基准数据集 - 的ACL文集网络语料库。实验结果表明，该方法已经能够胜过所有在每一个测量指标的比较方法。</font>
</div>


<hr>
<div id="paper16"> <b>16. Improving Conversational Recommender Systems via Knowledge Graph based  Semantic Fusion</b>  <a href="https://arxiv.org/pdf/2007.04032" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kun Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W+X" target="_blank" rel="noopener" style="color:#0000EE;">Wayne Xin Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bian%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuqing Bian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuanhang Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Ji-Rong Wen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingsong Yu</a><br>
<font size="3">
 Abstract: Conversational recommender systems (CRS) aim to recommend high-quality items to users through interactive conversations. Although several efforts have been made for CRS, two major issues still remain to be solved. First, the conversation data itself lacks of sufficient contextual information for accurately understanding users' preference. Second, there is a semantic gap between natural language expression and item-level user preference. To address these issues, we incorporate both word-oriented and entity-oriented knowledge graphs (KG) to enhance the data representations in CRSs, and adopt Mutual Information Maximization to align the word-level and entity-level semantic spaces. Based on the aligned semantic representations, we further develop a KG-enhanced recommender component for making accurate recommendations, and a KG-enhanced dialog component that can generate informative keywords or entities in the response text. Extensive experiments have demonstrated the effectiveness of our approach in yielding better performance on both recommendation and conversation tasks.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：会话推荐系统（CRS）的目标是通过交流互动推荐优质项目给用户。虽然一些努力已经进行了CRS，两大问题仍有待解决。首先，对话数据本身缺乏对准确了解用户的偏好充分的背景资料。第二，有自然语言表达和项目级别的用户偏好之间的语义差距。为了解决这些问题，我们二者结合字的和面向实体知识图（KG）加强CRS用的数据表示，并采用互信息最大化对齐字级和实体级语义空间。基于对齐的语义表示，我们进一步发展作出准确的建议一KG增强推荐器组件，并且可以在响应文本生成信息的关键字或实体KG增强对话的组成部分。大量的实验已经证明，在收益上都推荐和谈话任务更好的性能我们的方法的有效性。</font>
</div>


<hr>
<div id="paper17"> <b>17. Generalizing Tensor Decomposition for N-ary Relational Knowledge Bases</b>  <a href="https://arxiv.org/pdf/2007.03988" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Quanming Yao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yong Li</a><br>
<font size="3">
 Abstract: With the rapid development of knowledge bases (KBs), link prediction task, which completes KBs with missing facts, has been broadly studied in especially binary relational KBs (a.k.a knowledge graph) with powerful tensor decomposition related methods. However, the ubiquitous n-ary relational KBs with higher-arity relational facts are paid less attention, in which existing translation based and neural network based approaches have weak expressiveness and high complexity in modeling various relations. Tensor decomposition has not been considered for n-ary relational KBs, while directly extending tensor decomposition related methods of binary relational KBs to the n-ary case does not yield satisfactory results due to exponential model complexity and their strong assumptions on binary relations. To generalize tensor decomposition for n-ary relational KBs, in this work, we propose GETD, a generalized model based on Tucker decomposition and Tensor Ring decomposition. The existing negative sampling technique is also generalized to the n-ary case for GETD. In addition, we theoretically prove that GETD is fully expressive to completely represent any KBs. Extensive evaluations on two representative n-ary relational KB datasets demonstrate the superior performance of GETD, significantly improving the state-of-the-art methods by over 15\%. Moreover, GETD further obtains the state-of-the-art results on the benchmark binary relational KB datasets.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着知识库（KBS），链路预测的任务，完成与缺少事实KB的快速发展，已在尤其是二元关系知识库系统（a.k.a知识图）具有强大分解张相关的方法被广泛研究。然而，与更高元数的关系的事实普遍存在的n元关系KB的被关注较少，其中和基于神经网络现有的翻译方法在造型各方面的关系较弱的表现力和高复杂性。张量分解并没有被认为是n元关系知识库系统，而直接扩展二元关系KB的张量分解相关方法，以n进制情况下不会产生由于指数模型的复杂性及其对二元关系强的假设令人满意的结果。为了概括为n元关系KB的张量分解，在这项工作中，我们提出GETD的基础上，塔克分解和张量环分解广义模型。现有的负采样技术也推广到用于GETD n进制的情况。此外，我们从理论上证明了GETD完全表现力完全代表任何KB的。上两个代表性n进制关系KB数据集广泛评价显示GETD的性能优越，通过在15 \％显著提高国家的最先进的方法。此外，GETD进一步获得关于基准二元关系KB数据集的状态的最先进的结果。</font>
</div>


<hr>
<div id="paper18"> <b>18. Best-First Beam Search</b>  <a href="https://arxiv.org/pdf/2007.03909" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Meister%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Clara Meister</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cotterell%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ryan Cotterell</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vieira%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tim Vieira</a><br>
<font size="3">
 Abstract: Decoding for many NLP tasks requires a heuristic algorithm for approximating exact search since the full search space is often intractable if not simply too large to traverse efficiently. The default algorithm for this job is beam search--a pruned version of breadth-first search--which in practice, returns better results than exact inference due to beneficial search bias. In this work, we show that standard beam search is a computationally inefficient choice for many decoding tasks; specifically, when the scoring function is a monotonic function in sequence length, other search algorithms can be used to reduce the number of calls to the scoring function (e.g., a neural network), which is often the bottleneck computation. We propose best-first beam search, an algorithm that provably returns the same set of results as standard beam search, albeit in the minimum number of scoring function calls to guarantee optimality (modulo beam size). We show that best-first beam search can be used with length normalization and mutual information decoding, among other rescoring functions. Lastly, we propose a memory-reduced variant of best-first beam search, which has a similar search bias in terms of downstream performance, but runs in a fraction of the time.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：解码许多NLP任务需要近似精确搜索，因为全搜索空间往往是棘手的如果不是太大了有效穿越启发式算法。此作业的默认算法是波束搜索 - 广度优先搜索的修剪版本 - 这在实践中，不是精确推断返回更好的结果，由于有利的搜索偏见。在这项工作中，我们表明，标准束搜索是许多解码任务的计算效率低下的选择;具体地，当计分的功能是在序列长度的单调函数，其它搜索算法可以用来减少调用评分函数（例如，神经网络），这是经常的瓶颈计算的数量。我们建议最好先束搜索，一个算法，可证明返回相同的结果集为标准束搜索，虽然在得分函数调用，以保证最优（模波束尺寸）的最小数量的。我们表明，最佳优先束搜索可以与长度归一化和互信息解码，其他再评分功能中使用。最后，我们建议最好先定向搜索，这在下游性能方面类似的搜索偏见，但运行的内存降低了变体的一小部分时间。</font>
</div>


<hr>
<div id="paper19"> <b>19. Audio-Visual Understanding of Passenger Intents for In-Cabin  Conversational Agents</b>  <a href="https://arxiv.org/pdf/2007.03876" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Okur%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eda Okur</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+S+H" target="_blank" rel="noopener" style="color:#0000EE;">Shachi H Kumar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sahay%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Saurav Sahay</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nachman%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lama Nachman</a><br>
<font size="3">
 Abstract: Building multimodal dialogue understanding capabilities situated in the in-cabin context is crucial to enhance passenger comfort in autonomous vehicle (AV) interaction systems. To this end, understanding passenger intents from spoken interactions and vehicle vision systems is a crucial component for developing contextual and visually grounded conversational agents for AV. Towards this goal, we explore AMIE (Automated-vehicle Multimodal In-cabin Experience), the in-cabin agent responsible for handling multimodal passenger-vehicle interactions. In this work, we discuss the benefits of a multimodal understanding of in-cabin utterances by incorporating verbal/language input together with the non-verbal/acoustic and visual clues from inside and outside the vehicle. Our experimental results outperformed text-only baselines as we achieved improved performances for intent detection with a multimodal approach.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：位于建设多式联运对话的理解能力，舱内环境是增强自主汽车（AV）交互系统乘客的舒适性是至关重要的。为此，从语音交互和车辆视觉系统的理解乘客的意图是为发展背景和视觉接地对话剂AV的重要组成部分。为了实现这一目标，我们将探讨AMIE（自动车多式联运舱内体验），舱内代理负责处理多乘用车相互作用。在这项工作中，我们通过引入言语/语言输入共同探讨在机舱话语的理解多的好处，非言语/声学和从内部和车外的视觉线索。我们的实验结果优于纯文本基线，因为我们实现了目标探测性能改善与多模式的方法。</font>
</div>


<hr>
<div id="paper20"> <b>20. KQA Pro: A Large Diagnostic Dataset for Complex Question Answering over  Knowledge Base</b>  <a href="https://arxiv.org/pdf/2007.03875" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiaxin Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shulin Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liangming Pan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yutong Xiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei Hou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Juanzi Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hanwang Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=He%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bin He</a><br>
<font size="3">
 Abstract: Complex question answering over knowledge base (Complex KBQA) is challenging because it requires the compositional reasoning capability. Existing benchmarks have three shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning processes; 2) questions are either generated by templates, leading to poor diversity, or on a small scale; and 3) they mostly only consider the relations among entities but not attributes. To this end, we introduce KQA Pro, a large-scale dataset for Complex KBQA. We generate questions, SPARQLs, and functional programs with recursive templates and then paraphrase the questions by crowdsourcing, giving rise to around 120K diverse instances. The SPARQLs and programs depict the reasoning processes in various manners, which can benefit a large spectrum of QA methods. We contribute a unified codebase and conduct extensive evaluations for baselines and state-of-the-arts: a blind GRU obtains 31.58\%, the best model achieves only 35.15\%, and humans top at 97.5\%, which offers great research potential to fill the gap.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：复杂的问题回答了知识基础（复杂KBQA）是具有挑战性的，因为它需要组合推理能力。现有的基准有三个缺点，限制了复杂KBQA的发展：1）他们只提供没有明确的推理过程QA对; 2）的问题是由模板生成任一，导致多样性差，或在一个小规模; 3），他们大多只考虑实体之间的关系，但不是属性。为此，我们引入KQA临，大规模数据集的复杂KBQA。我们产生疑问，SPARQLs，并用递归模板功能的程序，然后通过意译众包，从而引发周围120K多样化实例的问题。该SPARQLs和计划描绘了各种方式，它可以受益的QA方法大范围的推理过程。我们贡献一个统一的代码库，并进行了基线和国家的最艺术的广泛评估：以97.5 \％的盲GRU取得31.58 \％，最好的模式只能达到35.15 \％，和人类的顶部，提供了巨大的研究潜力填补了国内空白。</font>
</div>


<hr>
<div id="paper21"> <b>21. Research on multi-dimensional end-to-end phrase recognition algorithm  based on background knowledge</b>  <a href="https://arxiv.org/pdf/2007.03860" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zheng Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gang Tu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guang Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhan%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhi-Qiang Zhan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yi-Jian Liu</a><br>
<font size="3">
 Abstract: At present, the deep end-to-end method based on supervised learning is used in entity recognition and dependency analysis. There are two problems in this method: firstly, background knowledge cannot be introduced; secondly, multi granularity and nested features of natural language cannot be recognized. In order to solve these problems, the annotation rules based on phrase window are proposed, and the corresponding multi-dimensional end-to-end phrase recognition algorithm is designed. This annotation rule divides sentences into seven types of nested phrases, and indicates the dependency between phrases. The algorithm can not only introduce background knowledge, recognize all kinds of nested phrases in sentences, but also recognize the dependency between phrases. The experimental results show that the annotation rule is easy to use and has no ambiguity; the matching algorithm is more consistent with the multi granularity and diversity characteristics of syntax than the traditional end-to-end algorithm. The experiment on CPWD dataset, by introducing background knowledge, the new algorithm improves the accuracy of the end-to-end method by more than one point. The corresponding method was applied to the CCL 2018 competition and won the first place in the task of Chinese humor type recognition.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：目前，基于监督学习深端至端方法在实体识别和依赖性分析使用。有在此法的两个问题：首先，无法引进的背景知识;其次，多粒度和自然语言的嵌套功能不能被识别。为了解决这些问题，根据短语窗口中的注释规则提出，和相应的多维端至端短语识别算法被设计。此注释规则划分句子翻译成七种类型的嵌套短语，并指示短语之间的相关性。该算法不仅可以介绍背景知识，认识各种嵌套短语在句子，但也承认短语之间的相关性。实验结果表明，该注释规则是易于使用，并且没有歧义;匹配算法是具有语法比传统的端至端算法的多粒度和多样性的特点更加一致。上CPWD数据集中的实验中，通过引入的背景知识，新算法由一个以上的点改善了端 - 端方法的准确性。相应的方法应用于覆铜板2018竞争，中国的幽默类型识别的任务，获得了第一名。</font>
</div>


<hr>
<div id="paper22"> <b>22. Language Modeling with Reduced Densities</b>  <a href="https://arxiv.org/pdf/2007.03834" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bradley%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tai-Danae Bradley</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vlassopoulos%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yiannis Vlassopoulos</a><br>
<font size="3">
 Abstract: We present a framework for modeling words, phrases, and longer expressions in a natural language using reduced density operators. We show these operators capture something of the meaning of these expressions and, under the Loewner order on positive semidefinite operators, preserve both a simple form of entailment and the relevant statistics therein. Pulling back the curtain, the assignment is shown to be a functor between categories enriched over probabilities.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出了一个框架，在使用密度降低运营商的自然语言建模单词，短语，和更长的表达式。我们展示的这些表达的意义，这些运营商捕捉的东西，并且在Loewner方程组顺序下上半正定运营，维护蕴涵了简单的形式和相关统计在其中。拉回帘，分配被示出为经富集的概率类别之间函子。</font>
</div>


<hr>
<div id="paper23"> <b>23. ISA: An Intelligent Shopping Assistant</b>  <a href="https://arxiv.org/pdf/2007.03805" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+T+M" target="_blank" rel="noopener" style="color:#0000EE;">Tuan Manh Lai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bui%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Trung Bui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lipka%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nedim Lipka</a><br>
<font size="3">
 Abstract: Despite the growth of e-commerce, brick-and-mortar stores are still the preferred destinations for many people. In this paper, we present ISA, a mobile-based intelligent shopping assistant that is designed to improve shopping experience in physical stores. ISA assists users by leveraging advanced techniques in computer vision, speech processing, and natural language processing. An in-store user only needs to take a picture or scan the barcode of the product of interest, and then the user can talk to the assistant about the product. The assistant can also guide the user through the purchase process or recommend other similar products to the user. We take a data-driven approach in building the engines of ISA's natural language processing component, and the engines achieve good performance.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管电子商务的发展，砖和迫击炮商店仍然是许多人的首选目的地。在本文中，我们提出ISA，是旨在提高实体店的购物体验基于移动智能购物助手。 ISA通过利用计算机视觉，语音处理和自然语言处理的先进技术帮助用户。店内用户只需要拍照或扫描感兴趣的产品的条形码，然后用户可以交谈的关于产品的助手。该助理也可以通过购买过程中引导用户或推荐其他同类产品提供给用户。我们取一个数据驱动的方法构建ISA的自然语言处理组件的发动机，并且发动机达到良好的性能。</font>
</div>


<hr>
<div id="paper24"> <b>24. The curious case of developmental BERTology: On sparsity, transfer  learning, generalization and the brain</b>  <a href="https://arxiv.org/pdf/2007.03774" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Wang</a><br>
<font size="3">
 Abstract: In this essay, we explore a point of intersection between deep learning and neuroscience, through the lens of large language models, transfer learning and network compression. Just like perceptual and cognitive neurophysiology has inspired effective deep neural network architectures which in turn make a useful model for understanding the brain, here we explore how biological neural development might inspire efficient and robust optimization procedures which in turn serve as a useful model for the maturation and aging of the brain.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这篇文章中，我们将探讨深度学习和神经科学之间的交叉点通过大型语言模型，传递学习和网络压缩镜头。就像知觉和认知神经生理学激发有效的深层神经网络架构而这又做出了有益的模型对理解大脑，在这里我们探索的神经如何生物的发展可能会激发效率和强大的优化程序，这反过来又作为成熟的有用模型和老化的大脑。</font>
</div>


<hr>
<div id="paper25"> <b>25. Cross-lingual Inductive Transfer to Detect Offensive Language</b>  <a href="https://arxiv.org/pdf/2007.03771" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Pant%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kartikey Pant</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dadu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tanvi Dadu</a><br>
<font size="3">
 Abstract: With the growing use of social media and its availability, many instances of the use of offensive language have been observed across multiple languages and domains. This phenomenon has given rise to the growing need to detect the offensive language used in social media cross-lingually. In OffensEval 2020, the organizers have released the \textit{multilingual Offensive Language Identification Dataset} (mOLID), which contains tweets in five different languages, to detect offensive language. In this work, we introduce a cross-lingual inductive approach to identify the offensive language in tweets using the contextual word embedding \textit{XLM-RoBERTa} (XLM-R). We show that our model performs competitively on all five languages, obtaining the fourth position in the English task with an F1-score of $0.919$ and eighth position in the Turkish task with an F1-score of $0.781$. Further experimentation proves that our model works competitively in a zero-shot learning environment, and is extensible to other languages.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着越来越多地使用社交媒体和其可用性，使用攻击性语言的许多情况下，已经在多个语言和域名观察。这种现象已经引起了越来越多需要检测的社交媒体使用的交叉舌的攻击性语言。在OffensEval 2020年，主办方发布了\ {textit多种语言的攻击性语言识别数据集}（摩利），其中包含了五种不同的语言鸣叫，来检测攻击性语言。在这项工作中，我们引入一个跨语种归纳的方法使用上下文字嵌入\ textit {XLM-罗伯塔}（XLM-R）来标识在鸣叫的冒犯性的语言。我们证明了我们的竞争力模型对所有五种语言进行，获得在英国工作的第四位为$ 0.919 $ F1的得分和第八的位置在土耳其的任务为$ 0.781 $的F1-得分。进一步的实验证明，我们的模型作品竞争在零射门的学习环境，并可以扩展到其他语言。</font>
</div>


<hr>
<div id="paper26"> <b>26. Evaluating German Transformer Language Models with Syntactic Agreement  Tests</b>  <a href="https://arxiv.org/pdf/2007.03765" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zaczynska%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Karolina Zaczynska</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feldhus%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nils Feldhus</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Schwarzenberg%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Robert Schwarzenberg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gabryszak%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aleksandra Gabryszak</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=M%C3%B6ller%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sebastian Möller</a><br>
<font size="3">
 Abstract: Pre-trained transformer language models (TLMs) have recently refashioned natural language processing (NLP): Most state-of-the-art NLP models now operate on top of TLMs to benefit from contextualization and knowledge induction. To explain their success, the scientific community conducted numerous analyses. Besides other methods, syntactic agreement tests were utilized to analyse TLMs. Most of the studies were conducted for the English language, however. In this work, we analyse German TLMs. To this end, we design numerous agreement tasks, some of which consider peculiarities of the German language. Our experimental results show that state-of-the-art German TLMs generally perform well on agreement tasks, but we also identify and discuss syntactic structures that push them to their limits.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：预先训练变压器语言模型（TLM的）最近重制自然语言处理（NLP）：大多数国家的最先进的NLP模型现在从语境和知识归纳TLM的受益的顶部运行。为了解释他们的成功，科学界进行了无数次的分析。此外其他的方法，句法协议测试用于分析TLM的。大多数研究都是针对英语进行的，但是。在这项工作中，我们分析了德国TLM的。为此，我们设计了许多协议的任务，其中一些考虑德语的特点。我们的实验结果表明，国家的最先进的德国TLM的普遍对协议的任务表现良好，但我们还确定并讨论他们推到自己的极限句法结构。</font>
</div>


<hr>
<div id="paper27"> <b>27. Learning Speech Representations from Raw Audio by Joint Audiovisual  Self-Supervision</b>  <a href="https://arxiv.org/pdf/2007.04134" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Shukla%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Abhinav Shukla</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Petridis%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stavros Petridis</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Pantic%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maja Pantic</a><br>
<font size="3">
 Abstract: The intuitive interaction between the audio and visual modalities is valuable for cross-modal self-supervised learning. This concept has been demonstrated for generic audiovisual tasks like video action recognition and acoustic scene classification. However, self-supervision remains under-explored for audiovisual speech. We propose a method to learn self-supervised speech representations from the raw audio waveform. We train a raw audio encoder by combining audio-only self-supervision (by predicting informative audio attributes) with visual self-supervision (by generating talking faces from audio). The visual pretext task drives the audio representations to capture information related to lip movements. This enriches the audio encoder with visual information and the encoder can be used for evaluation without the visual modality. Our method attains competitive performance with respect to existing self-supervised audio features on established isolated word classification benchmarks, and significantly outperforms other methods at learning from fewer labels. Notably, our method also outperforms fully supervised training, thus providing a strong initialization for speech related tasks. Our results demonstrate the potential of multimodal self-supervision in audiovisual speech for learning good audio representations.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在音频和视频模式之间的直观的人机交互是有价值的跨模态自我监督学习。这个概念已经被证明对于像视频行为识别和听觉场景分类通用视听任务。然而，自我监督遗体充分开发的视听讲话。我们建议学习从原始音频波形自我监督的讲话表示的方法。我们培养的结合仅音频自检（通过预测信息的音频属性）与视觉自检（由音频生成交谈面）原始音频编码器。视觉借口任务驱动的音频交涉与嘴唇动作捕捉信息。这种丰富视觉信息的音频编码器和编码器可用于评估，而不视觉模态。我们对于现有的既定孤立词的分类基准，自我监督的音频功能，并显著方法有竞争力的无所获表现在从较少的标签学习优于其他方法。值得注意的是，我们的方法也优于完全监督下的训练，从而为言语相关的任务，强大的初始化。我们的研究结果表明多式联运自我监督的视听讲话学习好音频表示的潜力。</font>
</div>


<hr>
<div id="paper28"> <b>28. Streaming End-to-End Bilingual ASR Systems with Joint Language  Identification</b>  <a href="https://arxiv.org/pdf/2007.03900" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Punjabi%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Surabhi Punjabi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Arsikere%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Harish Arsikere</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Raeesy%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zeynab Raeesy</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Chandak%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chander Chandak</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Bhave%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nikhil Bhave</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Bansal%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ankish Bansal</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=M%C3%BCller%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Markus Müller</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Murillo%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sergio Murillo</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Rastrow%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ariya Rastrow</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Garimella%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sri Garimella</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Maas%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roland Maas</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Hans%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mat Hans</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Mouchtaris%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Athanasios Mouchtaris</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Kunzmann%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Siegfried Kunzmann</a><br>
<font size="3">
 Abstract: Multilingual ASR technology simplifies model training and deployment, but its accuracy is known to depend on the availability of language information at runtime. Since language identity is seldom known beforehand in real-world scenarios, it must be inferred on-the-fly with minimum latency. Furthermore, in voice-activated smart assistant systems, language identity is also required for downstream processing of ASR output. In this paper, we introduce streaming, end-to-end, bilingual systems that perform both ASR and language identification (LID) using the recurrent neural network transducer (RNN-T) architecture. On the input side, embeddings from pretrained acoustic-only LID classifiers are used to guide RNN-T training and inference, while on the output side, language targets are jointly modeled with ASR targets. The proposed method is applied to two language pairs: English-Spanish as spoken in the United States, and English-Hindi as spoken in India. Experiments show that for English-Spanish, the bilingual joint ASR-LID architecture matches monolingual ASR and acoustic-only LID accuracies. For the more challenging (owing to within-utterance code switching) case of English-Hindi, English ASR and LID metrics show degradation. Overall, in scenarios where users switch dynamically between languages, the proposed architecture offers a promising simplification over running multiple monolingual ASR models and an LID classifier in parallel.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：多语种ASR技术简化模型的训练和部署，但其精度是已知的依赖于在运行时的语言信息的可用性。由于语言标识很少在现实世界的场景事先知道，它必须在即时与最小的延迟来推断。此外，在语音激活智能辅助系统中，还需要对ASR的输出端的下游处理语言标识。在本文中，我们引入流，端至端，执行既ASR以及使用所述回归神经网络传感器（RNN-T）结构语言识别（LID）双语系统。在投入方面，从预训练的声学只LID分类的嵌入用于指导RNN-T训练和推理，而在输出侧，语言指标均会同ASR目标建模。在印度说英语 - 西班牙语所讲的在美国和英国，印地文：该方法适用于两张语言对。实验表明，英语 - 西班牙语，双语联合ASR-LID架构的单语ASR和声学只LID精度匹配。对于更有挑战性（由于内发声代码转换）英语 - 印地文，英语ASR和LID度量的情况下表现出的降解。总体而言，在用户语言之间进行动态切换，所提出的架构提供了运行多个单语ASR模型有希望简化和并行的LID分类方案。</font>
</div>


<hr>
<div id="paper29"> <b>29. Spatio-Temporal Scene Graphs for Video Dialog</b>  <a href="https://arxiv.org/pdf/2007.03848" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Geng%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shijie Geng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peng Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hori%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chiori Hori</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Roux%2C+J+L" target="_blank" rel="noopener" style="color:#0000EE;">Jonathan Le Roux</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cherian%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anoop Cherian</a><br>
<font size="3">
 Abstract: The Audio-Visual Scene-aware Dialog (AVSD) task requires an agent to indulge in a natural conversation with a human about a given video. Specifically, apart from the video frames, the agent receives the audio, brief captions, and a dialog history, and the task is to produce the correct answer to a question about the video. Due to the diversity in the type of inputs, this task poses a very challenging multimodal reasoning problem. Current approaches to AVSD either use global video-level features or those from a few sampled frames, and thus lack the ability to explicitly capture relevant visual regions or their interactions for answer generation. To this end, we propose a novel spatio-temporal scene graph representation (STSGR) modeling fine-grained information flows within videos. Specifically, on an input video sequence, STSGR (i) creates a two-stream visual and semantic scene graph on every frame, (ii) conducts intra-graph reasoning using node and edge convolutions generating visual memories, and (iii) applies inter-graph aggregation to capture their temporal evolutions. These visual memories are then combined with other modalities and the question embeddings using a novel semantics-controlled multi-head shuffled transformer, which then produces the answer recursively. Our entire pipeline is trained end-to-end. We present experiments on the AVSD dataset and demonstrate state-of-the-art results. A human evaluation on the quality of our generated answers shows 12% relative improvement against prior methods.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：视听场景感知对话框（AVSD）的任务，需要有一个在一个给定的视频的人的代理人沉迷于一个自然对话。具体而言，除了视频帧，代理接收音频，字幕短暂和对话历史，任务就是产生正确回答关于视频的问题。由于投入的类型的多样性，这个任务提出了一个非常具有挑战性的多模态推理问题。到AVSD目前的做法既可以使用全球视频级功能或者那些从几个采样帧，因此缺乏明确捕获有关视觉的区域或他们的答案产生相互作用的能力。为此，我们提出了一种新颖的时空场景图的表示（STSGR）建模细粒度信息的视频内流动。具体地，在输入视频序列，STSGR（ⅰ）创建每个帧上的两流视觉和语义场景图，（ⅱ）进行-图表帧内推理使用节点和边的卷积产生的视觉记忆，和（iii）适用帧间图聚集捕捉他们的时间的演化。这些视觉记忆然后用使用新颖的语义控制多头其他模式和问题的嵌入组合改组变压器，然后产生答案递归。我们的整个管道被训练结束到终端。我们在AVSD数据集目前的实验和展示国家的最先进的成果。对我们产生的回答显示了对现有技术的方法12％的相对改善质量进行人工评估。</font>
</div>


<hr>
<div id="paper30"> <b>30. Expressive Interviewing: A Conversational System for Coping with  COVID-19</b>  <a href="https://arxiv.org/pdf/2007.03819" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Welch%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Charles Welch</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lahnala%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Allison Lahnala</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=P%C3%A9rez-Rosas%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Verónica Pérez-Rosas</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Siqi Shen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Seraj%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sarah Seraj</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=An%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Larry An</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Resnicow%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kenneth Resnicow</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pennebaker%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">James Pennebaker</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mihalcea%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rada Mihalcea</a><br>
<font size="3">
 Abstract: The ongoing COVID-19 pandemic has raised concerns for many regarding personal and public health implications, financial security and economic stability. Alongside many other unprecedented challenges, there are increasing concerns over social isolation and mental health. We introduce \textit{Expressive Interviewing}--an interview-style conversational system that draws on ideas from motivational interviewing and expressive writing. Expressive Interviewing seeks to encourage users to express their thoughts and feelings through writing by asking them questions about how COVID-19 has impacted their lives. We present relevant aspects of the system's design and implementation as well as quantitative and qualitative analyses of user interactions with the system. In addition, we conduct a comparative evaluation with a general purpose dialogue system for mental health that shows our system potential in helping users to cope with COVID-19 issues.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：正在进行COVID-19大流行已经提高了很多关于个人和公众健康的影响，金融安全和经济稳定的担忧。除了其他许多前所未有的挑战，也有对社会的隔离和心理健康日益关注。我们推出\ {textit表现面谈}  - 即从动机访谈和表达性写作思路借鉴的采访式对话系统。表现访旨在鼓励用户通过询问COVID-19是如何影响他们的生活他们的问题写来表达自己的想法和感受。我们提出了系统的设计的相关方面，实施以及与系统的用户交互的定量和定性分析。此外，我们还与心理健康的通用对话系统，显示我们的系统在帮助用户应对COVID-19潜在的问题进行了对比评测。</font>
</div>


<hr>
<div id="paper31"> <b>31. Placepedia: Comprehensive Place Understanding with Multi-Faceted  Annotations</b>  <a href="https://arxiv.org/pdf/2007.03777" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huaiyi Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuqi Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qingqiu Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhengkui Guo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ziwei Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dahua Lin</a><br>
<font size="3">
 Abstract: Place is an important element in visual understanding. Given a photo of a building, people can often tell its functionality, e.g. a restaurant or a shop, its cultural style, e.g. Asian or European, as well as its economic type, e.g. industry oriented or tourism oriented. While place recognition has been widely studied in previous work, there remains a long way towards comprehensive place understanding, which is far beyond categorizing a place with an image and requires information of multiple aspects. In this work, we contribute Placepedia, a large-scale place dataset with more than 35M photos from 240K unique places. Besides the photos, each place also comes with massive multi-faceted information, e.g. GDP, population, etc., and labels at multiple levels, including function, city, country, etc.. This dataset, with its large amount of data and rich annotations, allows various studies to be conducted. Particularly, in our studies, we develop 1) PlaceNet, a unified framework for multi-level place recognition, and 2) a method for city embedding, which can produce a vector representation for a city that captures both visual and multi-faceted side information. Such studies not only reveal key challenges in place understanding, but also establish connections between visual observations and underlying socioeconomic/cultural implications.   </font>
<br>
<font size="2" style="line-height:30px;">
摘要：Place是直观的理解的重要元素。由于建筑物的照片，人们可以经常告诉它的功能，例如餐馆或商店，它的文化风格，例如亚洲或欧洲，以及其经济类型，例如面向行业或旅游为主。尽管地方肯定已被广泛研究在以前的工作中，仍然存在对综合处了解到，这远远超出了同一个图像分类的地方，需要多个方面的信息很长的路要走。在这项工作中，我们贡献Placepedia，大规模数据集的地方从240K独特的地方超过35M的照片。除了照片，每个地方还带有大量的多方面的信息，例如GDP，人口等，多层次的，包括功能，城市，国家等。该数据集，其大量的数据和丰富的注释标签，允许进行了各种研究。特别是，在我们的研究中，我们开发1）PlaceNet，多层次的地方识别一个统一的框架，和2）城市嵌入的方法，它可以产生一个向量表示，对于一座捕获视觉和多方位的侧信息。这样的研究不仅揭示了地方理解的关键挑战，同时也建立目视观测和潜在的社会经济/文化内涵之间的连接。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！封面为论文标题词云图！</font></p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>PROCJX
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://procjx.github.io/2020/07/09/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-07-09/" title="【arxiv论文】 Computation and Language 2020-07-09">https://procjx.github.io/2020/07/09/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-07-09/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">

        
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2020/07/08/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-07-08/" rel="next" title="【arxiv论文】 Computer Vision and Pattern Recognition 2020-07-08">
                  <i class="fa fa-chevron-left"></i> 【arxiv论文】 Computer Vision and Pattern Recognition 2020-07-08
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/2020/07/09/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-07-09/" rel="prev" title="【arxiv论文】 Computer Vision and Pattern Recognition 2020-07-09">
                  【arxiv论文】 Computer Vision and Pattern Recognition 2020-07-09 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>

        
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang -->
<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9197824246"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="gitalk-container"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#目录"><span class="nav-number">1.</span> <span class="nav-text">目录</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#摘要"><span class="nav-number">2.</span> <span class="nav-text">摘要</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="PROCJX"
    src="/images/procjx.png">
  <p class="site-author-name" itemprop="name">PROCJX</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">334</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">71</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/procjx" title="GitHub &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;procjx" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:procjx@gmail.com" title="E-Mail &amp;rarr; mailto:procjx@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>


<!--
      
        <script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
        <script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
        <div class="widget-wrap">
            <h3 class="widget-title">标签云</h3>
            <div id="myCanvasContainer" class="widget tagcloud">
                <canvas width="250" height="250" id="resCanvas" style="width=100%">
                    <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AAAI/" rel="tag">AAAI</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ACL/" rel="tag">ACL</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Accepted-Papers/" rel="tag">Accepted Papers</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ArXiv/" rel="tag">ArXiv</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/" rel="tag">BERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS20SI/" rel="tag">CS20SI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS224d/" rel="tag">CS224d</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/" rel="tag">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CVPR/" rel="tag">CVPR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Context/" rel="tag">Context</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cross-Lingual/" rel="tag">Cross Lingual</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dialog-System/" rel="tag">Dialog System</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse/" rel="tag">Discourse</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Ranking/" rel="tag">Discourse Ranking</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Discourse-Structure/" rel="tag">Discourse Structure</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Document-NMT/" rel="tag">Document NMT</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EMNLP/" rel="tag">EMNLP</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Extractive/" rel="tag">Extractive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICLR/" rel="tag">ICLR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICML/" rel="tag">ICML</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IJCAI/" rel="tag">IJCAI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Inter-Sentence/" rel="tag">Inter-Sentence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Keyphrase-Generation/" rel="tag">Keyphrase Generation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NAACL/" rel="tag">NAACL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIPS/" rel="tag">NIPS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NMT/" rel="tag">NMT</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Relation-Extraction/" rel="tag">Neural Relation Extraction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RST/" rel="tag">RST</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Relation-Constraints/" rel="tag">Relation Constraints</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Summarization/" rel="tag">Summarization</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Translation/" rel="tag">Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Word-Translation/" rel="tag">Word Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/alias/" rel="tag">alias</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/git/" rel="tag">git</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pip/" rel="tag">pip</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/screen/" rel="tag">screen</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/" rel="tag">shell</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tgz/" rel="tag">tgz</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tts/" rel="tag">tts</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1/" rel="tag">冒泡</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/" rel="tag">冒泡排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%99%E4%BD%9C%E5%8A%A9%E6%89%8B/" rel="tag">写作助手</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8E%8B%E7%BC%A9/" rel="tag">压缩</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/" rel="tag">发送邮件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%88%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">合并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%90%8E%E5%8F%B0/" rel="tag">后台</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/" rel="tag">基数排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F/" rel="tag">希尔排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6/" rel="tag">归并</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/" rel="tag">归并排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/" rel="tag">快速排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F/" rel="tag">批量</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4/" rel="tag">批量删除</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F/" rel="tag">排序</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5/" rel="tag">插入</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/" rel="tag">插入排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%99%E7%A8%8B/" rel="tag">教程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/" rel="tag">斐波那契数列</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9D%80%E6%AD%BB%E8%BF%9B%E7%A8%8B/" rel="tag">杀死进程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B1%89%E8%AF%BA%E5%A1%94/" rel="tag">汉诺塔</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%A7%A3%E5%8E%8B/" rel="tag">解压</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%B0%B7%E6%AD%8C%E7%BF%BB%E8%AF%91/" rel="tag">谷歌翻译</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BF%AD%E4%BB%A3%E5%9B%9E%E7%BF%BB/" rel="tag">迭代回翻</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9/" rel="tag">选择</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/" rel="tag">选择排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%99%84%E4%BB%B6/" rel="tag">附件</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3/" rel="tag">非监督</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%A2%86%E5%9F%9F%E9%80%82%E5%BA%94/" rel="tag">领域适应</a><span class="tag-list-count">1</span></li></ul>
                </canvas>
            </div>
        </div>
        
-->
        
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-hengfu -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="9879871597"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-chuizhi -->
<!--
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="1662238719"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->

<!-- procjx-zhengfangxing -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1179774715076800"
     data-ad-slot="6699421902"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PROCJX</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.4.2
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>












        
      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  <script src="/js/local-search.js"></script>













  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '2286ab64f5194d9d79ce',
      clientSecret: 'f912492bec2391664b40478f50f2f943376768d6',
      repo: 'procjx.github.io',
      owner: 'procjx',
      admin: ['procjx'],
      id: 'efea09a5e5d70758454ad388229814f0',
        language: 'zh-CN',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
